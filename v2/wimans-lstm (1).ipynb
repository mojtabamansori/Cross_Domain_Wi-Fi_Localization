{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1927781",
   "metadata": {
    "papermill": {
     "duration": 0.004443,
     "end_time": "2025-01-17T05:46:16.088254",
     "exception": false,
     "start_time": "2025-01-17T05:46:16.083811",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "*preset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54bf3db3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-17T05:46:16.096882Z",
     "iopub.status.busy": "2025-01-17T05:46:16.096569Z",
     "iopub.status.idle": "2025-01-17T05:46:16.104948Z",
     "shell.execute_reply": "2025-01-17T05:46:16.104286Z"
    },
    "papermill": {
     "duration": 0.014027,
     "end_time": "2025-01-17T05:46:16.106190",
     "exception": false,
     "start_time": "2025-01-17T05:46:16.092163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          preset.py\n",
    "[description]   default settings of WiFi-based models\n",
    "\"\"\"\n",
    "#\n",
    "##\n",
    "model123 = \"THAT_location\"\n",
    "preset = {\n",
    "    #\n",
    "    ## define model\n",
    "    \"model\": \"LSTM\",                                    # \"ST-RF\", \"MLP\", \"LSTM\", \"CNN-1D\", \"CNN-2D\", \"CLSTM\", \"ABLSTM\", \"THAT\"\n",
    "    #\n",
    "    ## define task\n",
    "    \"task\": \"location\",                                 # \"identity\", \"activity\", \"location\"\n",
    "    #\n",
    "    ## number of repeated experiments\n",
    "    \"repeat\": 1,\n",
    "    #\n",
    "    ## path of data\n",
    "    \"path\": {\n",
    "        \"data_x\": \"/kaggle/input/wimans/wifi_csi/amp\",               # directory of CSI amplitude files\n",
    "        \"data_y\": \"/kaggle/input/wimans/annotation.csv\",             # path of annotation file\n",
    "        \"save\": f\"result_{model123}.json\"                           # path to save results\n",
    "    },\n",
    "    #\n",
    "    ## data selection for experiments\n",
    "    \"data\": {\n",
    "        \"num_users\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"],    # select number(s) of users, (e.g., [\"0\", \"1\"], [\"2\", \"3\", \"4\", \"5\"])\n",
    "        \"wifi_band\": [\"2.4\"],                           # select WiFi band(s) (e.g., [\"2.4\"], [\"5\"], [\"2.4\", \"5\"])\n",
    "        \"environment\": [\"empty_room\"],                   # select environment(s) (e.g., [\"classroom\"], [\"meeting_room\"], [\"empty_room\"])\n",
    "        \"length\": 3000,                                 # default length of CSI\n",
    "    },\n",
    "    #\n",
    "    ## hyperparameters of models\n",
    "    \"nn\": {\n",
    "        \"lr\": 1e-3,                                     # learning rate\n",
    "        \"epoch\": 300,                                   # number of epochs\n",
    "        \"batch_size\": 128,                              # batch size\n",
    "        \"threshold\": 0.5,                               # threshold to binarize sigmoid outputs\n",
    "    },\n",
    "    #\n",
    "    ## encoding of activities and locations\n",
    "    \"encoding\": {\n",
    "        \"activity\": {                                   # encoding of different activities\n",
    "            \"nan\":      [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"nothing\":  [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"walk\":     [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"rotation\": [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            \"jump\":     [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            \"wave\":     [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "            \"lie_down\": [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "            \"pick_up\":  [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "            \"sit_down\": [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "            \"stand_up\": [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        },\n",
    "        \"location\": {                                   # encoding of different locations\n",
    "            \"nan\":  [0, 0, 0, 0, 0],\n",
    "            \"a\":    [1, 0, 0, 0, 0],\n",
    "            \"b\":    [0, 1, 0, 0, 0],\n",
    "            \"c\":    [0, 0, 1, 0, 0],\n",
    "            \"d\":    [0, 0, 0, 1, 0],\n",
    "            \"e\":    [0, 0, 0, 0, 1],\n",
    "        },\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3131a62",
   "metadata": {
    "papermill": {
     "duration": 0.003376,
     "end_time": "2025-01-17T05:46:16.113194",
     "exception": false,
     "start_time": "2025-01-17T05:46:16.109818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "pre process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95c5d39b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T05:46:16.121448Z",
     "iopub.status.busy": "2025-01-17T05:46:16.121163Z",
     "iopub.status.idle": "2025-01-17T05:46:16.461869Z",
     "shell.execute_reply": "2025-01-17T05:46:16.460912Z"
    },
    "papermill": {
     "duration": 0.346771,
     "end_time": "2025-01-17T05:46:16.463439",
     "exception": false,
     "start_time": "2025-01-17T05:46:16.116668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          preprocess.py\n",
    "[description]   preprocess WiFi CSI data\n",
    "\"\"\"\n",
    "#\n",
    "##\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "\n",
    "#\n",
    "##\n",
    "def mat_to_amp(data_mat):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : calculate amplitude of raw WiFi CSI data\n",
    "    [parameter]\n",
    "    : data_mat: dict, raw WiFi CSI data from *.mat files\n",
    "    [return]\n",
    "    : data_csi_amp: numpy array, CSI amplitude\n",
    "    \"\"\"\n",
    "    #\n",
    "    ## \n",
    "    var_length = data_mat[\"trace\"].shape[0]\n",
    "    #\n",
    "    data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "    #\n",
    "    data_csi_amp = np.array(data_csi_amp, dtype = np.float32)\n",
    "    #\n",
    "    return data_csi_amp\n",
    "\n",
    "#\n",
    "##\n",
    "def extract_csi_amp(var_dir_mat, \n",
    "                    var_dir_amp):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : read raw WiFi CSI files (*.mat), calcuate CSI amplitude, and save amplitude (*.npy)\n",
    "    [parameter]\n",
    "    : var_dir_mat: string, directory to read raw WiFi CSI files (*.mat)\n",
    "    : var_dir_amp: string, directory to save WiFi CSI amplitude (*.npy)\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    var_path_mat = os.listdir(var_dir_mat)\n",
    "    #\n",
    "    for var_c, var_path in enumerate(var_path_mat):\n",
    "        #\n",
    "        data_mat = scio.loadmat(os.path.join(var_dir_mat, var_path))\n",
    "        #\n",
    "        data_csi_amp = mat_to_amp(data_mat)\n",
    "        #\n",
    "        \n",
    "        #\n",
    "        var_path_save = os.path.join(var_dir_amp, var_path.replace(\".mat\", \".npy\"))\n",
    "        #\n",
    "        with open(var_path_save, \"wb\") as var_file:\n",
    "            np.save(var_file, data_csi_amp)\n",
    "\n",
    "#\n",
    "##\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : parse arguments from input\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    var_args = argparse.ArgumentParser()\n",
    "    #\n",
    "    var_args.add_argument(\"--dir_mat\", default = \"/kaggle/input/wimans/wifi_csi/mat\", type = str)\n",
    "    var_args.add_argument(\"--dir_amp\", default = \"/kaggle/input/wimans/wifi_csi/amp\", type = str)\n",
    "    #\n",
    "    return var_args.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb98150",
   "metadata": {
    "papermill": {
     "duration": 0.003553,
     "end_time": "2025-01-17T05:46:16.470898",
     "exception": false,
     "start_time": "2025-01-17T05:46:16.467345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d915cbde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T05:46:16.479053Z",
     "iopub.status.busy": "2025-01-17T05:46:16.478708Z",
     "iopub.status.idle": "2025-01-17T05:46:17.175410Z",
     "shell.execute_reply": "2025-01-17T05:46:17.174721Z"
    },
    "papermill": {
     "duration": 0.702468,
     "end_time": "2025-01-17T05:46:17.177090",
     "exception": false,
     "start_time": "2025-01-17T05:46:16.474622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          load_data.py\n",
    "[description]   load annotation file and CSI amplitude, and encode labels\n",
    "\"\"\"\n",
    "#\n",
    "##\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#\n",
    "\n",
    "\n",
    "#\n",
    "##\n",
    "def load_data_y(var_path_data_y,\n",
    "                var_environment = None, \n",
    "                var_wifi_band = None, \n",
    "                var_num_users = None):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : load annotation file (*.csv) as a pandas dataframe\n",
    "    : according to selected environment(s), WiFi band(s), and number(s) of users\n",
    "    [parameter]\n",
    "    : var_path_data_y: string, path of annotation file\n",
    "    : var_environment: list, selected environment(s), e.g., [\"classroom\"]\n",
    "    : var_wifi_band: list, selected WiFi band(s), e.g., [\"2.4\"]\n",
    "    : var_num_users: list, selected number(s) of users, e.g., [\"0\", \"1\", \"2\"]\n",
    "    [return]\n",
    "    : data_pd_y: pandas dataframe, labels of selected data\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_pd_y = pd.read_csv(var_path_data_y, dtype = str)\n",
    "    #\n",
    "    if var_environment is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "    #\n",
    "    if var_wifi_band is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "    #\n",
    "    if var_num_users is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "    #\n",
    "    return data_pd_y\n",
    "\n",
    "#\n",
    "##\n",
    "def load_data_x(var_path_data_x, \n",
    "                var_label_list):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : load CSI amplitude (*.npy)\n",
    "    : according to a label list of selected data\n",
    "    [parameter]\n",
    "    : var_path_data_x: string, directory of CSI amplitude files\n",
    "    : var_label_list: list, selected labels\n",
    "    [return]\n",
    "    : data_x: numpy array, CSI amplitude\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "    #\n",
    "    data_x = []\n",
    "    #\n",
    "    for var_path in var_path_list:\n",
    "        #\n",
    "        data_csi = np.load(var_path)\n",
    "        #\n",
    "        var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "        #\n",
    "        data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "        #\n",
    "        data_x.append(data_csi_pad)\n",
    "    #\n",
    "    data_x = np.array(data_x)\n",
    "    #\n",
    "    return data_x\n",
    "\n",
    "#\n",
    "##\n",
    "def encode_data_y(data_pd_y, \n",
    "                  var_task):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : encode labels according to specific task\n",
    "    [parameter]\n",
    "    : data_pd_y: pandas dataframe, labels of different tasks\n",
    "    : var_task: string, indicate task\n",
    "    [return]\n",
    "    : data_y: numpy array, label encoding of task\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    if var_task == \"identity\":\n",
    "        #\n",
    "        data_y = encode_identity(data_pd_y)\n",
    "    #\n",
    "    elif var_task == \"activity\":\n",
    "        #\n",
    "        data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "    #\n",
    "    elif var_task == \"location\":\n",
    "        #\n",
    "        data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "    #\n",
    "    return data_y\n",
    "\n",
    "#\n",
    "##\n",
    "def encode_identity(data_pd_y):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : encode identity labels in a pandas dataframe\n",
    "    [parameter]\n",
    "    : data_pd_y: pandas dataframe, labels of different tasks\n",
    "    [return]\n",
    "    : data_identity_onehot_y: numpy array, onehot encoding for identity labels\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "                                    \"user_3_location\", \"user_4_location\", \n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    # \n",
    "    data_identity_y = data_location_pd_y.to_numpy(copy = True).astype(str)\n",
    "    #\n",
    "    data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "    data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "    #\n",
    "    data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "    #\n",
    "    return data_identity_onehot_y\n",
    "\n",
    "#\n",
    "##\n",
    "def encode_activity(data_pd_y, \n",
    "                    var_encoding):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : encode activity labels in a pandas dataframe\n",
    "    [parameter]\n",
    "    : data_pd_y: pandas dataframe, labels of different tasks\n",
    "    : var_encoding: dict, encoding of different activities\n",
    "    [return]\n",
    "    : data_activity_onehot_y: numpy array, onehot encoding for activity labels\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "                                    \"user_3_activity\", \"user_4_activity\", \n",
    "                                    \"user_5_activity\", \"user_6_activity\"]]\n",
    "    #\n",
    "    data_activity_y = data_activity_pd_y.to_numpy(copy = True).astype(str)\n",
    "    #\n",
    "    data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "    #\n",
    "    return data_activity_onehot_y\n",
    "\n",
    "#\n",
    "##\n",
    "def encode_location(data_pd_y, \n",
    "                    var_encoding):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : encode location labels in a pandas dataframe\n",
    "    [parameter]\n",
    "    : data_pd_y: pandas dataframe, labels of different tasks\n",
    "    : var_encoding: dict, encoding of different locations\n",
    "    [return]\n",
    "    : data_location_onehot_y: numpy array, onehot encoding for location labels\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "                                    \"user_3_location\", \"user_4_location\", \n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    #\n",
    "    data_location_y = data_location_pd_y.to_numpy(copy = True).astype(str)\n",
    "    #\n",
    "    data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "    #\n",
    "    return data_location_onehot_y\n",
    "\n",
    "#\n",
    "##\n",
    "def test_load_data_y():\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : test load_data_y() function\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "\n",
    "\n",
    "#\n",
    "##\n",
    "def test_load_data_x():\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : test load_data_x() function\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_pd_y = load_data_y(preset[\"path\"][\"data_y\"],\n",
    "                            var_environment = [\"meeting_room\"], \n",
    "                            var_wifi_band = [\"2.4\"], \n",
    "                            var_num_users = None)\n",
    "    #\n",
    "    var_label_list = data_pd_y[\"label\"].to_list()\n",
    "    #\n",
    "    data_x = load_data_x(preset[\"path\"][\"data_x\"], var_label_list)\n",
    "    #\n",
    "  \n",
    "\n",
    "#\n",
    "##\n",
    "def test_encode_identity():\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : test encode_identity() function\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype = str)\n",
    "    #\n",
    "    data_identity_onehot_y = encode_identity(data_pd_y)\n",
    "    #\n",
    "\n",
    "\n",
    "#\n",
    "##\n",
    "def test_encode_activity():\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : test encode_activity() function\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype = str)\n",
    "    #\n",
    "    data_activity_onehot_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "    #\n",
    "\n",
    "#\n",
    "##\n",
    "def test_encode_location():\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : test encode_location() function\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype = str)\n",
    "    #\n",
    "    data_location_onehot_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "    #\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd61d6",
   "metadata": {
    "papermill": {
     "duration": 0.003547,
     "end_time": "2025-01-17T05:46:17.184850",
     "exception": false,
     "start_time": "2025-01-17T05:46:17.181303",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87f7a6e6",
   "metadata": {
    "papermill": {
     "duration": 0.003448,
     "end_time": "2025-01-17T05:46:17.191881",
     "exception": false,
     "start_time": "2025-01-17T05:46:17.188433",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755940df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T05:46:17.200448Z",
     "iopub.status.busy": "2025-01-17T05:46:17.200015Z",
     "iopub.status.idle": "2025-01-17T05:46:21.540730Z",
     "shell.execute_reply": "2025-01-17T05:46:21.539923Z"
    },
    "papermill": {
     "duration": 4.346926,
     "end_time": "2025-01-17T05:46:21.542517",
     "exception": false,
     "start_time": "2025-01-17T05:46:17.195591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import device\n",
    "from torch.nn import Module\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: Module,\n",
    "    optimizer: Optimizer,\n",
    "    loss: Module,\n",
    "    data_train_set: TensorDataset,\n",
    "    data_test_set: TensorDataset,\n",
    "    var_threshold: float,\n",
    "    var_batch_size: int,\n",
    "    var_epochs: int,\n",
    "    device: device,\n",
    "    best_model_path: str = \"best_model_lstm_epoch_300_lr_10-3_schaler.pth\",\n",
    "):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : تابع آموزش (train) برای مدل‌های WiFi-based (مثلاً LSTM)\n",
    "    \n",
    "    [parameter]\n",
    "    : model (Module): شیء مدل PyTorch\n",
    "    : optimizer (Optimizer): بهینه‌ساز (مثلاً Adam)\n",
    "    : loss (Module): تابع لاش (مثلاً BCEWithLogitsLoss)\n",
    "    : data_train_set (TensorDataset): دیتاست آموزشی\n",
    "    : data_test_set (TensorDataset): دیتاست آزمایشی\n",
    "    : var_threshold (float): آستانه‌ی باینری کردن خروجی سیگموید\n",
    "    : var_batch_size (int): سایز بچ\n",
    "    : var_epochs (int): تعداد اپوک\n",
    "    : device (device):cuda یا cpu\n",
    "    : best_model_path (str): مسیر ذخیره‌ی بهترین مدل\n",
    "    \"\"\"\n",
    "\n",
    "    data_train_loader = DataLoader(data_train_set, var_batch_size, shuffle=True, pin_memory=True)\n",
    "    data_test_loader = DataLoader(data_test_set, batch_size=len(data_test_set), shuffle=False)\n",
    "\n",
    "    var_best_accuracy = 0.0\n",
    "    var_best_weight = None\n",
    "    var_best_optimizer_state = None\n",
    "    var_no_improvement_count = 0\n",
    "\n",
    "    for var_epoch in range(var_epochs):\n",
    "        var_time_e0 = time.time()\n",
    "\n",
    "        # -------------------------- Train -----------------------------\n",
    "        model.train()\n",
    "        for data_batch in data_train_loader:\n",
    "            data_batch_x, data_batch_y = data_batch\n",
    "            data_batch_x = data_batch_x.to(device)\n",
    "            data_batch_y = data_batch_y.to(device)\n",
    "\n",
    "            predict_train_y = model(data_batch_x)\n",
    "            var_loss_train = loss(\n",
    "                predict_train_y, data_batch_y.reshape(data_batch_y.shape[0], -1).float()\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            var_loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # محاسبه‌ی دقت روی داده‌های آموزشی (برای نمایش)\n",
    "        predict_train_y = (torch.sigmoid(predict_train_y) > var_threshold).float()\n",
    "        data_batch_y = data_batch_y.detach().cpu().numpy()\n",
    "        predict_train_y = predict_train_y.detach().cpu().numpy()\n",
    "\n",
    "        predict_train_y = predict_train_y.reshape(-1, data_batch_y.shape[-1])\n",
    "        data_batch_y = data_batch_y.reshape(-1, data_batch_y.shape[-1])\n",
    "        var_accuracy_train = accuracy_score(data_batch_y.astype(int), predict_train_y.astype(int))\n",
    "\n",
    "        # -------------------------- Evaluate ---------------------------\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            data_test_x, data_test_y = next(iter(data_test_loader))\n",
    "            data_test_x = data_test_x.to(device)\n",
    "            data_test_y = data_test_y.to(device)\n",
    "\n",
    "            predict_test_y = model(data_test_x)\n",
    "            var_loss_test = loss(\n",
    "                predict_test_y, data_test_y.reshape(data_test_y.shape[0], -1).float()\n",
    "            )\n",
    "\n",
    "            predict_test_y = (torch.sigmoid(predict_test_y) > var_threshold).float()\n",
    "\n",
    "            data_test_y = data_test_y.detach().cpu().numpy()\n",
    "            predict_test_y = predict_test_y.detach().cpu().numpy()\n",
    "\n",
    "            predict_test_y = predict_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            data_test_y = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            var_accuracy_test = accuracy_score(data_test_y.astype(int), predict_test_y.astype(int))\n",
    "\n",
    "        # ------------------------- Print log ---------------------------\n",
    "        print(\n",
    "            f\"Epoch {var_epoch}/{var_epochs}\",\n",
    "            f\"- {time.time() - var_time_e0:.6f}s\",\n",
    "            f\"- Loss {var_loss_train.item():.6f}\",\n",
    "            f\"- Accuracy {var_accuracy_train:.6f}\",\n",
    "            f\"- Test Loss {var_loss_test.item():.6f}\",\n",
    "            f\"- Test Accuracy {var_accuracy_test:.6f}\",\n",
    "        )\n",
    "\n",
    "        # ------------------ Save best model & Early reduce lr ----------\n",
    "        if var_accuracy_test > var_best_accuracy:\n",
    "            var_best_accuracy = var_accuracy_test\n",
    "            var_best_weight = deepcopy(model.state_dict())\n",
    "            var_best_optimizer_state = deepcopy(optimizer.state_dict())\n",
    "            torch.save({\"best_weight\": var_best_weight}, best_model_path)\n",
    "            var_no_improvement_count = 0\n",
    "        else:\n",
    "            var_no_improvement_count += 1\n",
    "            if var_no_improvement_count >= 30:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group[\"lr\"] /= 2\n",
    "                var_no_improvement_count = 0\n",
    "\n",
    "    # --------------- Save final (best) model & optimizer --------------\n",
    "    # در اینجا صرفاً بهترین وضعیت مدل و اپتیمایزر را در فایل نهایی ذخیره می‌کنیم\n",
    "    torch.save(\n",
    "        {\n",
    "            \"best_weight\": var_best_weight,\n",
    "            \"best_optimizer_state\": var_best_optimizer_state,\n",
    "        },\n",
    "        best_model_path,\n",
    "    )\n",
    "\n",
    "    return var_best_weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3408006e",
   "metadata": {
    "papermill": {
     "duration": 0.003495,
     "end_time": "2025-01-17T05:46:21.549817",
     "exception": false,
     "start_time": "2025-01-17T05:46:21.546322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#model lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f87ef96f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T05:46:21.558015Z",
     "iopub.status.busy": "2025-01-17T05:46:21.557629Z",
     "iopub.status.idle": "2025-01-17T05:46:26.225598Z",
     "shell.execute_reply": "2025-01-17T05:46:26.224710Z"
    },
    "papermill": {
     "duration": 4.673742,
     "end_time": "2025-01-17T05:46:26.227321",
     "exception": false,
     "start_time": "2025-01-17T05:46:21.553579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ptflops\r\n",
      "  Downloading ptflops-0.7.4-py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from ptflops) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (2024.9.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0->ptflops) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\r\n",
      "Downloading ptflops-0.7.4-py3-none-any.whl (19 kB)\r\n",
      "Installing collected packages: ptflops\r\n",
      "Successfully installed ptflops-0.7.4\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ptflops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "609578ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T05:46:26.236950Z",
     "iopub.status.busy": "2025-01-17T05:46:26.236681Z",
     "iopub.status.idle": "2025-01-17T05:46:34.031908Z",
     "shell.execute_reply": "2025-01-17T05:46:34.031143Z"
    },
    "papermill": {
     "duration": 7.801704,
     "end_time": "2025-01-17T05:46:34.033479",
     "exception": false,
     "start_time": "2025-01-17T05:46:26.231775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          lstm.py\n",
    "[description]   implement and evaluate WiFi-based model LSTM\n",
    "\"\"\"\n",
    "#\n",
    "##\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "#\n",
    "from torch.utils.data import TensorDataset\n",
    "from ptflops import get_model_complexity_info\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "##\n",
    "## ------------------------------------------------------------------------------------------ ##\n",
    "## --------------------------------------- LSTM --------------------------------------------- ##\n",
    "## ------------------------------------------------------------------------------------------ ##\n",
    "class LSTMM(torch.nn.Module):\n",
    "    #\n",
    "    ##\n",
    "    def __init__(self,\n",
    "                 var_x_shape,\n",
    "                 var_y_shape):\n",
    "        #\n",
    "        ##\n",
    "        super(LSTMM, self).__init__()\n",
    "        #\n",
    "        var_dim_input = var_x_shape[-1]\n",
    "        var_dim_output = var_y_shape[-1]\n",
    "        #\n",
    "        self.layer_norm = torch.nn.BatchNorm1d(var_dim_input)\n",
    "        #\n",
    "        self.layer_pooling = torch.nn.AvgPool1d(10, 10)\n",
    "        #\n",
    "        self.layer_lstm = torch.nn.LSTM(input_size = var_dim_input,\n",
    "                                        hidden_size = 512, \n",
    "                                        batch_first = True)\n",
    "        #\n",
    "        self.layer_linear = torch.nn.Linear(512, var_dim_output)\n",
    "\n",
    "    #\n",
    "    ##\n",
    "    def forward(self,\n",
    "                var_input):\n",
    "        #\n",
    "        ##\n",
    "        var_t = var_input\n",
    "        #\n",
    "        var_t = torch.permute(var_t, (0, 2, 1))\n",
    "        var_t = self.layer_norm(var_t)\n",
    "        var_t = self.layer_pooling(var_t)\n",
    "        var_t = torch.permute(var_t, (0, 2, 1))\n",
    "        #\n",
    "        var_t, _ = self.layer_lstm(var_t)\n",
    "        #\n",
    "        var_t = var_t[:, -1, :]\n",
    "        #\n",
    "        var_t = self.layer_linear(var_t)\n",
    "        #\n",
    "        var_output = var_t\n",
    "        #\n",
    "        return var_output\n",
    "    \n",
    "#\n",
    "##\n",
    "def run_lstm(data_train_x, \n",
    "             data_train_y,\n",
    "             data_test_x,\n",
    "             data_test_y,\n",
    "             var_repeat = 10):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : run WiFi-based model LSTM\n",
    "    [parameter]\n",
    "    : data_train_x: numpy array, CSI amplitude to train model\n",
    "    : data_train_y: numpy array, labels to train model\n",
    "    : data_test_x: numpy array, CSI amplitude to test model\n",
    "    : data_test_y: numpy array, labels to test model\n",
    "    : var_repeat: int, number of repeated experiments\n",
    "    [return]\n",
    "    : result: dict, results of experiments\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #\n",
    "    ##\n",
    "    ## ============================================ Preprocess ============================================\n",
    "    #\n",
    "    ##\n",
    "    data_train_x = data_train_x.reshape(data_train_x.shape[0], data_train_x.shape[1], -1)\n",
    "    data_test_x = data_test_x.reshape(data_test_x.shape[0], data_test_x.shape[1], -1)\n",
    "    #\n",
    "    ## shape for model\n",
    "    var_x_shape, var_y_shape = data_train_x[0].shape, data_train_y[0].reshape(-1).shape\n",
    "    #\n",
    "    data_train_set = TensorDataset(torch.from_numpy(data_train_x), torch.from_numpy(data_train_y))\n",
    "    data_test_set = TensorDataset(torch.from_numpy(data_test_x), torch.from_numpy(data_test_y))\n",
    "    #\n",
    "    ##\n",
    "    ## ========================================= Train & Evaluate =========================================\n",
    "    #\n",
    "    ##\n",
    "    result = {}\n",
    "    result_accuracy = []\n",
    "    result_time_train = []\n",
    "    result_time_test = []\n",
    "    #\n",
    "    ##\n",
    "    var_macs, var_params = get_model_complexity_info(LSTMM(var_x_shape, var_y_shape), \n",
    "                                                     var_x_shape, as_strings = False)\n",
    "    #\n",
    "    print(\"Parameters:\", var_params, \"- FLOPs:\", var_macs * 2)\n",
    "    #\n",
    "    ##\n",
    "    for var_r in range(var_repeat):\n",
    "        #\n",
    "        ##\n",
    "        print(\"Repeat\", var_r)\n",
    "        #\n",
    "        torch.random.manual_seed(var_r + 39)\n",
    "        #\n",
    "        model_lstm = torch.compile(LSTMM(var_x_shape, var_y_shape).to(device))\n",
    "        #\n",
    "        optimizer = torch.optim.Adam(model_lstm.parameters(), \n",
    "                                     lr = preset[\"nn\"][\"lr\"],\n",
    "                                     weight_decay = 0)\n",
    "        #\n",
    "        loss = torch.nn.BCEWithLogitsLoss(pos_weight = torch.tensor([6] * var_y_shape[-1]).to(device))\n",
    "        #\n",
    "        var_time_0 = time.time()\n",
    "        #\n",
    "        ## ---------------------------------------- Train -----------------------------------------\n",
    "        #\n",
    "        var_best_weight = train(model = model_lstm, \n",
    "                                optimizer = optimizer, \n",
    "                                loss = loss, \n",
    "                                data_train_set = data_train_set,\n",
    "                                data_test_set = data_test_set,\n",
    "                                var_threshold = preset[\"nn\"][\"threshold\"],\n",
    "                                var_batch_size = preset[\"nn\"][\"batch_size\"],\n",
    "                                var_epochs = preset[\"nn\"][\"epoch\"],\n",
    "                                device = device)\n",
    "        #\n",
    "        var_time_1 = time.time()\n",
    "        #\n",
    "        ## ---------------------------------------- Test ------------------------------------------\n",
    "        #\n",
    "        model_lstm.load_state_dict(var_best_weight)\n",
    "        #\n",
    "        with torch.no_grad():\n",
    "            predict_test_y = model_lstm(torch.from_numpy(data_test_x).to(device))\n",
    "        #\n",
    "        predict_test_y = (torch.sigmoid(predict_test_y) > preset[\"nn\"][\"threshold\"]).float()\n",
    "        predict_test_y = predict_test_y.detach().cpu().numpy()\n",
    "        #\n",
    "        var_time_2 = time.time()\n",
    "        #\n",
    "        ## -------------------------------------- Evaluate ----------------------------------------\n",
    "        #\n",
    "        ##\n",
    "        data_test_y_c = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "        predict_test_y_c = predict_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "        #\n",
    "        ## Accuracy\n",
    "        result_acc = accuracy_score(data_test_y_c.astype(int), \n",
    "                                    predict_test_y_c.astype(int))\n",
    "        #\n",
    "        ## Report\n",
    "        result_dict = classification_report(data_test_y_c, \n",
    "                                            predict_test_y_c, \n",
    "                                            digits = 6, \n",
    "                                            zero_division = 0, \n",
    "                                            output_dict = True)\n",
    "        #\n",
    "        result[\"repeat_\" + str(var_r)] = result_dict\n",
    "        #\n",
    "        result_accuracy.append(result_acc)\n",
    "        result_time_train.append(var_time_1 - var_time_0)\n",
    "        result_time_test.append(var_time_2 - var_time_1)\n",
    "        #\n",
    "        print(\"repeat_\" + str(var_r), result_accuracy)\n",
    "        print(result)\n",
    "    #\n",
    "    ##\n",
    "    result[\"accuracy\"] = {\"avg\": np.mean(result_accuracy), \"std\": np.std(result_accuracy)}\n",
    "    result[\"time_train\"] = {\"avg\": np.mean(result_time_train), \"std\": np.std(result_time_train)}\n",
    "    result[\"time_test\"] = {\"avg\": np.mean(result_time_test), \"std\": np.std(result_time_test)}\n",
    "    result[\"complexity\"] = {\"parameter\": var_params, \"flops\": var_macs * 2}\n",
    "    #\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d70e37e",
   "metadata": {
    "papermill": {
     "duration": 0.003925,
     "end_time": "2025-01-17T05:46:34.042043",
     "exception": false,
     "start_time": "2025-01-17T05:46:34.038118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e2b6a67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T05:46:34.051167Z",
     "iopub.status.busy": "2025-01-17T05:46:34.050892Z",
     "iopub.status.idle": "2025-01-17T06:35:26.713761Z",
     "shell.execute_reply": "2025-01-17T06:35:26.712470Z"
    },
    "papermill": {
     "duration": 2932.669395,
     "end_time": "2025-01-17T06:35:26.715548",
     "exception": false,
     "start_time": "2025-01-17T05:46:34.046153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMM(\n",
      "  1.62 M, 100.000% Params, 485.67 MMac, 99.833% MACs, \n",
      "  (layer_norm): BatchNorm1d(540, 0.033% Params, 1.62 MMac, 0.333% MACs, 270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_pooling): AvgPool1d(0, 0.000% Params, 810.0 KMac, 0.167% MACs, kernel_size=(10,), stride=(10,), padding=(0,))\n",
      "  (layer_lstm): LSTM(1.61 M, 99.018% Params, 483.23 MMac, 99.331% MACs, 270, 512, batch_first=True)\n",
      "  (layer_linear): Linear(15.39 k, 0.949% Params, 15.39 KMac, 0.003% MACs, in_features=512, out_features=30, bias=True)\n",
      ")\n",
      "Parameters: 1621562 - FLOPs: 972961980\n",
      "Repeat 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-6-600dd44513d4> line 48 \n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] due to: \n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3348, in create_backend\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] \n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] \n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 885, in step\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return self.step_graph_break(inst)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 964, in step_graph_break\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] \n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] \n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3348, in create_backend\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] \n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] \n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 885, in step\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return self.step_graph_break(inst)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 964, in step_graph_break\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] \n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0117 05:47:46.238000 18 torch/_dynamo/convert_frame.py:1125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/300 - 8.910580s - Loss 0.877786 - Accuracy 0.453125 - Test Loss 0.859980 - Test Accuracy 0.480548\n",
      "Epoch 1/300 - 4.651071s - Loss 0.805523 - Accuracy 0.460069 - Test Loss 0.783675 - Test Accuracy 0.488506\n",
      "Epoch 2/300 - 4.683573s - Loss 0.739113 - Accuracy 0.494792 - Test Loss 0.750276 - Test Accuracy 0.471706\n",
      "Epoch 3/300 - 4.611439s - Loss 0.630379 - Accuracy 0.536458 - Test Loss 0.731456 - Test Accuracy 0.488948\n",
      "Epoch 4/300 - 4.650517s - Loss 0.618249 - Accuracy 0.552083 - Test Loss 0.716777 - Test Accuracy 0.502653\n",
      "Epoch 5/300 - 4.968754s - Loss 0.692940 - Accuracy 0.482639 - Test Loss 0.721527 - Test Accuracy 0.472591\n",
      "Epoch 6/300 - 4.699270s - Loss 0.642846 - Accuracy 0.524306 - Test Loss 0.710197 - Test Accuracy 0.480106\n",
      "Epoch 7/300 - 4.585065s - Loss 0.562256 - Accuracy 0.546875 - Test Loss 0.697629 - Test Accuracy 0.466401\n",
      "Epoch 8/300 - 4.580285s - Loss 0.557331 - Accuracy 0.546875 - Test Loss 0.711896 - Test Accuracy 0.458886\n",
      "Epoch 9/300 - 4.709992s - Loss 0.659191 - Accuracy 0.512153 - Test Loss 0.713377 - Test Accuracy 0.495137\n",
      "Epoch 10/300 - 4.699463s - Loss 0.585078 - Accuracy 0.531250 - Test Loss 0.695020 - Test Accuracy 0.482759\n",
      "Epoch 11/300 - 4.691813s - Loss 0.560251 - Accuracy 0.560764 - Test Loss 0.700487 - Test Accuracy 0.497347\n",
      "Epoch 12/300 - 5.012106s - Loss 0.583759 - Accuracy 0.526042 - Test Loss 0.687782 - Test Accuracy 0.485411\n",
      "Epoch 13/300 - 4.659728s - Loss 0.482136 - Accuracy 0.623264 - Test Loss 0.686173 - Test Accuracy 0.513705\n",
      "Epoch 14/300 - 4.631544s - Loss 0.597650 - Accuracy 0.472222 - Test Loss 0.695181 - Test Accuracy 0.476127\n",
      "Epoch 15/300 - 4.697249s - Loss 0.500820 - Accuracy 0.550347 - Test Loss 0.697274 - Test Accuracy 0.499116\n",
      "Epoch 16/300 - 4.646033s - Loss 0.504998 - Accuracy 0.531250 - Test Loss 0.707571 - Test Accuracy 0.501768\n",
      "Epoch 17/300 - 4.699131s - Loss 0.574991 - Accuracy 0.487847 - Test Loss 0.716472 - Test Accuracy 0.471706\n",
      "Epoch 18/300 - 4.721610s - Loss 0.562778 - Accuracy 0.526042 - Test Loss 0.681541 - Test Accuracy 0.516799\n",
      "Epoch 19/300 - 4.874247s - Loss 0.566864 - Accuracy 0.467014 - Test Loss 0.697471 - Test Accuracy 0.472591\n",
      "Epoch 20/300 - 4.597080s - Loss 0.476143 - Accuracy 0.578125 - Test Loss 0.704889 - Test Accuracy 0.506631\n",
      "Epoch 21/300 - 4.618934s - Loss 0.458420 - Accuracy 0.572917 - Test Loss 0.715879 - Test Accuracy 0.498674\n",
      "Epoch 22/300 - 4.604675s - Loss 0.466614 - Accuracy 0.586806 - Test Loss 0.730535 - Test Accuracy 0.500442\n",
      "Epoch 23/300 - 4.651136s - Loss 0.449766 - Accuracy 0.571181 - Test Loss 0.689471 - Test Accuracy 0.498232\n",
      "Epoch 24/300 - 4.636960s - Loss 0.440566 - Accuracy 0.571181 - Test Loss 0.703200 - Test Accuracy 0.504421\n",
      "Epoch 25/300 - 4.944273s - Loss 0.505252 - Accuracy 0.595486 - Test Loss 0.684306 - Test Accuracy 0.523873\n",
      "Epoch 26/300 - 4.602350s - Loss 0.348041 - Accuracy 0.671875 - Test Loss 0.697156 - Test Accuracy 0.523431\n",
      "Epoch 27/300 - 4.614382s - Loss 0.450766 - Accuracy 0.572917 - Test Loss 0.740358 - Test Accuracy 0.508400\n",
      "Epoch 28/300 - 4.647557s - Loss 0.366812 - Accuracy 0.649306 - Test Loss 0.725439 - Test Accuracy 0.527409\n",
      "Epoch 29/300 - 4.629635s - Loss 0.491561 - Accuracy 0.616319 - Test Loss 0.740993 - Test Accuracy 0.522546\n",
      "Epoch 30/300 - 4.649020s - Loss 0.455373 - Accuracy 0.569444 - Test Loss 0.765293 - Test Accuracy 0.509284\n",
      "Epoch 31/300 - 4.606496s - Loss 0.465791 - Accuracy 0.640625 - Test Loss 0.770031 - Test Accuracy 0.504421\n",
      "Epoch 32/300 - 5.010077s - Loss 0.344443 - Accuracy 0.703125 - Test Loss 0.768609 - Test Accuracy 0.511936\n",
      "Epoch 33/300 - 4.633374s - Loss 0.426989 - Accuracy 0.578125 - Test Loss 0.792997 - Test Accuracy 0.495579\n",
      "Epoch 34/300 - 4.642544s - Loss 0.395055 - Accuracy 0.638889 - Test Loss 0.771995 - Test Accuracy 0.531388\n",
      "Epoch 35/300 - 4.593506s - Loss 0.392297 - Accuracy 0.701389 - Test Loss 0.774668 - Test Accuracy 0.548630\n",
      "Epoch 36/300 - 4.949745s - Loss 0.378805 - Accuracy 0.656250 - Test Loss 0.764891 - Test Accuracy 0.534925\n",
      "Epoch 37/300 - 4.621842s - Loss 0.344570 - Accuracy 0.637153 - Test Loss 0.776012 - Test Accuracy 0.542440\n",
      "Epoch 38/300 - 4.631806s - Loss 0.302164 - Accuracy 0.717014 - Test Loss 0.781317 - Test Accuracy 0.571618\n",
      "Epoch 39/300 - 4.984521s - Loss 0.279353 - Accuracy 0.704861 - Test Loss 0.757611 - Test Accuracy 0.564987\n",
      "Epoch 40/300 - 4.685153s - Loss 0.264266 - Accuracy 0.725694 - Test Loss 0.794181 - Test Accuracy 0.546419\n",
      "Epoch 41/300 - 4.592941s - Loss 0.279033 - Accuracy 0.727431 - Test Loss 0.840920 - Test Accuracy 0.539788\n",
      "Epoch 42/300 - 4.561864s - Loss 0.295208 - Accuracy 0.713542 - Test Loss 0.827546 - Test Accuracy 0.545535\n",
      "Epoch 43/300 - 4.682105s - Loss 0.275856 - Accuracy 0.720486 - Test Loss 0.800225 - Test Accuracy 0.562334\n",
      "Epoch 44/300 - 4.602233s - Loss 0.287301 - Accuracy 0.715278 - Test Loss 0.858087 - Test Accuracy 0.571618\n",
      "Epoch 45/300 - 4.775757s - Loss 0.351583 - Accuracy 0.689236 - Test Loss 0.824385 - Test Accuracy 0.543324\n",
      "Epoch 46/300 - 4.814171s - Loss 0.310271 - Accuracy 0.699653 - Test Loss 0.842044 - Test Accuracy 0.549072\n",
      "Epoch 47/300 - 4.706790s - Loss 0.288500 - Accuracy 0.704861 - Test Loss 0.848347 - Test Accuracy 0.565429\n",
      "Epoch 48/300 - 4.692809s - Loss 0.249236 - Accuracy 0.777778 - Test Loss 0.865544 - Test Accuracy 0.574271\n",
      "Epoch 49/300 - 4.675942s - Loss 0.236472 - Accuracy 0.763889 - Test Loss 0.878333 - Test Accuracy 0.582670\n",
      "Epoch 50/300 - 4.653490s - Loss 0.283342 - Accuracy 0.701389 - Test Loss 0.900022 - Test Accuracy 0.575155\n",
      "Epoch 51/300 - 4.710067s - Loss 0.188650 - Accuracy 0.765625 - Test Loss 0.874443 - Test Accuracy 0.577807\n",
      "Epoch 52/300 - 5.155212s - Loss 0.170074 - Accuracy 0.800347 - Test Loss 0.915325 - Test Accuracy 0.587091\n",
      "Epoch 53/300 - 4.798405s - Loss 0.250622 - Accuracy 0.717014 - Test Loss 0.852598 - Test Accuracy 0.548630\n",
      "Epoch 54/300 - 4.761145s - Loss 0.253694 - Accuracy 0.751736 - Test Loss 0.820644 - Test Accuracy 0.578691\n",
      "Epoch 55/300 - 4.763989s - Loss 0.206822 - Accuracy 0.748264 - Test Loss 0.866067 - Test Accuracy 0.578691\n",
      "Epoch 56/300 - 4.736643s - Loss 0.217825 - Accuracy 0.770833 - Test Loss 0.897350 - Test Accuracy 0.587533\n",
      "Epoch 57/300 - 4.767331s - Loss 0.259854 - Accuracy 0.746528 - Test Loss 0.921731 - Test Accuracy 0.596817\n",
      "Epoch 58/300 - 4.707996s - Loss 0.208260 - Accuracy 0.750000 - Test Loss 0.928015 - Test Accuracy 0.586649\n",
      "Epoch 59/300 - 5.074750s - Loss 0.161484 - Accuracy 0.840278 - Test Loss 0.932805 - Test Accuracy 0.606985\n",
      "Epoch 60/300 - 4.716009s - Loss 0.181921 - Accuracy 0.810764 - Test Loss 0.947908 - Test Accuracy 0.606543\n",
      "Epoch 61/300 - 4.723352s - Loss 0.154392 - Accuracy 0.840278 - Test Loss 0.952981 - Test Accuracy 0.604775\n",
      "Epoch 62/300 - 4.752613s - Loss 0.187350 - Accuracy 0.817708 - Test Loss 0.971293 - Test Accuracy 0.603006\n",
      "Epoch 63/300 - 4.743951s - Loss 0.178011 - Accuracy 0.803819 - Test Loss 0.952952 - Test Accuracy 0.595049\n",
      "Epoch 64/300 - 4.749125s - Loss 0.143158 - Accuracy 0.871528 - Test Loss 0.977999 - Test Accuracy 0.598143\n",
      "Epoch 65/300 - 4.893866s - Loss 0.173024 - Accuracy 0.833333 - Test Loss 0.956576 - Test Accuracy 0.602122\n",
      "Epoch 66/300 - 4.875460s - Loss 0.098032 - Accuracy 0.881944 - Test Loss 1.031853 - Test Accuracy 0.606543\n",
      "Epoch 67/300 - 4.714219s - Loss 0.244321 - Accuracy 0.829861 - Test Loss 1.008524 - Test Accuracy 0.616269\n",
      "Epoch 68/300 - 4.776144s - Loss 0.127688 - Accuracy 0.836806 - Test Loss 0.982634 - Test Accuracy 0.611848\n",
      "Epoch 69/300 - 4.683481s - Loss 0.184354 - Accuracy 0.828125 - Test Loss 0.980266 - Test Accuracy 0.618037\n",
      "Epoch 70/300 - 4.768990s - Loss 0.199872 - Accuracy 0.809028 - Test Loss 1.008035 - Test Accuracy 0.604332\n",
      "Epoch 71/300 - 4.770103s - Loss 0.217458 - Accuracy 0.824653 - Test Loss 1.047637 - Test Accuracy 0.604332\n",
      "Epoch 72/300 - 5.167917s - Loss 0.195233 - Accuracy 0.776042 - Test Loss 0.979139 - Test Accuracy 0.564545\n",
      "Epoch 73/300 - 4.737764s - Loss 0.159036 - Accuracy 0.817708 - Test Loss 0.928464 - Test Accuracy 0.581344\n",
      "Epoch 74/300 - 4.765188s - Loss 0.152914 - Accuracy 0.857639 - Test Loss 1.038875 - Test Accuracy 0.583112\n",
      "Epoch 75/300 - 4.750309s - Loss 0.194473 - Accuracy 0.848958 - Test Loss 1.060750 - Test Accuracy 0.594164\n",
      "Epoch 76/300 - 4.813811s - Loss 0.152608 - Accuracy 0.824653 - Test Loss 0.994940 - Test Accuracy 0.595491\n",
      "Epoch 77/300 - 4.671510s - Loss 0.172336 - Accuracy 0.826389 - Test Loss 1.032692 - Test Accuracy 0.602122\n",
      "Epoch 78/300 - 4.680857s - Loss 0.129390 - Accuracy 0.847222 - Test Loss 1.049731 - Test Accuracy 0.598585\n",
      "Epoch 79/300 - 4.957477s - Loss 0.133965 - Accuracy 0.862847 - Test Loss 1.053464 - Test Accuracy 0.619805\n",
      "Epoch 80/300 - 4.740612s - Loss 0.164383 - Accuracy 0.875000 - Test Loss 1.096706 - Test Accuracy 0.613174\n",
      "Epoch 81/300 - 4.635761s - Loss 0.110246 - Accuracy 0.904514 - Test Loss 1.126273 - Test Accuracy 0.608753\n",
      "Epoch 82/300 - 4.678709s - Loss 0.163828 - Accuracy 0.842014 - Test Loss 1.146598 - Test Accuracy 0.606543\n",
      "Epoch 83/300 - 4.640481s - Loss 0.162094 - Accuracy 0.836806 - Test Loss 1.123080 - Test Accuracy 0.600354\n",
      "Epoch 84/300 - 4.650850s - Loss 0.129449 - Accuracy 0.876736 - Test Loss 1.075571 - Test Accuracy 0.604775\n",
      "Epoch 85/300 - 4.658665s - Loss 0.142707 - Accuracy 0.842014 - Test Loss 1.100989 - Test Accuracy 0.586649\n",
      "Epoch 86/300 - 4.826164s - Loss 0.127096 - Accuracy 0.876736 - Test Loss 1.071484 - Test Accuracy 0.614943\n",
      "Epoch 87/300 - 4.697600s - Loss 0.093755 - Accuracy 0.902778 - Test Loss 1.118561 - Test Accuracy 0.618479\n",
      "Epoch 88/300 - 4.664250s - Loss 0.094015 - Accuracy 0.906250 - Test Loss 1.164905 - Test Accuracy 0.622900\n",
      "Epoch 89/300 - 4.743168s - Loss 0.109624 - Accuracy 0.880208 - Test Loss 1.143481 - Test Accuracy 0.610080\n",
      "Epoch 90/300 - 4.640531s - Loss 0.119303 - Accuracy 0.859375 - Test Loss 1.121554 - Test Accuracy 0.613616\n",
      "Epoch 91/300 - 4.721128s - Loss 0.114970 - Accuracy 0.885417 - Test Loss 1.173585 - Test Accuracy 0.627321\n",
      "Epoch 92/300 - 4.986467s - Loss 0.107379 - Accuracy 0.895833 - Test Loss 1.145366 - Test Accuracy 0.633952\n",
      "Epoch 93/300 - 4.715849s - Loss 0.133685 - Accuracy 0.895833 - Test Loss 1.134404 - Test Accuracy 0.638373\n",
      "Epoch 94/300 - 4.676194s - Loss 0.100247 - Accuracy 0.902778 - Test Loss 1.140957 - Test Accuracy 0.633952\n",
      "Epoch 95/300 - 4.784104s - Loss 0.083381 - Accuracy 0.930556 - Test Loss 1.186154 - Test Accuracy 0.625995\n",
      "Epoch 96/300 - 4.698520s - Loss 0.072246 - Accuracy 0.920139 - Test Loss 1.227288 - Test Accuracy 0.633510\n",
      "Epoch 97/300 - 4.734586s - Loss 0.068671 - Accuracy 0.940972 - Test Loss 1.249965 - Test Accuracy 0.633952\n",
      "Epoch 98/300 - 4.676033s - Loss 0.103367 - Accuracy 0.932292 - Test Loss 1.305282 - Test Accuracy 0.623784\n",
      "Epoch 99/300 - 5.062500s - Loss 0.079920 - Accuracy 0.935764 - Test Loss 1.239804 - Test Accuracy 0.628647\n",
      "Epoch 100/300 - 4.684557s - Loss 0.084064 - Accuracy 0.923611 - Test Loss 1.295230 - Test Accuracy 0.636605\n",
      "Epoch 101/300 - 4.741600s - Loss 0.095223 - Accuracy 0.918403 - Test Loss 1.319790 - Test Accuracy 0.629973\n",
      "Epoch 102/300 - 4.732857s - Loss 0.110352 - Accuracy 0.914931 - Test Loss 1.344070 - Test Accuracy 0.637931\n",
      "Epoch 103/300 - 4.714854s - Loss 0.099718 - Accuracy 0.947917 - Test Loss 1.372329 - Test Accuracy 0.637047\n",
      "Epoch 104/300 - 4.674613s - Loss 0.098352 - Accuracy 0.921875 - Test Loss 1.376687 - Test Accuracy 0.641026\n",
      "Epoch 105/300 - 4.725854s - Loss 0.066649 - Accuracy 0.932292 - Test Loss 1.339897 - Test Accuracy 0.637489\n",
      "Epoch 106/300 - 5.085992s - Loss 0.122012 - Accuracy 0.901042 - Test Loss 1.258527 - Test Accuracy 0.631300\n",
      "Epoch 107/300 - 4.672872s - Loss 0.073443 - Accuracy 0.925347 - Test Loss 1.247055 - Test Accuracy 0.628647\n",
      "Epoch 108/300 - 4.724852s - Loss 0.155784 - Accuracy 0.892361 - Test Loss 1.306036 - Test Accuracy 0.619805\n",
      "Epoch 109/300 - 4.684231s - Loss 0.077537 - Accuracy 0.913194 - Test Loss 1.259037 - Test Accuracy 0.611848\n",
      "Epoch 110/300 - 4.735960s - Loss 0.080056 - Accuracy 0.932292 - Test Loss 1.202372 - Test Accuracy 0.620690\n",
      "Epoch 111/300 - 4.700272s - Loss 0.107110 - Accuracy 0.901042 - Test Loss 1.203504 - Test Accuracy 0.623342\n",
      "Epoch 112/300 - 5.029159s - Loss 0.066860 - Accuracy 0.925347 - Test Loss 1.226957 - Test Accuracy 0.625111\n",
      "Epoch 113/300 - 4.646641s - Loss 0.061870 - Accuracy 0.928819 - Test Loss 1.209864 - Test Accuracy 0.643678\n",
      "Epoch 114/300 - 4.671292s - Loss 0.103325 - Accuracy 0.902778 - Test Loss 1.544247 - Test Accuracy 0.590186\n",
      "Epoch 115/300 - 4.647480s - Loss 0.204491 - Accuracy 0.810764 - Test Loss 1.128029 - Test Accuracy 0.580460\n",
      "Epoch 116/300 - 4.721483s - Loss 0.221736 - Accuracy 0.776042 - Test Loss 1.113890 - Test Accuracy 0.592396\n",
      "Epoch 117/300 - 4.671198s - Loss 0.125681 - Accuracy 0.861111 - Test Loss 1.101403 - Test Accuracy 0.617595\n",
      "Epoch 118/300 - 4.737167s - Loss 0.108090 - Accuracy 0.913194 - Test Loss 1.175638 - Test Accuracy 0.620248\n",
      "Epoch 119/300 - 4.950451s - Loss 0.153015 - Accuracy 0.861111 - Test Loss 1.214407 - Test Accuracy 0.629531\n",
      "Epoch 120/300 - 4.732335s - Loss 0.092965 - Accuracy 0.911458 - Test Loss 1.170117 - Test Accuracy 0.620248\n",
      "Epoch 121/300 - 4.733474s - Loss 0.058140 - Accuracy 0.944444 - Test Loss 1.214196 - Test Accuracy 0.640584\n",
      "Epoch 122/300 - 4.716828s - Loss 0.060736 - Accuracy 0.956597 - Test Loss 1.238867 - Test Accuracy 0.637489\n",
      "Epoch 123/300 - 4.669146s - Loss 0.113697 - Accuracy 0.906250 - Test Loss 1.254820 - Test Accuracy 0.642352\n",
      "Epoch 124/300 - 4.737532s - Loss 0.069790 - Accuracy 0.942708 - Test Loss 1.266737 - Test Accuracy 0.638373\n",
      "Epoch 125/300 - 4.726486s - Loss 0.080479 - Accuracy 0.913194 - Test Loss 1.309278 - Test Accuracy 0.645447\n",
      "Epoch 126/300 - 5.047286s - Loss 0.060952 - Accuracy 0.946181 - Test Loss 1.357402 - Test Accuracy 0.640584\n",
      "Epoch 127/300 - 4.701842s - Loss 0.063411 - Accuracy 0.944444 - Test Loss 1.387581 - Test Accuracy 0.650309\n",
      "Epoch 128/300 - 4.667558s - Loss 0.069967 - Accuracy 0.934028 - Test Loss 1.368981 - Test Accuracy 0.631742\n",
      "Epoch 129/300 - 4.647152s - Loss 0.072398 - Accuracy 0.923611 - Test Loss 1.243355 - Test Accuracy 0.631300\n",
      "Epoch 130/300 - 4.663799s - Loss 0.069975 - Accuracy 0.930556 - Test Loss 1.234635 - Test Accuracy 0.630858\n",
      "Epoch 131/300 - 4.705670s - Loss 0.067778 - Accuracy 0.937500 - Test Loss 1.268131 - Test Accuracy 0.635279\n",
      "Epoch 132/300 - 4.961081s - Loss 0.226646 - Accuracy 0.881944 - Test Loss 1.272970 - Test Accuracy 0.603006\n",
      "Epoch 133/300 - 4.679224s - Loss 0.153611 - Accuracy 0.871528 - Test Loss 1.133610 - Test Accuracy 0.620248\n",
      "Epoch 134/300 - 4.629429s - Loss 0.107499 - Accuracy 0.904514 - Test Loss 1.145653 - Test Accuracy 0.595049\n",
      "Epoch 135/300 - 4.705127s - Loss 0.133971 - Accuracy 0.885417 - Test Loss 1.127106 - Test Accuracy 0.608753\n",
      "Epoch 136/300 - 4.669500s - Loss 0.101447 - Accuracy 0.895833 - Test Loss 1.142472 - Test Accuracy 0.618921\n",
      "Epoch 137/300 - 4.705421s - Loss 0.078064 - Accuracy 0.932292 - Test Loss 1.170406 - Test Accuracy 0.629531\n",
      "Epoch 138/300 - 4.683924s - Loss 0.085052 - Accuracy 0.928819 - Test Loss 1.190201 - Test Accuracy 0.628205\n",
      "Epoch 139/300 - 5.044372s - Loss 0.067360 - Accuracy 0.942708 - Test Loss 1.236156 - Test Accuracy 0.637489\n",
      "Epoch 140/300 - 4.715692s - Loss 0.072087 - Accuracy 0.932292 - Test Loss 1.276124 - Test Accuracy 0.639699\n",
      "Epoch 141/300 - 4.696061s - Loss 0.156486 - Accuracy 0.888889 - Test Loss 1.351737 - Test Accuracy 0.592396\n",
      "Epoch 142/300 - 4.639560s - Loss 0.159459 - Accuracy 0.866319 - Test Loss 1.194220 - Test Accuracy 0.586649\n",
      "Epoch 143/300 - 4.638868s - Loss 0.108836 - Accuracy 0.899306 - Test Loss 1.168258 - Test Accuracy 0.612290\n",
      "Epoch 144/300 - 4.649297s - Loss 0.412282 - Accuracy 0.793403 - Test Loss 1.116486 - Test Accuracy 0.586207\n",
      "Epoch 145/300 - 4.679584s - Loss 0.240302 - Accuracy 0.793403 - Test Loss 0.974803 - Test Accuracy 0.567197\n",
      "Epoch 146/300 - 5.070455s - Loss 0.200289 - Accuracy 0.779514 - Test Loss 0.992791 - Test Accuracy 0.586207\n",
      "Epoch 147/300 - 4.626178s - Loss 0.146973 - Accuracy 0.866319 - Test Loss 1.049246 - Test Accuracy 0.598143\n",
      "Epoch 148/300 - 4.678261s - Loss 0.149538 - Accuracy 0.857639 - Test Loss 1.066028 - Test Accuracy 0.606543\n",
      "Epoch 149/300 - 4.670000s - Loss 0.113261 - Accuracy 0.897569 - Test Loss 1.110515 - Test Accuracy 0.618037\n",
      "Epoch 150/300 - 4.663790s - Loss 0.091685 - Accuracy 0.885417 - Test Loss 1.130994 - Test Accuracy 0.617153\n",
      "Epoch 151/300 - 4.677020s - Loss 0.101910 - Accuracy 0.920139 - Test Loss 1.139058 - Test Accuracy 0.624668\n",
      "Epoch 152/300 - 4.790848s - Loss 0.080278 - Accuracy 0.932292 - Test Loss 1.178416 - Test Accuracy 0.634394\n",
      "Epoch 153/300 - 4.774441s - Loss 0.077345 - Accuracy 0.934028 - Test Loss 1.185607 - Test Accuracy 0.628205\n",
      "Epoch 154/300 - 4.648370s - Loss 0.081704 - Accuracy 0.937500 - Test Loss 1.180287 - Test Accuracy 0.631742\n",
      "Epoch 155/300 - 4.673882s - Loss 0.055795 - Accuracy 0.958333 - Test Loss 1.204645 - Test Accuracy 0.627763\n",
      "Epoch 156/300 - 4.696400s - Loss 0.068301 - Accuracy 0.944444 - Test Loss 1.242016 - Test Accuracy 0.638815\n",
      "Epoch 157/300 - 4.669163s - Loss 0.053214 - Accuracy 0.953125 - Test Loss 1.256202 - Test Accuracy 0.632184\n",
      "Epoch 158/300 - 4.701154s - Loss 0.070744 - Accuracy 0.940972 - Test Loss 1.260780 - Test Accuracy 0.637047\n",
      "Epoch 159/300 - 5.006153s - Loss 0.053567 - Accuracy 0.961806 - Test Loss 1.299695 - Test Accuracy 0.633952\n",
      "Epoch 160/300 - 4.702187s - Loss 0.062318 - Accuracy 0.940972 - Test Loss 1.279813 - Test Accuracy 0.637931\n",
      "Epoch 161/300 - 4.666216s - Loss 0.072365 - Accuracy 0.934028 - Test Loss 1.260518 - Test Accuracy 0.644120\n",
      "Epoch 162/300 - 4.659666s - Loss 0.046348 - Accuracy 0.953125 - Test Loss 1.288065 - Test Accuracy 0.646331\n",
      "Epoch 163/300 - 4.669843s - Loss 0.056669 - Accuracy 0.961806 - Test Loss 1.304057 - Test Accuracy 0.636163\n",
      "Epoch 164/300 - 4.657145s - Loss 0.041596 - Accuracy 0.967014 - Test Loss 1.263058 - Test Accuracy 0.637489\n",
      "Epoch 165/300 - 4.703077s - Loss 0.043381 - Accuracy 0.979167 - Test Loss 1.270699 - Test Accuracy 0.646773\n",
      "Epoch 166/300 - 4.990993s - Loss 0.049963 - Accuracy 0.954861 - Test Loss 1.287902 - Test Accuracy 0.650752\n",
      "Epoch 167/300 - 4.685776s - Loss 0.040658 - Accuracy 0.972222 - Test Loss 1.298085 - Test Accuracy 0.654730\n",
      "Epoch 168/300 - 4.609048s - Loss 0.040490 - Accuracy 0.960069 - Test Loss 1.315484 - Test Accuracy 0.661804\n",
      "Epoch 169/300 - 4.674073s - Loss 0.049761 - Accuracy 0.960069 - Test Loss 1.341147 - Test Accuracy 0.658267\n",
      "Epoch 170/300 - 4.653750s - Loss 0.032863 - Accuracy 0.977431 - Test Loss 1.350879 - Test Accuracy 0.652962\n",
      "Epoch 171/300 - 4.665272s - Loss 0.091643 - Accuracy 0.942708 - Test Loss 1.454563 - Test Accuracy 0.636605\n",
      "Epoch 172/300 - 4.677047s - Loss 0.058235 - Accuracy 0.954861 - Test Loss 1.338289 - Test Accuracy 0.636163\n",
      "Epoch 173/300 - 5.006458s - Loss 0.044692 - Accuracy 0.954861 - Test Loss 1.315864 - Test Accuracy 0.630416\n",
      "Epoch 174/300 - 4.707806s - Loss 0.053790 - Accuracy 0.944444 - Test Loss 1.320771 - Test Accuracy 0.635279\n",
      "Epoch 175/300 - 4.691370s - Loss 0.058763 - Accuracy 0.960069 - Test Loss 1.336507 - Test Accuracy 0.642352\n",
      "Epoch 176/300 - 4.717582s - Loss 0.036800 - Accuracy 0.960069 - Test Loss 1.363362 - Test Accuracy 0.645004\n",
      "Epoch 177/300 - 4.693448s - Loss 0.035485 - Accuracy 0.967014 - Test Loss 1.361078 - Test Accuracy 0.644562\n",
      "Epoch 178/300 - 4.682944s - Loss 0.036940 - Accuracy 0.972222 - Test Loss 1.383100 - Test Accuracy 0.645889\n",
      "Epoch 179/300 - 4.951531s - Loss 0.043945 - Accuracy 0.965278 - Test Loss 1.380157 - Test Accuracy 0.647215\n",
      "Epoch 180/300 - 4.699806s - Loss 0.031278 - Accuracy 0.975694 - Test Loss 1.381026 - Test Accuracy 0.652078\n",
      "Epoch 181/300 - 4.685882s - Loss 0.030107 - Accuracy 0.980903 - Test Loss 1.397648 - Test Accuracy 0.658267\n",
      "Epoch 182/300 - 4.805949s - Loss 0.037688 - Accuracy 0.972222 - Test Loss 1.419219 - Test Accuracy 0.653404\n",
      "Epoch 183/300 - 4.779104s - Loss 0.031847 - Accuracy 0.982639 - Test Loss 1.406819 - Test Accuracy 0.654288\n",
      "Epoch 184/300 - 4.817755s - Loss 0.031848 - Accuracy 0.973958 - Test Loss 1.428622 - Test Accuracy 0.654730\n",
      "Epoch 185/300 - 4.778577s - Loss 0.033303 - Accuracy 0.986111 - Test Loss 1.430049 - Test Accuracy 0.659593\n",
      "Epoch 186/300 - 5.159009s - Loss 0.036482 - Accuracy 0.980903 - Test Loss 1.432901 - Test Accuracy 0.658709\n",
      "Epoch 187/300 - 4.773808s - Loss 0.023195 - Accuracy 0.991319 - Test Loss 1.449698 - Test Accuracy 0.659593\n",
      "Epoch 188/300 - 4.826150s - Loss 0.024117 - Accuracy 0.986111 - Test Loss 1.474791 - Test Accuracy 0.658267\n",
      "Epoch 189/300 - 4.800533s - Loss 0.024006 - Accuracy 0.984375 - Test Loss 1.459844 - Test Accuracy 0.659151\n",
      "Epoch 190/300 - 4.789564s - Loss 0.056079 - Accuracy 0.963542 - Test Loss 1.452079 - Test Accuracy 0.657825\n",
      "Epoch 191/300 - 4.797188s - Loss 0.023120 - Accuracy 0.987847 - Test Loss 1.456541 - Test Accuracy 0.658267\n",
      "Epoch 192/300 - 4.888528s - Loss 0.067658 - Accuracy 0.965278 - Test Loss 1.485727 - Test Accuracy 0.654288\n",
      "Epoch 193/300 - 4.959501s - Loss 0.028849 - Accuracy 0.984375 - Test Loss 1.476345 - Test Accuracy 0.653846\n",
      "Epoch 194/300 - 4.752895s - Loss 0.037314 - Accuracy 0.980903 - Test Loss 1.468164 - Test Accuracy 0.656499\n",
      "Epoch 195/300 - 4.662933s - Loss 0.024239 - Accuracy 0.986111 - Test Loss 1.463160 - Test Accuracy 0.653404\n",
      "Epoch 196/300 - 4.749906s - Loss 0.029396 - Accuracy 0.987847 - Test Loss 1.481114 - Test Accuracy 0.659593\n",
      "Epoch 197/300 - 4.723976s - Loss 0.030837 - Accuracy 0.984375 - Test Loss 1.499153 - Test Accuracy 0.658709\n",
      "Epoch 198/300 - 4.779767s - Loss 0.026006 - Accuracy 0.986111 - Test Loss 1.516634 - Test Accuracy 0.657383\n",
      "Epoch 199/300 - 4.996206s - Loss 0.025217 - Accuracy 0.987847 - Test Loss 1.502324 - Test Accuracy 0.657825\n",
      "Epoch 200/300 - 4.702334s - Loss 0.022646 - Accuracy 0.984375 - Test Loss 1.505275 - Test Accuracy 0.657825\n",
      "Epoch 201/300 - 4.673157s - Loss 0.022534 - Accuracy 0.980903 - Test Loss 1.507832 - Test Accuracy 0.657383\n",
      "Epoch 202/300 - 4.717205s - Loss 0.021597 - Accuracy 0.979167 - Test Loss 1.514215 - Test Accuracy 0.660920\n",
      "Epoch 203/300 - 4.736778s - Loss 0.029414 - Accuracy 0.980903 - Test Loss 1.525204 - Test Accuracy 0.664456\n",
      "Epoch 204/300 - 4.688253s - Loss 0.029722 - Accuracy 0.975694 - Test Loss 1.523488 - Test Accuracy 0.662688\n",
      "Epoch 205/300 - 4.676733s - Loss 0.028555 - Accuracy 0.972222 - Test Loss 1.532310 - Test Accuracy 0.664014\n",
      "Epoch 206/300 - 5.022340s - Loss 0.019561 - Accuracy 0.986111 - Test Loss 1.541878 - Test Accuracy 0.659593\n",
      "Epoch 207/300 - 4.665500s - Loss 0.023801 - Accuracy 0.986111 - Test Loss 1.546723 - Test Accuracy 0.664456\n",
      "Epoch 208/300 - 4.663548s - Loss 0.028092 - Accuracy 0.979167 - Test Loss 1.556196 - Test Accuracy 0.664456\n",
      "Epoch 209/300 - 4.693356s - Loss 0.021505 - Accuracy 0.987847 - Test Loss 1.553394 - Test Accuracy 0.664456\n",
      "Epoch 210/300 - 4.678422s - Loss 0.027453 - Accuracy 0.979167 - Test Loss 1.562758 - Test Accuracy 0.661804\n",
      "Epoch 211/300 - 4.684261s - Loss 0.028125 - Accuracy 0.987847 - Test Loss 1.574050 - Test Accuracy 0.663572\n",
      "Epoch 212/300 - 4.654871s - Loss 0.017208 - Accuracy 0.991319 - Test Loss 1.574779 - Test Accuracy 0.660477\n",
      "Epoch 213/300 - 4.974601s - Loss 0.018023 - Accuracy 0.991319 - Test Loss 1.554281 - Test Accuracy 0.661362\n",
      "Epoch 214/300 - 4.718533s - Loss 0.019342 - Accuracy 0.994792 - Test Loss 1.535007 - Test Accuracy 0.664456\n",
      "Epoch 215/300 - 4.707719s - Loss 0.025505 - Accuracy 0.982639 - Test Loss 1.529143 - Test Accuracy 0.660477\n",
      "Epoch 216/300 - 4.687838s - Loss 0.022141 - Accuracy 0.989583 - Test Loss 1.530581 - Test Accuracy 0.658267\n",
      "Epoch 217/300 - 4.678778s - Loss 0.016675 - Accuracy 0.993056 - Test Loss 1.524555 - Test Accuracy 0.661362\n",
      "Epoch 218/300 - 4.696926s - Loss 0.019662 - Accuracy 0.986111 - Test Loss 1.541406 - Test Accuracy 0.663572\n",
      "Epoch 219/300 - 4.914437s - Loss 0.020359 - Accuracy 0.982639 - Test Loss 1.553186 - Test Accuracy 0.660920\n",
      "Epoch 220/300 - 4.881817s - Loss 0.022465 - Accuracy 0.986111 - Test Loss 1.547632 - Test Accuracy 0.659593\n",
      "Epoch 221/300 - 4.787066s - Loss 0.013896 - Accuracy 0.998264 - Test Loss 1.539539 - Test Accuracy 0.660477\n",
      "Epoch 222/300 - 4.750751s - Loss 0.019934 - Accuracy 0.984375 - Test Loss 1.525659 - Test Accuracy 0.663572\n",
      "Epoch 223/300 - 4.722555s - Loss 0.028820 - Accuracy 0.984375 - Test Loss 1.531802 - Test Accuracy 0.660920\n",
      "Epoch 224/300 - 4.714430s - Loss 0.021506 - Accuracy 0.984375 - Test Loss 1.544882 - Test Accuracy 0.658709\n",
      "Epoch 225/300 - 4.695267s - Loss 0.019254 - Accuracy 0.991319 - Test Loss 1.552273 - Test Accuracy 0.663130\n",
      "Epoch 226/300 - 4.986449s - Loss 0.017052 - Accuracy 0.991319 - Test Loss 1.550816 - Test Accuracy 0.661362\n",
      "Epoch 227/300 - 4.730741s - Loss 0.020244 - Accuracy 0.993056 - Test Loss 1.554740 - Test Accuracy 0.664898\n",
      "Epoch 228/300 - 4.735934s - Loss 0.016705 - Accuracy 0.994792 - Test Loss 1.562945 - Test Accuracy 0.666225\n",
      "Epoch 229/300 - 4.696022s - Loss 0.015021 - Accuracy 0.996528 - Test Loss 1.562148 - Test Accuracy 0.664456\n",
      "Epoch 230/300 - 4.704086s - Loss 0.015562 - Accuracy 0.996528 - Test Loss 1.583821 - Test Accuracy 0.666225\n",
      "Epoch 231/300 - 4.747123s - Loss 0.015507 - Accuracy 0.996528 - Test Loss 1.585525 - Test Accuracy 0.665782\n",
      "Epoch 232/300 - 4.823366s - Loss 0.011780 - Accuracy 0.996528 - Test Loss 1.576427 - Test Accuracy 0.667109\n",
      "Epoch 233/300 - 5.066462s - Loss 0.013400 - Accuracy 0.994792 - Test Loss 1.589174 - Test Accuracy 0.666225\n",
      "Epoch 234/300 - 4.819464s - Loss 0.027743 - Accuracy 0.980903 - Test Loss 1.606467 - Test Accuracy 0.666225\n",
      "Epoch 235/300 - 4.787206s - Loss 0.021059 - Accuracy 0.982639 - Test Loss 1.620452 - Test Accuracy 0.659151\n",
      "Epoch 236/300 - 4.749892s - Loss 0.014681 - Accuracy 0.994792 - Test Loss 1.617662 - Test Accuracy 0.667109\n",
      "Epoch 237/300 - 4.705122s - Loss 0.018418 - Accuracy 0.982639 - Test Loss 1.625014 - Test Accuracy 0.666667\n",
      "Epoch 238/300 - 4.697750s - Loss 0.012335 - Accuracy 0.994792 - Test Loss 1.615942 - Test Accuracy 0.666667\n",
      "Epoch 239/300 - 4.817726s - Loss 0.013806 - Accuracy 0.994792 - Test Loss 1.606686 - Test Accuracy 0.670203\n",
      "Epoch 240/300 - 4.897135s - Loss 0.018941 - Accuracy 0.989583 - Test Loss 1.605903 - Test Accuracy 0.671088\n",
      "Epoch 241/300 - 4.800247s - Loss 0.015592 - Accuracy 0.993056 - Test Loss 1.624926 - Test Accuracy 0.671530\n",
      "Epoch 242/300 - 4.719266s - Loss 0.015432 - Accuracy 0.994792 - Test Loss 1.620097 - Test Accuracy 0.671530\n",
      "Epoch 243/300 - 4.702288s - Loss 0.020277 - Accuracy 0.996528 - Test Loss 1.633563 - Test Accuracy 0.668435\n",
      "Epoch 244/300 - 4.692887s - Loss 0.012279 - Accuracy 0.996528 - Test Loss 1.633480 - Test Accuracy 0.667551\n",
      "Epoch 245/300 - 4.673370s - Loss 0.016120 - Accuracy 0.998264 - Test Loss 1.623522 - Test Accuracy 0.666667\n",
      "Epoch 246/300 - 5.093765s - Loss 0.021969 - Accuracy 0.986111 - Test Loss 1.629573 - Test Accuracy 0.667109\n",
      "Epoch 247/300 - 4.673041s - Loss 0.020956 - Accuracy 0.993056 - Test Loss 1.647781 - Test Accuracy 0.664898\n",
      "Epoch 248/300 - 4.769756s - Loss 0.023423 - Accuracy 0.996528 - Test Loss 1.640667 - Test Accuracy 0.671972\n",
      "Epoch 249/300 - 4.677425s - Loss 0.051279 - Accuracy 0.979167 - Test Loss 1.625508 - Test Accuracy 0.667551\n",
      "Epoch 250/300 - 4.681275s - Loss 0.016647 - Accuracy 0.987847 - Test Loss 1.628355 - Test Accuracy 0.669761\n",
      "Epoch 251/300 - 4.712711s - Loss 0.021065 - Accuracy 0.984375 - Test Loss 1.621200 - Test Accuracy 0.667109\n",
      "Epoch 252/300 - 4.680202s - Loss 0.013688 - Accuracy 0.994792 - Test Loss 1.611771 - Test Accuracy 0.670645\n",
      "Epoch 253/300 - 4.960240s - Loss 0.017234 - Accuracy 0.989583 - Test Loss 1.617825 - Test Accuracy 0.671088\n",
      "Epoch 254/300 - 4.668786s - Loss 0.019670 - Accuracy 0.994792 - Test Loss 1.625654 - Test Accuracy 0.671972\n",
      "Epoch 255/300 - 4.730482s - Loss 0.014317 - Accuracy 0.996528 - Test Loss 1.640058 - Test Accuracy 0.669761\n",
      "Epoch 256/300 - 4.740352s - Loss 0.012847 - Accuracy 0.993056 - Test Loss 1.652888 - Test Accuracy 0.670203\n",
      "Epoch 257/300 - 4.684409s - Loss 0.010242 - Accuracy 0.996528 - Test Loss 1.647426 - Test Accuracy 0.669319\n",
      "Epoch 258/300 - 4.641965s - Loss 0.011558 - Accuracy 0.996528 - Test Loss 1.657033 - Test Accuracy 0.673298\n",
      "Epoch 259/300 - 4.708826s - Loss 0.015540 - Accuracy 0.998264 - Test Loss 1.674705 - Test Accuracy 0.667993\n",
      "Epoch 260/300 - 4.832544s - Loss 0.012017 - Accuracy 0.994792 - Test Loss 1.663817 - Test Accuracy 0.664898\n",
      "Epoch 261/300 - 4.812876s - Loss 0.016431 - Accuracy 0.991319 - Test Loss 1.670843 - Test Accuracy 0.664456\n",
      "Epoch 262/300 - 4.813197s - Loss 0.031153 - Accuracy 0.979167 - Test Loss 1.661374 - Test Accuracy 0.669319\n",
      "Epoch 263/300 - 4.821772s - Loss 0.012112 - Accuracy 0.993056 - Test Loss 1.644849 - Test Accuracy 0.674182\n",
      "Epoch 264/300 - 4.735431s - Loss 0.016779 - Accuracy 0.989583 - Test Loss 1.661760 - Test Accuracy 0.664898\n",
      "Epoch 265/300 - 4.780919s - Loss 0.045206 - Accuracy 0.970486 - Test Loss 1.670982 - Test Accuracy 0.662688\n",
      "Epoch 266/300 - 5.073273s - Loss 0.018439 - Accuracy 0.993056 - Test Loss 1.677859 - Test Accuracy 0.667993\n",
      "Epoch 267/300 - 4.703924s - Loss 0.017424 - Accuracy 0.986111 - Test Loss 1.694246 - Test Accuracy 0.663572\n",
      "Epoch 268/300 - 4.734380s - Loss 0.016326 - Accuracy 0.996528 - Test Loss 1.684900 - Test Accuracy 0.666225\n",
      "Epoch 269/300 - 4.701185s - Loss 0.020807 - Accuracy 0.986111 - Test Loss 1.686013 - Test Accuracy 0.672414\n",
      "Epoch 270/300 - 4.679597s - Loss 0.012319 - Accuracy 0.996528 - Test Loss 1.688161 - Test Accuracy 0.666225\n",
      "Epoch 271/300 - 4.689465s - Loss 0.016076 - Accuracy 0.991319 - Test Loss 1.682090 - Test Accuracy 0.673740\n",
      "Epoch 272/300 - 4.709115s - Loss 0.043158 - Accuracy 0.979167 - Test Loss 1.685770 - Test Accuracy 0.668877\n",
      "Epoch 273/300 - 5.016198s - Loss 0.008756 - Accuracy 0.996528 - Test Loss 1.673622 - Test Accuracy 0.665782\n",
      "Epoch 274/300 - 4.687711s - Loss 0.022598 - Accuracy 0.986111 - Test Loss 1.693750 - Test Accuracy 0.667551\n",
      "Epoch 275/300 - 4.634548s - Loss 0.017333 - Accuracy 0.991319 - Test Loss 1.706039 - Test Accuracy 0.666667\n",
      "Epoch 276/300 - 4.695326s - Loss 0.044983 - Accuracy 0.984375 - Test Loss 1.719245 - Test Accuracy 0.664898\n",
      "Epoch 277/300 - 4.660825s - Loss 0.034254 - Accuracy 0.980903 - Test Loss 1.676980 - Test Accuracy 0.668877\n",
      "Epoch 278/300 - 4.697486s - Loss 0.026708 - Accuracy 0.968750 - Test Loss 1.671818 - Test Accuracy 0.673740\n",
      "Epoch 279/300 - 4.844233s - Loss 0.016176 - Accuracy 0.994792 - Test Loss 1.663121 - Test Accuracy 0.675066\n",
      "Epoch 280/300 - 4.823892s - Loss 0.018632 - Accuracy 0.993056 - Test Loss 1.662568 - Test Accuracy 0.677277\n",
      "Epoch 281/300 - 4.700417s - Loss 0.013271 - Accuracy 0.991319 - Test Loss 1.685824 - Test Accuracy 0.674182\n",
      "Epoch 282/300 - 4.715050s - Loss 0.016763 - Accuracy 0.991319 - Test Loss 1.690949 - Test Accuracy 0.671088\n",
      "Epoch 283/300 - 4.701138s - Loss 0.028402 - Accuracy 0.980903 - Test Loss 1.698583 - Test Accuracy 0.669761\n",
      "Epoch 284/300 - 4.690261s - Loss 0.014460 - Accuracy 0.993056 - Test Loss 1.699804 - Test Accuracy 0.665782\n",
      "Epoch 285/300 - 4.752017s - Loss 0.017301 - Accuracy 0.991319 - Test Loss 1.712223 - Test Accuracy 0.665340\n",
      "Epoch 286/300 - 5.087816s - Loss 0.012327 - Accuracy 0.998264 - Test Loss 1.709431 - Test Accuracy 0.669761\n",
      "Epoch 287/300 - 4.775583s - Loss 0.010881 - Accuracy 0.998264 - Test Loss 1.704376 - Test Accuracy 0.673298\n",
      "Epoch 288/300 - 4.695284s - Loss 0.016614 - Accuracy 0.993056 - Test Loss 1.723528 - Test Accuracy 0.672414\n",
      "Epoch 289/300 - 4.743777s - Loss 0.012875 - Accuracy 0.994792 - Test Loss 1.711213 - Test Accuracy 0.674624\n",
      "Epoch 290/300 - 4.677351s - Loss 0.015530 - Accuracy 0.991319 - Test Loss 1.710390 - Test Accuracy 0.674624\n",
      "Epoch 291/300 - 4.689526s - Loss 0.014844 - Accuracy 0.986111 - Test Loss 1.717234 - Test Accuracy 0.676393\n",
      "Epoch 292/300 - 4.688304s - Loss 0.013745 - Accuracy 0.994792 - Test Loss 1.719559 - Test Accuracy 0.678161\n",
      "Epoch 293/300 - 5.013529s - Loss 0.011536 - Accuracy 0.996528 - Test Loss 1.725711 - Test Accuracy 0.673740\n",
      "Epoch 294/300 - 4.658290s - Loss 0.010352 - Accuracy 0.998264 - Test Loss 1.716224 - Test Accuracy 0.677277\n",
      "Epoch 295/300 - 4.760257s - Loss 0.010478 - Accuracy 1.000000 - Test Loss 1.745852 - Test Accuracy 0.679045\n",
      "Epoch 296/300 - 4.667795s - Loss 0.026570 - Accuracy 0.975694 - Test Loss 1.739635 - Test Accuracy 0.670203\n",
      "Epoch 297/300 - 4.701554s - Loss 0.014158 - Accuracy 0.989583 - Test Loss 1.757024 - Test Accuracy 0.673298\n",
      "Epoch 298/300 - 4.681330s - Loss 0.011993 - Accuracy 0.993056 - Test Loss 1.761476 - Test Accuracy 0.678161\n",
      "Epoch 299/300 - 4.702043s - Loss 0.008764 - Accuracy 1.000000 - Test Loss 1.764831 - Test Accuracy 0.675066\n",
      "repeat_0 [0.6790450928381963]\n",
      "{'repeat_0': {'0': {'precision': 0.4065934065934066, 'recall': 0.3627450980392157, 'f1-score': 0.383419689119171, 'support': 204}, '1': {'precision': 0.49230769230769234, 'recall': 0.4729064039408867, 'f1-score': 0.4824120603015076, 'support': 203}, '2': {'precision': 0.4342857142857143, 'recall': 0.40860215053763443, 'f1-score': 0.42105263157894735, 'support': 186}, '3': {'precision': 0.421875, 'recall': 0.43548387096774194, 'f1-score': 0.42857142857142855, 'support': 186}, '4': {'precision': 0.4472049689440994, 'recall': 0.35467980295566504, 'f1-score': 0.3956043956043956, 'support': 203}, 'micro avg': {'precision': 0.4408839779005525, 'recall': 0.4063136456211813, 'f1-score': 0.42289348171701113, 'support': 982}, 'macro avg': {'precision': 0.44045335642618255, 'recall': 0.4068834652882288, 'f1-score': 0.42221204103509, 'support': 982}, 'weighted avg': {'precision': 0.44084726887608094, 'recall': 0.4063136456211813, 'f1-score': 0.422082517634602, 'support': 982}, 'samples avg': {'precision': 0.16312997347480107, 'recall': 0.17639257294429708, 'f1-score': 0.1674771588564692, 'support': 982}}}\n",
      "Repeat 1\n",
      "Epoch 0/300 - 4.840263s - Loss 0.792904 - Accuracy 0.519097 - Test Loss 0.876561 - Test Accuracy 0.513263\n",
      "Epoch 1/300 - 4.680842s - Loss 0.720280 - Accuracy 0.510417 - Test Loss 0.780097 - Test Accuracy 0.477454\n",
      "Epoch 2/300 - 4.634743s - Loss 0.738932 - Accuracy 0.489583 - Test Loss 0.744633 - Test Accuracy 0.493369\n",
      "Epoch 3/300 - 4.725384s - Loss 0.714450 - Accuracy 0.468750 - Test Loss 0.727759 - Test Accuracy 0.468612\n",
      "Epoch 4/300 - 4.706466s - Loss 0.630785 - Accuracy 0.512153 - Test Loss 0.709650 - Test Accuracy 0.501326\n",
      "Epoch 5/300 - 4.735272s - Loss 0.572801 - Accuracy 0.538194 - Test Loss 0.708829 - Test Accuracy 0.484527\n",
      "Epoch 6/300 - 5.032293s - Loss 0.699866 - Accuracy 0.503472 - Test Loss 0.699215 - Test Accuracy 0.486295\n",
      "Epoch 7/300 - 4.694766s - Loss 0.569533 - Accuracy 0.560764 - Test Loss 0.691290 - Test Accuracy 0.490716\n",
      "Epoch 8/300 - 4.656478s - Loss 0.586139 - Accuracy 0.531250 - Test Loss 0.684897 - Test Accuracy 0.471264\n",
      "Epoch 9/300 - 4.638790s - Loss 0.659258 - Accuracy 0.491319 - Test Loss 0.726873 - Test Accuracy 0.470822\n",
      "Epoch 10/300 - 4.684341s - Loss 0.691956 - Accuracy 0.468750 - Test Loss 0.722620 - Test Accuracy 0.482759\n",
      "Epoch 11/300 - 4.671880s - Loss 0.570996 - Accuracy 0.536458 - Test Loss 0.705011 - Test Accuracy 0.465959\n",
      "Epoch 12/300 - 4.773525s - Loss 0.485546 - Accuracy 0.592014 - Test Loss 0.698284 - Test Accuracy 0.512821\n",
      "Epoch 13/300 - 5.036752s - Loss 0.561401 - Accuracy 0.500000 - Test Loss 0.698506 - Test Accuracy 0.463749\n",
      "Epoch 14/300 - 4.802721s - Loss 0.558200 - Accuracy 0.546875 - Test Loss 0.711593 - Test Accuracy 0.488506\n",
      "Epoch 15/300 - 4.762076s - Loss 0.479071 - Accuracy 0.569444 - Test Loss 0.703144 - Test Accuracy 0.493369\n",
      "Epoch 16/300 - 4.802276s - Loss 0.468087 - Accuracy 0.569444 - Test Loss 0.689913 - Test Accuracy 0.486737\n",
      "Epoch 17/300 - 4.795565s - Loss 0.455609 - Accuracy 0.557292 - Test Loss 0.694209 - Test Accuracy 0.497790\n",
      "Epoch 18/300 - 4.769883s - Loss 0.487147 - Accuracy 0.550347 - Test Loss 0.689724 - Test Accuracy 0.500442\n",
      "Epoch 19/300 - 4.839262s - Loss 0.510298 - Accuracy 0.513889 - Test Loss 0.677646 - Test Accuracy 0.503979\n",
      "Epoch 20/300 - 4.814167s - Loss 0.421815 - Accuracy 0.598958 - Test Loss 0.705955 - Test Accuracy 0.509726\n",
      "Epoch 21/300 - 4.697496s - Loss 0.477686 - Accuracy 0.572917 - Test Loss 0.713301 - Test Accuracy 0.513263\n",
      "Epoch 22/300 - 4.759753s - Loss 0.377246 - Accuracy 0.642361 - Test Loss 0.699072 - Test Accuracy 0.496905\n",
      "Epoch 23/300 - 4.656549s - Loss 0.372148 - Accuracy 0.654514 - Test Loss 0.707093 - Test Accuracy 0.521220\n",
      "Epoch 24/300 - 4.741405s - Loss 0.514991 - Accuracy 0.546875 - Test Loss 0.722999 - Test Accuracy 0.520336\n",
      "Epoch 25/300 - 4.714535s - Loss 0.407400 - Accuracy 0.567708 - Test Loss 0.726605 - Test Accuracy 0.472591\n",
      "Epoch 26/300 - 4.959139s - Loss 0.476940 - Accuracy 0.598958 - Test Loss 0.755628 - Test Accuracy 0.526083\n",
      "Epoch 27/300 - 4.694218s - Loss 0.472736 - Accuracy 0.609375 - Test Loss 0.728991 - Test Accuracy 0.504863\n",
      "Epoch 28/300 - 4.733806s - Loss 0.360366 - Accuracy 0.635417 - Test Loss 0.716536 - Test Accuracy 0.534483\n",
      "Epoch 29/300 - 4.681008s - Loss 0.404947 - Accuracy 0.637153 - Test Loss 0.735452 - Test Accuracy 0.518126\n",
      "Epoch 30/300 - 4.659184s - Loss 0.317257 - Accuracy 0.680556 - Test Loss 0.732114 - Test Accuracy 0.523873\n",
      "Epoch 31/300 - 4.675268s - Loss 0.322869 - Accuracy 0.668403 - Test Loss 0.745259 - Test Accuracy 0.536251\n",
      "Epoch 32/300 - 4.673442s - Loss 0.362433 - Accuracy 0.612847 - Test Loss 0.742872 - Test Accuracy 0.535367\n",
      "Epoch 33/300 - 4.990992s - Loss 0.248262 - Accuracy 0.737847 - Test Loss 0.762882 - Test Accuracy 0.542882\n",
      "Epoch 34/300 - 4.640461s - Loss 0.263392 - Accuracy 0.758681 - Test Loss 0.775450 - Test Accuracy 0.541114\n",
      "Epoch 35/300 - 4.686580s - Loss 0.265771 - Accuracy 0.746528 - Test Loss 0.771773 - Test Accuracy 0.547745\n",
      "Epoch 36/300 - 4.676681s - Loss 0.267273 - Accuracy 0.730903 - Test Loss 0.768707 - Test Accuracy 0.556587\n",
      "Epoch 37/300 - 4.674484s - Loss 0.329371 - Accuracy 0.710069 - Test Loss 0.780152 - Test Accuracy 0.573828\n",
      "Epoch 38/300 - 4.669802s - Loss 0.349164 - Accuracy 0.638889 - Test Loss 0.791447 - Test Accuracy 0.546419\n",
      "Epoch 39/300 - 4.668046s - Loss 0.296440 - Accuracy 0.706597 - Test Loss 0.764009 - Test Accuracy 0.548187\n",
      "Epoch 40/300 - 4.943628s - Loss 0.274830 - Accuracy 0.717014 - Test Loss 0.807794 - Test Accuracy 0.549072\n",
      "Epoch 41/300 - 4.700506s - Loss 0.289991 - Accuracy 0.729167 - Test Loss 0.808196 - Test Accuracy 0.573828\n",
      "Epoch 42/300 - 4.732597s - Loss 0.326019 - Accuracy 0.666667 - Test Loss 0.817755 - Test Accuracy 0.548630\n",
      "Epoch 43/300 - 4.693510s - Loss 0.237436 - Accuracy 0.751736 - Test Loss 0.789551 - Test Accuracy 0.568966\n",
      "Epoch 44/300 - 4.671211s - Loss 0.230337 - Accuracy 0.753472 - Test Loss 0.866854 - Test Accuracy 0.591512\n",
      "Epoch 45/300 - 4.696124s - Loss 0.181803 - Accuracy 0.835069 - Test Loss 0.871582 - Test Accuracy 0.609195\n",
      "Epoch 46/300 - 4.832994s - Loss 0.230949 - Accuracy 0.758681 - Test Loss 0.893213 - Test Accuracy 0.591512\n",
      "Epoch 47/300 - 4.830773s - Loss 0.243195 - Accuracy 0.748264 - Test Loss 0.912857 - Test Accuracy 0.576481\n",
      "Epoch 48/300 - 4.704932s - Loss 0.235805 - Accuracy 0.753472 - Test Loss 0.879456 - Test Accuracy 0.566313\n",
      "Epoch 49/300 - 4.673941s - Loss 0.205857 - Accuracy 0.795139 - Test Loss 0.840441 - Test Accuracy 0.579134\n",
      "Epoch 50/300 - 4.698082s - Loss 0.213213 - Accuracy 0.786458 - Test Loss 0.905773 - Test Accuracy 0.573828\n",
      "Epoch 51/300 - 4.690299s - Loss 0.202911 - Accuracy 0.802083 - Test Loss 0.861736 - Test Accuracy 0.581786\n",
      "Epoch 52/300 - 4.747402s - Loss 0.194148 - Accuracy 0.798611 - Test Loss 0.852768 - Test Accuracy 0.586207\n",
      "Epoch 53/300 - 5.045362s - Loss 0.186509 - Accuracy 0.829861 - Test Loss 0.886050 - Test Accuracy 0.601680\n",
      "Epoch 54/300 - 4.737012s - Loss 0.176158 - Accuracy 0.805556 - Test Loss 0.887718 - Test Accuracy 0.596375\n",
      "Epoch 55/300 - 4.696447s - Loss 0.141319 - Accuracy 0.866319 - Test Loss 0.931173 - Test Accuracy 0.585323\n",
      "Epoch 56/300 - 4.758816s - Loss 0.148254 - Accuracy 0.845486 - Test Loss 0.962186 - Test Accuracy 0.578249\n",
      "Epoch 57/300 - 4.648992s - Loss 0.204782 - Accuracy 0.826389 - Test Loss 0.949221 - Test Accuracy 0.598585\n",
      "Epoch 58/300 - 4.682592s - Loss 0.278176 - Accuracy 0.781250 - Test Loss 0.890636 - Test Accuracy 0.596817\n",
      "Epoch 59/300 - 4.699065s - Loss 0.205249 - Accuracy 0.769097 - Test Loss 0.880122 - Test Accuracy 0.589302\n",
      "Epoch 60/300 - 5.056946s - Loss 0.273091 - Accuracy 0.800347 - Test Loss 0.965455 - Test Accuracy 0.580460\n",
      "Epoch 61/300 - 4.652347s - Loss 0.290738 - Accuracy 0.725694 - Test Loss 0.839100 - Test Accuracy 0.543767\n",
      "Epoch 62/300 - 4.677001s - Loss 0.257714 - Accuracy 0.706597 - Test Loss 0.815549 - Test Accuracy 0.552608\n",
      "Epoch 63/300 - 4.654861s - Loss 0.232200 - Accuracy 0.793403 - Test Loss 0.843122 - Test Accuracy 0.567197\n",
      "Epoch 64/300 - 4.725177s - Loss 0.211153 - Accuracy 0.791667 - Test Loss 0.859825 - Test Accuracy 0.577365\n",
      "Epoch 65/300 - 4.670805s - Loss 0.171881 - Accuracy 0.835069 - Test Loss 0.891916 - Test Accuracy 0.575597\n",
      "Epoch 66/300 - 4.964777s - Loss 0.224248 - Accuracy 0.789931 - Test Loss 0.945729 - Test Accuracy 0.579576\n",
      "Epoch 67/300 - 4.748849s - Loss 0.156279 - Accuracy 0.854167 - Test Loss 0.966653 - Test Accuracy 0.586649\n",
      "Epoch 68/300 - 4.698684s - Loss 0.140568 - Accuracy 0.845486 - Test Loss 0.974976 - Test Accuracy 0.602564\n",
      "Epoch 69/300 - 4.681509s - Loss 0.176782 - Accuracy 0.862847 - Test Loss 0.972638 - Test Accuracy 0.612290\n",
      "Epoch 70/300 - 4.697273s - Loss 0.163185 - Accuracy 0.838542 - Test Loss 0.965315 - Test Accuracy 0.600354\n",
      "Epoch 71/300 - 4.723183s - Loss 0.140701 - Accuracy 0.868056 - Test Loss 1.005937 - Test Accuracy 0.611848\n",
      "Epoch 72/300 - 4.660795s - Loss 0.122036 - Accuracy 0.904514 - Test Loss 0.997412 - Test Accuracy 0.624668\n",
      "Epoch 73/300 - 5.007848s - Loss 0.123226 - Accuracy 0.852431 - Test Loss 1.013631 - Test Accuracy 0.612290\n",
      "Epoch 74/300 - 4.762309s - Loss 0.116723 - Accuracy 0.890625 - Test Loss 1.001154 - Test Accuracy 0.620690\n",
      "Epoch 75/300 - 4.790608s - Loss 0.127038 - Accuracy 0.885417 - Test Loss 1.009858 - Test Accuracy 0.614943\n",
      "Epoch 76/300 - 4.706636s - Loss 0.102138 - Accuracy 0.894097 - Test Loss 1.020617 - Test Accuracy 0.625111\n",
      "Epoch 77/300 - 4.817935s - Loss 0.092563 - Accuracy 0.913194 - Test Loss 1.037406 - Test Accuracy 0.634394\n",
      "Epoch 78/300 - 4.750729s - Loss 0.117585 - Accuracy 0.892361 - Test Loss 1.079481 - Test Accuracy 0.628205\n",
      "Epoch 79/300 - 4.763850s - Loss 0.125151 - Accuracy 0.897569 - Test Loss 1.087589 - Test Accuracy 0.615827\n",
      "Epoch 80/300 - 5.065365s - Loss 0.101906 - Accuracy 0.885417 - Test Loss 1.110446 - Test Accuracy 0.624668\n",
      "Epoch 81/300 - 4.740320s - Loss 0.088150 - Accuracy 0.904514 - Test Loss 1.147871 - Test Accuracy 0.628205\n",
      "Epoch 82/300 - 4.775163s - Loss 0.111518 - Accuracy 0.911458 - Test Loss 1.137868 - Test Accuracy 0.636605\n",
      "Epoch 83/300 - 4.704720s - Loss 0.075942 - Accuracy 0.927083 - Test Loss 1.131653 - Test Accuracy 0.638373\n",
      "Epoch 84/300 - 4.723361s - Loss 0.089750 - Accuracy 0.913194 - Test Loss 1.131925 - Test Accuracy 0.629089\n",
      "Epoch 85/300 - 4.733724s - Loss 0.181318 - Accuracy 0.880208 - Test Loss 1.125098 - Test Accuracy 0.621132\n",
      "Epoch 86/300 - 4.904164s - Loss 0.118815 - Accuracy 0.878472 - Test Loss 1.063846 - Test Accuracy 0.609637\n",
      "Epoch 87/300 - 4.864770s - Loss 0.121457 - Accuracy 0.888889 - Test Loss 1.085148 - Test Accuracy 0.627321\n",
      "Epoch 88/300 - 4.734172s - Loss 0.085999 - Accuracy 0.904514 - Test Loss 1.077055 - Test Accuracy 0.632184\n",
      "Epoch 89/300 - 4.751961s - Loss 0.129750 - Accuracy 0.918403 - Test Loss 1.149534 - Test Accuracy 0.629531\n",
      "Epoch 90/300 - 4.724408s - Loss 0.096172 - Accuracy 0.911458 - Test Loss 1.129344 - Test Accuracy 0.631300\n",
      "Epoch 91/300 - 4.694283s - Loss 0.106406 - Accuracy 0.901042 - Test Loss 1.144659 - Test Accuracy 0.641910\n",
      "Epoch 92/300 - 4.723807s - Loss 0.074250 - Accuracy 0.940972 - Test Loss 1.164991 - Test Accuracy 0.641910\n",
      "Epoch 93/300 - 5.126682s - Loss 0.088787 - Accuracy 0.916667 - Test Loss 1.139078 - Test Accuracy 0.640141\n",
      "Epoch 94/300 - 4.690779s - Loss 0.103573 - Accuracy 0.892361 - Test Loss 1.181221 - Test Accuracy 0.623342\n",
      "Epoch 95/300 - 4.685178s - Loss 0.083575 - Accuracy 0.901042 - Test Loss 1.140332 - Test Accuracy 0.602564\n",
      "Epoch 96/300 - 4.669658s - Loss 0.102649 - Accuracy 0.906250 - Test Loss 1.103744 - Test Accuracy 0.618921\n",
      "Epoch 97/300 - 4.697994s - Loss 0.069079 - Accuracy 0.934028 - Test Loss 1.110277 - Test Accuracy 0.633952\n",
      "Epoch 98/300 - 4.772440s - Loss 0.073646 - Accuracy 0.932292 - Test Loss 1.152805 - Test Accuracy 0.641910\n",
      "Epoch 99/300 - 4.735087s - Loss 0.064256 - Accuracy 0.942708 - Test Loss 1.166077 - Test Accuracy 0.633068\n",
      "Epoch 100/300 - 5.092525s - Loss 0.096060 - Accuracy 0.934028 - Test Loss 1.208165 - Test Accuracy 0.624668\n",
      "Epoch 101/300 - 4.682742s - Loss 0.079917 - Accuracy 0.925347 - Test Loss 1.184887 - Test Accuracy 0.640141\n",
      "Epoch 102/300 - 4.719793s - Loss 0.076238 - Accuracy 0.914931 - Test Loss 1.244448 - Test Accuracy 0.639699\n",
      "Epoch 103/300 - 4.695691s - Loss 0.067799 - Accuracy 0.949653 - Test Loss 1.289559 - Test Accuracy 0.640584\n",
      "Epoch 104/300 - 4.728026s - Loss 0.111297 - Accuracy 0.901042 - Test Loss 1.253748 - Test Accuracy 0.627321\n",
      "Epoch 105/300 - 4.751980s - Loss 0.112918 - Accuracy 0.888889 - Test Loss 1.185591 - Test Accuracy 0.624668\n",
      "Epoch 106/300 - 5.073052s - Loss 0.071294 - Accuracy 0.944444 - Test Loss 1.149515 - Test Accuracy 0.637047\n",
      "Epoch 107/300 - 4.725837s - Loss 0.059129 - Accuracy 0.946181 - Test Loss 1.167657 - Test Accuracy 0.631300\n",
      "Epoch 108/300 - 4.736578s - Loss 0.072152 - Accuracy 0.937500 - Test Loss 1.204889 - Test Accuracy 0.636605\n",
      "Epoch 109/300 - 4.701551s - Loss 0.080664 - Accuracy 0.934028 - Test Loss 1.221831 - Test Accuracy 0.641468\n",
      "Epoch 110/300 - 4.779482s - Loss 0.059423 - Accuracy 0.949653 - Test Loss 1.248176 - Test Accuracy 0.642794\n",
      "Epoch 111/300 - 4.705169s - Loss 0.079018 - Accuracy 0.920139 - Test Loss 1.243257 - Test Accuracy 0.634394\n",
      "Epoch 112/300 - 4.717984s - Loss 0.100425 - Accuracy 0.911458 - Test Loss 1.203493 - Test Accuracy 0.633510\n",
      "Epoch 113/300 - 5.070813s - Loss 0.072351 - Accuracy 0.923611 - Test Loss 1.245427 - Test Accuracy 0.622900\n",
      "Epoch 114/300 - 4.832745s - Loss 0.060644 - Accuracy 0.953125 - Test Loss 1.206084 - Test Accuracy 0.641910\n",
      "Epoch 115/300 - 4.805158s - Loss 0.067860 - Accuracy 0.939236 - Test Loss 1.254139 - Test Accuracy 0.633510\n",
      "Epoch 116/300 - 4.792668s - Loss 0.064296 - Accuracy 0.953125 - Test Loss 1.247543 - Test Accuracy 0.647657\n",
      "Epoch 117/300 - 4.769873s - Loss 0.076930 - Accuracy 0.937500 - Test Loss 1.237031 - Test Accuracy 0.633952\n",
      "Epoch 118/300 - 4.766877s - Loss 0.079895 - Accuracy 0.927083 - Test Loss 1.241593 - Test Accuracy 0.625553\n",
      "Epoch 119/300 - 4.911068s - Loss 0.082251 - Accuracy 0.930556 - Test Loss 1.185364 - Test Accuracy 0.627763\n",
      "Epoch 120/300 - 4.849714s - Loss 0.074333 - Accuracy 0.927083 - Test Loss 1.189481 - Test Accuracy 0.643236\n",
      "Epoch 121/300 - 4.740956s - Loss 0.061897 - Accuracy 0.935764 - Test Loss 1.239511 - Test Accuracy 0.628647\n",
      "Epoch 122/300 - 4.697713s - Loss 0.089547 - Accuracy 0.937500 - Test Loss 1.251587 - Test Accuracy 0.633068\n",
      "Epoch 123/300 - 4.732002s - Loss 0.085811 - Accuracy 0.925347 - Test Loss 1.213811 - Test Accuracy 0.630416\n",
      "Epoch 124/300 - 4.678865s - Loss 0.059276 - Accuracy 0.963542 - Test Loss 1.206421 - Test Accuracy 0.638815\n",
      "Epoch 125/300 - 4.780103s - Loss 0.049204 - Accuracy 0.953125 - Test Loss 1.245861 - Test Accuracy 0.650309\n",
      "Epoch 126/300 - 5.050260s - Loss 0.043114 - Accuracy 0.956597 - Test Loss 1.260717 - Test Accuracy 0.656499\n",
      "Epoch 127/300 - 4.735069s - Loss 0.051557 - Accuracy 0.960069 - Test Loss 1.257886 - Test Accuracy 0.655172\n",
      "Epoch 128/300 - 4.688550s - Loss 0.045608 - Accuracy 0.968750 - Test Loss 1.279481 - Test Accuracy 0.657825\n",
      "Epoch 129/300 - 4.723382s - Loss 0.037752 - Accuracy 0.977431 - Test Loss 1.296665 - Test Accuracy 0.666225\n",
      "Epoch 130/300 - 4.745211s - Loss 0.034563 - Accuracy 0.968750 - Test Loss 1.337943 - Test Accuracy 0.660920\n",
      "Epoch 131/300 - 4.748804s - Loss 0.051670 - Accuracy 0.951389 - Test Loss 1.317924 - Test Accuracy 0.651194\n",
      "Epoch 132/300 - 4.707055s - Loss 0.032674 - Accuracy 0.977431 - Test Loss 1.429113 - Test Accuracy 0.647657\n",
      "Epoch 133/300 - 5.196883s - Loss 0.132302 - Accuracy 0.901042 - Test Loss 1.180168 - Test Accuracy 0.630858\n",
      "Epoch 134/300 - 4.746401s - Loss 0.165344 - Accuracy 0.878472 - Test Loss 1.137723 - Test Accuracy 0.621132\n",
      "Epoch 135/300 - 4.784993s - Loss 0.138220 - Accuracy 0.873264 - Test Loss 1.169378 - Test Accuracy 0.614058\n",
      "Epoch 136/300 - 4.767661s - Loss 0.117246 - Accuracy 0.885417 - Test Loss 1.131662 - Test Accuracy 0.632184\n",
      "Epoch 137/300 - 4.785716s - Loss 0.079138 - Accuracy 0.937500 - Test Loss 1.201174 - Test Accuracy 0.640141\n",
      "Epoch 138/300 - 4.719945s - Loss 0.074900 - Accuracy 0.928819 - Test Loss 1.152152 - Test Accuracy 0.641910\n",
      "Epoch 139/300 - 5.059429s - Loss 0.104965 - Accuracy 0.916667 - Test Loss 1.100193 - Test Accuracy 0.642352\n",
      "Epoch 140/300 - 4.719833s - Loss 0.065513 - Accuracy 0.934028 - Test Loss 1.122491 - Test Accuracy 0.634394\n",
      "Epoch 141/300 - 4.739082s - Loss 0.090685 - Accuracy 0.911458 - Test Loss 1.072779 - Test Accuracy 0.649867\n",
      "Epoch 142/300 - 4.736635s - Loss 0.068451 - Accuracy 0.932292 - Test Loss 1.080956 - Test Accuracy 0.659151\n",
      "Epoch 143/300 - 4.674222s - Loss 0.079561 - Accuracy 0.935764 - Test Loss 1.150346 - Test Accuracy 0.650309\n",
      "Epoch 144/300 - 4.772703s - Loss 0.056061 - Accuracy 0.961806 - Test Loss 1.117845 - Test Accuracy 0.657825\n",
      "Epoch 145/300 - 4.703025s - Loss 0.039981 - Accuracy 0.967014 - Test Loss 1.131976 - Test Accuracy 0.667993\n",
      "Epoch 146/300 - 5.036759s - Loss 0.049964 - Accuracy 0.956597 - Test Loss 1.214529 - Test Accuracy 0.666667\n",
      "Epoch 147/300 - 4.670152s - Loss 0.036810 - Accuracy 0.965278 - Test Loss 1.234354 - Test Accuracy 0.671530\n",
      "Epoch 148/300 - 4.725850s - Loss 0.062273 - Accuracy 0.965278 - Test Loss 1.243017 - Test Accuracy 0.671088\n",
      "Epoch 149/300 - 4.711424s - Loss 0.048458 - Accuracy 0.954861 - Test Loss 1.242211 - Test Accuracy 0.665340\n",
      "Epoch 150/300 - 4.745421s - Loss 0.082567 - Accuracy 0.954861 - Test Loss 1.186916 - Test Accuracy 0.675508\n",
      "Epoch 151/300 - 4.688401s - Loss 0.032447 - Accuracy 0.972222 - Test Loss 1.202863 - Test Accuracy 0.682140\n",
      "Epoch 152/300 - 4.792793s - Loss 0.027611 - Accuracy 0.972222 - Test Loss 1.219905 - Test Accuracy 0.679487\n",
      "Epoch 153/300 - 4.938339s - Loss 0.032954 - Accuracy 0.979167 - Test Loss 1.251675 - Test Accuracy 0.688329\n",
      "Epoch 154/300 - 4.795463s - Loss 0.033289 - Accuracy 0.967014 - Test Loss 1.278586 - Test Accuracy 0.681698\n",
      "Epoch 155/300 - 4.768520s - Loss 0.033605 - Accuracy 0.986111 - Test Loss 1.320149 - Test Accuracy 0.685234\n",
      "Epoch 156/300 - 4.774385s - Loss 0.026930 - Accuracy 0.977431 - Test Loss 1.303813 - Test Accuracy 0.693634\n",
      "Epoch 157/300 - 4.786532s - Loss 0.039711 - Accuracy 0.970486 - Test Loss 1.291620 - Test Accuracy 0.677277\n",
      "Epoch 158/300 - 4.825646s - Loss 0.037939 - Accuracy 0.972222 - Test Loss 1.325737 - Test Accuracy 0.679487\n",
      "Epoch 159/300 - 5.040538s - Loss 0.046752 - Accuracy 0.961806 - Test Loss 1.330925 - Test Accuracy 0.683908\n",
      "Epoch 160/300 - 4.839195s - Loss 0.041100 - Accuracy 0.965278 - Test Loss 1.356476 - Test Accuracy 0.688329\n",
      "Epoch 161/300 - 4.787959s - Loss 0.023399 - Accuracy 0.987847 - Test Loss 1.391976 - Test Accuracy 0.689655\n",
      "Epoch 162/300 - 4.800803s - Loss 0.020404 - Accuracy 0.987847 - Test Loss 1.401493 - Test Accuracy 0.693634\n",
      "Epoch 163/300 - 4.858402s - Loss 0.025343 - Accuracy 0.984375 - Test Loss 1.412021 - Test Accuracy 0.682140\n",
      "Epoch 164/300 - 4.818329s - Loss 0.029326 - Accuracy 0.980903 - Test Loss 1.422369 - Test Accuracy 0.681698\n",
      "Epoch 165/300 - 4.827948s - Loss 0.038263 - Accuracy 0.967014 - Test Loss 1.455088 - Test Accuracy 0.679045\n",
      "Epoch 166/300 - 5.167331s - Loss 0.031378 - Accuracy 0.972222 - Test Loss 1.431447 - Test Accuracy 0.667551\n",
      "Epoch 167/300 - 4.821382s - Loss 0.056692 - Accuracy 0.963542 - Test Loss 1.557363 - Test Accuracy 0.656941\n",
      "Epoch 168/300 - 4.856158s - Loss 0.054943 - Accuracy 0.949653 - Test Loss 1.360561 - Test Accuracy 0.665782\n",
      "Epoch 169/300 - 4.831176s - Loss 0.083968 - Accuracy 0.916667 - Test Loss 1.253803 - Test Accuracy 0.639257\n",
      "Epoch 170/300 - 4.809767s - Loss 0.092044 - Accuracy 0.892361 - Test Loss 1.122684 - Test Accuracy 0.631300\n",
      "Epoch 171/300 - 4.808876s - Loss 0.059450 - Accuracy 0.940972 - Test Loss 1.239175 - Test Accuracy 0.646331\n",
      "Epoch 172/300 - 4.861590s - Loss 0.047504 - Accuracy 0.949653 - Test Loss 1.276557 - Test Accuracy 0.640141\n",
      "Epoch 173/300 - 4.855451s - Loss 0.047587 - Accuracy 0.953125 - Test Loss 1.272000 - Test Accuracy 0.670203\n",
      "Epoch 174/300 - 4.683582s - Loss 0.039818 - Accuracy 0.972222 - Test Loss 1.362218 - Test Accuracy 0.667993\n",
      "Epoch 175/300 - 4.742724s - Loss 0.033360 - Accuracy 0.970486 - Test Loss 1.365125 - Test Accuracy 0.676393\n",
      "Epoch 176/300 - 4.744678s - Loss 0.035892 - Accuracy 0.967014 - Test Loss 1.374065 - Test Accuracy 0.673298\n",
      "Epoch 177/300 - 4.761263s - Loss 0.041796 - Accuracy 0.970486 - Test Loss 1.381493 - Test Accuracy 0.666225\n",
      "Epoch 178/300 - 4.748450s - Loss 0.034542 - Accuracy 0.970486 - Test Loss 1.378803 - Test Accuracy 0.684792\n",
      "Epoch 179/300 - 5.085787s - Loss 0.035261 - Accuracy 0.968750 - Test Loss 1.373185 - Test Accuracy 0.677719\n",
      "Epoch 180/300 - 4.709202s - Loss 0.038654 - Accuracy 0.963542 - Test Loss 1.406557 - Test Accuracy 0.665340\n",
      "Epoch 181/300 - 4.723044s - Loss 0.029092 - Accuracy 0.980903 - Test Loss 1.423584 - Test Accuracy 0.667551\n",
      "Epoch 182/300 - 4.722968s - Loss 0.038850 - Accuracy 0.972222 - Test Loss 1.426506 - Test Accuracy 0.679045\n",
      "Epoch 183/300 - 4.892776s - Loss 0.022424 - Accuracy 0.984375 - Test Loss 1.439889 - Test Accuracy 0.672414\n",
      "Epoch 184/300 - 4.739978s - Loss 0.034875 - Accuracy 0.975694 - Test Loss 1.416187 - Test Accuracy 0.687445\n",
      "Epoch 185/300 - 4.782132s - Loss 0.022419 - Accuracy 0.984375 - Test Loss 1.446152 - Test Accuracy 0.671972\n",
      "Epoch 186/300 - 5.144780s - Loss 0.022830 - Accuracy 0.982639 - Test Loss 1.447441 - Test Accuracy 0.679487\n",
      "Epoch 187/300 - 4.757085s - Loss 0.022157 - Accuracy 0.986111 - Test Loss 1.469989 - Test Accuracy 0.683908\n",
      "Epoch 188/300 - 4.738857s - Loss 0.020461 - Accuracy 0.989583 - Test Loss 1.478837 - Test Accuracy 0.682582\n",
      "Epoch 189/300 - 4.780244s - Loss 0.019467 - Accuracy 0.982639 - Test Loss 1.551054 - Test Accuracy 0.685676\n",
      "Epoch 190/300 - 4.749649s - Loss 0.014968 - Accuracy 0.984375 - Test Loss 1.523434 - Test Accuracy 0.691866\n",
      "Epoch 191/300 - 4.793156s - Loss 0.030596 - Accuracy 0.977431 - Test Loss 1.485469 - Test Accuracy 0.685676\n",
      "Epoch 192/300 - 5.147361s - Loss 0.027890 - Accuracy 0.975694 - Test Loss 1.473517 - Test Accuracy 0.679929\n",
      "Epoch 193/300 - 4.818310s - Loss 0.061827 - Accuracy 0.972222 - Test Loss 1.424600 - Test Accuracy 0.681256\n",
      "Epoch 194/300 - 4.727910s - Loss 0.028293 - Accuracy 0.973958 - Test Loss 1.417601 - Test Accuracy 0.687003\n",
      "Epoch 195/300 - 4.794172s - Loss 0.020932 - Accuracy 0.986111 - Test Loss 1.402545 - Test Accuracy 0.692308\n",
      "Epoch 196/300 - 4.851096s - Loss 0.014218 - Accuracy 0.998264 - Test Loss 1.427761 - Test Accuracy 0.692750\n",
      "Epoch 197/300 - 4.822977s - Loss 0.022507 - Accuracy 0.980903 - Test Loss 1.468539 - Test Accuracy 0.686118\n",
      "Epoch 198/300 - 4.849096s - Loss 0.019068 - Accuracy 0.982639 - Test Loss 1.498325 - Test Accuracy 0.688329\n",
      "Epoch 199/300 - 5.209463s - Loss 0.012165 - Accuracy 0.994792 - Test Loss 1.505698 - Test Accuracy 0.691866\n",
      "Epoch 200/300 - 4.760208s - Loss 0.027813 - Accuracy 0.975694 - Test Loss 1.488786 - Test Accuracy 0.691424\n",
      "Epoch 201/300 - 4.781399s - Loss 0.014261 - Accuracy 0.991319 - Test Loss 1.470256 - Test Accuracy 0.691866\n",
      "Epoch 202/300 - 4.803855s - Loss 0.025764 - Accuracy 0.987847 - Test Loss 1.509496 - Test Accuracy 0.686561\n",
      "Epoch 203/300 - 4.739740s - Loss 0.015873 - Accuracy 0.987847 - Test Loss 1.481699 - Test Accuracy 0.681698\n",
      "Epoch 204/300 - 4.749237s - Loss 0.017941 - Accuracy 0.982639 - Test Loss 1.454915 - Test Accuracy 0.689655\n",
      "Epoch 205/300 - 5.030220s - Loss 0.020204 - Accuracy 0.993056 - Test Loss 1.463981 - Test Accuracy 0.692308\n",
      "Epoch 206/300 - 4.772490s - Loss 0.015048 - Accuracy 0.991319 - Test Loss 1.475418 - Test Accuracy 0.695844\n",
      "Epoch 207/300 - 4.731052s - Loss 0.018330 - Accuracy 0.986111 - Test Loss 1.499018 - Test Accuracy 0.690981\n",
      "Epoch 208/300 - 4.728272s - Loss 0.017647 - Accuracy 0.987847 - Test Loss 1.501036 - Test Accuracy 0.691866\n",
      "Epoch 209/300 - 4.708260s - Loss 0.011774 - Accuracy 0.994792 - Test Loss 1.492292 - Test Accuracy 0.696286\n",
      "Epoch 210/300 - 4.746827s - Loss 0.019839 - Accuracy 0.982639 - Test Loss 1.512914 - Test Accuracy 0.697171\n",
      "Epoch 211/300 - 4.717430s - Loss 0.013484 - Accuracy 0.993056 - Test Loss 1.513582 - Test Accuracy 0.697171\n",
      "Epoch 212/300 - 5.020722s - Loss 0.011490 - Accuracy 0.993056 - Test Loss 1.512479 - Test Accuracy 0.700707\n",
      "Epoch 213/300 - 4.743371s - Loss 0.019629 - Accuracy 0.982639 - Test Loss 1.523916 - Test Accuracy 0.697613\n",
      "Epoch 214/300 - 4.740576s - Loss 0.013077 - Accuracy 0.991319 - Test Loss 1.536044 - Test Accuracy 0.700265\n",
      "Epoch 215/300 - 4.737201s - Loss 0.011801 - Accuracy 0.998264 - Test Loss 1.536236 - Test Accuracy 0.700265\n",
      "Epoch 216/300 - 4.771162s - Loss 0.011264 - Accuracy 1.000000 - Test Loss 1.544309 - Test Accuracy 0.697171\n",
      "Epoch 217/300 - 4.762510s - Loss 0.009085 - Accuracy 0.996528 - Test Loss 1.558361 - Test Accuracy 0.697171\n",
      "Epoch 218/300 - 4.816546s - Loss 0.015927 - Accuracy 0.994792 - Test Loss 1.555875 - Test Accuracy 0.697613\n",
      "Epoch 219/300 - 5.033974s - Loss 0.019924 - Accuracy 0.989583 - Test Loss 1.572637 - Test Accuracy 0.695844\n",
      "Epoch 220/300 - 4.803237s - Loss 0.017854 - Accuracy 0.979167 - Test Loss 1.575124 - Test Accuracy 0.698497\n",
      "Epoch 221/300 - 4.743553s - Loss 0.010418 - Accuracy 0.996528 - Test Loss 1.575733 - Test Accuracy 0.699823\n",
      "Epoch 222/300 - 4.705225s - Loss 0.016680 - Accuracy 0.986111 - Test Loss 1.570724 - Test Accuracy 0.698939\n",
      "Epoch 223/300 - 4.712228s - Loss 0.013219 - Accuracy 0.998264 - Test Loss 1.567687 - Test Accuracy 0.698055\n",
      "Epoch 224/300 - 4.746522s - Loss 0.039802 - Accuracy 0.986111 - Test Loss 1.575581 - Test Accuracy 0.686561\n",
      "Epoch 225/300 - 5.137310s - Loss 0.017534 - Accuracy 0.984375 - Test Loss 1.570365 - Test Accuracy 0.687445\n",
      "Epoch 226/300 - 4.745482s - Loss 0.021240 - Accuracy 0.986111 - Test Loss 1.570654 - Test Accuracy 0.688771\n",
      "Epoch 227/300 - 4.745657s - Loss 0.011696 - Accuracy 0.994792 - Test Loss 1.582671 - Test Accuracy 0.695844\n",
      "Epoch 228/300 - 4.735421s - Loss 0.013745 - Accuracy 0.994792 - Test Loss 1.595713 - Test Accuracy 0.692750\n",
      "Epoch 229/300 - 4.755634s - Loss 0.009488 - Accuracy 0.996528 - Test Loss 1.587304 - Test Accuracy 0.696286\n",
      "Epoch 230/300 - 4.718030s - Loss 0.018769 - Accuracy 0.989583 - Test Loss 1.606485 - Test Accuracy 0.695844\n",
      "Epoch 231/300 - 4.776330s - Loss 0.018011 - Accuracy 0.996528 - Test Loss 1.635365 - Test Accuracy 0.695402\n",
      "Epoch 232/300 - 5.051606s - Loss 0.011426 - Accuracy 0.994792 - Test Loss 1.622439 - Test Accuracy 0.694518\n",
      "Epoch 233/300 - 4.717147s - Loss 0.033372 - Accuracy 0.979167 - Test Loss 1.671055 - Test Accuracy 0.674624\n",
      "Epoch 234/300 - 4.724092s - Loss 0.033928 - Accuracy 0.972222 - Test Loss 1.619094 - Test Accuracy 0.678603\n",
      "Epoch 235/300 - 4.748232s - Loss 0.015705 - Accuracy 0.991319 - Test Loss 1.554026 - Test Accuracy 0.677719\n",
      "Epoch 236/300 - 4.700815s - Loss 0.024772 - Accuracy 0.977431 - Test Loss 1.546032 - Test Accuracy 0.685676\n",
      "Epoch 237/300 - 4.741029s - Loss 0.035414 - Accuracy 0.977431 - Test Loss 1.536598 - Test Accuracy 0.677719\n",
      "Epoch 238/300 - 4.919852s - Loss 0.027665 - Accuracy 0.973958 - Test Loss 1.555749 - Test Accuracy 0.678161\n",
      "Epoch 239/300 - 4.772284s - Loss 0.021958 - Accuracy 0.982639 - Test Loss 1.550051 - Test Accuracy 0.681698\n",
      "Epoch 240/300 - 4.707743s - Loss 0.015721 - Accuracy 0.989583 - Test Loss 1.569898 - Test Accuracy 0.685234\n",
      "Epoch 241/300 - 4.715066s - Loss 0.024152 - Accuracy 0.977431 - Test Loss 1.583389 - Test Accuracy 0.688771\n",
      "Epoch 242/300 - 4.710567s - Loss 0.035660 - Accuracy 0.977431 - Test Loss 1.579337 - Test Accuracy 0.687887\n",
      "Epoch 243/300 - 4.728243s - Loss 0.016438 - Accuracy 0.991319 - Test Loss 1.561791 - Test Accuracy 0.689655\n",
      "Epoch 244/300 - 4.734743s - Loss 0.019852 - Accuracy 0.980903 - Test Loss 1.551024 - Test Accuracy 0.695402\n",
      "Epoch 245/300 - 4.975112s - Loss 0.016676 - Accuracy 0.982639 - Test Loss 1.543492 - Test Accuracy 0.694518\n",
      "Epoch 246/300 - 4.705331s - Loss 0.032896 - Accuracy 0.982639 - Test Loss 1.539590 - Test Accuracy 0.694518\n",
      "Epoch 247/300 - 4.708129s - Loss 0.022098 - Accuracy 0.986111 - Test Loss 1.553279 - Test Accuracy 0.690981\n",
      "Epoch 248/300 - 4.705655s - Loss 0.012412 - Accuracy 0.993056 - Test Loss 1.551674 - Test Accuracy 0.695402\n",
      "Epoch 249/300 - 4.638766s - Loss 0.013208 - Accuracy 0.994792 - Test Loss 1.557807 - Test Accuracy 0.696729\n",
      "Epoch 250/300 - 4.697778s - Loss 0.009567 - Accuracy 0.993056 - Test Loss 1.568768 - Test Accuracy 0.695844\n",
      "Epoch 251/300 - 4.706545s - Loss 0.013640 - Accuracy 0.989583 - Test Loss 1.560900 - Test Accuracy 0.692308\n",
      "Epoch 252/300 - 5.060434s - Loss 0.012025 - Accuracy 0.991319 - Test Loss 1.574309 - Test Accuracy 0.690539\n",
      "Epoch 253/300 - 4.681670s - Loss 0.012858 - Accuracy 0.991319 - Test Loss 1.586546 - Test Accuracy 0.691424\n",
      "Epoch 254/300 - 4.663567s - Loss 0.012681 - Accuracy 0.993056 - Test Loss 1.592588 - Test Accuracy 0.694518\n",
      "Epoch 255/300 - 4.688041s - Loss 0.017464 - Accuracy 0.987847 - Test Loss 1.596804 - Test Accuracy 0.693634\n",
      "Epoch 256/300 - 4.722225s - Loss 0.016214 - Accuracy 0.994792 - Test Loss 1.622313 - Test Accuracy 0.692750\n",
      "Epoch 257/300 - 4.676165s - Loss 0.008373 - Accuracy 0.998264 - Test Loss 1.614143 - Test Accuracy 0.693192\n",
      "Epoch 258/300 - 4.778963s - Loss 0.009197 - Accuracy 0.994792 - Test Loss 1.608578 - Test Accuracy 0.696286\n",
      "Epoch 259/300 - 4.825935s - Loss 0.011256 - Accuracy 0.993056 - Test Loss 1.595435 - Test Accuracy 0.698939\n",
      "Epoch 260/300 - 4.769382s - Loss 0.015387 - Accuracy 0.991319 - Test Loss 1.601623 - Test Accuracy 0.697171\n",
      "Epoch 261/300 - 4.733265s - Loss 0.016314 - Accuracy 0.991319 - Test Loss 1.602009 - Test Accuracy 0.700265\n",
      "Epoch 262/300 - 4.767141s - Loss 0.009925 - Accuracy 0.996528 - Test Loss 1.634416 - Test Accuracy 0.696729\n",
      "Epoch 263/300 - 4.749046s - Loss 0.011439 - Accuracy 0.993056 - Test Loss 1.624092 - Test Accuracy 0.699381\n",
      "Epoch 264/300 - 4.762587s - Loss 0.011143 - Accuracy 0.987847 - Test Loss 1.619741 - Test Accuracy 0.695844\n",
      "Epoch 265/300 - 5.096849s - Loss 0.009555 - Accuracy 0.994792 - Test Loss 1.617828 - Test Accuracy 0.693634\n",
      "Epoch 266/300 - 4.776514s - Loss 0.009179 - Accuracy 0.994792 - Test Loss 1.614295 - Test Accuracy 0.693192\n",
      "Epoch 267/300 - 4.662411s - Loss 0.018488 - Accuracy 0.993056 - Test Loss 1.637003 - Test Accuracy 0.693192\n",
      "Epoch 268/300 - 4.676671s - Loss 0.007946 - Accuracy 0.998264 - Test Loss 1.641989 - Test Accuracy 0.696729\n",
      "Epoch 269/300 - 4.714538s - Loss 0.008173 - Accuracy 0.996528 - Test Loss 1.643896 - Test Accuracy 0.690981\n",
      "Epoch 270/300 - 4.652979s - Loss 0.011249 - Accuracy 0.996528 - Test Loss 1.623260 - Test Accuracy 0.699381\n",
      "Epoch 271/300 - 4.733733s - Loss 0.007946 - Accuracy 0.993056 - Test Loss 1.630956 - Test Accuracy 0.694960\n",
      "Epoch 272/300 - 5.032752s - Loss 0.008171 - Accuracy 0.994792 - Test Loss 1.628985 - Test Accuracy 0.698939\n",
      "Epoch 273/300 - 4.711042s - Loss 0.008451 - Accuracy 0.996528 - Test Loss 1.638213 - Test Accuracy 0.697171\n",
      "Epoch 274/300 - 4.665201s - Loss 0.008196 - Accuracy 0.996528 - Test Loss 1.644226 - Test Accuracy 0.698939\n",
      "Epoch 275/300 - 4.731486s - Loss 0.008526 - Accuracy 0.993056 - Test Loss 1.652767 - Test Accuracy 0.700265\n",
      "Epoch 276/300 - 4.751548s - Loss 0.014140 - Accuracy 0.994792 - Test Loss 1.646370 - Test Accuracy 0.699381\n",
      "Epoch 277/300 - 4.763629s - Loss 0.008203 - Accuracy 0.996528 - Test Loss 1.654467 - Test Accuracy 0.699381\n",
      "Epoch 278/300 - 4.830662s - Loss 0.011444 - Accuracy 0.993056 - Test Loss 1.653075 - Test Accuracy 0.696286\n",
      "Epoch 279/300 - 4.920186s - Loss 0.009522 - Accuracy 0.993056 - Test Loss 1.670813 - Test Accuracy 0.696729\n",
      "Epoch 280/300 - 4.746729s - Loss 0.010573 - Accuracy 0.989583 - Test Loss 1.668244 - Test Accuracy 0.695402\n",
      "Epoch 281/300 - 4.728921s - Loss 0.009670 - Accuracy 0.996528 - Test Loss 1.673019 - Test Accuracy 0.698939\n",
      "Epoch 282/300 - 4.749951s - Loss 0.008828 - Accuracy 0.993056 - Test Loss 1.672270 - Test Accuracy 0.698939\n",
      "Epoch 283/300 - 4.729244s - Loss 0.007594 - Accuracy 0.998264 - Test Loss 1.672241 - Test Accuracy 0.698939\n",
      "Epoch 284/300 - 4.746163s - Loss 0.010869 - Accuracy 0.987847 - Test Loss 1.663317 - Test Accuracy 0.702918\n",
      "Epoch 285/300 - 5.078123s - Loss 0.005933 - Accuracy 0.996528 - Test Loss 1.670845 - Test Accuracy 0.700265\n",
      "Epoch 286/300 - 4.777636s - Loss 0.008867 - Accuracy 0.996528 - Test Loss 1.679024 - Test Accuracy 0.698497\n",
      "Epoch 287/300 - 4.701951s - Loss 0.006760 - Accuracy 0.996528 - Test Loss 1.672817 - Test Accuracy 0.701592\n",
      "Epoch 288/300 - 4.654295s - Loss 0.006719 - Accuracy 0.996528 - Test Loss 1.675211 - Test Accuracy 0.702034\n",
      "Epoch 289/300 - 4.662074s - Loss 0.007557 - Accuracy 0.998264 - Test Loss 1.674480 - Test Accuracy 0.703360\n",
      "Epoch 290/300 - 4.656125s - Loss 0.005773 - Accuracy 0.998264 - Test Loss 1.678373 - Test Accuracy 0.703360\n",
      "Epoch 291/300 - 4.615053s - Loss 0.006995 - Accuracy 0.998264 - Test Loss 1.671621 - Test Accuracy 0.702918\n",
      "Epoch 292/300 - 4.955929s - Loss 0.008443 - Accuracy 0.991319 - Test Loss 1.678213 - Test Accuracy 0.700265\n",
      "Epoch 293/300 - 4.700465s - Loss 0.007330 - Accuracy 0.998264 - Test Loss 1.679152 - Test Accuracy 0.702476\n",
      "Epoch 294/300 - 4.720344s - Loss 0.011444 - Accuracy 0.994792 - Test Loss 1.682323 - Test Accuracy 0.702476\n",
      "Epoch 295/300 - 4.708199s - Loss 0.006236 - Accuracy 1.000000 - Test Loss 1.684580 - Test Accuracy 0.701592\n",
      "Epoch 296/300 - 4.770005s - Loss 0.005888 - Accuracy 0.998264 - Test Loss 1.686804 - Test Accuracy 0.704244\n",
      "Epoch 297/300 - 4.682846s - Loss 0.013294 - Accuracy 0.993056 - Test Loss 1.685166 - Test Accuracy 0.705570\n",
      "Epoch 298/300 - 4.703373s - Loss 0.005762 - Accuracy 0.998264 - Test Loss 1.670066 - Test Accuracy 0.703360\n",
      "Epoch 299/300 - 4.910289s - Loss 0.040460 - Accuracy 0.989583 - Test Loss 1.667826 - Test Accuracy 0.700707\n",
      "repeat_1 [0.6790450928381963, 0.7055702917771883]\n",
      "{'repeat_0': {'0': {'precision': 0.4065934065934066, 'recall': 0.3627450980392157, 'f1-score': 0.383419689119171, 'support': 204}, '1': {'precision': 0.49230769230769234, 'recall': 0.4729064039408867, 'f1-score': 0.4824120603015076, 'support': 203}, '2': {'precision': 0.4342857142857143, 'recall': 0.40860215053763443, 'f1-score': 0.42105263157894735, 'support': 186}, '3': {'precision': 0.421875, 'recall': 0.43548387096774194, 'f1-score': 0.42857142857142855, 'support': 186}, '4': {'precision': 0.4472049689440994, 'recall': 0.35467980295566504, 'f1-score': 0.3956043956043956, 'support': 203}, 'micro avg': {'precision': 0.4408839779005525, 'recall': 0.4063136456211813, 'f1-score': 0.42289348171701113, 'support': 982}, 'macro avg': {'precision': 0.44045335642618255, 'recall': 0.4068834652882288, 'f1-score': 0.42221204103509, 'support': 982}, 'weighted avg': {'precision': 0.44084726887608094, 'recall': 0.4063136456211813, 'f1-score': 0.422082517634602, 'support': 982}, 'samples avg': {'precision': 0.16312997347480107, 'recall': 0.17639257294429708, 'f1-score': 0.1674771588564692, 'support': 982}}, 'repeat_1': {'0': {'precision': 0.4787234042553192, 'recall': 0.4411764705882353, 'f1-score': 0.4591836734693877, 'support': 204}, '1': {'precision': 0.5255102040816326, 'recall': 0.5073891625615764, 'f1-score': 0.5162907268170426, 'support': 203}, '2': {'precision': 0.5058823529411764, 'recall': 0.46236559139784944, 'f1-score': 0.4831460674157303, 'support': 186}, '3': {'precision': 0.4777777777777778, 'recall': 0.46236559139784944, 'f1-score': 0.4699453551912568, 'support': 186}, '4': {'precision': 0.5345911949685535, 'recall': 0.4187192118226601, 'f1-score': 0.4696132596685083, 'support': 203}, 'micro avg': {'precision': 0.503919372900336, 'recall': 0.45824847250509165, 'f1-score': 0.48, 'support': 982}, 'macro avg': {'precision': 0.5044969868048919, 'recall': 0.4584032055536341, 'f1-score': 0.4796358165123851, 'support': 982}, 'weighted avg': {'precision': 0.504909310375762, 'recall': 0.45824847250509165, 'f1-score': 0.47972197886886103, 'support': 982}, 'samples avg': {'precision': 0.1874447391688771, 'recall': 0.1989389920424403, 'f1-score': 0.19127615679339816, 'support': 982}}}\n",
      "{'repeat_0': {'0': {'precision': 0.4065934065934066, 'recall': 0.3627450980392157, 'f1-score': 0.383419689119171, 'support': 204}, '1': {'precision': 0.49230769230769234, 'recall': 0.4729064039408867, 'f1-score': 0.4824120603015076, 'support': 203}, '2': {'precision': 0.4342857142857143, 'recall': 0.40860215053763443, 'f1-score': 0.42105263157894735, 'support': 186}, '3': {'precision': 0.421875, 'recall': 0.43548387096774194, 'f1-score': 0.42857142857142855, 'support': 186}, '4': {'precision': 0.4472049689440994, 'recall': 0.35467980295566504, 'f1-score': 0.3956043956043956, 'support': 203}, 'micro avg': {'precision': 0.4408839779005525, 'recall': 0.4063136456211813, 'f1-score': 0.42289348171701113, 'support': 982}, 'macro avg': {'precision': 0.44045335642618255, 'recall': 0.4068834652882288, 'f1-score': 0.42221204103509, 'support': 982}, 'weighted avg': {'precision': 0.44084726887608094, 'recall': 0.4063136456211813, 'f1-score': 0.422082517634602, 'support': 982}, 'samples avg': {'precision': 0.16312997347480107, 'recall': 0.17639257294429708, 'f1-score': 0.1674771588564692, 'support': 982}}, 'repeat_1': {'0': {'precision': 0.4787234042553192, 'recall': 0.4411764705882353, 'f1-score': 0.4591836734693877, 'support': 204}, '1': {'precision': 0.5255102040816326, 'recall': 0.5073891625615764, 'f1-score': 0.5162907268170426, 'support': 203}, '2': {'precision': 0.5058823529411764, 'recall': 0.46236559139784944, 'f1-score': 0.4831460674157303, 'support': 186}, '3': {'precision': 0.4777777777777778, 'recall': 0.46236559139784944, 'f1-score': 0.4699453551912568, 'support': 186}, '4': {'precision': 0.5345911949685535, 'recall': 0.4187192118226601, 'f1-score': 0.4696132596685083, 'support': 203}, 'micro avg': {'precision': 0.503919372900336, 'recall': 0.45824847250509165, 'f1-score': 0.48, 'support': 982}, 'macro avg': {'precision': 0.5044969868048919, 'recall': 0.4584032055536341, 'f1-score': 0.4796358165123851, 'support': 982}, 'weighted avg': {'precision': 0.504909310375762, 'recall': 0.45824847250509165, 'f1-score': 0.47972197886886103, 'support': 982}, 'samples avg': {'precision': 0.1874447391688771, 'recall': 0.1989389920424403, 'f1-score': 0.19127615679339816, 'support': 982}}, 'accuracy': {'avg': 0.6923076923076923, 'std': 0.01326259946949604}, 'time_train': {'avg': 1431.640007019043, 'std': 2.7684125900268555}, 'time_test': {'avg': 0.36323559284210205, 'std': 0.00011360645294189453}, 'complexity': {'parameter': 1621562, 'flops': 972961980}, 'model': 'LSTM', 'task': 'location', 'data': {'num_users': ['0', '1', '2', '3', '4', '5'], 'wifi_band': ['2.4'], 'environment': ['empty_room'], 'length': 3000}, 'nn': {'lr': 0.001, 'epoch': 300, 'batch_size': 128, 'threshold': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "[file]          run.py\n",
    "[description]   run WiFi-based models\n",
    "\"\"\"\n",
    "#\n",
    "##\n",
    "import json\n",
    "import argparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "#\n",
    "## parse arguments from input\n",
    "\n",
    "#\n",
    "var_task = \"location\"\n",
    "var_model = \"LSTM\"\n",
    "var_repeat = 2\n",
    "#\n",
    "## load annotation file as labels\n",
    "data_pd_y = load_data_y(preset[\"path\"][\"data_y\"],\n",
    "                        var_environment = preset[\"data\"][\"environment\"],\n",
    "                        var_wifi_band = preset[\"data\"][\"wifi_band\"],\n",
    "                        var_num_users = preset[\"data\"][\"num_users\"])\n",
    "#\n",
    "var_label_list = data_pd_y[\"label\"].to_list()\n",
    "#\n",
    "## load CSI amplitude\n",
    "data_x = load_data_x(preset[\"path\"][\"data_x\"], var_label_list)\n",
    "#\n",
    "## encode labels\n",
    "data_y = encode_data_y(data_pd_y, var_task)\n",
    "\n",
    "\n",
    "## a training set (80%) and a test set (20%)\n",
    "data_train_x, data_test_x, data_train_y, data_test_y = train_test_split(data_x, data_y,\n",
    "                                                                        test_size = 0.2,\n",
    "                                                                        shuffle = True,\n",
    "                                                                        random_state = 39)\n",
    "#\n",
    "## select a WiFi-based model\n",
    "# if var_model == \"ST-RF\": run_model = run_strf\n",
    "# #\n",
    "# if var_model == \"MLP\": run_model = run_mlp\n",
    "#\n",
    "if var_model == \"LSTM\": run_model = run_lstm\n",
    "# #\n",
    "# if var_model == \"CNN-1D\": run_model = run_cnn_1d\n",
    "# #\n",
    "# if var_model == \"CNN-2D\": run_model = run_cnn_2d\n",
    "# #\n",
    "# if var_model == \"CLSTM\": run_model = run_cnn_lstm\n",
    "# #\n",
    "# if var_model == \"ABLSTM\": run_model = run_ablstm\n",
    "# #\n",
    "if var_model == \"THAT\": run_model = run_that\n",
    "#\n",
    "## run WiFi-based model\n",
    "result = run_model(data_train_x, data_train_y,\n",
    "                   data_test_x, data_test_y, var_repeat)\n",
    "#\n",
    "##\n",
    "result[\"model\"] = var_model\n",
    "result[\"task\"] = var_task\n",
    "result[\"data\"] = preset[\"data\"]\n",
    "result[\"nn\"] = preset[\"nn\"]\n",
    "#\n",
    "print(result)\n",
    "#\n",
    "## save results\n",
    "var_file = open(preset[\"path\"][\"save\"], 'w')\n",
    "json.dump(result, var_file, indent = 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9def9a5d",
   "metadata": {
    "papermill": {
     "duration": 0.033912,
     "end_time": "2025-01-17T06:35:26.787591",
     "exception": false,
     "start_time": "2025-01-17T06:35:26.753679",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0d8662",
   "metadata": {
    "papermill": {
     "duration": 0.033687,
     "end_time": "2025-01-17T06:35:26.855698",
     "exception": false,
     "start_time": "2025-01-17T06:35:26.822011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a45d037",
   "metadata": {
    "papermill": {
     "duration": 0.03431,
     "end_time": "2025-01-17T06:35:26.924389",
     "exception": false,
     "start_time": "2025-01-17T06:35:26.890079",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad0ed9",
   "metadata": {
    "papermill": {
     "duration": 0.032902,
     "end_time": "2025-01-17T06:35:26.990432",
     "exception": false,
     "start_time": "2025-01-17T06:35:26.957530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4451316,
     "sourceId": 7638081,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2957.458818,
   "end_time": "2025-01-17T06:35:30.789940",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-17T05:46:13.331122",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
