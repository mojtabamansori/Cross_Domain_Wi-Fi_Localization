{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76ea8300",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T07:32:00.791520Z",
     "iopub.status.busy": "2025-01-17T07:32:00.791124Z",
     "iopub.status.idle": "2025-01-17T07:32:13.626817Z",
     "shell.execute_reply": "2025-01-17T07:32:13.625349Z"
    },
    "papermill": {
     "duration": 12.843464,
     "end_time": "2025-01-17T07:32:13.629230",
     "exception": false,
     "start_time": "2025-01-17T07:32:00.785766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ptflops\r\n",
      "  Downloading ptflops-0.7.4-py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from ptflops) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (2024.9.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0->ptflops) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\r\n",
      "Downloading ptflops-0.7.4-py3-none-any.whl (19 kB)\r\n",
      "Installing collected packages: ptflops\r\n",
      "Successfully installed ptflops-0.7.4\r\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "!pip install ptflops\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08f0aad4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-17T07:32:13.640457Z",
     "iopub.status.busy": "2025-01-17T07:32:13.639557Z",
     "iopub.status.idle": "2025-01-17T07:32:13.821572Z",
     "shell.execute_reply": "2025-01-17T07:32:13.820085Z"
    },
    "papermill": {
     "duration": 0.18998,
     "end_time": "2025-01-17T07:32:13.823967",
     "exception": false,
     "start_time": "2025-01-17T07:32:13.633987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "preset = {\n",
    "    #\n",
    "    ## define model\n",
    "    \"model\": \"LSTM\",                                    # \"ST-RF\", \"MLP\", \"LSTM\", \"CNN-1D\", \"CNN-2D\", \"CLSTM\", \"ABLSTM\", \"THAT\"\n",
    "    #\n",
    "    ## define task\n",
    "    \"task\": \"location\",                                 # \"identity\", \"activity\", \"location\"\n",
    "    #\n",
    "    ## number of repeated experiments\n",
    "    \"repeat\": 3,\n",
    "    #\n",
    "    ## path of data\n",
    "    \"path\": {\n",
    "        \"data_x\": \"/kaggle/input/wimans/wifi_csi/amp\",               # directory of CSI amplitude files\n",
    "        \"data_y\": \"/kaggle/input/wimans/annotation.csv\",             # path of annotation file\n",
    "        \"save\": f\"result_-.json\"                           # path to save results\n",
    "    },\n",
    "    #\n",
    "    ## data selection for experiments\n",
    "    \"data\": {\n",
    "        \"num_users\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"],    # select number(s) of users, (e.g., [\"0\", \"1\"], [\"2\", \"3\", \"4\", \"5\"])\n",
    "        \"wifi_band\": [\"2.4\"],                           # select WiFi band(s) (e.g., [\"2.4\"], [\"5\"], [\"2.4\", \"5\"])\n",
    "        \"environment\": [\"classroom\"],                   # select environment(s) (e.g., [\"classroom\"], [\"meeting_room\"], [\"empty_room\"])\n",
    "        \"length\": 3000,                                 # default length of CSI\n",
    "    },\n",
    "    #\n",
    "    ## hyperparameters of models\n",
    "    \"nn\": {\n",
    "        \"lr\": 1e-3,                                     # learning rate\n",
    "        \"epoch\": 300,                                   # number of epochs\n",
    "        \"batch_size\": 128,                              # batch size\n",
    "        \"threshold\": 0.5,                               # threshold to binarize sigmoid outputs\n",
    "    },\n",
    "    #\n",
    "    ## encoding of activities and locations\n",
    "    \"encoding\": {\n",
    "        \"activity\": {                                   # encoding of different activities\n",
    "            \"nan\":      [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"nothing\":  [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"walk\":     [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"rotation\": [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            \"jump\":     [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            \"wave\":     [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "            \"lie_down\": [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "            \"pick_up\":  [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "            \"sit_down\": [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "            \"stand_up\": [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        },\n",
    "        \"location\": {                                   # encoding of different locations\n",
    "            \"nan\":  [0, 0, 0, 0, 0],\n",
    "            \"a\":    [1, 0, 0, 0, 0],\n",
    "            \"b\":    [0, 1, 0, 0, 0],\n",
    "            \"c\":    [0, 0, 1, 0, 0],\n",
    "            \"d\":    [0, 0, 0, 1, 0],\n",
    "            \"e\":    [0, 0, 0, 0, 1],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "def mat_to_amp(data_mat):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : calculate amplitude of raw WiFi CSI data\n",
    "    [parameter]\n",
    "    : data_mat: dict, raw WiFi CSI data from *.mat files\n",
    "    [return]\n",
    "    : data_csi_amp: numpy array, CSI amplitude\n",
    "    \"\"\"\n",
    "    #\n",
    "    ## \n",
    "    var_length = data_mat[\"trace\"].shape[0]\n",
    "    #\n",
    "    data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "    #\n",
    "    data_csi_amp = np.array(data_csi_amp, dtype = np.float32)\n",
    "    #\n",
    "    return data_csi_amp\n",
    "\n",
    "#\n",
    "##\n",
    "def extract_csi_amp(var_dir_mat, \n",
    "                    var_dir_amp):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : read raw WiFi CSI files (*.mat), calcuate CSI amplitude, and save amplitude (*.npy)\n",
    "    [parameter]\n",
    "    : var_dir_mat: string, directory to read raw WiFi CSI files (*.mat)\n",
    "    : var_dir_amp: string, directory to save WiFi CSI amplitude (*.npy)\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    var_path_mat = os.listdir(var_dir_mat)\n",
    "    #\n",
    "    for var_c, var_path in enumerate(var_path_mat):\n",
    "        #\n",
    "        data_mat = scio.loadmat(os.path.join(var_dir_mat, var_path))\n",
    "        #\n",
    "        data_csi_amp = mat_to_amp(data_mat)\n",
    "        #\n",
    "        \n",
    "        #\n",
    "        var_path_save = os.path.join(var_dir_amp, var_path.replace(\".mat\", \".npy\"))\n",
    "        #\n",
    "        with open(var_path_save, \"wb\") as var_file:\n",
    "            np.save(var_file, data_csi_amp)\n",
    "\n",
    "#\n",
    "##\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : parse arguments from input\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    var_args = argparse.ArgumentParser()\n",
    "    #\n",
    "    var_args.add_argument(\"--dir_mat\", default = \"/kaggle/input/wimans/wifi_csi/mat\", type = str)\n",
    "    var_args.add_argument(\"--dir_amp\", default = \"/kaggle/input/wimans/wifi_csi/amp\", type = str)\n",
    "    #\n",
    "    return var_args.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03a86b29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T07:32:13.835215Z",
     "iopub.status.busy": "2025-01-17T07:32:13.834815Z",
     "iopub.status.idle": "2025-01-17T07:33:25.615603Z",
     "shell.execute_reply": "2025-01-17T07:33:25.614385Z"
    },
    "papermill": {
     "duration": 71.788197,
     "end_time": "2025-01-17T07:33:25.617608",
     "exception": false,
     "start_time": "2025-01-17T07:32:13.829411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_data_y(var_path_data_y,\n",
    "                var_environment = None, \n",
    "                var_wifi_band = None, \n",
    "                var_num_users = None):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : load annotation file (*.csv) as a pandas dataframe\n",
    "    : according to selected environment(s), WiFi band(s), and number(s) of users\n",
    "    [parameter]\n",
    "    : var_path_data_y: string, path of annotation file\n",
    "    : var_environment: list, selected environment(s), e.g., [\"classroom\"]\n",
    "    : var_wifi_band: list, selected WiFi band(s), e.g., [\"2.4\"]\n",
    "    : var_num_users: list, selected number(s) of users, e.g., [\"0\", \"1\", \"2\"]\n",
    "    [return]\n",
    "    : data_pd_y: pandas dataframe, labels of selected data\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_pd_y = pd.read_csv(var_path_data_y, dtype = str)\n",
    "    #\n",
    "    if var_environment is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "    #\n",
    "    if var_wifi_band is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "    #\n",
    "    if var_num_users is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "    #\n",
    "    return data_pd_y\n",
    "\n",
    "#\n",
    "##\n",
    "def load_data_x(var_path_data_x, \n",
    "                var_label_list):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : load CSI amplitude (*.npy)\n",
    "    : according to a label list of selected data\n",
    "    [parameter]\n",
    "    : var_path_data_x: string, directory of CSI amplitude files\n",
    "    : var_label_list: list, selected labels\n",
    "    [return]\n",
    "    : data_x: numpy array, CSI amplitude\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "    #\n",
    "    data_x = []\n",
    "    #\n",
    "    for var_path in var_path_list:\n",
    "        #\n",
    "        data_csi = np.load(var_path)\n",
    "        #\n",
    "        var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "        #\n",
    "        data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "        #\n",
    "        data_x.append(data_csi_pad)\n",
    "    #\n",
    "    data_x = np.array(data_x)\n",
    "    #\n",
    "    return data_x\n",
    "\n",
    "#\n",
    "##\n",
    "def encode_data_y(data_pd_y, \n",
    "                  var_task):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : encode labels according to specific task\n",
    "    [parameter]\n",
    "    : data_pd_y: pandas dataframe, labels of different tasks\n",
    "    : var_task: string, indicate task\n",
    "    [return]\n",
    "    : data_y: numpy array, label encoding of task\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    if var_task == \"identity\":\n",
    "        #\n",
    "        data_y = encode_identity(data_pd_y)\n",
    "    #\n",
    "    elif var_task == \"activity\":\n",
    "        #\n",
    "        data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "    #\n",
    "    elif var_task == \"location\":\n",
    "        #\n",
    "        data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "    #\n",
    "    return data_y\n",
    "\n",
    "#\n",
    "##\n",
    "def encode_identity(data_pd_y):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : encode identity labels in a pandas dataframe\n",
    "    [parameter]\n",
    "    : data_pd_y: pandas dataframe, labels of different tasks\n",
    "    [return]\n",
    "    : data_identity_onehot_y: numpy array, onehot encoding for identity labels\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "                                    \"user_3_location\", \"user_4_location\", \n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    # \n",
    "    data_identity_y = data_location_pd_y.to_numpy(copy = True).astype(str)\n",
    "    #\n",
    "    data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "    data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "    #\n",
    "    data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "    #\n",
    "    return data_identity_onehot_y\n",
    "\n",
    "#\n",
    "##\n",
    "def encode_activity(data_pd_y, \n",
    "                    var_encoding):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : encode activity labels in a pandas dataframe\n",
    "    [parameter]\n",
    "    : data_pd_y: pandas dataframe, labels of different tasks\n",
    "    : var_encoding: dict, encoding of different activities\n",
    "    [return]\n",
    "    : data_activity_onehot_y: numpy array, onehot encoding for activity labels\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "                                    \"user_3_activity\", \"user_4_activity\", \n",
    "                                    \"user_5_activity\", \"user_6_activity\"]]\n",
    "    #\n",
    "    data_activity_y = data_activity_pd_y.to_numpy(copy = True).astype(str)\n",
    "    #\n",
    "    data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "    #\n",
    "    return data_activity_onehot_y\n",
    "\n",
    "#\n",
    "##\n",
    "def encode_location(data_pd_y, \n",
    "                    var_encoding):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : encode location labels in a pandas dataframe\n",
    "    [parameter]\n",
    "    : data_pd_y: pandas dataframe, labels of different tasks\n",
    "    : var_encoding: dict, encoding of different locations\n",
    "    [return]\n",
    "    : data_location_onehot_y: numpy array, onehot encoding for location labels\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "                                    \"user_3_location\", \"user_4_location\", \n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    #\n",
    "    data_location_y = data_location_pd_y.to_numpy(copy = True).astype(str)\n",
    "    #\n",
    "    data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "    #\n",
    "    return data_location_onehot_y\n",
    "\n",
    "#\n",
    "##\n",
    "def test_load_data_y():\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : test load_data_y() function\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "\n",
    "\n",
    "#\n",
    "##\n",
    "def test_load_data_x():\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : test load_data_x() function\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_pd_y = load_data_y(preset[\"path\"][\"data_y\"],\n",
    "                            var_environment = [\"meeting_room\"], \n",
    "                            var_wifi_band = [\"2.4\"], \n",
    "                            var_num_users = None)\n",
    "    #\n",
    "    var_label_list = data_pd_y[\"label\"].to_list()\n",
    "    #\n",
    "    data_x = load_data_x(preset[\"path\"][\"data_x\"], var_label_list)\n",
    "    #\n",
    "  \n",
    "\n",
    "#\n",
    "##\n",
    "def test_encode_identity():\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : test encode_identity() function\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype = str)\n",
    "    #\n",
    "    data_identity_onehot_y = encode_identity(data_pd_y)\n",
    "    #\n",
    "\n",
    "\n",
    "#\n",
    "##\n",
    "def test_encode_activity():\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : test encode_activity() function\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype = str)\n",
    "    #\n",
    "    data_activity_onehot_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "    #\n",
    "\n",
    "#\n",
    "##\n",
    "def test_encode_location():\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : test encode_location() function\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype = str)\n",
    "    #\n",
    "    data_location_onehot_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "    #\n",
    "   \n",
    "\"\"\"\n",
    "[file]          run.py\n",
    "[description]   Run WiFi-based models\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "var_task = \"location\"\n",
    "var_model = \"LSTM\"\n",
    "var_repeat = 2\n",
    "\n",
    "# Load annotation file as labels\n",
    "data_pd_y = load_data_y(\n",
    "    preset[\"path\"][\"data_y\"],\n",
    "    var_environment=preset[\"data\"][\"environment\"],\n",
    "    var_wifi_band=preset[\"data\"][\"wifi_band\"],\n",
    "    var_num_users=preset[\"data\"][\"num_users\"]\n",
    ")\n",
    "var_label_list = data_pd_y[\"label\"].to_list()\n",
    "data_x = load_data_x(preset[\"path\"][\"data_x\"], var_label_list)\n",
    "data_y = encode_data_y(data_pd_y, var_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3683777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T07:33:25.627729Z",
     "iopub.status.busy": "2025-01-17T07:33:25.627314Z",
     "iopub.status.idle": "2025-01-17T07:33:28.845375Z",
     "shell.execute_reply": "2025-01-17T07:33:28.844085Z"
    },
    "papermill": {
     "duration": 3.225133,
     "end_time": "2025-01-17T07:33:28.847189",
     "exception": false,
     "start_time": "2025-01-17T07:33:25.622056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1881, 3000, 3, 3, 30)\n",
      "(1881, 3000, 270)\n",
      "(1881, 6, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "shapee = data_x.shape\n",
    "print(shapee)\n",
    "data_x_1 = np.resize(data_x, (shapee[0], 3000, 270)) \n",
    "print(data_x_1.shape)\n",
    "\n",
    "print(data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a581070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T07:33:28.857070Z",
     "iopub.status.busy": "2025-01-17T07:33:28.856689Z",
     "iopub.status.idle": "2025-01-17T07:33:28.870899Z",
     "shell.execute_reply": "2025-01-17T07:33:28.869777Z"
    },
    "papermill": {
     "duration": 0.021365,
     "end_time": "2025-01-17T07:33:28.872888",
     "exception": false,
     "start_time": "2025-01-17T07:33:28.851523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# تعریف مدل LSTM\n",
    "class LSTMM(torch.nn.Module):\n",
    "    def __init__(self, var_x_shape, var_y_shape):\n",
    "        super(LSTMM, self).__init__()\n",
    "        var_dim_input = var_x_shape[-1]  # باید 270 باشد\n",
    "        var_dim_output = var_y_shape[-1]  # باید 30 باشد\n",
    "        self.layer_norm = torch.nn.BatchNorm1d(var_dim_input)\n",
    "        self.layer_pooling = torch.nn.AvgPool1d(kernel_size=10, stride=10)\n",
    "        self.layer_lstm = torch.nn.LSTM(\n",
    "            input_size=var_dim_input,\n",
    "            hidden_size=512,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.layer_linear = torch.nn.Linear(512, var_dim_output)\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        # شکل ورودی: (batch, time, features)\n",
    "        var_t = torch.permute(var_input, (0, 2, 1))  # (batch, features, time)\n",
    "        var_t = self.layer_norm(var_t)\n",
    "        var_t = self.layer_pooling(var_t)  # (batch, features, pooled_time)\n",
    "        var_t = torch.permute(var_t, (0, 2, 1))  # (batch, pooled_time, features)\n",
    "        var_t, _ = self.layer_lstm(var_t)  # (batch, pooled_time, hidden)\n",
    "        var_t = var_t[:, -1, :]  # (batch, hidden)\n",
    "        var_output = self.layer_linear(var_t)  # (batch, var_dim_output)\n",
    "        return var_output\n",
    "\n",
    "# تعریف مدل ResNet\n",
    "def build_resnet(var_y_shape):\n",
    "    model_resnet = models.resnet18(weights=None)  # استفاده از 'weights' به جای 'pretrained'\n",
    "\n",
    "    # تغییر لایه کانولوشن اول برای پذیرش ورودی با 1 کانال\n",
    "    model_resnet.conv1 = torch.nn.Conv2d(\n",
    "        in_channels=1,\n",
    "        out_channels=64,\n",
    "        kernel_size=7,\n",
    "        stride=2,\n",
    "        padding=3,\n",
    "        bias=False\n",
    "    )\n",
    "\n",
    "    # تغییر لایه Fully Connected نهایی برای خروجی تعداد کلاس‌های مورد نظر\n",
    "    in_features_fc = model_resnet.fc.in_features  # معمولاً 512\n",
    "    out_features_fc = var_y_shape[-1]  # 30\n",
    "    model_resnet.fc = torch.nn.Linear(in_features_fc, out_features_fc)\n",
    "\n",
    "    return model_resnet\n",
    "\n",
    "# تابع برای پاکسازی پیشوند 'module.' یا '_orig_mod.'\n",
    "def remove_module_prefix(state_dict):\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            new_key = k[len('module.'):]\n",
    "        elif k.startswith('_orig_mod.'):\n",
    "            new_key = k[len('_orig_mod.'):]\n",
    "        else:\n",
    "            new_key = k\n",
    "        new_state_dict[new_key] = v\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "def compute_accuracy_batch(model, dataloader, device, var_threshold=0.5):\n",
    "    \"\"\"\n",
    "    محاسبه دقت مدل به صورت دسته‌ای بدون ذخیره‌سازی تمام پیش‌بینی‌ها و برچسب‌ها.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): مدل PyTorch برای ارزیابی.\n",
    "        dataloader (DataLoader): DataLoader برای داده‌ها.\n",
    "        device (torch.device): دستگاه (GPU یا CPU) برای اجرای مدل.\n",
    "        var_threshold (float): آستانه برای تبدیل احتمالات به مقادیر باینری.\n",
    "\n",
    "    Returns:\n",
    "        float: دقت مدل بر روی داده‌های ارزیابی.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sum = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # پیش‌بینی مدل\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # اعمال سیگموید برای تبدیل خروجی‌ها به احتمالات\n",
    "            preds = torch.sigmoid(outputs)\n",
    "     \n",
    "            # اعمال آستانه برای تبدیل احتمالات به مقادیر باینری\n",
    "            preds = (preds > var_threshold).float()\n",
    "    \n",
    "            predict_test_y = preds.detach().cpu().numpy()\n",
    "            data_test_y = labels.detach().cpu().numpy()\n",
    "            \n",
    "            # محاسبه تعداد نمونه‌های درست پیش‌بینی شده\n",
    "            # در اینجا فرض بر این است که دقت به صورت تطابق کامل برچسب‌ها محاسبه می‌شود\n",
    "\n",
    "            predict_test_y = predict_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            data_test_y = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            var_accuracy_test = accuracy_score(data_test_y.astype(int), predict_test_y.astype(int))\n",
    "            sum = var_accuracy_test + sum\n",
    "        ave = sum/len(dataloader)\n",
    "    return ave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b12ecec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T07:33:28.884362Z",
     "iopub.status.busy": "2025-01-17T07:33:28.884002Z",
     "iopub.status.idle": "2025-01-17T07:34:07.353917Z",
     "shell.execute_reply": "2025-01-17T07:34:07.350806Z"
    },
    "papermill": {
     "duration": 38.480958,
     "end_time": "2025-01-17T07:34:07.359600",
     "exception": false,
     "start_time": "2025-01-17T07:33:28.878642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_features(model_lstm)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-cc5e10dad423>:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"/kaggle/input/models/lstm_mr.pth\", map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "class LSTMM(torch.nn.Module):\n",
    "    def __init__(self, var_x_shape, var_y_shape):\n",
    "        super(LSTMM, self).__init__()\n",
    "        var_dim_input = var_x_shape[-1]  # باید 270 باشد\n",
    "        var_dim_output = var_y_shape[-1]  # باید 30 باشد\n",
    "        self.layer_norm = torch.nn.BatchNorm1d(var_dim_input)\n",
    "        self.layer_pooling = torch.nn.AvgPool1d(kernel_size=10, stride=10)\n",
    "        self.layer_lstm = torch.nn.LSTM(\n",
    "            input_size=var_dim_input,\n",
    "            hidden_size=512,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.layer_linear = torch.nn.Linear(512, var_dim_output)\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        # شکل ورودی: (batch, time, features)\n",
    "        var_t = torch.permute(var_input, (0, 2, 1))  # (batch, features, time)\n",
    "        var_t = self.layer_norm(var_t)\n",
    "        var_t = self.layer_pooling(var_t)  # (batch, features, pooled_time)\n",
    "        var_t = torch.permute(var_t, (0, 2, 1))  # (batch, pooled_time, features)\n",
    "        var_t, _ = self.layer_lstm(var_t)  # (batch, pooled_time, hidden)\n",
    "        var_t = var_t[:, -1, :]  # (batch, hidden)\n",
    "        var_features = var_t  # ویژگی‌ها از LSTM\n",
    "        var_output = self.layer_linear(var_t)  # خروجی نهایی\n",
    "        return var_features, var_output\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(model, dataloader, device, model_type='lstm'):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            if model_type == 'lstm':\n",
    "                var_features, _ = model(inputs)  # Unpack the tuple\n",
    "            else:\n",
    "                var_features = model(inputs)\n",
    "            \n",
    "            \n",
    "            var_features = var_features.view(var_features.size(0), -1)  # Flatten\n",
    "            features.append(var_features.cpu().numpy())\n",
    "            labels_list.append(labels.cpu().numpy())\n",
    "            \n",
    "\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels_list, axis=0)\n",
    "    return features, labels\n",
    "\n",
    "var_x = torch.from_numpy(data_x_1).float()\n",
    "var_y = torch.from_numpy(data_y).long()\n",
    "\n",
    "var_x = var_x.to(device)\n",
    "var_y = var_y.to(device)\n",
    "\n",
    "var_x_shape = var_x.shape\n",
    "var_y_shape = var_y.shape\n",
    "\n",
    "\n",
    "batch_size = 8  \n",
    "dataset_lstm = TensorDataset(var_x, var_y)\n",
    "dataloader_lstm = DataLoader(dataset_lstm, batch_size=batch_size, shuffle=False)\n",
    "print('extract_features(model_lstm)')\n",
    "# استخراج ویژگی‌ها از LSTM\n",
    "checkpoint = torch.load(\"/kaggle/input/models/lstm_mr.pth\", map_location=device)\n",
    "checkpoint = remove_module_prefix(checkpoint['best_weight'])\n",
    "model_lstm = LSTMM(var_x_shape, (var_y_shape[0],30)).to(device)\n",
    "model_lstm.load_state_dict(checkpoint)  \n",
    "features_lstm, labels_lstm = extract_features(model_lstm, dataloader_lstm, device, model_type='lstm')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a16adc11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T07:34:07.376123Z",
     "iopub.status.busy": "2025-01-17T07:34:07.375662Z",
     "iopub.status.idle": "2025-01-17T07:54:33.336351Z",
     "shell.execute_reply": "2025-01-17T07:54:33.334113Z"
    },
    "papermill": {
     "duration": 1225.971021,
     "end_time": "2025-01-17T07:54:33.339579",
     "exception": false,
     "start_time": "2025-01-17T07:34:07.368558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\r\n",
      "extract_features(model_resnet)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-96efcfc07c44>:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint1 = torch.load(\"/kaggle/input/models/resnet_mr.pth\", map_location=device)\n",
      "100%|██████████| 236/236 [20:19<00:00,  5.17s/it]\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_resnet(var_y_shape):\n",
    "    model_resnet = models.resnet18(weights=None)  # استفاده از 'weights' به جای 'pretrained'\n",
    "\n",
    "    # تغییر لایه کانولوشن اول برای پذیرش ورودی با 1 کانال\n",
    "    model_resnet.conv1 = torch.nn.Conv2d(\n",
    "        in_channels=1,\n",
    "        out_channels=64,\n",
    "        kernel_size=7,\n",
    "        stride=2,\n",
    "        padding=3,\n",
    "        bias=False\n",
    "    )\n",
    "\n",
    "    # تغییر لایه Fully Connected نهایی برای خروجی تعداد کلاس‌های مورد نظر\n",
    "    in_features_fc = model_resnet.fc.in_features  # معمولاً 512\n",
    "    out_features_fc = var_y_shape[-1]  # 30\n",
    "    model_resnet.fc = torch.nn.Linear(in_features_fc, out_features_fc)\n",
    "    \n",
    "    return model_resnet\n",
    "\n",
    "def extract_features(model, dataloader, device, model_type='resnet'):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            var_features = model(inputs)  # خروجی مدل ویژگی‌های 512 بعدی است\n",
    "            \n",
    "            var_features = var_features.view(var_features.size(0), -1)  # Flatten\n",
    "            features.append(var_features.cpu().numpy())\n",
    "            labels_list.append(labels.cpu().numpy())\n",
    "            \n",
    "            \n",
    "    features = np.concatenate(features, axis=0)\n",
    "    \n",
    "    labels = np.concatenate(labels_list, axis=0)\n",
    "    return features, labels\n",
    "\n",
    "var_x = torch.from_numpy(data_x_1).float()\n",
    "var_y = torch.from_numpy(data_y).long()\n",
    "\n",
    "var_x = var_x.to(device)\n",
    "var_y = var_y.to(device)\n",
    "\n",
    "var_x_shape = var_x.shape\n",
    "var_y_shape = var_y.shape\n",
    "batch_size = 8\n",
    "print('extract_features(model_resnet)')\n",
    "checkpoint1 = torch.load(\"/kaggle/input/models/resnet_mr.pth\", map_location=device)\n",
    "checkpoint1 = remove_module_prefix(checkpoint1)\n",
    "model_resnet = build_resnet((var_y_shape[0], 30)).to(device)\n",
    "model_resnet.load_state_dict(checkpoint1)\n",
    "\n",
    "# تغییر لایه‌ی FC به Identity برای استخراج ویژگی‌های 512 بعدی\n",
    "model_resnet.fc = nn.Identity()\n",
    "\n",
    "var_x_resnet = var_x.unsqueeze(1)  # افزودن بعد کانال\n",
    "dataset_resnet = TensorDataset(var_x_resnet, var_y)\n",
    "dataloader_resnet = DataLoader(dataset_resnet, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# استخراج ویژگی‌ها از ResNet\n",
    "features_resnet, labels_resnet = extract_features(model_resnet, dataloader_resnet, device, model_type='resnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d435097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T07:54:33.379176Z",
     "iopub.status.busy": "2025-01-17T07:54:33.378603Z",
     "iopub.status.idle": "2025-01-17T07:54:48.587154Z",
     "shell.execute_reply": "2025-01-17T07:54:48.585985Z"
    },
    "papermill": {
     "duration": 15.230832,
     "end_time": "2025-01-17T07:54:48.589063",
     "exception": false,
     "start_time": "2025-01-17T07:54:33.358231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature concate\n",
      "test train splite\n",
      "X_train (1504, 1024)\n",
      "X_test (377, 1024)\n",
      "y_train (1504, 30)\n",
      "y_test (377, 30)\n",
      "random forest start\n",
      "Accuracy of Random Forest: 0.2016\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# اطمینان از هم‌خوانی برچسب‌ها\n",
    "assert np.array_equal(labels_lstm, labels_resnet), \"برچسب‌ها با هم همخوانی ندارند\"\n",
    "print('feature concate')\n",
    "# ترکیب ویژگی‌ها\n",
    "features_combined = np.concatenate([features_lstm, features_resnet], axis=1)\n",
    "\n",
    "# آماده‌سازی داده‌ها برای Random Forest\n",
    "X = features_combined\n",
    "y = labels_lstm  # یا labels_resnet\n",
    "\n",
    "# تقسیم داده‌ها به مجموعه آموزش و تست (اختیاری)\n",
    "from sklearn.model_selection import train_test_split\n",
    "print('test train splite')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train = y_train.reshape((y_train.shape[0],30))\n",
    "y_test = y_test.reshape((y_test.shape[0],30))\n",
    "print('X_train',X_train.shape)\n",
    "print('X_test',X_test.shape)\n",
    "print('y_train',y_train.shape)\n",
    "print('y_test',y_test.shape)\n",
    "\n",
    "print('random forest start')\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# پیش‌بینی و ارزیابی مدل\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of Random Forest: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df00d2",
   "metadata": {
    "papermill": {
     "duration": 0.017249,
     "end_time": "2025-01-17T07:54:48.624135",
     "exception": false,
     "start_time": "2025-01-17T07:54:48.606886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8655aa2",
   "metadata": {
    "papermill": {
     "duration": 0.017687,
     "end_time": "2025-01-17T07:54:48.659356",
     "exception": false,
     "start_time": "2025-01-17T07:54:48.641669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4451316,
     "sourceId": 7638081,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6493733,
     "sourceId": 10488113,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1374.851618,
   "end_time": "2025-01-17T07:54:52.244457",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-17T07:31:57.392839",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
