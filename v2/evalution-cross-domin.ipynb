{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c90772d",
   "metadata": {
    "papermill": {
     "duration": 0.005077,
     "end_time": "2025-01-20T09:55:53.471218",
     "exception": false,
     "start_time": "2025-01-20T09:55:53.466141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "*preset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606bc08f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-20T09:55:53.481839Z",
     "iopub.status.busy": "2025-01-20T09:55:53.481356Z",
     "iopub.status.idle": "2025-01-20T09:55:53.492654Z",
     "shell.execute_reply": "2025-01-20T09:55:53.491234Z"
    },
    "papermill": {
     "duration": 0.019184,
     "end_time": "2025-01-20T09:55:53.494956",
     "exception": false,
     "start_time": "2025-01-20T09:55:53.475772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          preset.py\n",
    "[description]   default settings of WiFi-based models\n",
    "\"\"\"\n",
    "#\n",
    "##\n",
    "model123 = \"THAT_location\"\n",
    "preset = {\n",
    "    #\n",
    "    ## define model\n",
    "    \"model\": \"LSTM\",                                    # \"ST-RF\", \"MLP\", \"LSTM\", \"CNN-1D\", \"CNN-2D\", \"CLSTM\", \"ABLSTM\", \"THAT\"\n",
    "    #\n",
    "    ## define task\n",
    "    \"task\": \"location\",                                 # \"identity\", \"activity\", \"location\"\n",
    "    #\n",
    "    ## number of repeated experiments\n",
    "    \"repeat\": 3,\n",
    "    #\n",
    "    ## path of data\n",
    "    \"path\": {\n",
    "        \"data_x\": \"/kaggle/input/wimans/wifi_csi/amp\",               # directory of CSI amplitude files\n",
    "        \"data_y\": \"/kaggle/input/wimans/annotation.csv\",             # path of annotation file\n",
    "        \"save\": f\"result_{model123}.json\"                           # path to save results\n",
    "    },\n",
    "    #\n",
    "    ## data selection for experiments\n",
    "    \"data\": {\n",
    "        \"num_users\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"],    # select number(s) of users, (e.g., [\"0\", \"1\"], [\"2\", \"3\", \"4\", \"5\"])\n",
    "        \"wifi_band\": [\"2.4\"],                           # select WiFi band(s) (e.g., [\"2.4\"], [\"5\"], [\"2.4\", \"5\"])\n",
    "        \"environment\": [\"meeting_room\"],                   # select environment(s) (e.g., [\"classroom\"], [\"meeting_room\"], [\"empty_room\"])\n",
    "        \"length\": 3000,                                 # default length of CSI\n",
    "    },\n",
    "    #\n",
    "    ## hyperparameters of models\n",
    "    \"nn\": {\n",
    "        \"lr\": 1e-3,                                     # learning rate\n",
    "        \"epoch\": 300,                                   # number of epochs\n",
    "        \"batch_size\": 128,                              # batch size\n",
    "        \"threshold\": 0.5,                               # threshold to binarize sigmoid outputs\n",
    "    },\n",
    "    #\n",
    "    ## encoding of activities and locations\n",
    "    \"encoding\": {\n",
    "        \"activity\": {                                   # encoding of different activities\n",
    "            \"nan\":      [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"nothing\":  [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"walk\":     [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"rotation\": [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            \"jump\":     [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            \"wave\":     [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "            \"lie_down\": [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "            \"pick_up\":  [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "            \"sit_down\": [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "            \"stand_up\": [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        },\n",
    "        \"location\": {                                   # encoding of different locations\n",
    "            \"nan\":  [0, 0, 0, 0, 0],\n",
    "            \"a\":    [1, 0, 0, 0, 0],\n",
    "            \"b\":    [0, 1, 0, 0, 0],\n",
    "            \"c\":    [0, 0, 1, 0, 0],\n",
    "            \"d\":    [0, 0, 0, 1, 0],\n",
    "            \"e\":    [0, 0, 0, 0, 1],\n",
    "        },\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd2e34a",
   "metadata": {
    "papermill": {
     "duration": 0.00438,
     "end_time": "2025-01-20T09:55:53.504293",
     "exception": false,
     "start_time": "2025-01-20T09:55:53.499913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "pre process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f371e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T09:55:53.515002Z",
     "iopub.status.busy": "2025-01-20T09:55:53.514616Z",
     "iopub.status.idle": "2025-01-20T09:55:54.059580Z",
     "shell.execute_reply": "2025-01-20T09:55:54.058160Z"
    },
    "papermill": {
     "duration": 0.552878,
     "end_time": "2025-01-20T09:55:54.062010",
     "exception": false,
     "start_time": "2025-01-20T09:55:53.509132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          preprocess.py\n",
    "[description]   preprocess WiFi CSI data\n",
    "\"\"\"\n",
    "#\n",
    "##\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "\n",
    "#\n",
    "##\n",
    "def mat_to_amp(data_mat):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : calculate amplitude of raw WiFi CSI data\n",
    "    [parameter]\n",
    "    : data_mat: dict, raw WiFi CSI data from *.mat files\n",
    "    [return]\n",
    "    : data_csi_amp: numpy array, CSI amplitude\n",
    "    \"\"\"\n",
    "    #\n",
    "    ## \n",
    "    var_length = data_mat[\"trace\"].shape[0]\n",
    "    #\n",
    "    data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "    #\n",
    "    data_csi_amp = np.array(data_csi_amp, dtype = np.float32)\n",
    "    #\n",
    "    return data_csi_amp\n",
    "\n",
    "#\n",
    "##\n",
    "def extract_csi_amp(var_dir_mat, \n",
    "                    var_dir_amp):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : read raw WiFi CSI files (*.mat), calcuate CSI amplitude, and save amplitude (*.npy)\n",
    "    [parameter]\n",
    "    : var_dir_mat: string, directory to read raw WiFi CSI files (*.mat)\n",
    "    : var_dir_amp: string, directory to save WiFi CSI amplitude (*.npy)\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    var_path_mat = os.listdir(var_dir_mat)\n",
    "    #\n",
    "    for var_c, var_path in enumerate(var_path_mat):\n",
    "        #\n",
    "        data_mat = scio.loadmat(os.path.join(var_dir_mat, var_path))\n",
    "        #\n",
    "        data_csi_amp = mat_to_amp(data_mat)\n",
    "        #\n",
    "        \n",
    "        #\n",
    "        var_path_save = os.path.join(var_dir_amp, var_path.replace(\".mat\", \".npy\"))\n",
    "        #\n",
    "        with open(var_path_save, \"wb\") as var_file:\n",
    "            np.save(var_file, data_csi_amp)\n",
    "\n",
    "#\n",
    "##\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : parse arguments from input\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    var_args = argparse.ArgumentParser()\n",
    "    #\n",
    "    var_args.add_argument(\"--dir_mat\", default = \"/kaggle/input/wimans/wifi_csi/mat\", type = str)\n",
    "    var_args.add_argument(\"--dir_amp\", default = \"/kaggle/input/wimans/wifi_csi/amp\", type = str)\n",
    "    #\n",
    "    return var_args.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721052a2",
   "metadata": {
    "papermill": {
     "duration": 0.004674,
     "end_time": "2025-01-20T09:55:54.071806",
     "exception": false,
     "start_time": "2025-01-20T09:55:54.067132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cfe57cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T09:55:54.083076Z",
     "iopub.status.busy": "2025-01-20T09:55:54.082465Z",
     "iopub.status.idle": "2025-01-20T09:55:55.190732Z",
     "shell.execute_reply": "2025-01-20T09:55:55.189473Z"
    },
    "papermill": {
     "duration": 1.116576,
     "end_time": "2025-01-20T09:55:55.192972",
     "exception": false,
     "start_time": "2025-01-20T09:55:54.076396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          load_data.py\n",
    "[description]   load annotation file and CSI amplitude, and encode labels\n",
    "\"\"\"\n",
    "#\n",
    "##\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#\n",
    "\n",
    "\n",
    "#\n",
    "##\n",
    "def load_data_y(var_path_data_y,\n",
    "                var_environment = None, \n",
    "                var_wifi_band = None, \n",
    "                var_num_users = None):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : load annotation file (*.csv) as a pandas dataframe\n",
    "    : according to selected environment(s), WiFi band(s), and number(s) of users\n",
    "    [parameter]\n",
    "    : var_path_data_y: string, path of annotation file\n",
    "    : var_environment: list, selected environment(s), e.g., [\"classroom\"]\n",
    "    : var_wifi_band: list, selected WiFi band(s), e.g., [\"2.4\"]\n",
    "    : var_num_users: list, selected number(s) of users, e.g., [\"0\", \"1\", \"2\"]\n",
    "    [return]\n",
    "    : data_pd_y: pandas dataframe, labels of selected data\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_pd_y = pd.read_csv(var_path_data_y, dtype = str)\n",
    "    #\n",
    "    if var_environment is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "    #\n",
    "    if var_wifi_band is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "    #\n",
    "    if var_num_users is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "    #\n",
    "    return data_pd_y\n",
    "\n",
    "#\n",
    "##\n",
    "def load_data_x(var_path_data_x, \n",
    "                var_label_list):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : load CSI amplitude (*.npy)\n",
    "    : according to a label list of selected data\n",
    "    [parameter]\n",
    "    : var_path_data_x: string, directory of CSI amplitude files\n",
    "    : var_label_list: list, selected labels\n",
    "    [return]\n",
    "    : data_x: numpy array, CSI amplitude\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "    #\n",
    "    data_x = []\n",
    "    #\n",
    "    for var_path in var_path_list:\n",
    "        #\n",
    "        data_csi = np.load(var_path)\n",
    "        #\n",
    "        var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "        #\n",
    "        data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "        #\n",
    "        data_x.append(data_csi_pad)\n",
    "    #\n",
    "    data_x = np.array(data_x)\n",
    "    #\n",
    "    return data_x\n",
    "\n",
    "#\n",
    "##\n",
    "def encode_data_y(data_pd_y, \n",
    "                  var_task):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : encode labels according to specific task\n",
    "    [parameter]\n",
    "    : data_pd_y: pandas dataframe, labels of different tasks\n",
    "    : var_task: string, indicate task\n",
    "    [return]\n",
    "    : data_y: numpy array, label encoding of task\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    if var_task == \"identity\":\n",
    "        #\n",
    "        data_y = encode_identity(data_pd_y)\n",
    "    #\n",
    "    elif var_task == \"activity\":\n",
    "        #\n",
    "        data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "    #\n",
    "    elif var_task == \"location\":\n",
    "        #\n",
    "        data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "    #\n",
    "    return data_y\n",
    "\n",
    "#\n",
    "##\n",
    "def encode_identity(data_pd_y):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : encode identity labels in a pandas dataframe\n",
    "    [parameter]\n",
    "    : data_pd_y: pandas dataframe, labels of different tasks\n",
    "    [return]\n",
    "    : data_identity_onehot_y: numpy array, onehot encoding for identity labels\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "                                    \"user_3_location\", \"user_4_location\", \n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    # \n",
    "    data_identity_y = data_location_pd_y.to_numpy(copy = True).astype(str)\n",
    "    #\n",
    "    data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "    data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "    #\n",
    "    data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "    #\n",
    "    return data_identity_onehot_y\n",
    "\n",
    "#\n",
    "##\n",
    "def encode_activity(data_pd_y, \n",
    "                    var_encoding):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : encode activity labels in a pandas dataframe\n",
    "    [parameter]\n",
    "    : data_pd_y: pandas dataframe, labels of different tasks\n",
    "    : var_encoding: dict, encoding of different activities\n",
    "    [return]\n",
    "    : data_activity_onehot_y: numpy array, onehot encoding for activity labels\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "                                    \"user_3_activity\", \"user_4_activity\", \n",
    "                                    \"user_5_activity\", \"user_6_activity\"]]\n",
    "    #\n",
    "    data_activity_y = data_activity_pd_y.to_numpy(copy = True).astype(str)\n",
    "    #\n",
    "    data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "    #\n",
    "    return data_activity_onehot_y\n",
    "\n",
    "#\n",
    "##\n",
    "def encode_location(data_pd_y, \n",
    "                    var_encoding):\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : encode location labels in a pandas dataframe\n",
    "    [parameter]\n",
    "    : data_pd_y: pandas dataframe, labels of different tasks\n",
    "    : var_encoding: dict, encoding of different locations\n",
    "    [return]\n",
    "    : data_location_onehot_y: numpy array, onehot encoding for location labels\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "                                    \"user_3_location\", \"user_4_location\", \n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    #\n",
    "    data_location_y = data_location_pd_y.to_numpy(copy = True).astype(str)\n",
    "    #\n",
    "    data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "    #\n",
    "    return data_location_onehot_y\n",
    "\n",
    "#\n",
    "##\n",
    "def test_load_data_y():\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : test load_data_y() function\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "\n",
    "\n",
    "#\n",
    "##\n",
    "def test_load_data_x():\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : test load_data_x() function\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_pd_y = load_data_y(preset[\"path\"][\"data_y\"],\n",
    "                            var_environment = [\"meeting_room\"], \n",
    "                            var_wifi_band = [\"2.4\"], \n",
    "                            var_num_users = None)\n",
    "    #\n",
    "    var_label_list = data_pd_y[\"label\"].to_list()\n",
    "    #\n",
    "    data_x = load_data_x(preset[\"path\"][\"data_x\"], var_label_list)\n",
    "    #\n",
    "  \n",
    "\n",
    "#\n",
    "##\n",
    "def test_encode_identity():\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : test encode_identity() function\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype = str)\n",
    "    #\n",
    "    data_identity_onehot_y = encode_identity(data_pd_y)\n",
    "    #\n",
    "\n",
    "\n",
    "#\n",
    "##\n",
    "def test_encode_activity():\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : test encode_activity() function\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype = str)\n",
    "    #\n",
    "    data_activity_onehot_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "    #\n",
    "\n",
    "#\n",
    "##\n",
    "def test_encode_location():\n",
    "    \"\"\"\n",
    "    [description]\n",
    "    : test encode_location() function\n",
    "    \"\"\"\n",
    "    #\n",
    "    ##\n",
    "    data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype = str)\n",
    "    #\n",
    "    data_location_onehot_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "    #\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df7b998",
   "metadata": {
    "papermill": {
     "duration": 0.004051,
     "end_time": "2025-01-20T09:55:55.201877",
     "exception": false,
     "start_time": "2025-01-20T09:55:55.197826",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b62ce9f7",
   "metadata": {
    "papermill": {
     "duration": 0.00399,
     "end_time": "2025-01-20T09:55:55.210109",
     "exception": false,
     "start_time": "2025-01-20T09:55:55.206119",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4bca4eb",
   "metadata": {
    "papermill": {
     "duration": 0.003889,
     "end_time": "2025-01-20T09:55:55.218327",
     "exception": false,
     "start_time": "2025-01-20T09:55:55.214438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#model lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5a4ce9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T09:55:55.228578Z",
     "iopub.status.busy": "2025-01-20T09:55:55.228009Z",
     "iopub.status.idle": "2025-01-20T09:56:01.601524Z",
     "shell.execute_reply": "2025-01-20T09:56:01.599965Z"
    },
    "papermill": {
     "duration": 6.381623,
     "end_time": "2025-01-20T09:56:01.604256",
     "exception": false,
     "start_time": "2025-01-20T09:55:55.222633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ptflops\r\n",
      "  Downloading ptflops-0.7.4-py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from ptflops) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (2024.9.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0->ptflops) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\r\n",
      "Downloading ptflops-0.7.4-py3-none-any.whl (19 kB)\r\n",
      "Installing collected packages: ptflops\r\n",
      "Successfully installed ptflops-0.7.4\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ptflops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1539053",
   "metadata": {
    "papermill": {
     "duration": 0.004505,
     "end_time": "2025-01-20T09:56:01.614145",
     "exception": false,
     "start_time": "2025-01-20T09:56:01.609640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d82ebfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T09:56:01.626580Z",
     "iopub.status.busy": "2025-01-20T09:56:01.626025Z",
     "iopub.status.idle": "2025-01-20T09:57:46.823834Z",
     "shell.execute_reply": "2025-01-20T09:57:46.821810Z"
    },
    "papermill": {
     "duration": 105.209865,
     "end_time": "2025-01-20T09:57:46.828848",
     "exception": false,
     "start_time": "2025-01-20T09:56:01.618983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          run.py\n",
    "[description]   Run WiFi-based models\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "var_task = \"location\"\n",
    "var_model = \"LSTM\"\n",
    "var_repeat = 2\n",
    "\n",
    "# Load annotation file as labels\n",
    "data_pd_y = load_data_y(\n",
    "    preset[\"path\"][\"data_y\"],\n",
    "    var_environment=preset[\"data\"][\"environment\"],\n",
    "    var_wifi_band=preset[\"data\"][\"wifi_band\"],\n",
    "    var_num_users=preset[\"data\"][\"num_users\"]\n",
    ")\n",
    "var_label_list = data_pd_y[\"label\"].to_list()\n",
    "data_x = load_data_x(preset[\"path\"][\"data_x\"], var_label_list)\n",
    "data_y = encode_data_y(data_pd_y, var_task)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba1fbb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T09:57:46.844367Z",
     "iopub.status.busy": "2025-01-20T09:57:46.843872Z",
     "iopub.status.idle": "2025-01-20T09:57:50.258933Z",
     "shell.execute_reply": "2025-01-20T09:57:50.256954Z"
    },
    "papermill": {
     "duration": 3.424966,
     "end_time": "2025-01-20T09:57:50.261338",
     "exception": false,
     "start_time": "2025-01-20T09:57:46.836372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1881, 3000, 3, 3, 30)\n",
      "(1881, 3000, 270)\n",
      "(1881, 6, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "shapee = data_x.shape\n",
    "print(shapee)\n",
    "data_x_1 = np.resize(data_x, (shapee[0], 3000, 270)) \n",
    "print(data_x_1.shape)\n",
    "\n",
    "print(data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7444590a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T09:57:50.273825Z",
     "iopub.status.busy": "2025-01-20T09:57:50.273427Z",
     "iopub.status.idle": "2025-01-20T09:57:50.286952Z",
     "shell.execute_reply": "2025-01-20T09:57:50.285591Z"
    },
    "papermill": {
     "duration": 0.021985,
     "end_time": "2025-01-20T09:57:50.288986",
     "exception": false,
     "start_time": "2025-01-20T09:57:50.267001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# تعریف مدل LSTM\n",
    "class LSTMM(torch.nn.Module):\n",
    "    def __init__(self, var_x_shape, var_y_shape):\n",
    "        super(LSTMM, self).__init__()\n",
    "        var_dim_input = var_x_shape[-1]  # باید 270 باشد\n",
    "        var_dim_output = var_y_shape[-1]  # باید 30 باشد\n",
    "        self.layer_norm = torch.nn.BatchNorm1d(var_dim_input)\n",
    "        self.layer_pooling = torch.nn.AvgPool1d(kernel_size=10, stride=10)\n",
    "        self.layer_lstm = torch.nn.LSTM(\n",
    "            input_size=var_dim_input,\n",
    "            hidden_size=512,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.layer_linear = torch.nn.Linear(512, var_dim_output)\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        # شکل ورودی: (batch, time, features)\n",
    "        var_t = torch.permute(var_input, (0, 2, 1))  # (batch, features, time)\n",
    "        var_t = self.layer_norm(var_t)\n",
    "        var_t = self.layer_pooling(var_t)  # (batch, features, pooled_time)\n",
    "        var_t = torch.permute(var_t, (0, 2, 1))  # (batch, pooled_time, features)\n",
    "        var_t, _ = self.layer_lstm(var_t)  # (batch, pooled_time, hidden)\n",
    "        var_t = var_t[:, -1, :]  # (batch, hidden)\n",
    "        var_output = self.layer_linear(var_t)  # (batch, var_dim_output)\n",
    "        return var_output\n",
    "\n",
    "# تعریف مدل ResNet\n",
    "def build_resnet(var_y_shape):\n",
    "    model_resnet = models.resnet18(weights=None)  # استفاده از 'weights' به جای 'pretrained'\n",
    "\n",
    "    # تغییر لایه کانولوشن اول برای پذیرش ورودی با 1 کانال\n",
    "    model_resnet.conv1 = torch.nn.Conv2d(\n",
    "        in_channels=1,\n",
    "        out_channels=64,\n",
    "        kernel_size=7,\n",
    "        stride=2,\n",
    "        padding=3,\n",
    "        bias=False\n",
    "    )\n",
    "\n",
    "    # تغییر لایه Fully Connected نهایی برای خروجی تعداد کلاس‌های مورد نظر\n",
    "    in_features_fc = model_resnet.fc.in_features  # معمولاً 512\n",
    "    out_features_fc = var_y_shape[-1]  # 30\n",
    "    model_resnet.fc = torch.nn.Linear(in_features_fc, out_features_fc)\n",
    "\n",
    "    return model_resnet\n",
    "\n",
    "# تابع برای پاکسازی پیشوند 'module.' یا '_orig_mod.'\n",
    "def remove_module_prefix(state_dict):\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            new_key = k[len('module.'):]\n",
    "        elif k.startswith('_orig_mod.'):\n",
    "            new_key = k[len('_orig_mod.'):]\n",
    "        else:\n",
    "            new_key = k\n",
    "        new_state_dict[new_key] = v\n",
    "    return new_state_dict\n",
    "\n",
    "# تابع برای محاسبه دقت به صورت دسته‌ای\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13b87a97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T09:57:50.300980Z",
     "iopub.status.busy": "2025-01-20T09:57:50.300433Z",
     "iopub.status.idle": "2025-01-20T10:32:01.316435Z",
     "shell.execute_reply": "2025-01-20T10:32:01.313579Z"
    },
    "papermill": {
     "duration": 2051.033637,
     "end_time": "2025-01-20T10:32:01.327609",
     "exception": false,
     "start_time": "2025-01-20T09:57:50.293972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-2f46404e3c2d>:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"/kaggle/input/models-v1/lstm_cr.pth\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm acc\n",
      "LSTM Accuracy  : 0.5049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-2f46404e3c2d>:178: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint1 = torch.load(\"/kaggle/input/models-v1/resnet_cr.pth\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm acc\n",
      "ResNet Accuracy: 0.3898\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_accuracy_batch(model, dataloader, device, var_threshold=0.5):\n",
    "    \"\"\"\n",
    "    محاسبه دقت مدل به صورت دسته‌ای بدون ذخیره‌سازی تمام پیش‌بینی‌ها و برچسب‌ها.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): مدل PyTorch برای ارزیابی.\n",
    "        dataloader (DataLoader): DataLoader برای داده‌ها.\n",
    "        device (torch.device): دستگاه (GPU یا CPU) برای اجرای مدل.\n",
    "        var_threshold (float): آستانه برای تبدیل احتمالات به مقادیر باینری.\n",
    "\n",
    "    Returns:\n",
    "        float: دقت مدل بر روی داده‌های ارزیابی.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sum = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # پیش‌بینی مدل\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # اعمال سیگموید برای تبدیل خروجی‌ها به احتمالات\n",
    "            preds = torch.sigmoid(outputs)\n",
    "     \n",
    "            # اعمال آستانه برای تبدیل احتمالات به مقادیر باینری\n",
    "            preds = (preds > var_threshold).float()\n",
    "    \n",
    "            predict_test_y = preds.detach().cpu().numpy()\n",
    "            data_test_y = labels.detach().cpu().numpy()\n",
    "            \n",
    "            # محاسبه تعداد نمونه‌های درست پیش‌بینی شده\n",
    "            # در اینجا فرض بر این است که دقت به صورت تطابق کامل برچسب‌ها محاسبه می‌شود\n",
    "\n",
    "            predict_test_y = predict_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            data_test_y = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            var_accuracy_test = accuracy_score(data_test_y.astype(int), predict_test_y.astype(int))\n",
    "            sum = var_accuracy_test + sum\n",
    "        ave = sum/len(dataloader)\n",
    "    return ave\n",
    "\n",
    "\n",
    "\n",
    "var_x = torch.from_numpy(data_x_1).float()\n",
    "var_y = torch.from_numpy(data_y).long()\n",
    "# تعیین دستگاه (GPU در صورت موجود بودن)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# انتقال تنسورها به دستگاه انتخاب شده\n",
    "var_x = var_x.to(device)\n",
    "var_y = var_y.to(device)\n",
    "\n",
    "var_x_shape = var_x.shape\n",
    "var_y_shape = var_y.shape\n",
    "\n",
    "\n",
    "batch_size = 8  # در صورت نیاز می‌توانید کاهش دهید (مثلاً 32 یا 16)\n",
    "dataset_lstm = TensorDataset(var_x, var_y)\n",
    "dataloader_lstm = DataLoader(dataset_lstm, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "checkpoint = torch.load(\"/kaggle/input/models-v1/lstm_cr.pth\", map_location=device)\n",
    "\n",
    "checkpoint = remove_module_prefix(checkpoint['best_weight'])\n",
    "\n",
    "model_lstm = LSTMM(var_x_shape, (var_y_shape[0],30)).to(device)\n",
    "model_lstm.load_state_dict(checkpoint)  \n",
    "model_lstm.eval()\n",
    "\n",
    "\n",
    "print('lstm acc')\n",
    "# محاسبه دقت برای مدل LSTM\n",
    "accuracy_lstm = compute_accuracy_batch(model_lstm, dataloader_lstm, device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"LSTM Accuracy  : {accuracy_lstm:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_accuracy_batch(model, dataloader, device, var_threshold=0.5):\n",
    "    \"\"\"\n",
    "    محاسبه دقت مدل به صورت دسته‌ای بدون ذخیره‌سازی تمام پیش‌بینی‌ها و برچسب‌ها.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): مدل PyTorch برای ارزیابی.\n",
    "        dataloader (DataLoader): DataLoader برای داده‌ها.\n",
    "        device (torch.device): دستگاه (GPU یا CPU) برای اجرای مدل.\n",
    "        var_threshold (float): آستانه برای تبدیل احتمالات به مقادیر باینری.\n",
    "\n",
    "    Returns:\n",
    "        float: دقت مدل بر روی داده‌های ارزیابی.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sum = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # پیش‌بینی مدل\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # اعمال سیگموید برای تبدیل خروجی‌ها به احتمالات\n",
    "            preds = torch.sigmoid(outputs)\n",
    "     \n",
    "            # اعمال آستانه برای تبدیل احتمالات به مقادیر باینری\n",
    "            preds = (preds > var_threshold).float()\n",
    "    \n",
    "            predict_test_y = preds.detach().cpu().numpy()\n",
    "            data_test_y = labels.detach().cpu().numpy()\n",
    "            \n",
    "            # محاسبه تعداد نمونه‌های درست پیش‌بینی شده\n",
    "            # در اینجا فرض بر این است که دقت به صورت تطابق کامل برچسب‌ها محاسبه می‌شود\n",
    "\n",
    "            predict_test_y = predict_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            data_test_y = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            var_accuracy_test = accuracy_score(data_test_y.astype(int), predict_test_y.astype(int))\n",
    "            sum = var_accuracy_test + sum\n",
    "        ave = sum/len(dataloader)\n",
    "    return ave\n",
    "\n",
    "\n",
    "    \n",
    "# تبدیل داده‌های ورودی از آرایه‌های NumPy به تنسورهای PyTorch\n",
    "# فرض بر این است که data_x_1 و data_y آرایه‌های NumPy هستند\n",
    "var_x = torch.from_numpy(data_x_1).float()\n",
    "var_y = torch.from_numpy(data_y).long()\n",
    "# تعیین دستگاه (GPU در صورت موجود بودن)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# انتقال تنسورها به دستگاه انتخاب شده\n",
    "var_x = var_x.to(device)\n",
    "var_y = var_y.to(device)\n",
    "\n",
    "var_x_shape = var_x.shape\n",
    "var_y_shape = var_y.shape\n",
    "\n",
    "\n",
    "batch_size = 8  # در صورت نیاز می‌توانید کاهش دهید (مثلاً 32 یا 16)\n",
    "\n",
    "\n",
    "var_x_resnet = var_x.unsqueeze(1)  # افزودن بعد کانال\n",
    "dataset_resnet = TensorDataset(var_x_resnet, var_y)\n",
    "dataloader_resnet = DataLoader(dataset_resnet, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "checkpoint1 = torch.load(\"/kaggle/input/models-v1/resnet_cr.pth\", map_location=device)\n",
    "\n",
    "\n",
    "checkpoint1 = remove_module_prefix(checkpoint1)\n",
    "\n",
    "\n",
    "# # بارگذاری مدل ResNet\n",
    "model_resnet = build_resnet((var_y_shape[0],30)).to(device)\n",
    "model_resnet.load_state_dict(checkpoint1)\n",
    "model_resnet.eval()\n",
    "print('lstm acc')\n",
    "\n",
    "\n",
    "# print('resnet acc')\n",
    "accuracy_resnet = compute_accuracy_batch(model_resnet, dataloader_resnet, device,)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(f\"ResNet Accuracy: {accuracy_resnet:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c318cf",
   "metadata": {
    "papermill": {
     "duration": 0.005565,
     "end_time": "2025-01-20T10:32:01.339583",
     "exception": false,
     "start_time": "2025-01-20T10:32:01.334018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5527cd38",
   "metadata": {
    "papermill": {
     "duration": 0.00536,
     "end_time": "2025-01-20T10:32:01.350953",
     "exception": false,
     "start_time": "2025-01-20T10:32:01.345593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4451316,
     "sourceId": 7638081,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6493733,
     "sourceId": 10488113,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6512794,
     "sourceId": 10523099,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2174.229816,
   "end_time": "2025-01-20T10:32:04.627418",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-20T09:55:50.397602",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
