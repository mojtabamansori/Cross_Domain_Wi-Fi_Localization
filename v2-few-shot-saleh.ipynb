{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2bab4d",
   "metadata": {
    "papermill": {
     "duration": 0.008505,
     "end_time": "2025-12-28T20:24:52.124377",
     "exception": false,
     "start_time": "2025-12-28T20:24:52.115872",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 1: Library Imports\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0096ac0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:24:52.139744Z",
     "iopub.status.busy": "2025-12-28T20:24:52.139489Z",
     "iopub.status.idle": "2025-12-28T20:25:02.153453Z",
     "shell.execute_reply": "2025-12-28T20:25:02.152650Z"
    },
    "papermill": {
     "duration": 10.023289,
     "end_time": "2025-12-28T20:25:02.154936",
     "exception": false,
     "start_time": "2025-12-28T20:24:52.131647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 1: Library Imports\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as scio\n",
    "import time\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch._dynamo\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# from ptflops import get_model_complexity_info\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from copy import deepcopy\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e96bd3",
   "metadata": {
    "papermill": {
     "duration": 0.006747,
     "end_time": "2025-12-28T20:25:02.169143",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.162396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 2: preset.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afca6679",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.184116Z",
     "iopub.status.busy": "2025-12-28T20:25:02.183411Z",
     "iopub.status.idle": "2025-12-28T20:25:02.192430Z",
     "shell.execute_reply": "2025-12-28T20:25:02.191709Z"
    },
    "papermill": {
     "duration": 0.01763,
     "end_time": "2025-12-28T20:25:02.193477",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.175847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "few=empty_room,100,5,m=THAT,t=activity,epoch=300,batch=64,environment=['classroom']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "[file]          preset.py\n",
    "[description]   default settings of WiFi-based models\n",
    "\"\"\"\n",
    "minidata_set = 1\n",
    "preset = {\n",
    "    # define model\n",
    "    \"model\": \"THAT\",  # \"ST-RF\", \"MLP\", \"LSTM\", \"CNN-1D\", \"CNN-2D\", \"CLSTM\", \"ABLSTM\", \"THAT\", \"bi-LSTM\", \"ResNet18\"\n",
    "    # define task\n",
    "    \"task\": \"activity\",  # \"identity\", \"activity\", \"location\", \"count\"\n",
    "    # number of repeated experiments\n",
    "    \"repeat\": 1,\n",
    "    # path of data\n",
    "    \"path\": {\n",
    "        # \"data_x\": \"/kaggle/input/wimans/wifi_csi/mat\",   # directory of CSI amplitude files \n",
    "        \"data_x\": \"/kaggle/input/wimans/wifi_csi/amp\",   # directory of CSI amplitude files \n",
    "        \"data_y\": \"/kaggle/input/wimans/annotation.csv\", # path of annotation file\n",
    "        \"save\": \"result_lstm_epoch=80_batchsize=32_envs=empty_room_wifiband=2.4.json\"               # path to save results\n",
    "    },\n",
    "    # data selection for experiments\n",
    "    \"data\": {\n",
    "        \"num_users\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"],  # select number(s) of users\n",
    "        \"wifi_band\": [\"2.4\"],                         # select WiFi band(s)\n",
    "        \"environment\": [\"classroom\"],                 # select environment(s) [\"classroom\"], [\"meeting_room\"], [\"empty_room\"]\n",
    "        \"length\": 3000,                               # default length of CSI\n",
    "    },\n",
    "    # hyperparameters of models\n",
    "    \"nn\": {\n",
    "        \"lr\": 1e-3,           # learning rate\n",
    "        \"epoch\": 300,         # number of epochs\n",
    "        \"batch_size\": 64,    # batch size\n",
    "        \"threshold\": 0.5,     # threshold to binarize sigmoid outputs\n",
    "    },\n",
    "    # encoding of activities and locations\n",
    "    \"encoding\": {\n",
    "        \"activity\": {  # encoding of different activities\n",
    "            \"nan\":      [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"nothing\":  [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"walk\":     [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"rotation\": [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            \"jump\":     [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            \"wave\":     [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "            \"lie_down\": [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "            \"pick_up\":  [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "            \"sit_down\": [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "            \"stand_up\": [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        },\n",
    "        \"location\": {  # encoding of different locations\n",
    "            \"nan\":  [0, 0, 0, 0, 0],\n",
    "            \"a\":    [1, 0, 0, 0, 0],\n",
    "            \"b\":    [0, 1, 0, 0, 0],\n",
    "            \"c\":    [0, 0, 1, 0, 0],\n",
    "            \"d\":    [0, 0, 0, 1, 0],\n",
    "            \"e\":    [0, 0, 0, 0, 1],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Few-shot parameters (manually set)\n",
    "dest_env = \"empty_room\"       # Destination environment[\"classroom\"], [\"meeting_room\"], [\"empty_room\"]\n",
    "few_shot_epochs = 100         # Number of epochs for few-shot training\n",
    "few_shot_num_samples = 5     # Number of samples to use from the destination test data\n",
    "\n",
    "Confusion_matrix = 1\n",
    "\n",
    "name_run = \"few={},{},{},m={},t={},epoch={},batch={},environment={}\".format(dest_env, few_shot_epochs, few_shot_num_samples, preset[\"model\"], preset[\"task\"], preset[\"nn\"][\"epoch\"], preset[\"nn\"][\"batch_size\"], preset[\"data\"][\"environment\"])\n",
    "print(name_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113f7f3",
   "metadata": {
    "papermill": {
     "duration": 0.006569,
     "end_time": "2025-12-28T20:25:02.206973",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.200404",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 3: load_data.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9cf8e8",
   "metadata": {
    "papermill": {
     "duration": 0.00659,
     "end_time": "2025-12-28T20:25:02.220231",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.213641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "raw + sparse\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8542367e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.235028Z",
     "iopub.status.busy": "2025-12-28T20:25:02.234786Z",
     "iopub.status.idle": "2025-12-28T20:25:02.260702Z",
     "shell.execute_reply": "2025-12-28T20:25:02.260160Z"
    },
    "papermill": {
     "duration": 0.034846,
     "end_time": "2025-12-28T20:25:02.261733",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.226887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 3: load_data.py  (AMP + RPCA sparse, and RAW + alpha*SPARSE)\n",
    "# =========================\n",
    "\"\"\"\n",
    "[file]          load_data.py\n",
    "[description]   load annotation file and CSI amplitude, and encode labels\n",
    "                (Test mode) X_input = X_raw + alpha * X_sparse\n",
    "                where X_sparse is RPCA sparse component computed per (Tx,Rx) link\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd  # Cell1 already imports, but safe here too\n",
    "\n",
    "# =========================================================\n",
    "# NEW TEST SETTINGS\n",
    "# =========================================================\n",
    "# \"raw\"            : use amplitude as-is\n",
    "# \"sparse\"         : use RPCA sparse component only\n",
    "# \"raw_plus_sparse\": raw + alpha*sparse  (your requested test)\n",
    "CSI_INPUT_MODE = \"raw_plus_sparse\"   # <-- \"raw\" / \"sparse\" / \"raw_plus_sparse\"\n",
    "\n",
    "# weight of sparse when adding to raw\n",
    "SPARSE_ALPHA = 1.0\n",
    "\n",
    "# keep amplitudes non-negative after addition (recommended)\n",
    "CLAMP_NONNEG = True\n",
    "\n",
    "# RPCA settings (IALM)\n",
    "RPCA_MAX_ITER = 60\n",
    "RPCA_TOL      = 1e-5\n",
    "RPCA_RHO      = 1.5\n",
    "RPCA_MU_INIT  = None\n",
    "\n",
    "# lambda options you asked before\n",
    "# \"classic\": 1/sqrt(max(m,n))\n",
    "# \"median\" : 1/sqrt(max(m,n)) * median(abs(M))\n",
    "# \"scaled\" : 1.2/sqrt(max(m,n))\n",
    "RPCA_LAMBDA_MODE = \"median\"   # <-- \"classic\" / \"median\" / \"scaled\"\n",
    "\n",
    "# cache for speed\n",
    "CACHE_ENABLED = True\n",
    "CACHE_ROOT = \"/kaggle/working/csi_cache_amp_raw_plus_sparse\"\n",
    "\n",
    "# debug print once to confirm amplitude/phase\n",
    "_DEBUG_PRINT_ONCE = True\n",
    "\n",
    "# expected antenna/subcarrier dims (WiMANS typical)\n",
    "TX = 3\n",
    "RX = 3\n",
    "SC = 30\n",
    "# =========================================================\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RPCA helpers (IALM)\n",
    "# =========================\n",
    "def _compute_lambda(M: np.ndarray) -> float:\n",
    "    m, n = M.shape\n",
    "    base = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "    if RPCA_LAMBDA_MODE == \"classic\":\n",
    "        return base\n",
    "\n",
    "    if RPCA_LAMBDA_MODE == \"scaled\":\n",
    "        return 1.2 * base\n",
    "\n",
    "    if RPCA_LAMBDA_MODE == \"median\":\n",
    "        med = np.median(np.abs(M))\n",
    "        med = float(med) if med > 1e-12 else 1e-12\n",
    "        return base * med\n",
    "\n",
    "    raise ValueError(f\"Unknown RPCA_LAMBDA_MODE: {RPCA_LAMBDA_MODE}\")\n",
    "\n",
    "\n",
    "def _soft_threshold(X: np.ndarray, tau: float) -> np.ndarray:\n",
    "    return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "\n",
    "def _svt(X: np.ndarray, tau: float) -> np.ndarray:\n",
    "    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    s_thr = np.maximum(s - tau, 0.0)\n",
    "    if np.all(s_thr == 0):\n",
    "        return np.zeros_like(X)\n",
    "    return (U * s_thr) @ Vt\n",
    "\n",
    "\n",
    "def _rpca_ialm(M: np.ndarray,\n",
    "              max_iter: int = 60,\n",
    "              tol: float = 1e-5,\n",
    "              rho: float = 1.5,\n",
    "              mu: float | None = None):\n",
    "    \"\"\"\n",
    "    Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "    Decompose: M = L + S\n",
    "    Returns: L, S (float32)\n",
    "    \"\"\"\n",
    "    M = M.astype(np.float64, copy=False)\n",
    "    lam = _compute_lambda(M)\n",
    "\n",
    "    if mu is None:\n",
    "        # spectral norm approx via top singular value\n",
    "        s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "        mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "    L = np.zeros_like(M)\n",
    "    S = np.zeros_like(M)\n",
    "    Y = np.zeros_like(M)\n",
    "\n",
    "    normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "        S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "        R = M - L - S\n",
    "        Y = Y + mu * R\n",
    "\n",
    "        err = np.linalg.norm(R, ord=\"fro\") / normM\n",
    "        if err < tol:\n",
    "            break\n",
    "\n",
    "        mu *= rho\n",
    "\n",
    "    return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Data shape helpers\n",
    "# =========================\n",
    "def _ensure_shape_4d_amp(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Expect amp npy to be either:\n",
    "      - (T, 3, 3, 30)\n",
    "      - (T, 270)  -> reshape to (T,3,3,30)\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim == 4:\n",
    "        return x\n",
    "    if x.ndim == 2 and x.shape[1] == TX * RX * SC:\n",
    "        T = x.shape[0]\n",
    "        return x.reshape(T, TX, RX, SC)\n",
    "    raise ValueError(f\"Unexpected AMP shape: {x.shape} (expected (T,{TX},{RX},{SC}) or (T,{TX*RX*SC}))\")\n",
    "\n",
    "\n",
    "def _debug_print_once(label: str, arr: np.ndarray):\n",
    "    global _DEBUG_PRINT_ONCE\n",
    "    if not _DEBUG_PRINT_ONCE:\n",
    "        return\n",
    "    _DEBUG_PRINT_ONCE = False\n",
    "    print(f\"\\n[DEBUG] First loaded AMP sample: {label}\")\n",
    "    print(f\"[DEBUG] shape={arr.shape}, dtype={arr.dtype}, complex={np.iscomplexobj(arr)}\")\n",
    "    if np.iscomplexobj(arr):\n",
    "        print(\"[DEBUG] ==> WARNING: This looks complex; amp folder usually should be real amplitude.\")\n",
    "    else:\n",
    "        print(\"[DEBUG] ==> Input is REAL -> amplitude-only (no true phase).\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def _cache_path(label: str, mode: str) -> str:\n",
    "    return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "def _make_sparse_component_pairwise(x4: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    x4: (T,3,3,30) real amplitude\n",
    "    Run RPCA on each (Tx,Rx) link separately:\n",
    "      M = (T,30) for each link, compute S, place back.\n",
    "    Return: S_out same shape as x4 (float32)\n",
    "    \"\"\"\n",
    "    x4 = x4.astype(np.float32, copy=False)\n",
    "    T = x4.shape[0]\n",
    "    S_out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "    for tx in range(TX):\n",
    "        for rx in range(RX):\n",
    "            M = x4[:, tx, rx, :]  # (T,30)\n",
    "            _, S = _rpca_ialm(\n",
    "                M,\n",
    "                max_iter=RPCA_MAX_ITER,\n",
    "                tol=RPCA_TOL,\n",
    "                rho=RPCA_RHO,\n",
    "                mu=RPCA_MU_INIT\n",
    "            )\n",
    "            S_out[:, tx, rx, :] = S\n",
    "\n",
    "    return S_out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Public API (used by run.py)\n",
    "# =========================\n",
    "def load_data_y(var_path_data_y,\n",
    "                var_environment=None,\n",
    "                var_wifi_band=None,\n",
    "                var_num_users=None):\n",
    "    \"\"\"\n",
    "    Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "    \"\"\"\n",
    "    data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "    if var_environment is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "    if var_wifi_band is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "    if var_num_users is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "    return data_pd_y\n",
    "\n",
    "\n",
    "def load_data_x(var_path_data_x, var_label_list):\n",
    "    \"\"\"\n",
    "    Load CSI amplitude (*.npy) files based on a label list.\n",
    "\n",
    "    Modes:\n",
    "      - raw:            x_out = x_raw\n",
    "      - sparse:         x_out = S\n",
    "      - raw_plus_sparse:x_out = x_raw + alpha*S\n",
    "    \"\"\"\n",
    "    var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "    data_x = []\n",
    "    target_len = preset[\"data\"][\"length\"]\n",
    "\n",
    "    for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "        x_raw = np.load(var_path)\n",
    "        x_raw = _ensure_shape_4d_amp(x_raw).astype(np.float32, copy=False)\n",
    "\n",
    "        _debug_print_once(var_label, x_raw)\n",
    "\n",
    "        mode = CSI_INPUT_MODE\n",
    "\n",
    "        if mode == \"raw\":\n",
    "            x_out = x_raw\n",
    "\n",
    "        else:\n",
    "            if CACHE_ENABLED:\n",
    "                os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "                p = _cache_path(var_label, mode)\n",
    "                if os.path.exists(p):\n",
    "                    x_out = np.load(p).astype(np.float32, copy=False)\n",
    "                else:\n",
    "                    S = _make_sparse_component_pairwise(x_raw)\n",
    "                    if mode == \"sparse\":\n",
    "                        x_out = S\n",
    "                    elif mode == \"raw_plus_sparse\":\n",
    "                        x_out = x_raw + (SPARSE_ALPHA * S)\n",
    "                        if CLAMP_NONNEG:\n",
    "                            x_out = np.maximum(x_out, 0.0)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown CSI_INPUT_MODE={mode}\")\n",
    "\n",
    "                    np.save(p, x_out.astype(np.float32))\n",
    "            else:\n",
    "                S = _make_sparse_component_pairwise(x_raw)\n",
    "                if mode == \"sparse\":\n",
    "                    x_out = S\n",
    "                elif mode == \"raw_plus_sparse\":\n",
    "                    x_out = x_raw + (SPARSE_ALPHA * S)\n",
    "                    if CLAMP_NONNEG:\n",
    "                        x_out = np.maximum(x_out, 0.0)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown CSI_INPUT_MODE={mode}\")\n",
    "\n",
    "        # trim/pad to target_len (same behavior as your existing code expects)\n",
    "        if x_out.shape[0] > target_len:\n",
    "            x_out = x_out[-target_len:, :, :, :]\n",
    "\n",
    "        pad_len = target_len - x_out.shape[0]\n",
    "        if pad_len > 0:\n",
    "            x_out = np.pad(x_out, ((pad_len, 0), (0, 0), (0, 0), (0, 0)))\n",
    "\n",
    "        data_x.append(x_out)\n",
    "\n",
    "    return np.array(data_x)\n",
    "\n",
    "\n",
    "def encode_data_y(data_pd_y, var_task):\n",
    "    \"\"\"\n",
    "    Encode labels according to specific task.\n",
    "    \"\"\"\n",
    "    if var_task == \"identity\":\n",
    "        data_y = encode_identity(data_pd_y)\n",
    "    elif var_task == \"activity\":\n",
    "        data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "    elif var_task == \"location\":\n",
    "        data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "    elif var_task == \"count\":\n",
    "        data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "    return data_y\n",
    "\n",
    "\n",
    "def encode_identity(data_pd_y):\n",
    "    \"\"\"\n",
    "    Onehot encoding for identity labels.\n",
    "    \"\"\"\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "                                    \"user_3_location\", \"user_4_location\",\n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "    data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "    data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "    return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "def encode_activity(data_pd_y, var_encoding):\n",
    "    \"\"\"\n",
    "    Onehot encoding for activity labels.\n",
    "    \"\"\"\n",
    "    data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "                                    \"user_3_activity\", \"user_4_activity\",\n",
    "                                    \"user_5_activity\", \"user_6_activity\"]]\n",
    "    data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "    return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "def encode_location(data_pd_y, var_encoding):\n",
    "    \"\"\"\n",
    "    Onehot encoding for location labels.\n",
    "    \"\"\"\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "                                    \"user_3_location\", \"user_4_location\",\n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "    return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "def encode_count(data_pd_y, var_encoding):\n",
    "    \"\"\"\n",
    "    Onehot encoding for count labels.\n",
    "    \"\"\"\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "                                    \"user_3_location\", \"user_4_location\",\n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "    data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "    data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "    data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "    count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6ff467",
   "metadata": {
    "papermill": {
     "duration": 0.007259,
     "end_time": "2025-12-28T20:25:02.276206",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.268947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "last stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74309063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.292487Z",
     "iopub.status.busy": "2025-12-28T20:25:02.292281Z",
     "iopub.status.idle": "2025-12-28T20:25:02.300246Z",
     "shell.execute_reply": "2025-12-28T20:25:02.299698Z"
    },
    "papermill": {
     "duration": 0.017128,
     "end_time": "2025-12-28T20:25:02.301229",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.284101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI (from .mat), and encode labels\n",
    "# \"\"\"\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import scipy.io as scio\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from numpy.fft import ifft\n",
    "\n",
    "# # =========================================================\n",
    "# #  Settings\n",
    "# # =========================================================\n",
    "# # \"raw\"     : abs(raw complex CSI)  -> float\n",
    "# # \"lowrank\" : abs(L) after RPCA     -> float\n",
    "# # \"sparse\"  : abs(S) after RPCA     -> float\n",
    "# CSI_INPUT_MODE = \"sparse\"   # raw / lowrank / sparse\n",
    "\n",
    "# # expected dimensions\n",
    "# TX = 3\n",
    "# RX = 3\n",
    "# SC = 30\n",
    "\n",
    "# # optional IFFT (frequency -> delay)\n",
    "# USE_IFFT = True\n",
    "\n",
    "# # RPCA iterations (برای شروع کم بگذار؛ اگر لازم شد بیشتر کن)\n",
    "# RPCA_MAX_ITER = 50\n",
    "\n",
    "# # lambda mode (اختیاری)\n",
    "# # \"classic\" : 1/sqrt(max(m,n))\n",
    "# # \"median\"  : 1/sqrt(max(m,n)) * median(abs(M))\n",
    "# # \"scaled\"  : 1.2/sqrt(max(m,n))\n",
    "# RPCA_LAMBDA_MODE = \"classic\"\n",
    "\n",
    "# # cache (very important for speed)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT = \"/kaggle/working/csi_cache_mat_pipeline\"\n",
    "\n",
    "# # print once to verify input is amplitude or complex/phase\n",
    "# _DEBUG_PRINT_ONCE = True\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# # =========================================================\n",
    "# # Minimal R_pca implementation (no external dependency)\n",
    "# # Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "# # =========================================================\n",
    "# class R_pca:\n",
    "#     def __init__(self, D):\n",
    "#         self.D = np.asarray(D, dtype=np.float64)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _shrink(M, tau):\n",
    "#         return np.sign(M) * np.maximum(np.abs(M) - tau, 0.0)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _svt(M, tau):\n",
    "#         U, s, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "#         s = np.maximum(s - tau, 0.0)\n",
    "#         if np.all(s == 0):\n",
    "#             return np.zeros_like(M)\n",
    "#         return (U * s) @ Vt\n",
    "\n",
    "#     def _lambda(self, D):\n",
    "#         m, n = D.shape\n",
    "#         base = 1.0 / np.sqrt(max(m, n))\n",
    "#         if RPCA_LAMBDA_MODE == \"classic\":\n",
    "#             return base\n",
    "#         if RPCA_LAMBDA_MODE == \"scaled\":\n",
    "#             return 1.2 * base\n",
    "#         if RPCA_LAMBDA_MODE == \"median\":\n",
    "#             med = np.median(np.abs(D))\n",
    "#             med = float(med) if med > 1e-12 else 1e-12\n",
    "#             return base * med\n",
    "#         raise ValueError(f\"Unknown RPCA_LAMBDA_MODE={RPCA_LAMBDA_MODE}\")\n",
    "\n",
    "#     def fit(self, max_iter=200, tol=1e-6, rho=1.5, mu=None):\n",
    "#         \"\"\"\n",
    "#         Returns L, S such that D ≈ L + S\n",
    "#         \"\"\"\n",
    "#         D = self.D\n",
    "#         m, n = D.shape\n",
    "#         lam = self._lambda(D)\n",
    "\n",
    "#         # auto mu\n",
    "#         if mu is None:\n",
    "#             s0 = np.linalg.svd(D, compute_uv=False, full_matrices=False)[0] if D.size else 1.0\n",
    "#             mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#         L = np.zeros_like(D)\n",
    "#         S = np.zeros_like(D)\n",
    "#         Y = np.zeros_like(D)\n",
    "\n",
    "#         normD = np.linalg.norm(D, ord=\"fro\") + 1e-12\n",
    "\n",
    "#         for _ in range(max_iter):\n",
    "#             L = self._svt(D - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#             S = self._shrink(D - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#             R = D - L - S\n",
    "#             Y = Y + mu * R\n",
    "\n",
    "#             if (np.linalg.norm(R, ord=\"fro\") / normD) < tol:\n",
    "#                 break\n",
    "\n",
    "#             mu *= rho\n",
    "\n",
    "#         return L.astype(np.float64), S.astype(np.float64)\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 1) Phase calibration (sanitize) - vectorized\n",
    "# # --------------------------------------------------\n",
    "# def phase_sanitize_matrix(X):\n",
    "#     \"\"\"\n",
    "#     X: complex matrix (N_subcarriers, T)\n",
    "#     \"\"\"\n",
    "#     phase = np.unwrap(np.angle(X), axis=0)  # (N,T)\n",
    "#     N, T = phase.shape\n",
    "#     k = np.arange(N, dtype=np.float64)[:, None]  # (N,1)\n",
    "\n",
    "#     A = np.concatenate([k, np.ones((N, 1), dtype=np.float64)], axis=1)  # (N,2)\n",
    "#     pinvA = np.linalg.pinv(A)  # (2,N)\n",
    "#     coeff = pinvA @ phase      # (2,T)\n",
    "#     a = coeff[0:1, :]\n",
    "#     b = coeff[1:2, :]\n",
    "\n",
    "#     phase_corr = phase - (k @ a + b)\n",
    "#     return np.abs(X) * np.exp(1j * phase_corr)\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 2) Preprocess CSI matrix\n",
    "# # --------------------------------------------------\n",
    "# def preprocess_csi(X):\n",
    "#     \"\"\"\n",
    "#     X : complex CSI matrix (N_subcarriers, T)\n",
    "#     \"\"\"\n",
    "#     X_corr = phase_sanitize_matrix(X).astype(np.complex128, copy=False)\n",
    "#     fro = np.linalg.norm(X_corr, \"fro\")\n",
    "#     if fro > 0:\n",
    "#         X_corr /= fro\n",
    "#     return X_corr\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 3) Optional IFFT\n",
    "# # --------------------------------------------------\n",
    "# def csi_to_cir(X):\n",
    "#     return ifft(X, axis=0)\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 4) RPCA on complex matrix (real & imag separately)\n",
    "# # --------------------------------------------------\n",
    "# def rpca_complex(X, max_iter=200):\n",
    "#     Xr = np.real(X)\n",
    "#     Xi = np.imag(X)\n",
    "\n",
    "#     rpca_r = R_pca(Xr)\n",
    "#     Lr, Sr = rpca_r.fit(max_iter=max_iter)\n",
    "\n",
    "#     rpca_i = R_pca(Xi)\n",
    "#     Li, Si = rpca_i.fit(max_iter=max_iter)\n",
    "\n",
    "#     L = Lr + 1j * Li\n",
    "#     S = Sr + 1j * Si\n",
    "#     return L, S\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 5) Full pipeline\n",
    "# # --------------------------------------------------\n",
    "# def csi_lowrank_sparse_pipeline(X, use_ifft=True, max_iter=200):\n",
    "#     Xp = preprocess_csi(X)\n",
    "#     if use_ifft:\n",
    "#         Xp = csi_to_cir(Xp)\n",
    "#     L, S = rpca_complex(Xp, max_iter=max_iter)\n",
    "#     return L, S\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # MAT loader -> (T,3,3,30) complex\n",
    "# # --------------------------------------------------\n",
    "# def load_csi_from_mat(mat_path):\n",
    "#     m = scio.loadmat(mat_path)\n",
    "#     if \"trace\" not in m:\n",
    "#         raise KeyError(f\"'trace' not found in {mat_path}\")\n",
    "\n",
    "#     trace = m[\"trace\"]  # (T,1) object array\n",
    "#     T = trace.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.complex128)\n",
    "\n",
    "#     for t in range(T):\n",
    "#         out[t] = trace[t, 0][\"csi\"][0, 0]  # (3,3,30) complex\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def _debug_print_once(label, csi_4d):\n",
    "#     global _DEBUG_PRINT_ONCE\n",
    "#     if not _DEBUG_PRINT_ONCE:\n",
    "#         return\n",
    "#     _DEBUG_PRINT_ONCE = False\n",
    "\n",
    "#     is_cplx = np.iscomplexobj(csi_4d)\n",
    "#     print(f\"\\n[DEBUG] First MAT sample: {label}\")\n",
    "#     print(f\"[DEBUG] shape={csi_4d.shape}, dtype={csi_4d.dtype}, complex={is_cplx}\")\n",
    "\n",
    "#     x0 = csi_4d[0, 0, 0, :]\n",
    "#     if is_cplx:\n",
    "#         ang = np.angle(x0)\n",
    "#         print(f\"[DEBUG] abs range:  min={np.min(np.abs(x0)):.6f}, max={np.max(np.abs(x0)):.6f}\")\n",
    "#         print(f\"[DEBUG] phase stats: mean={np.mean(ang):.6f}, std={np.std(ang):.6f}\")\n",
    "#         print(\"[DEBUG] ==> Input is COMPLEX (phase exists).\")\n",
    "#     else:\n",
    "#         print(f\"[DEBUG] value range: min={np.min(x0):.6f}, max={np.max(x0):.6f}\")\n",
    "#         print(\"[DEBUG] ==> Input is REAL (likely amplitude-only).\")\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _apply_pipeline_pairwise_9links(csi_4d_complex, label):\n",
    "#     \"\"\"\n",
    "#     csi_4d_complex: (T,3,3,30) complex\n",
    "#     Run pipeline on each link separately: (30,T) -> RPCA -> abs -> back to (T,3,3,30) float32\n",
    "#     \"\"\"\n",
    "#     _debug_print_once(label, csi_4d_complex)\n",
    "\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return np.abs(csi_4d_complex).astype(np.float32)\n",
    "\n",
    "#     mode = CSI_INPUT_MODE  # lowrank / sparse\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             return np.load(p).astype(np.float32, copy=False)\n",
    "\n",
    "#     T = csi_4d_complex.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "#     for tx in range(TX):\n",
    "#         for rx in range(RX):\n",
    "#             X = csi_4d_complex[:, tx, rx, :].T  # (30,T) complex\n",
    "#             L, S = csi_lowrank_sparse_pipeline(X, use_ifft=USE_IFFT, max_iter=RPCA_MAX_ITER)\n",
    "#             Y = L if mode == \"lowrank\" else S\n",
    "#             out[:, tx, rx, :] = np.abs(Y.T).astype(np.float32)\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out)\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # Existing label loading/encoding\n",
    "# # --------------------------------------------------\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None,\n",
    "#                 var_wifi_band=None,\n",
    "#                 var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     var_path_data_x should point to MAT directory, e.g. /kaggle/input/wimans/wifi_csi/mat\n",
    "#     Each label corresponds to <label>.mat\n",
    "#     \"\"\"\n",
    "#     data_x = []\n",
    "#     target_len = preset[\"data\"][\"length\"]\n",
    "\n",
    "#     for label in var_label_list:\n",
    "#         mat_path = os.path.join(var_path_data_x, label + \".mat\")\n",
    "#         if not os.path.exists(mat_path):\n",
    "#             raise FileNotFoundError(f\"MAT file not found: {mat_path}\")\n",
    "\n",
    "#         csi_complex = load_csi_from_mat(mat_path)  # (T,3,3,30) complex\n",
    "\n",
    "#         # if longer than target_len, keep last target_len frames\n",
    "#         if csi_complex.shape[0] > target_len:\n",
    "#             csi_complex = csi_complex[-target_len:, :, :, :]\n",
    "\n",
    "#         csi_feat = _apply_pipeline_pairwise_9links(csi_complex, label)\n",
    "\n",
    "#         # pad if shorter\n",
    "#         pad_len = target_len - csi_feat.shape[0]\n",
    "#         if pad_len > 0:\n",
    "#             csi_feat = np.pad(csi_feat, ((pad_len, 0), (0, 0), (0, 0), (0, 0)))\n",
    "\n",
    "#         data_x.append(csi_feat)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y)\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "#                                     \"user_3_activity\", \"user_4_activity\",\n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b54a76c",
   "metadata": {
    "papermill": {
     "duration": 0.006697,
     "end_time": "2025-12-28T20:25:02.314810",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.308113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "rpca 30 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db565541",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.330237Z",
     "iopub.status.busy": "2025-12-28T20:25:02.329999Z",
     "iopub.status.idle": "2025-12-28T20:25:02.336334Z",
     "shell.execute_reply": "2025-12-28T20:25:02.335831Z"
    },
    "papermill": {
     "duration": 0.015724,
     "end_time": "2025-12-28T20:25:02.337377",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.321653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # =========================================================\n",
    "# # انتخاب نوع دیتای ورودی مدل:\n",
    "# # \"raw\"     : amp خام\n",
    "# # \"lowrank\" : خروجی Low-rank (L) از RPCA\n",
    "# # \"sparse\"  : خروجی Sparse (S) از RPCA\n",
    "# CSI_INPUT_MODE = \"lowrank\"   # <-- raw / lowrank / sparse\n",
    "\n",
    "# # ابعاد مورد انتظار CSI\n",
    "# TX = 3\n",
    "# RX = 3\n",
    "# SC = 30\n",
    "\n",
    "# # RPCA (IALM) تنظیمات سریع‌تر\n",
    "# RPCA_MAX_ITER = 60\n",
    "# RPCA_TOL      = 1e-5\n",
    "# RPCA_RHO      = 1.5\n",
    "# RPCA_MU_INIT  = None\n",
    "\n",
    "# # ✅ انتخاب لامبدا:\n",
    "# # \"median\" : lam = 1/sqrt(max(m,n)) * median(abs(M))\n",
    "# # \"scaled\" : lam = 1.2/sqrt(max(m,n))\n",
    "# RPCA_LAMBDA_MODE = \"median\"  # <-- \"median\" یا \"scaled\"\n",
    "\n",
    "# # Cache (خیلی مهم برای سرعت)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT    = \"/kaggle/working/csi_cache_pairwise\"\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _soft_threshold(X, tau):\n",
    "#     return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "# def _svt(X, tau):\n",
    "#     # Singular Value Thresholding\n",
    "#     U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "#     s = np.maximum(s - tau, 0.0)\n",
    "#     if np.all(s == 0):\n",
    "#         return np.zeros_like(X)\n",
    "#     return (U * s) @ Vt\n",
    "\n",
    "# def _compute_lambda(M, mode):\n",
    "#     \"\"\"\n",
    "#     mode:\n",
    "#       - 'median': 1/sqrt(max(m,n)) * median(abs(M))\n",
    "#       - 'scaled': 1.2/sqrt(max(m,n))\n",
    "#     \"\"\"\n",
    "#     m, n = M.shape\n",
    "#     base = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "#     if mode == \"median\":\n",
    "#         med = np.median(np.abs(M))\n",
    "#         # اگر med خیلی کوچک بود برای پایداری:\n",
    "#         med = float(med) if med > 1e-12 else 1e-12\n",
    "#         return base * med\n",
    "\n",
    "#     if mode == \"scaled\":\n",
    "#         return 1.2 * base\n",
    "\n",
    "#     raise ValueError(f\"Unknown RPCA_LAMBDA_MODE: {mode}. Use 'median' or 'scaled'.\")\n",
    "\n",
    "\n",
    "# def _rpca_ialm(M, mu=None, rho=1.5, max_iter=60, tol=1e-5):\n",
    "#     \"\"\"\n",
    "#     Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "#     Decompose: M = L + S\n",
    "#     M shape: (T, SC) = (3000, 30)\n",
    "#     \"\"\"\n",
    "#     M = M.astype(np.float64, copy=False)\n",
    "#     m, n = M.shape\n",
    "\n",
    "#     # ✅ lambda طبق انتخاب کاربر\n",
    "#     lam = _compute_lambda(M, RPCA_LAMBDA_MODE)\n",
    "\n",
    "#     # mu خودکار یا دستی\n",
    "#     if mu is None:\n",
    "#         s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "#         mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     Y = np.zeros_like(M)\n",
    "\n",
    "#     normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#         S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#         R = M - L - S\n",
    "#         Y = Y + mu * R\n",
    "\n",
    "#         if (np.linalg.norm(R, ord=\"fro\") / normM) < tol:\n",
    "#             break\n",
    "\n",
    "#         mu *= rho\n",
    "\n",
    "#     return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# def _ensure_shape_4d(data_csi):\n",
    "#     \"\"\"\n",
    "#     Ensure CSI shape is (T, TX, RX, SC).\n",
    "#     If input is (T, 270) we reshape to (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     if data_csi.ndim == 4:\n",
    "#         return data_csi\n",
    "\n",
    "#     if data_csi.ndim == 2 and data_csi.shape[1] == TX * RX * SC:\n",
    "#         T = data_csi.shape[0]\n",
    "#         return data_csi.reshape(T, TX, RX, SC)\n",
    "\n",
    "#     raise ValueError(\n",
    "#         f\"Unexpected CSI shape {data_csi.shape}. Expected (T,{TX},{RX},{SC}) or (T,{TX*RX*SC}).\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _rpca_pairwise_9links(data_csi_4d):\n",
    "#     \"\"\"\n",
    "#     data_csi_4d: (T, TX, RX, SC)\n",
    "#     Run RPCA on each (tx,rx) separately on matrix (T, SC) and reassemble.\n",
    "#     Output shape stays (T, TX, RX, SC).\n",
    "#     \"\"\"\n",
    "#     T = data_csi_4d.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "#     for tx in range(TX):\n",
    "#         for rx in range(RX):\n",
    "#             M = data_csi_4d[:, tx, rx, :]  # (T,30)\n",
    "#             L, S = _rpca_ialm(\n",
    "#                 M,\n",
    "#                 mu=RPCA_MU_INIT,\n",
    "#                 rho=RPCA_RHO,\n",
    "#                 max_iter=RPCA_MAX_ITER,\n",
    "#                 tol=RPCA_TOL\n",
    "#             )\n",
    "#             out[:, tx, rx, :] = L if CSI_INPUT_MODE == \"lowrank\" else S\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def _apply_mode_with_cache(data_csi, label):\n",
    "#     \"\"\"\n",
    "#     Apply raw/lowrank/sparse. For lowrank/sparse use pairwise RPCA (9 links of 3000x30).\n",
    "#     Keeps the original 4D shape (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     data_csi = _ensure_shape_4d(np.asarray(data_csi, dtype=np.float32))\n",
    "\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     mode = CSI_INPUT_MODE\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             cached = np.load(p)\n",
    "#             return _ensure_shape_4d(cached).astype(np.float32, copy=False)\n",
    "\n",
    "#     out = _rpca_pairwise_9links(data_csi)\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out.astype(np.float32))\n",
    "\n",
    "#     return out.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None,\n",
    "#                 var_wifi_band=None,\n",
    "#                 var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ RPCA روی 9 لینک جدا (3000x30) و بعد اتصال\n",
    "#         data_csi = _apply_mode_with_cache(data_csi, var_label)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "#                                     \"user_3_activity\", \"user_4_activity\",\n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f43bc3b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.352748Z",
     "iopub.status.busy": "2025-12-28T20:25:02.352563Z",
     "iopub.status.idle": "2025-12-28T20:25:02.359251Z",
     "shell.execute_reply": "2025-12-28T20:25:02.358552Z"
    },
    "papermill": {
     "duration": 0.01582,
     "end_time": "2025-12-28T20:25:02.360390",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.344570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # =========================================================\n",
    "# # انتخاب نوع دیتای ورودی مدل:\n",
    "# # \"raw\"     : amp خام\n",
    "# # \"lowrank\" : خروجی Low-rank (L) از RPCA\n",
    "# # \"sparse\"  : خروجی Sparse (S) از RPCA\n",
    "# CSI_INPUT_MODE = \"sparse\"   # <-- raw / lowrank / sparse\n",
    "\n",
    "# # ابعاد مورد انتظار CSI\n",
    "# TX = 3\n",
    "# RX = 3\n",
    "# SC = 30\n",
    "\n",
    "# # RPCA (IALM) تنظیمات سریع‌تر\n",
    "# RPCA_MAX_ITER = 60\n",
    "# RPCA_TOL      = 1e-5\n",
    "# RPCA_RHO      = 1.5\n",
    "# RPCA_MU_INIT  = None\n",
    "# RPCA_LAMBDA   = None     # None -> 1/sqrt(max(m,n))\n",
    "\n",
    "# # Cache (خیلی مهم برای سرعت)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT    = \"/kaggle/working/csi_cache_pairwise\"  # خروجی‌ها اینجا ذخیره میشن\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _soft_threshold(X, tau):\n",
    "#     return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "# def _svt(X, tau):\n",
    "#     # Singular Value Thresholding\n",
    "#     U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "#     s = np.maximum(s - tau, 0.0)\n",
    "#     if np.all(s == 0):\n",
    "#         return np.zeros_like(X)\n",
    "#     return (U * s) @ Vt\n",
    "\n",
    "# def _rpca_ialm(M, lam=None, mu=None, rho=1.5, max_iter=60, tol=1e-5):\n",
    "#     \"\"\"\n",
    "#     Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "#     Decompose: M = L + S\n",
    "#     M shape: (T, SC) = (3000, 30)\n",
    "#     \"\"\"\n",
    "#     M = M.astype(np.float64, copy=False)\n",
    "#     m, n = M.shape\n",
    "\n",
    "#     if lam is None:\n",
    "#         lam = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "#     if mu is None:\n",
    "#         # top singular value as spectral norm approximation\n",
    "#         s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "#         mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     Y = np.zeros_like(M)\n",
    "\n",
    "#     normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#         S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#         R = M - L - S\n",
    "#         Y = Y + mu * R\n",
    "\n",
    "#         if (np.linalg.norm(R, ord=\"fro\") / normM) < tol:\n",
    "#             break\n",
    "\n",
    "#         mu *= rho\n",
    "\n",
    "#     return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# def _ensure_shape_4d(data_csi):\n",
    "#     \"\"\"\n",
    "#     Ensure CSI shape is (T, TX, RX, SC).\n",
    "#     If input is (T, 270) we reshape to (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     if data_csi.ndim == 4:\n",
    "#         return data_csi\n",
    "\n",
    "#     if data_csi.ndim == 2 and data_csi.shape[1] == TX * RX * SC:\n",
    "#         T = data_csi.shape[0]\n",
    "#         return data_csi.reshape(T, TX, RX, SC)\n",
    "\n",
    "#     raise ValueError(\n",
    "#         f\"Unexpected CSI shape {data_csi.shape}. Expected (T,{TX},{RX},{SC}) or (T,{TX*RX*SC}).\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     # mode: \"lowrank\" or \"sparse\"\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _rpca_pairwise_9links(data_csi_4d):\n",
    "#     \"\"\"\n",
    "#     data_csi_4d: (T, TX, RX, SC)\n",
    "#     Run RPCA on each (tx,rx) separately on matrix (T, SC) and reassemble.\n",
    "#     Output shape stays (T, TX, RX, SC).\n",
    "#     \"\"\"\n",
    "#     T = data_csi_4d.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "#     # 9 times RPCA: for each tx-rx link\n",
    "#     for tx in range(TX):\n",
    "#         for rx in range(RX):\n",
    "#             M = data_csi_4d[:, tx, rx, :]  # (T, SC) => (3000,30)\n",
    "#             L, S = _rpca_ialm(\n",
    "#                 M,\n",
    "#                 lam=RPCA_LAMBDA,\n",
    "#                 mu=RPCA_MU_INIT,\n",
    "#                 rho=RPCA_RHO,\n",
    "#                 max_iter=RPCA_MAX_ITER,\n",
    "#                 tol=RPCA_TOL\n",
    "#             )\n",
    "#             if CSI_INPUT_MODE == \"lowrank\":\n",
    "#                 out[:, tx, rx, :] = L\n",
    "#             else:  # \"sparse\"\n",
    "#                 out[:, tx, rx, :] = S\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def _apply_mode_with_cache(data_csi, label):\n",
    "#     \"\"\"\n",
    "#     Apply raw/lowrank/sparse. For lowrank/sparse use pairwise RPCA (9 links of 3000x30).\n",
    "#     Keeps the original 4D shape (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     data_csi = _ensure_shape_4d(np.asarray(data_csi, dtype=np.float32))\n",
    "\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     mode = CSI_INPUT_MODE\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             cached = np.load(p)\n",
    "#             return _ensure_shape_4d(cached).astype(np.float32, copy=False)\n",
    "\n",
    "#     out = _rpca_pairwise_9links(data_csi)\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out.astype(np.float32))\n",
    "\n",
    "#     return out.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     \"\"\"\n",
    "#     Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "#     \"\"\"\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     Load CSI amplitude (*.npy) files based on a label list.\n",
    "#     \"\"\"\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ NEW: RPCA روی 9 لینک جداگانه (3000x30) و بعد چسباندن\n",
    "#         data_csi = _apply_mode_with_cache(data_csi, var_label)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     \"\"\"\n",
    "#     Encode labels according to specific task.\n",
    "#     \"\"\"\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     return data_identity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for activity labels.\n",
    "#     \"\"\"\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "#                                     \"user_3_activity\", \"user_4_activity\", \n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "#     return data_activity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for location labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "#     return data_location_onehot_y\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1)\n",
    "#     count_data = count_data.reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     count_data_onehot = encoder.fit_transform(count_data).astype(\"int8\")\n",
    "#     return count_data_onehot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de9a8e",
   "metadata": {
    "papermill": {
     "duration": 0.006956,
     "end_time": "2025-12-28T20:25:02.374286",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.367330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "rpca\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e41fa1cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.389207Z",
     "iopub.status.busy": "2025-12-28T20:25:02.388943Z",
     "iopub.status.idle": "2025-12-28T20:25:02.394458Z",
     "shell.execute_reply": "2025-12-28T20:25:02.393947Z"
    },
    "papermill": {
     "duration": 0.014263,
     "end_time": "2025-12-28T20:25:02.395500",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.381237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # =========================================================\n",
    "# # انتخاب ورودی مدل:\n",
    "# # \"raw\"     : همون amp خام\n",
    "# # \"lowrank\" : مؤلفه Low-rank از RPCA (L)\n",
    "# # \"sparse\"  : مؤلفه Sparse از RPCA (S)\n",
    "# CSI_INPUT_MODE = \"lowrank\"   # <-- raw / lowrank / sparse\n",
    "\n",
    "# # RPCA (IALM) تنظیمات سریع‌تر\n",
    "# RPCA_MAX_ITER = 80\n",
    "# RPCA_TOL      = 1e-5\n",
    "# RPCA_RHO      = 1.5\n",
    "# RPCA_MU_INIT  = None\n",
    "# RPCA_LAMBDA   = None     # None -> 1/sqrt(max(m,n))\n",
    "\n",
    "# # Cache (خیلی مهم برای سرعت)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT    = \"/kaggle/working/csi_cache\"  # خروجی‌ها اینجا ذخیره میشن\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _soft_threshold(X, tau):\n",
    "#     return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "# def _svt(X, tau):\n",
    "#     U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "#     s = np.maximum(s - tau, 0.0)\n",
    "#     if np.all(s == 0):\n",
    "#         return np.zeros_like(X)\n",
    "#     return (U * s) @ Vt\n",
    "\n",
    "# def _rpca_ialm(M, lam=None, mu=None, rho=1.5, max_iter=80, tol=1e-5):\n",
    "#     M = M.astype(np.float64, copy=False)\n",
    "#     m, n = M.shape\n",
    "\n",
    "#     if lam is None:\n",
    "#         lam = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "#     if mu is None:\n",
    "#         s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "#         mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     Y = np.zeros_like(M)\n",
    "\n",
    "#     normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#         S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#         R = M - L - S\n",
    "#         Y = Y + mu * R\n",
    "\n",
    "#         if (np.linalg.norm(R, ord=\"fro\") / normM) < tol:\n",
    "#             break\n",
    "#         mu *= rho\n",
    "\n",
    "#     return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# def _rpca_keep_shape(X):\n",
    "#     \"\"\"RPCA روی (T,F) و بازگرداندن دقیقاً به shape اولیه\"\"\"\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "#     if X.ndim == 1:\n",
    "#         M = X[:, None]\n",
    "#         L, S = _rpca_ialm(M, RPCA_LAMBDA, RPCA_MU_INIT, RPCA_RHO, RPCA_MAX_ITER, RPCA_TOL)\n",
    "#         return L[:, 0], S[:, 0]\n",
    "\n",
    "#     T = X.shape[0]\n",
    "#     F = int(np.prod(X.shape[1:]))\n",
    "#     M = X.reshape(T, F)\n",
    "\n",
    "#     L, S = _rpca_ialm(M, RPCA_LAMBDA, RPCA_MU_INIT, RPCA_RHO, RPCA_MAX_ITER, RPCA_TOL)\n",
    "#     return L.reshape(X.shape), S.reshape(X.shape)\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     # mode: \"lowrank\" or \"sparse\"\n",
    "#     # فایل خروجی: /kaggle/working/csi_cache/lowrank/<label>.npy\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _apply_mode_with_cache(data_csi, label):\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     # lowrank یا sparse\n",
    "#     mode = CSI_INPUT_MODE\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             return np.load(p).astype(np.float32, copy=False)\n",
    "\n",
    "#     L, S = _rpca_keep_shape(data_csi)\n",
    "#     out = L if mode == \"lowrank\" else S\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out.astype(np.float32))\n",
    "\n",
    "#     return out.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ RPCA lowrank/sparse بدون تغییر shape + با cache\n",
    "#         data_csi = _apply_mode_with_cache(data_csi, var_label)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "#                                     \"user_3_activity\", \"user_4_activity\",\n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccfe604",
   "metadata": {
    "papermill": {
     "duration": 0.0069,
     "end_time": "2025-12-28T20:25:02.409489",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.402589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "svd\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc0fdc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.424885Z",
     "iopub.status.busy": "2025-12-28T20:25:02.424663Z",
     "iopub.status.idle": "2025-12-28T20:25:02.431818Z",
     "shell.execute_reply": "2025-12-28T20:25:02.431220Z"
    },
    "papermill": {
     "duration": 0.01642,
     "end_time": "2025-12-28T20:25:02.432948",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.416528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # Note: All necessary libraries (os, numpy, pandas, etc.) are imported in Cell 1.\n",
    "# # from preset import preset   --> preset is already defined in Cell 2.\n",
    "\n",
    "# # =========================================================\n",
    "# # 🔧 NEW: Choose CSI representation mode here (ONLY EDIT THIS)\n",
    "# # ---------------------------------------------------------\n",
    "# # \"raw\"     : use original CSI amplitude as-is\n",
    "# # \"lowrank\" : use low-rank approximation (SVD)\n",
    "# # \"sparse\"  : keep only large-magnitude entries (dense array with many zeros)\n",
    "# CSI_INPUT_MODE = \"sparse\"     # <-- set to: \"raw\" / \"lowrank\" / \"sparse\"\n",
    "\n",
    "# # Low-rank settings\n",
    "# LOW_RANK_ENERGY = 0.95     # keep enough singular values to preserve this energy\n",
    "# LOW_RANK_RANK   = None     # if set to an int (e.g., 10), it overrides ENERGY\n",
    "\n",
    "# # Sparse settings\n",
    "# SPARSE_KEEP_RATIO = 0.10   # keep top 10% magnitudes (globally per sample)\n",
    "# SPARSE_MIN_ABS    = None   # if set (e.g., 0.5), keeps |x|>=threshold instead of keep_ratio\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _low_rank_approx_keep_shape(X, rank=None, energy=0.95):\n",
    "#     \"\"\"\n",
    "#     Low-rank approximation using SVD while preserving the original shape.\n",
    "#     Works for 1D/2D/ND by flattening all non-time dims into features.\n",
    "#     Assumes first axis is time.\n",
    "#     \"\"\"\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "\n",
    "#     if X.ndim == 1:\n",
    "#         M = X[:, None]  # (T,1)\n",
    "#         U, S, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "#         if rank is None:\n",
    "#             s2 = S**2\n",
    "#             cum = np.cumsum(s2) / (np.sum(s2) + 1e-12)\n",
    "#             rank = int(np.searchsorted(cum, energy) + 1)\n",
    "#         rank = max(1, min(rank, S.shape[0]))\n",
    "#         M_lr = (U[:, :rank] * S[:rank]) @ Vt[:rank, :]\n",
    "#         return M_lr[:, 0].astype(np.float32)\n",
    "\n",
    "#     # ND: reshape to (T, F)\n",
    "#     T = X.shape[0]\n",
    "#     F = int(np.prod(X.shape[1:]))\n",
    "#     M = X.reshape(T, F)\n",
    "\n",
    "#     U, S, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "\n",
    "#     if rank is None:\n",
    "#         s2 = S**2\n",
    "#         cum = np.cumsum(s2) / (np.sum(s2) + 1e-12)\n",
    "#         rank = int(np.searchsorted(cum, energy) + 1)\n",
    "\n",
    "#     rank = max(1, min(rank, S.shape[0]))\n",
    "#     M_lr = (U[:, :rank] * S[:rank]) @ Vt[:rank, :]\n",
    "\n",
    "#     return M_lr.reshape(X.shape).astype(np.float32)\n",
    "\n",
    "\n",
    "# def _to_sparse_dense_keep_shape(X, keep_ratio=0.10, min_abs=None):\n",
    "#     \"\"\"\n",
    "#     Makes X sparse-in-content (many zeros) but keeps it as a dense numpy array\n",
    "#     so the rest of the pipeline (np.save/np.load/pad/model) doesn't change.\n",
    "#     Keeps the same shape.\n",
    "#     \"\"\"\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "#     flat = X.ravel()\n",
    "#     if flat.size == 0:\n",
    "#         return X.astype(np.float32)\n",
    "\n",
    "#     absflat = np.abs(flat)\n",
    "\n",
    "#     if min_abs is not None:\n",
    "#         thr = float(min_abs)\n",
    "#         mask = absflat >= thr\n",
    "#     else:\n",
    "#         k = int(np.ceil(keep_ratio * flat.size))\n",
    "#         k = max(1, min(k, flat.size))\n",
    "#         if k == flat.size:\n",
    "#             mask = np.ones_like(absflat, dtype=bool)\n",
    "#         else:\n",
    "#             thr = np.partition(absflat, -k)[-k]\n",
    "#             mask = absflat >= thr\n",
    "\n",
    "#     out = np.zeros_like(flat, dtype=np.float32)\n",
    "#     out[mask] = flat[mask]\n",
    "#     return out.reshape(X.shape).astype(np.float32)\n",
    "\n",
    "\n",
    "# def _apply_csi_mode(data_csi):\n",
    "#     \"\"\"\n",
    "#     Apply selected CSI_INPUT_MODE to a single sample array.\n",
    "#     \"\"\"\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     elif CSI_INPUT_MODE == \"lowrank\":\n",
    "#         return _low_rank_approx_keep_shape(\n",
    "#             data_csi,\n",
    "#             rank=LOW_RANK_RANK,\n",
    "#             energy=LOW_RANK_ENERGY\n",
    "#         )\n",
    "\n",
    "#     elif CSI_INPUT_MODE == \"sparse\":\n",
    "#         return _to_sparse_dense_keep_shape(\n",
    "#             data_csi,\n",
    "#             keep_ratio=SPARSE_KEEP_RATIO,\n",
    "#             min_abs=SPARSE_MIN_ABS\n",
    "#         )\n",
    "\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown CSI_INPUT_MODE: {CSI_INPUT_MODE}. Use 'raw', 'lowrank', or 'sparse'.\")\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     \"\"\"\n",
    "#     Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "#     \"\"\"\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     Load CSI amplitude (*.npy) files based on a label list.\n",
    "#     \"\"\"\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_path in var_path_list:\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ NEW: convert input CSI according to selected mode (raw/lowrank/sparse)\n",
    "#         data_csi = _apply_csi_mode(data_csi)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "#     data_x = np.array(data_x)\n",
    "#     return data_x\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     \"\"\"\n",
    "#     Encode labels according to specific task.\n",
    "#     \"\"\"\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     return data_identity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for activity labels.\n",
    "#     \"\"\"\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "#                                     \"user_3_activity\", \"user_4_activity\", \n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "#     return data_activity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for location labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "#     return data_location_onehot_y\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     print(\"data_identity_onehot_y\",data_identity_onehot_y.shape)\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1)\n",
    "#     print(\"count_data\",count_data.shape)\n",
    "#     count_data = count_data.reshape(-1, 1)  # shape = (11286, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)  \n",
    "#     count_data_onehot = encoder.fit_transform(count_data)\n",
    "#     print(count_data_onehot.shape)  \n",
    "#     count_data_onehot = count_data_onehot.astype(\"int8\")\n",
    "\n",
    "#     return count_data_onehot\n",
    "\n",
    "\n",
    "# # Test functions (optional)\n",
    "# def test_load_data_y():\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"classroom\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=[\"1\", \"2\", \"3\"]).describe())\n",
    "\n",
    "# def test_load_data_x():\n",
    "#     data_pd_y = load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=None)\n",
    "#     var_label_list = data_pd_y[\"label\"].to_list()\n",
    "#     data_x = load_data_x(preset[\"path\"][\"data_x\"], var_label_list)\n",
    "#     print(data_x.shape)\n",
    "\n",
    "# def test_encode_identity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_identity_onehot_y = encode_identity(data_pd_y)\n",
    "#     print(data_identity_onehot_y.shape)\n",
    "#     print(data_identity_onehot_y[2000])\n",
    "\n",
    "# def test_encode_activity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_activity_onehot_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     print(data_activity_onehot_y.shape)\n",
    "#     print(data_activity_onehot_y[1560])\n",
    "\n",
    "# def test_encode_location():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_location_onehot_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     print(data_location_onehot_y.shape)\n",
    "#     print(data_location_onehot_y[1560])\n",
    "\n",
    "# def test_encode_count():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_count_onehot_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     print(data_count_onehot_y.shape)\n",
    "#     print(data_count_onehot_y[20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22e12c54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.448300Z",
     "iopub.status.busy": "2025-12-28T20:25:02.448091Z",
     "iopub.status.idle": "2025-12-28T20:25:02.453480Z",
     "shell.execute_reply": "2025-12-28T20:25:02.452811Z"
    },
    "papermill": {
     "duration": 0.014243,
     "end_time": "2025-12-28T20:25:02.454557",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.440314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # Note: All necessary libraries (os, numpy, pandas, etc.) are imported in Cell 1.\n",
    "# # from preset import preset   --> preset is already defined in Cell 2.\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     \"\"\"\n",
    "#     Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "#     \"\"\"\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     Load CSI amplitude (*.npy) files based on a label list.\n",
    "#     \"\"\"\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_path in var_path_list:\n",
    "#         data_csi = np.load(var_path)\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "#     data_x = np.array(data_x)\n",
    "#     return data_x\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     \"\"\"\n",
    "#     Encode labels according to specific task.\n",
    "#     \"\"\"\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     return data_identity_onehot_y\n",
    "\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for activity labels.\n",
    "#     \"\"\"\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "#                                     \"user_3_activity\", \"user_4_activity\", \n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "#     return data_activity_onehot_y\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for location labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "#     return data_location_onehot_y\n",
    "\n",
    "# # Test functions (optional)\n",
    "# def test_load_data_y():\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"classroom\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=[\"1\", \"2\", \"3\"]).describe())\n",
    "\n",
    "# def test_load_data_x():\n",
    "#     data_pd_y = load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=None)\n",
    "#     var_label_list = data_pd_y[\"label\"].to_list()\n",
    "#     data_x = load_data_x(preset[\"path\"][\"data_x\"], var_label_list)\n",
    "#     print(data_x.shape)\n",
    "\n",
    "# def test_encode_identity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_identity_onehot_y = encode_identity(data_pd_y)\n",
    "#     print(data_identity_onehot_y.shape)\n",
    "#     print(data_identity_onehot_y[2000])\n",
    "\n",
    "# def test_encode_activity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_activity_onehot_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     print(data_activity_onehot_y.shape)\n",
    "#     print(data_activity_onehot_y[1560])\n",
    "\n",
    "# def test_encode_location():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_location_onehot_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     print(data_location_onehot_y.shape)\n",
    "#     print(data_location_onehot_y[1560])\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     print(\"data_identity_onehot_y\",data_identity_onehot_y.shape)\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1)\n",
    "#     print(\"count_data\",count_data.shape)\n",
    "#     count_data = count_data.reshape(-1, 1)  # shape = (11286, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)  \n",
    "#     count_data_onehot = encoder.fit_transform(count_data)\n",
    "#     print(count_data_onehot.shape)  \n",
    "#     count_data_onehot = count_data_onehot.astype(\"int8\")\n",
    "\n",
    "#     return count_data_onehot\n",
    "\n",
    "\n",
    "# def test_encode_count():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_count_onehot_y = encode_count(data_pd_y)\n",
    "#     print(data_count_onehot_y.shape)\n",
    "#     print(data_count_onehot_y[20])\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     test_encode_count()\n",
    "# #     test_load_data_y()\n",
    "# #     test_load_data_x()\n",
    "# #     test_encode_identity()\n",
    "# #     test_encode_activity()\n",
    "# #     test_encode_location()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe825c8",
   "metadata": {
    "papermill": {
     "duration": 0.006834,
     "end_time": "2025-12-28T20:25:02.468515",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.461681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 4: preprocess.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9be42d3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.484184Z",
     "iopub.status.busy": "2025-12-28T20:25:02.483934Z",
     "iopub.status.idle": "2025-12-28T20:25:02.498832Z",
     "shell.execute_reply": "2025-12-28T20:25:02.498157Z"
    },
    "papermill": {
     "duration": 0.024304,
     "end_time": "2025-12-28T20:25:02.499863",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.475559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          preprocess.py\n",
    "[description]   preprocess WiFi CSI data\n",
    "\"\"\"\n",
    "\n",
    "# All necessary libraries are already imported in Cell 1.\n",
    "\n",
    "# def mat_to_amp(data_mat):\n",
    "#     \"\"\"\n",
    "#     Calculate amplitude of raw WiFi CSI data.\n",
    "#     \"\"\"\n",
    "#     var_length = data_mat[\"trace\"].shape[0]\n",
    "#     data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "#     data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "#     return data_csi_amp\n",
    "\n",
    "def extract_csi_amp(var_dir_mat, var_dir_amp):\n",
    "    \"\"\"\n",
    "    Read raw WiFi CSI (*.mat) files, calculate CSI amplitude, and save as (*.npy).\n",
    "    \"\"\"\n",
    "    var_path_mat = os.listdir(var_dir_mat)\n",
    "    for var_c, var_path in enumerate(var_path_mat):\n",
    "        data_mat = scio.loadmat(os.path.join(var_dir_mat, var_path))\n",
    "        data_csi_amp = mat_to_amp(data_mat)\n",
    "        # print(var_c, data_csi_amp.shape)\n",
    "        var_path_save = os.path.join(var_dir_amp, var_path.replace(\".mat\", \".npy\"))\n",
    "        with open(var_path_save, \"wb\") as var_file:\n",
    "            np.save(var_file, data_csi_amp)\n",
    "\n",
    "\n",
    "\n",
    "# # تنظیمات low-rank (بدون تغییر ورودی mat_to_amp)\n",
    "# LOW_RANK_ENERGY = 0.95   # مثلاً 95% انرژی\n",
    "# LOW_RANK_RANK = None     # اگر عدد بذاری (مثلاً 5)، به جای ENERGY از rank ثابت استفاده میشه\n",
    "\n",
    "# def _low_rank_approx(X, rank=None, energy=0.95):\n",
    "#     X = np.asarray(X)\n",
    "\n",
    "#     was_1d = (X.ndim == 1)\n",
    "#     if was_1d:\n",
    "#         X = X[:, None]\n",
    "\n",
    "#     U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "#     if rank is None:\n",
    "#         s2 = S**2\n",
    "#         cum = np.cumsum(s2) / (np.sum(s2) + 1e-12)\n",
    "#         rank = int(np.searchsorted(cum, energy) + 1)\n",
    "\n",
    "#     rank = max(1, min(rank, S.shape[0]))\n",
    "#     X_lr = (U[:, :rank] * S[:rank]) @ Vt[:rank, :]\n",
    "\n",
    "#     if was_1d:\n",
    "#         X_lr = X_lr[:, 0]\n",
    "\n",
    "#     return X_lr.astype(np.float32)\n",
    "\n",
    "# def mat_to_amp(data_mat):\n",
    "#     \"\"\"\n",
    "#     Calculate amplitude of raw WiFi CSI data, then return its low-rank approximation.\n",
    "#     (ورودی تابع تغییر نکرده)\n",
    "#     \"\"\"\n",
    "#     var_length = data_mat[\"trace\"].shape[0]\n",
    "#     data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "#     data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "#     # خروجی low-rank با همان ابعاد\n",
    "#     data_csi_amp_lr = _low_rank_approx(\n",
    "#         data_csi_amp,\n",
    "#         rank=LOW_RANK_RANK,\n",
    "#         energy=LOW_RANK_ENERGY\n",
    "#     )\n",
    "#     return data_csi_amp_lr\n",
    "\n",
    "\n",
    "\n",
    "# # تنظیمات sparsity (بدون تغییر ورودی mat_to_amp)\n",
    "# SPARSE_KEEP_RATIO = 0.10   # مثلا فقط 10% بزرگترین مقادیر نگه داشته بشن\n",
    "# SPARSE_MIN_ABS = None      # اگر عدد بذاری (مثلا 0.5)، به جای keep_ratio آستانه ثابت میشه\n",
    "\n",
    "# def _to_sparse(X, keep_ratio=0.10, min_abs=None):\n",
    "#     \"\"\"\n",
    "#     Convert X to a sparse representation by keeping only large-magnitude entries.\n",
    "#     Returns:\n",
    "#       - scipy.sparse.csr_matrix if SciPy is available\n",
    "#       - otherwise returns a dense array with many zeros (still \"sparse\" in content)\n",
    "#     \"\"\"\n",
    "#     X = np.asarray(X)\n",
    "#     flat = X.ravel()\n",
    "#     absflat = np.abs(flat)\n",
    "\n",
    "#     if flat.size == 0:\n",
    "#         return X.astype(np.float32)\n",
    "\n",
    "#     # انتخاب آستانه\n",
    "#     if min_abs is not None:\n",
    "#         thr = float(min_abs)\n",
    "#         mask = absflat >= thr\n",
    "#     else:\n",
    "#         k = int(np.ceil(keep_ratio * flat.size))\n",
    "#         k = max(1, min(k, flat.size))\n",
    "#         if k == flat.size:\n",
    "#             mask = np.ones_like(absflat, dtype=bool)\n",
    "#         else:\n",
    "#             thr = np.partition(absflat, -k)[-k]  # kth largest magnitude\n",
    "#             mask = absflat >= thr\n",
    "\n",
    "#     idx = np.nonzero(mask)[0]\n",
    "#     data = flat[idx].astype(np.float32)\n",
    "\n",
    "#     # اگر SciPy هست: sparse واقعی بساز\n",
    "#     try:\n",
    "#         # معمولاً تو Cell1 یا از قبل import شده؛ اگر هم نشده باشه اینجا تلاش می‌کنه.\n",
    "#         import scipy.sparse as sp\n",
    "\n",
    "#         if X.ndim == 1:\n",
    "#             rows = idx\n",
    "#             cols = np.zeros_like(rows)\n",
    "#             shape = (X.shape[0], 1)\n",
    "#         else:\n",
    "#             rows, cols = np.unravel_index(idx, X.shape)\n",
    "#             shape = X.shape\n",
    "\n",
    "#         return sp.coo_matrix((data, (rows, cols)), shape=shape).tocsr()\n",
    "\n",
    "#     except Exception:\n",
    "#         # fallback: آرایه‌ی dense با صفرهای زیاد\n",
    "#         out = np.zeros_like(flat, dtype=np.float32)\n",
    "#         out[idx] = data\n",
    "#         return out.reshape(X.shape)\n",
    "\n",
    "def mat_to_amp(data_mat):\n",
    "    \"\"\"\n",
    "    Calculate amplitude of raw WiFi CSI data, then return its sparse version.\n",
    "    (ورودی تابع تغییر نکرده)\n",
    "    \"\"\"\n",
    "    var_length = data_mat[\"trace\"].shape[0]\n",
    "    data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "    data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "    # خروجی sparse (CSR اگر SciPy باشد)\n",
    "    return _to_sparse(data_csi_amp, keep_ratio=SPARSE_KEEP_RATIO, min_abs=SPARSE_MIN_ABS)\n",
    "\n",
    "# تنظیمات RPCA (می‌تونی عوضشون کنی)\n",
    "RPCA_MAX_ITER = 500\n",
    "RPCA_TOL = 1e-7\n",
    "RPCA_RHO = 1.5\n",
    "RPCA_MU_INIT = None     # None یعنی خودکار\n",
    "RPCA_LAMBDA = None      # None یعنی 1/sqrt(max(m,n))\n",
    "\n",
    "def _soft_threshold(X, tau):\n",
    "    return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "def _svt(X, tau):\n",
    "    # Singular Value Thresholding\n",
    "    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    s_thr = np.maximum(s - tau, 0.0)\n",
    "    # اگر همه صفر شد، سریع برگرد\n",
    "    if np.all(s_thr == 0):\n",
    "        return np.zeros_like(X)\n",
    "    return (U * s_thr) @ Vt\n",
    "\n",
    "def _rpca_ialm(M, lam=None, mu=None, rho=1.5, max_iter=500, tol=1e-7):\n",
    "    \"\"\"\n",
    "    Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "    Decompose: M = L + S\n",
    "    Returns: L, S (same shape as M)\n",
    "    \"\"\"\n",
    "    M = M.astype(np.float64, copy=False)\n",
    "    m, n = M.shape\n",
    "\n",
    "    if lam is None:\n",
    "        lam = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "    # mu پیشنهادی (خودکار)\n",
    "    if mu is None:\n",
    "        # ||M||_2 تقریباً بزرگ‌ترین singular value است\n",
    "        norm2 = np.linalg.svd(M, compute_uv=False)[0] if M.size else 1.0\n",
    "        mu = 1.25 / (norm2 + 1e-12)\n",
    "\n",
    "    L = np.zeros_like(M)\n",
    "    S = np.zeros_like(M)\n",
    "    Y = np.zeros_like(M)\n",
    "\n",
    "    normM = np.linalg.norm(M, ord='fro') + 1e-12\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # L update\n",
    "        L = _svt(M - S + (1.0/mu)*Y, 1.0/mu)\n",
    "\n",
    "        # S update (sparse)\n",
    "        S = _soft_threshold(M - L + (1.0/mu)*Y, lam/mu)\n",
    "\n",
    "        # dual update\n",
    "        R = M - L - S\n",
    "        Y = Y + mu * R\n",
    "\n",
    "        # stop\n",
    "        err = np.linalg.norm(R, ord='fro') / normM\n",
    "        if err < tol:\n",
    "            break\n",
    "\n",
    "        mu *= rho\n",
    "\n",
    "    return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "# def mat_to_amp(data_mat):\n",
    "#     \"\"\"\n",
    "#     Calculate amplitude of raw WiFi CSI data, then return RPCA sparse component S.\n",
    "#     (ورودی تابع تغییر نکرده)\n",
    "#     \"\"\"\n",
    "#     var_length = data_mat[\"trace\"].shape[0]\n",
    "#     data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "#     data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "#     was_1d = (data_csi_amp.ndim == 1)\n",
    "#     M = data_csi_amp[:, None] if was_1d else data_csi_amp\n",
    "\n",
    "#     _, S = _rpca_ialm(\n",
    "#         M,\n",
    "#         lam=RPCA_LAMBDA,\n",
    "#         mu=RPCA_MU_INIT,\n",
    "#         rho=RPCA_RHO,\n",
    "#         max_iter=RPCA_MAX_ITER,\n",
    "#         tol=RPCA_TOL\n",
    "#     )\n",
    "\n",
    "#     if was_1d:\n",
    "#         S = S[:, 0]\n",
    "\n",
    "#     return S\n",
    "\n",
    "def mat_to_amp(data_mat):\n",
    "    \"\"\"\n",
    "    Calculate amplitude of raw WiFi CSI data, then return RPCA low-rank component L.\n",
    "    (ورودی تابع تغییر نکرده)\n",
    "    \"\"\"\n",
    "    var_length = data_mat[\"trace\"].shape[0]\n",
    "    data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "    data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "    was_1d = (data_csi_amp.ndim == 1)\n",
    "    M = data_csi_amp[:, None] if was_1d else data_csi_amp\n",
    "\n",
    "    L, _ = _rpca_ialm(\n",
    "        M,\n",
    "        lam=RPCA_LAMBDA,\n",
    "        mu=RPCA_MU_INIT,\n",
    "        rho=RPCA_RHO,\n",
    "        max_iter=RPCA_MAX_ITER,\n",
    "        tol=RPCA_TOL\n",
    "    )\n",
    "\n",
    "    if was_1d:\n",
    "        L = L[:, 0]\n",
    "\n",
    "    return L\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parse arguments from input.\n",
    "    \"\"\"\n",
    "    var_args = argparse.ArgumentParser()\n",
    "    var_args.add_argument(\"--dir_mat\", default=\"/kaggle/input/wimans/wifi_csi/mat\", type=str)\n",
    "    var_args.add_argument(\"--dir_amp\", default=\"/kaggle/input/wimans/wifi_csi/amp\", type=str)\n",
    "    return var_args.parse_args()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     var_args = parse_args()\n",
    "#     extract_csi_amp(var_dir_mat=var_args.dir_mat, var_dir_amp=var_args.dir_amp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52973b68",
   "metadata": {
    "papermill": {
     "duration": 0.006854,
     "end_time": "2025-12-28T20:25:02.514310",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.507456",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 5: that.py (WiFi-based Model THAT)\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "510711d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.529419Z",
     "iopub.status.busy": "2025-12-28T20:25:02.529213Z",
     "iopub.status.idle": "2025-12-28T20:25:02.553533Z",
     "shell.execute_reply": "2025-12-28T20:25:02.553053Z"
    },
    "papermill": {
     "duration": 0.033298,
     "end_time": "2025-12-28T20:25:02.554547",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.521249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          that.py\n",
    "[description]   implement and evaluate WiFi-based model THAT\n",
    "                https://github.com/windofshadow/THAT\n",
    "\"\"\"\n",
    "\n",
    "# All necessary libraries are imported in Cell 1.\n",
    "# from train import train   --> Defined in Cell 6.\n",
    "# from preset import preset --> Defined in Cell 2.\n",
    "\n",
    "class Gaussian_Position(torch.nn.Module):\n",
    "    def __init__(self, var_dim_feature, var_dim_time, var_num_gaussian=10):\n",
    "        super(Gaussian_Position, self).__init__()\n",
    "        var_embedding = torch.zeros([var_num_gaussian, var_dim_feature], dtype=torch.float)\n",
    "        self.var_embedding = torch.nn.Parameter(var_embedding, requires_grad=True)\n",
    "        torch.nn.init.xavier_uniform_(self.var_embedding)\n",
    "        var_position = torch.arange(0.0, var_dim_time).unsqueeze(1).repeat(1, var_num_gaussian)\n",
    "        self.var_position = torch.nn.Parameter(var_position, requires_grad=False)\n",
    "        var_mu = torch.arange(0.0, var_dim_time, var_dim_time/var_num_gaussian).unsqueeze(0)\n",
    "        self.var_mu = torch.nn.Parameter(var_mu, requires_grad=True)\n",
    "        var_sigma = torch.tensor([50.0] * var_num_gaussian).unsqueeze(0)\n",
    "        self.var_sigma = torch.nn.Parameter(var_sigma, requires_grad=True)\n",
    "\n",
    "    def calculate_pdf(self, var_position, var_mu, var_sigma):\n",
    "        var_pdf = var_position - var_mu\n",
    "        var_pdf = - var_pdf * var_pdf\n",
    "        var_pdf = var_pdf / var_sigma / var_sigma / 2\n",
    "        var_pdf = var_pdf - torch.log(var_sigma)\n",
    "        return var_pdf\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        var_pdf = self.calculate_pdf(self.var_position, self.var_mu, self.var_sigma)\n",
    "        var_pdf = torch.softmax(var_pdf, dim=-1)\n",
    "        var_position_encoding = torch.matmul(var_pdf, self.var_embedding)\n",
    "        var_output = var_input + var_position_encoding.unsqueeze(0)\n",
    "        return var_output\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, var_dim_feature, var_num_head=10, var_size_cnn=[1, 3, 5]):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layer_norm_0 = torch.nn.LayerNorm(var_dim_feature, eps=1e-6)\n",
    "        self.layer_attention = torch.nn.MultiheadAttention(var_dim_feature, var_num_head, batch_first=True)\n",
    "        self.layer_dropout_0 = torch.nn.Dropout(0.1)\n",
    "        self.layer_norm_1 = torch.nn.LayerNorm(var_dim_feature, 1e-6)\n",
    "        layer_cnn = []\n",
    "        for var_size in var_size_cnn:\n",
    "            layer = torch.nn.Sequential(\n",
    "                torch.nn.Conv1d(var_dim_feature, var_dim_feature, var_size, padding=\"same\"),\n",
    "                torch.nn.BatchNorm1d(var_dim_feature),\n",
    "                torch.nn.Dropout(0.1),\n",
    "                torch.nn.LeakyReLU()\n",
    "            )\n",
    "            layer_cnn.append(layer)\n",
    "        self.layer_cnn = torch.nn.ModuleList(layer_cnn)\n",
    "        self.layer_dropout_1 = torch.nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        var_t = var_input\n",
    "        var_t = self.layer_norm_0(var_t)\n",
    "        var_t, _ = self.layer_attention(var_t, var_t, var_t)\n",
    "        var_t = self.layer_dropout_0(var_t)\n",
    "        var_t = var_t + var_input\n",
    "        var_s = self.layer_norm_1(var_t)\n",
    "        var_s = torch.permute(var_s, (0, 2, 1))\n",
    "        var_c = torch.stack([layer(var_s) for layer in self.layer_cnn], dim=0)\n",
    "        var_s = torch.sum(var_c, dim=0) / len(self.layer_cnn)\n",
    "        var_s = self.layer_dropout_1(var_s)\n",
    "        var_s = torch.permute(var_s, (0, 2, 1))\n",
    "        var_output = var_s + var_t\n",
    "        return var_output\n",
    "\n",
    "class THAT(torch.nn.Module):\n",
    "    def __init__(self, var_x_shape, var_y_shape):\n",
    "        super(THAT, self).__init__()\n",
    "        var_dim_feature = var_x_shape[-1]\n",
    "        var_dim_time = var_x_shape[-2]\n",
    "        var_dim_output = var_y_shape[-1]\n",
    "        # Left branch\n",
    "        self.layer_left_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "        self.layer_left_gaussian = Gaussian_Position(var_dim_feature, var_dim_time // 20)\n",
    "        var_num_left = 4\n",
    "        var_dim_left = var_dim_feature\n",
    "        self.layer_left_encoder = torch.nn.ModuleList([\n",
    "            Encoder(var_dim_feature=var_dim_left, var_num_head=10, var_size_cnn=[1, 3, 5])\n",
    "            for _ in range(var_num_left)\n",
    "        ])\n",
    "        self.layer_left_norm = torch.nn.LayerNorm(var_dim_left, eps=1e-6)\n",
    "        self.layer_left_cnn_0 = torch.nn.Conv1d(in_channels=var_dim_left, out_channels=128, kernel_size=8)\n",
    "        self.layer_left_cnn_1 = torch.nn.Conv1d(in_channels=var_dim_left, out_channels=128, kernel_size=16)\n",
    "        self.layer_left_dropout = torch.nn.Dropout(0.5)\n",
    "        # Right branch\n",
    "        self.layer_right_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "        var_num_right = 1\n",
    "        var_dim_right = var_dim_time // 20\n",
    "        self.layer_right_encoder = torch.nn.ModuleList([\n",
    "            Encoder(var_dim_feature=var_dim_right, var_num_head=10, var_size_cnn=[1, 2, 3])\n",
    "            for _ in range(var_num_right)\n",
    "        ])\n",
    "        self.layer_right_norm = torch.nn.LayerNorm(var_dim_right, eps=1e-6)\n",
    "        self.layer_right_cnn_0 = torch.nn.Conv1d(in_channels=var_dim_right, out_channels=16, kernel_size=2)\n",
    "        self.layer_right_cnn_1 = torch.nn.Conv1d(in_channels=var_dim_right, out_channels=16, kernel_size=4)\n",
    "        self.layer_right_dropout = torch.nn.Dropout(0.5)\n",
    "        self.layer_leakyrelu = torch.nn.LeakyReLU()\n",
    "        self.layer_output = torch.nn.Linear(256 + 32, var_dim_output)\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        var_t = var_input  # shape: (batch_size, time_steps, features)\n",
    "        # Left branch\n",
    "        var_left = torch.permute(var_t, (0, 2, 1))\n",
    "        var_left = self.layer_left_pooling(var_left)\n",
    "        var_left = torch.permute(var_left, (0, 2, 1))\n",
    "        var_left = self.layer_left_gaussian(var_left)\n",
    "        for layer in self.layer_left_encoder:\n",
    "            var_left = layer(var_left)\n",
    "        var_left = self.layer_left_norm(var_left)\n",
    "        var_left = torch.permute(var_left, (0, 2, 1))\n",
    "        var_left_0 = self.layer_leakyrelu(self.layer_left_cnn_0(var_left))\n",
    "        var_left_1 = self.layer_leakyrelu(self.layer_left_cnn_1(var_left))\n",
    "        var_left_0 = torch.sum(var_left_0, dim=-1)\n",
    "        var_left_1 = torch.sum(var_left_1, dim=-1)\n",
    "        var_left = torch.concat([var_left_0, var_left_1], dim=-1)\n",
    "        var_left = self.layer_left_dropout(var_left)\n",
    "        # Right branch\n",
    "        var_right = torch.permute(var_t, (0, 2, 1))\n",
    "        var_right = self.layer_right_pooling(var_right)\n",
    "        for layer in self.layer_right_encoder:\n",
    "            var_right = layer(var_right)\n",
    "        var_right = self.layer_right_norm(var_right)\n",
    "        var_right = torch.permute(var_right, (0, 2, 1))\n",
    "        var_right_0 = self.layer_leakyrelu(self.layer_right_cnn_0(var_right))\n",
    "        var_right_1 = self.layer_leakyrelu(self.layer_right_cnn_1(var_right))\n",
    "        var_right_0 = torch.sum(var_right_0, dim=-1)\n",
    "        var_right_1 = torch.sum(var_right_1, dim=-1)\n",
    "        var_right = torch.concat([var_right_0, var_right_1], dim=-1)\n",
    "        var_right = self.layer_right_dropout(var_right)\n",
    "        # Concatenate branches\n",
    "        var_t = torch.concat([var_left, var_right], dim=-1)\n",
    "        var_output = self.layer_output(var_t)\n",
    "        return var_output\n",
    "\n",
    "def run_that(data_train_x, data_train_y, data_test_x, data_test_y, var_repeat=10, init_model=None):\n",
    "    \"\"\"\n",
    "    Run WiFi-based model THAT.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data_train_x = data_train_x.reshape(data_train_x.shape[0], data_train_x.shape[1], -1)\n",
    "    data_test_x = data_test_x.reshape(data_test_x.shape[0], data_test_x.shape[1], -1)\n",
    "    var_x_shape, var_y_shape = data_train_x[0].shape, data_train_y[0].reshape(-1).shape\n",
    "    data_train_set = TensorDataset(torch.from_numpy(data_train_x), torch.from_numpy(data_train_y))\n",
    "    data_test_set = TensorDataset(torch.from_numpy(data_test_x), torch.from_numpy(data_test_y))\n",
    "    \n",
    "    result = {}\n",
    "    result_accuracy = []\n",
    "    result_time_train = []\n",
    "    result_time_test = []\n",
    "    \n",
    "    # var_macs, var_params = get_model_complexity_info(THAT(var_x_shape, var_y_shape), var_x_shape, as_strings=False)\n",
    "    # print(\"Parameters:\", var_params, \"- FLOPs:\", var_macs * 2)\n",
    "    \n",
    "    for var_r in range(var_repeat):\n",
    "        print(\"Repeat\", var_r)\n",
    "        torch.random.manual_seed(var_r + 39)\n",
    "        if init_model is not None:\n",
    "            model_that = init_model\n",
    "            lr2 = preset[\"nn\"][\"lr\"] /10\n",
    "        else:\n",
    "            model_that = THAT(var_x_shape, var_y_shape).to(device)\n",
    "            lr2 = preset[\"nn\"][\"lr\"]\n",
    "\n",
    "        optimizer = torch.optim.Adam(model_that.parameters(), lr=lr2, weight_decay=0)\n",
    "        loss = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([4] * var_y_shape[-1]).to(device))\n",
    "        var_time_0 = time.time()\n",
    "        \n",
    "        # Train\n",
    "        var_best_weight = train(model=model_that, optimizer=optimizer, loss=loss, \n",
    "                                  data_train_set=data_train_set, data_test_set=data_test_set,\n",
    "                                  var_threshold=preset[\"nn\"][\"threshold\"],\n",
    "                                  var_batch_size=preset[\"nn\"][\"batch_size\"],\n",
    "                                  var_epochs=preset[\"nn\"][\"epoch\"],\n",
    "                                  device=device)\n",
    "        var_time_1 = time.time()\n",
    "        \n",
    "        # Test\n",
    "        model_that.load_state_dict(var_best_weight)\n",
    "        with torch.no_grad():\n",
    "            predict_test_y = model_that(torch.from_numpy(data_test_x).to(device))\n",
    "        predict_test_y = (torch.sigmoid(predict_test_y) > preset[\"nn\"][\"threshold\"]).float()\n",
    "        predict_test_y = predict_test_y.detach().cpu().numpy()\n",
    "        var_time_2 = time.time()\n",
    "        \n",
    "        # Evaluate\n",
    "        data_test_y_c = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "        predict_test_y_c = predict_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "        result_acc = accuracy_score(data_test_y_c.astype(int), predict_test_y_c.astype(int))\n",
    "        result_dict = classification_report(data_test_y_c, predict_test_y_c, digits=6, zero_division=0, output_dict=True)\n",
    "        result[\"repeat_\" + str(var_r)] = result_dict\n",
    "        result_accuracy.append(result_acc)\n",
    "        result_time_train.append(var_time_1 - var_time_0)\n",
    "        result_time_test.append(var_time_2 - var_time_1)\n",
    "        print(\"repeat_\" + str(var_r), result_accuracy)\n",
    "        print(result)\n",
    "    \n",
    "    result[\"accuracy\"] = {\"avg\": np.mean(result_accuracy), \"std\": np.std(result_accuracy)}\n",
    "    result[\"time_train\"] = {\"avg\": np.mean(result_time_train), \"std\": np.std(result_time_train)}\n",
    "    result[\"time_test\"] = {\"avg\": np.mean(result_time_test), \"std\": np.std(result_time_test)}\n",
    "    # result[\"complexity\"] = {\"parameter\": var_params, \"flops\": var_macs * 2}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1db7c",
   "metadata": {
    "papermill": {
     "duration": 0.006776,
     "end_time": "2025-12-28T20:25:02.568516",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.561740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2e52c33",
   "metadata": {
    "papermill": {
     "duration": 0.006943,
     "end_time": "2025-12-28T20:25:02.582394",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.575451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell7: for RESNET18 Model\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a21720e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.597433Z",
     "iopub.status.busy": "2025-12-28T20:25:02.597215Z",
     "iopub.status.idle": "2025-12-28T20:25:02.603575Z",
     "shell.execute_reply": "2025-12-28T20:25:02.602896Z"
    },
    "papermill": {
     "duration": 0.015439,
     "end_time": "2025-12-28T20:25:02.604724",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.589285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# import torch._dynamo\n",
    "# torch._dynamo.config.suppress_errors = True\n",
    "# import time\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "# import numpy as np\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# import torchvision.models as models\n",
    "# from copy import deepcopy\n",
    "\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "# torch._dynamo.config.cache_size_limit = 65536\n",
    "\n",
    "# # فرض می‌کنیم preset قبلاً تعریف شده باشه\n",
    "# # preset = { \"nn\": {\"lr\": 1e-3, \"epoch\": 10, \"batch_size\": 4, \"threshold\": 0.5}, ... }\n",
    "\n",
    "# class ResNet18Model(torch.nn.Module):\n",
    "#     def __init__(self, var_x_shape, var_y_shape):\n",
    "#         super(ResNet18Model, self).__init__()\n",
    "#         model_resnet = models.resnet18(weights=None)\n",
    "#         model_resnet.conv1 = torch.nn.Conv2d(1, 64, 7, 3, 2, bias=False)\n",
    "#         in_features_fc = model_resnet.fc.in_features  # معمولاً 512\n",
    "#         out_features_fc = var_y_shape[-1]\n",
    "#         model_resnet.fc = torch.nn.Linear(in_features_fc, out_features_fc)\n",
    "#         self.resnet = model_resnet\n",
    "\n",
    "#     def forward(self, var_input):\n",
    "#         var_input = var_input.reshape(var_input.size(0), 1, 3000, 270)\n",
    "#         return self.resnet(var_input)\n",
    "\n",
    "# def run_resnet(data_train_x, data_train_y, data_test_x, data_test_y, var_repeat=10, init_model=None):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     var_x_shape = data_train_x[0].shape\n",
    "#     var_y_shape = data_train_y[0].reshape(-1).shape\n",
    "\n",
    "#     # تغییر شکل داده‌ها روی CPU\n",
    "#     data_train_x = data_train_x.reshape(data_train_x.shape[0], 1, data_train_x.shape[1],\n",
    "#                                         data_train_x.shape[2]*data_train_x.shape[3]*data_train_x.shape[4])\n",
    "#     data_test_x  = data_test_x.reshape(data_test_x.shape[0], 1, data_test_x.shape[1],\n",
    "#                                        data_test_x.shape[2]*data_test_x.shape[3]*data_test_x.shape[4])\n",
    "    \n",
    "#     # دیتاست‌ها روی CPU\n",
    "#     data_train_set = TensorDataset(torch.from_numpy(data_train_x).float(),\n",
    "#                                    torch.from_numpy(data_train_y).float())\n",
    "#     data_test_set  = TensorDataset(torch.from_numpy(data_test_x).float(),\n",
    "#                                    torch.from_numpy(data_test_y).float())\n",
    "    \n",
    "#     result = {}\n",
    "#     result_accuracy = []\n",
    "#     result_time_train = []\n",
    "#     result_time_test = []\n",
    "    \n",
    "#     for var_r in range(var_repeat):\n",
    "#         print(\"Repeat\", var_r)\n",
    "#         torch.random.manual_seed(var_r + 39)\n",
    "        \n",
    "#         # ساخت مدل و انتقال به GPU\n",
    "#         if init_model is not None:\n",
    "#             model_resnet = init_model\n",
    "#             lr2 = preset[\"nn\"][\"lr\"] /10\n",
    "            \n",
    "#         else:\n",
    "#             model_resnet = ResNet18Model(var_x_shape, var_y_shape).to(device)\n",
    "#             lr2 = preset[\"nn\"][\"lr\"]\n",
    "\n",
    "#         optimizer = torch.optim.Adam(model_resnet.parameters(), lr=lr2, weight_decay=0)\n",
    "#         loss_func = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([6] * var_y_shape[-1]).to(device))\n",
    "        \n",
    "#         # تابع آموزش داخلی؛ دیتا روی CPU باقی می‌مونه و فقط هنگام محاسبه batch به GPU میره\n",
    "#         def train_inner():\n",
    "#             train_loader = DataLoader(data_train_set, preset[\"nn\"][\"batch_size\"], shuffle=True, pin_memory=False)\n",
    "#             test_loader = DataLoader(data_test_set, preset[\"nn\"][\"batch_size\"], shuffle=False, pin_memory=False)\n",
    "#             best_accuracy = 0\n",
    "#             best_weight = None\n",
    "            \n",
    "#             for epoch in range(preset[\"nn\"][\"epoch\"]):\n",
    "#                 t0 = time.time()\n",
    "#                 model_resnet.train()\n",
    "#                 # متغیرهای مربوط به آخرین batch آموزش\n",
    "#                 last_train_loss = None\n",
    "#                 last_train_acc = None\n",
    "#                 for batch in train_loader:\n",
    "#                     batch_x, batch_y = batch\n",
    "#                     batch_x = batch_x.to(device)\n",
    "#                     batch_y = batch_y.to(device)\n",
    "#                     outputs = model_resnet(batch_x)\n",
    "#                     loss_val = loss_func(outputs, batch_y.reshape(batch_y.shape[0], -1).float())\n",
    "#                     optimizer.zero_grad()\n",
    "#                     loss_val.backward()\n",
    "#                     optimizer.step()\n",
    "#                     last_train_loss = loss_val.item()\n",
    "#                     # محاسبه دقت آخرین batch آموزش\n",
    "#                     train_preds = (torch.sigmoid(outputs) > preset[\"nn\"][\"threshold\"]).float()\n",
    "#                     last_train_acc = accuracy_score(batch_y.reshape(batch_y.shape[0], -1).detach().cpu().numpy().astype(int),\n",
    "#                                                     train_preds.detach().cpu().numpy().astype(int))\n",
    "                \n",
    "#                 # ارزیابی روی دیتاست تست به صورت batch به batch\n",
    "#                 model_resnet.eval()\n",
    "#                 all_preds = []\n",
    "#                 all_labels = []\n",
    "#                 test_loss_val = None\n",
    "#                 with torch.no_grad():\n",
    "#                     for t_batch in test_loader:\n",
    "#                         t_x, t_y = t_batch\n",
    "#                         t_x = t_x.to(device)\n",
    "#                         outputs_test = model_resnet(t_x)\n",
    "#                         outputs_test = (torch.sigmoid(outputs_test) > preset[\"nn\"][\"threshold\"]).float()\n",
    "#                         all_preds.append(outputs_test.detach().cpu().numpy())\n",
    "#                         all_labels.append(t_y.cpu().numpy())  # اینجا تغییر دادیم\n",
    "#                 preds_cat = np.vstack(all_preds)\n",
    "#                 labels_cat = np.vstack(all_labels)\n",
    "#                 print(\"preds_cat\",preds_cat.shape)\n",
    "#                 # تبدیل به شکل (n, 6, 5)\n",
    "                \n",
    "#                 # preds_cat = preds_cat.reshape(-1, 6, 5)\n",
    "#                 # labels_cat = labels_cat.reshape(-1, 6, 5)\n",
    "\n",
    "#                 preds_cat = preds_cat.reshape(-1, 6)\n",
    "#                 labels_cat = labels_cat.reshape(-1, 6)\n",
    "                \n",
    "#                 # برای محاسبه دقت، مسطح می‌کنیم\n",
    "#                 test_acc = accuracy_score(labels_cat.reshape(labels_cat.shape[0], -1).astype(int),\n",
    "#                                           preds_cat.reshape(preds_cat.shape[0], -1).astype(int))\n",
    "#                 epoch_time = time.time() - t0\n",
    "#                 print(f\"Epoch {epoch}/{preset['nn']['epoch']} - \"\n",
    "#                       f\"Train Loss: {(last_train_loss if last_train_loss is not None else 0.0):.6f}, \"\n",
    "#                       f\"Train Acc: {(last_train_acc if last_train_acc is not None else 0.0):.6f}, \"\n",
    "#                       f\"Test Loss: {(test_loss_val if test_loss_val is not None else 0.0):.6f}, \"\n",
    "#                       f\"Test Acc: {(test_acc if test_acc is not None else 0.0):.6f} - \"\n",
    "#                       f\"Time: {epoch_time:.4f}s\")\n",
    "\n",
    "#                 if test_acc > best_accuracy:\n",
    "#                     best_accuracy = test_acc\n",
    "#                     print('-----***-----')\n",
    "#                     print(best_accuracy)\n",
    "#                     best_weight = deepcopy(model_resnet.state_dict())\n",
    "#             return best_weight\n",
    "        \n",
    "#         t0_run = time.time()\n",
    "#         best_weight = train_inner()\n",
    "#         t1_run = time.time()\n",
    "        \n",
    "#         torch.save(model_resnet.state_dict(), f\"{name_run}_model_final.pt\")\n",
    "#         model_resnet.load_state_dict(best_weight)\n",
    "#         torch.save(model_resnet.state_dict(), f\"{name_run}_best_model.pt\")\n",
    "\n",
    "#         # bad age niaz bod load koni\n",
    "#         # model_resnet = ResNet18Model(var_x_shape, var_y_shape).to(device)\n",
    "#         # model_resnet.load_state_dict(torch.load(\"resnet_model_repeat0.pt\"))\n",
    "#         # model_resnet.eval()\n",
    "\n",
    "        \n",
    "#         # ارزیابی نهایی مدل روی دیتاست تست (استفاده از batchهای کوچک)\n",
    "#         model_resnet.eval()\n",
    "#         all_preds = []\n",
    "#         test_loader_final = DataLoader(data_test_set, preset[\"nn\"][\"batch_size\"], shuffle=False, pin_memory=False)\n",
    "#         with torch.no_grad():\n",
    "#             for batch in test_loader_final:\n",
    "#                 batch_x, _ = batch\n",
    "#                 batch_x = batch_x.to(device)\n",
    "#                 all_preds.append(model_resnet(batch_x))\n",
    "#         preds_all = torch.cat(all_preds, dim=0)\n",
    "#         preds_final = (torch.sigmoid(preds_all) > preset[\"nn\"][\"threshold\"]).float().detach().cpu().numpy()\n",
    "#         t2_run = time.time()\n",
    "        \n",
    "#         data_test_y_np = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "#         preds_final = preds_final.reshape(-1, data_test_y.shape[-1])\n",
    "#         acc_final = accuracy_score(data_test_y_np.astype(int), preds_final.astype(int))\n",
    "#         result[f\"repeat_{var_r}\"] = {\"accuracy\": acc_final}\n",
    "#         result_accuracy.append(acc_final)\n",
    "#         result_time_train.append(t1_run - t0_run)\n",
    "#         result_time_test.append(t2_run - t1_run)\n",
    "#         print(\"Repeat\", var_r, \"Final Test Accuracy:\", acc_final)\n",
    "    \n",
    "#     result[\"accuracy\"] = {\"avg\": np.mean(result_accuracy), \"std\": np.std(result_accuracy)}\n",
    "#     result[\"time_train\"] = {\"avg\": np.mean(result_time_train), \"std\": np.std(result_time_train)}\n",
    "#     result[\"time_test\"] = {\"avg\": np.mean(result_time_test), \"std\": np.std(result_time_test)}\n",
    "#     return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4a7bdc",
   "metadata": {
    "papermill": {
     "duration": 0.007043,
     "end_time": "2025-12-28T20:25:02.618861",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.611818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 9: train.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c88626c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.633837Z",
     "iopub.status.busy": "2025-12-28T20:25:02.633646Z",
     "iopub.status.idle": "2025-12-28T20:25:03.312806Z",
     "shell.execute_reply": "2025-12-28T20:25:03.312047Z"
    },
    "papermill": {
     "duration": 0.688398,
     "end_time": "2025-12-28T20:25:03.314261",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.625863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          train.py\n",
    "[description]   function to train WiFi-based models\n",
    "\"\"\"\n",
    "\n",
    "# All necessary libraries are imported in Cell 1.\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch._dynamo.config.cache_size_limit = 65536\n",
    "\n",
    "def train(model, optimizer, loss, data_train_set, data_test_set, var_threshold, var_batch_size, var_epochs, device):\n",
    "    \"\"\"\n",
    "    Generic training function for WiFi-based models.\n",
    "    \"\"\"\n",
    "    # دیتا رو روی CPU نگه می‌داریم (pin_memory=False)\n",
    "    data_train_loader = DataLoader(data_train_set, var_batch_size, shuffle=True, pin_memory=False)\n",
    "    data_test_loader = DataLoader(data_test_set, batch_size=len(data_test_set), shuffle=False, pin_memory=False)\n",
    "    \n",
    "    var_best_accuracy = -1.0\n",
    "    var_best_weight   = deepcopy(model.state_dict())\n",
    "    \n",
    "    \n",
    "    for var_epoch in range(var_epochs):\n",
    "        var_time_e0 = time.time()\n",
    "        model.train()\n",
    "        for data_batch in data_train_loader:\n",
    "            data_batch_x, data_batch_y = data_batch\n",
    "            # انتقال موقتی داده به GPU فقط برای forward pass\n",
    "            data_batch_x = data_batch_x.to(device)\n",
    "            data_batch_y = data_batch_y.to(device)\n",
    "            predict_train_y = model(data_batch_x)\n",
    "            var_loss_train = loss(predict_train_y, data_batch_y.reshape(data_batch_y.shape[0], -1).float())\n",
    "            optimizer.zero_grad()\n",
    "            var_loss_train.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # محاسبه دقت روی آخرین batch و انتقال نتایج به CPU\n",
    "        predict_train_y = (torch.sigmoid(predict_train_y) > var_threshold).float()\n",
    "        data_batch_y = data_batch_y.detach().cpu().numpy()\n",
    "        predict_train_y = predict_train_y.detach().cpu().numpy()\n",
    "        \n",
    "        predict_train_y = predict_train_y.reshape(-1, data_batch_y.shape[-1])\n",
    "        data_batch_y = data_batch_y.reshape(-1, data_batch_y.shape[-1])\n",
    "        var_accuracy_train = accuracy_score(data_batch_y.astype(int), predict_train_y.astype(int))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            data_test_x, data_test_y = next(iter(data_test_loader))\n",
    "            # انتقال موقتی دیتا تست به GPU برای محاسبات\n",
    "            data_test_x = data_test_x.to(device)\n",
    "            data_test_y = data_test_y.to(device)\n",
    "            \n",
    "            predict_test_y = model(data_test_x)\n",
    "            var_loss_test = loss(predict_test_y, data_test_y.reshape(data_test_y.shape[0], -1).float())\n",
    "            \n",
    "            predict_test_y = (torch.sigmoid(predict_test_y) > var_threshold).float()\n",
    "            \n",
    "            # انتقال نتایج به CPU برای ارزیابی\n",
    "            data_test_y = data_test_y.detach().cpu().numpy()\n",
    "            predict_test_y = predict_test_y.detach().cpu().numpy()\n",
    "            \n",
    "            predict_test_y = predict_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            data_test_y = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            var_accuracy_test = accuracy_score(data_test_y.astype(int), predict_test_y.astype(int))\n",
    "        \n",
    "        print(f\"Epoch {var_epoch}/{var_epochs}\",\n",
    "              \"- %.6fs\"%(time.time() - var_time_e0),\n",
    "              \"- Loss %.6f\"%var_loss_train.cpu(),\n",
    "              \"- Accuracy %.6f\"%var_accuracy_train,\n",
    "              \"- Test Loss %.6f\"%var_loss_test.cpu(),\n",
    "              \"- Test Accuracy %.6f\"%var_accuracy_test)\n",
    "            \n",
    "        if var_accuracy_test > var_best_accuracy:\n",
    "            var_best_accuracy = var_accuracy_test\n",
    "            print('-----***-----')\n",
    "            print(var_best_accuracy)\n",
    "            var_best_weight = deepcopy(model.state_dict())\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{name_run}_model_final.pt\")\n",
    "    torch.save(var_best_weight, f\"{name_run}_best_model.pt\")\n",
    "\n",
    "    \n",
    "    return var_best_weight\n",
    "\n",
    "\n",
    "\n",
    "# === importsِ لازم را یک‌بار بالای فایل اضافه کن ===\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- تابع کمکی ----------\n",
    "def save_confusion_matrix(model, data_loader, threshold, device, pdf_path):\n",
    "    \"\"\"\n",
    "    Runs the model on `data_loader`, builds a confusion matrix and writes it to `pdf_path`.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "\n",
    "            preds = (torch.sigmoid(logits) > threshold).float().cpu().numpy().ravel()\n",
    "            yb    = yb.cpu().numpy().ravel()\n",
    "\n",
    "            y_true.extend(yb)\n",
    "            y_pred.extend(preds)\n",
    "\n",
    "    cm  = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    ConfusionMatrixDisplay(cm).plot(ax=ax)\n",
    "    ax.set_title(\"Confusion Matrix – Test\")\n",
    "\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "# ---------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc448aa7",
   "metadata": {
    "papermill": {
     "duration": 0.007028,
     "end_time": "2025-12-28T20:25:03.328945",
     "exception": false,
     "start_time": "2025-12-28T20:25:03.321917",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 11: run.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7397987c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:03.345189Z",
     "iopub.status.busy": "2025-12-28T20:25:03.344762Z",
     "iopub.status.idle": "2025-12-28T21:28:58.649696Z",
     "shell.execute_reply": "2025-12-28T21:28:58.648868Z"
    },
    "papermill": {
     "duration": 3835.314494,
     "end_time": "2025-12-28T21:28:58.650923",
     "exception": false,
     "start_time": "2025-12-28T20:25:03.336429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "\n",
      "[DEBUG] First loaded AMP sample: act_1_1\n",
      "[DEBUG] shape=(2835, 3, 3, 30), dtype=float32, complex=False\n",
      "[DEBUG] ==> Input is REAL -> amplitude-only (no true phase).\n",
      "\n",
      "Running model: THAT\n",
      "Repeat 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py:370: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1036.)\n",
      "  return F.conv1d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/300 - 5.977154s - Loss 2.266921 - Accuracy 0.125000 - Test Loss 1.255990 - Test Accuracy 0.312997\n",
      "-----***-----\n",
      "0.3129973474801061\n",
      "Epoch 1/300 - 5.029934s - Loss 0.871868 - Accuracy 0.364583 - Test Loss 0.728570 - Test Accuracy 0.542440\n",
      "-----***-----\n",
      "0.5424403183023873\n",
      "Epoch 2/300 - 5.020202s - Loss 0.786193 - Accuracy 0.302083 - Test Loss 0.577157 - Test Accuracy 0.555261\n",
      "-----***-----\n",
      "0.5552608311229\n",
      "Epoch 3/300 - 5.023449s - Loss 0.635026 - Accuracy 0.380208 - Test Loss 0.524987 - Test Accuracy 0.538019\n",
      "Epoch 4/300 - 5.019351s - Loss 0.636254 - Accuracy 0.427083 - Test Loss 0.526285 - Test Accuracy 0.560124\n",
      "-----***-----\n",
      "0.5601237842617153\n",
      "Epoch 5/300 - 5.021939s - Loss 0.634014 - Accuracy 0.411458 - Test Loss 0.534578 - Test Accuracy 0.536251\n",
      "Epoch 6/300 - 5.011743s - Loss 0.506476 - Accuracy 0.447917 - Test Loss 0.496913 - Test Accuracy 0.552608\n",
      "Epoch 7/300 - 5.017810s - Loss 0.571341 - Accuracy 0.432292 - Test Loss 0.493679 - Test Accuracy 0.562776\n",
      "-----***-----\n",
      "0.5627763041556145\n",
      "Epoch 8/300 - 5.016852s - Loss 0.579592 - Accuracy 0.406250 - Test Loss 0.504359 - Test Accuracy 0.559240\n",
      "Epoch 9/300 - 5.021710s - Loss 0.503881 - Accuracy 0.515625 - Test Loss 0.488207 - Test Accuracy 0.559240\n",
      "Epoch 10/300 - 5.013452s - Loss 0.501249 - Accuracy 0.473958 - Test Loss 0.473775 - Test Accuracy 0.534483\n",
      "Epoch 11/300 - 5.027814s - Loss 0.467407 - Accuracy 0.484375 - Test Loss 0.482302 - Test Accuracy 0.563218\n",
      "-----***-----\n",
      "0.5632183908045977\n",
      "Epoch 12/300 - 5.011752s - Loss 0.497353 - Accuracy 0.333333 - Test Loss 0.460064 - Test Accuracy 0.547745\n",
      "Epoch 13/300 - 5.018530s - Loss 0.418557 - Accuracy 0.515625 - Test Loss 0.476494 - Test Accuracy 0.571618\n",
      "-----***-----\n",
      "0.5716180371352785\n",
      "Epoch 14/300 - 5.017058s - Loss 0.422853 - Accuracy 0.458333 - Test Loss 0.467348 - Test Accuracy 0.549514\n",
      "Epoch 15/300 - 5.020563s - Loss 0.460393 - Accuracy 0.526042 - Test Loss 0.451014 - Test Accuracy 0.564103\n",
      "Epoch 16/300 - 5.025921s - Loss 0.437128 - Accuracy 0.541667 - Test Loss 0.450879 - Test Accuracy 0.545093\n",
      "Epoch 17/300 - 5.038182s - Loss 0.440776 - Accuracy 0.406250 - Test Loss 0.450537 - Test Accuracy 0.551282\n",
      "Epoch 18/300 - 5.024524s - Loss 0.414678 - Accuracy 0.458333 - Test Loss 0.447335 - Test Accuracy 0.536693\n",
      "Epoch 19/300 - 5.020939s - Loss 0.307613 - Accuracy 0.598958 - Test Loss 0.451237 - Test Accuracy 0.555261\n",
      "Epoch 20/300 - 5.012004s - Loss 0.398269 - Accuracy 0.468750 - Test Loss 0.450533 - Test Accuracy 0.564103\n",
      "Epoch 21/300 - 5.009404s - Loss 0.307501 - Accuracy 0.630208 - Test Loss 0.442728 - Test Accuracy 0.553492\n",
      "Epoch 22/300 - 5.014149s - Loss 0.287274 - Accuracy 0.640625 - Test Loss 0.453820 - Test Accuracy 0.562334\n",
      "Epoch 23/300 - 5.025581s - Loss 0.264688 - Accuracy 0.593750 - Test Loss 0.456327 - Test Accuracy 0.556145\n",
      "Epoch 24/300 - 5.012649s - Loss 0.260867 - Accuracy 0.614583 - Test Loss 0.445768 - Test Accuracy 0.556145\n",
      "Epoch 25/300 - 5.033488s - Loss 0.284865 - Accuracy 0.557292 - Test Loss 0.453093 - Test Accuracy 0.561892\n",
      "Epoch 26/300 - 5.019218s - Loss 0.263480 - Accuracy 0.619792 - Test Loss 0.444172 - Test Accuracy 0.572502\n",
      "-----***-----\n",
      "0.5725022104332449\n",
      "Epoch 27/300 - 5.028816s - Loss 0.269476 - Accuracy 0.635417 - Test Loss 0.464431 - Test Accuracy 0.564987\n",
      "Epoch 28/300 - 5.030845s - Loss 0.270137 - Accuracy 0.630208 - Test Loss 0.474343 - Test Accuracy 0.575155\n",
      "-----***-----\n",
      "0.5751547303271441\n",
      "Epoch 29/300 - 5.030998s - Loss 0.252676 - Accuracy 0.692708 - Test Loss 0.465077 - Test Accuracy 0.561892\n",
      "Epoch 30/300 - 5.045150s - Loss 0.226962 - Accuracy 0.619792 - Test Loss 0.491544 - Test Accuracy 0.580460\n",
      "-----***-----\n",
      "0.5804597701149425\n",
      "Epoch 31/300 - 5.045751s - Loss 0.238242 - Accuracy 0.708333 - Test Loss 0.513561 - Test Accuracy 0.591512\n",
      "-----***-----\n",
      "0.5915119363395226\n",
      "Epoch 32/300 - 5.038228s - Loss 0.170603 - Accuracy 0.697917 - Test Loss 0.533381 - Test Accuracy 0.594607\n",
      "-----***-----\n",
      "0.594606542882405\n",
      "Epoch 33/300 - 5.053696s - Loss 0.190464 - Accuracy 0.692708 - Test Loss 0.491337 - Test Accuracy 0.571618\n",
      "Epoch 34/300 - 5.031116s - Loss 0.209502 - Accuracy 0.630208 - Test Loss 0.524992 - Test Accuracy 0.583996\n",
      "Epoch 35/300 - 5.024488s - Loss 0.202740 - Accuracy 0.677083 - Test Loss 0.509270 - Test Accuracy 0.580902\n",
      "Epoch 36/300 - 5.032004s - Loss 0.169514 - Accuracy 0.666667 - Test Loss 0.572937 - Test Accuracy 0.579576\n",
      "Epoch 37/300 - 5.038080s - Loss 0.191540 - Accuracy 0.723958 - Test Loss 0.530126 - Test Accuracy 0.580018\n",
      "Epoch 38/300 - 5.023256s - Loss 0.170159 - Accuracy 0.796875 - Test Loss 0.565804 - Test Accuracy 0.583996\n",
      "Epoch 39/300 - 5.029516s - Loss 0.148336 - Accuracy 0.750000 - Test Loss 0.627950 - Test Accuracy 0.591954\n",
      "Epoch 40/300 - 5.026722s - Loss 0.179885 - Accuracy 0.703125 - Test Loss 0.598773 - Test Accuracy 0.591070\n",
      "Epoch 41/300 - 5.015097s - Loss 0.133209 - Accuracy 0.765625 - Test Loss 0.612463 - Test Accuracy 0.587091\n",
      "Epoch 42/300 - 5.023829s - Loss 0.133788 - Accuracy 0.729167 - Test Loss 0.610313 - Test Accuracy 0.596817\n",
      "-----***-----\n",
      "0.596816976127321\n",
      "Epoch 43/300 - 5.029401s - Loss 0.108873 - Accuracy 0.802083 - Test Loss 0.628794 - Test Accuracy 0.590628\n",
      "Epoch 44/300 - 5.018718s - Loss 0.098811 - Accuracy 0.854167 - Test Loss 0.649908 - Test Accuracy 0.597701\n",
      "-----***-----\n",
      "0.5977011494252874\n",
      "Epoch 45/300 - 5.031987s - Loss 0.100375 - Accuracy 0.791667 - Test Loss 0.691580 - Test Accuracy 0.599469\n",
      "-----***-----\n",
      "0.5994694960212201\n",
      "Epoch 46/300 - 5.034233s - Loss 0.144769 - Accuracy 0.770833 - Test Loss 0.718117 - Test Accuracy 0.592838\n",
      "Epoch 47/300 - 5.031183s - Loss 0.125266 - Accuracy 0.739583 - Test Loss 0.693356 - Test Accuracy 0.587975\n",
      "Epoch 48/300 - 5.015683s - Loss 0.094700 - Accuracy 0.859375 - Test Loss 0.858953 - Test Accuracy 0.588859\n",
      "Epoch 49/300 - 5.019836s - Loss 0.127777 - Accuracy 0.770833 - Test Loss 0.709462 - Test Accuracy 0.595933\n",
      "Epoch 50/300 - 5.016113s - Loss 0.120959 - Accuracy 0.765625 - Test Loss 0.757485 - Test Accuracy 0.591954\n",
      "Epoch 51/300 - 5.039857s - Loss 0.115938 - Accuracy 0.791667 - Test Loss 0.749434 - Test Accuracy 0.589302\n",
      "Epoch 52/300 - 5.022217s - Loss 0.091394 - Accuracy 0.833333 - Test Loss 0.708633 - Test Accuracy 0.593722\n",
      "Epoch 53/300 - 5.028290s - Loss 0.107117 - Accuracy 0.776042 - Test Loss 0.818480 - Test Accuracy 0.592838\n",
      "Epoch 54/300 - 5.015065s - Loss 0.079313 - Accuracy 0.838542 - Test Loss 0.838591 - Test Accuracy 0.589302\n",
      "Epoch 55/300 - 5.017558s - Loss 0.093816 - Accuracy 0.807292 - Test Loss 0.793619 - Test Accuracy 0.595933\n",
      "Epoch 56/300 - 5.028784s - Loss 0.100247 - Accuracy 0.817708 - Test Loss 0.815376 - Test Accuracy 0.593280\n",
      "Epoch 57/300 - 5.026901s - Loss 0.078660 - Accuracy 0.833333 - Test Loss 0.874088 - Test Accuracy 0.599912\n",
      "-----***-----\n",
      "0.5999115826702034\n",
      "Epoch 58/300 - 5.032928s - Loss 0.053374 - Accuracy 0.901042 - Test Loss 0.832844 - Test Accuracy 0.603890\n",
      "-----***-----\n",
      "0.6038903625110522\n",
      "Epoch 59/300 - 5.033600s - Loss 0.058664 - Accuracy 0.833333 - Test Loss 0.802162 - Test Accuracy 0.594607\n",
      "Epoch 60/300 - 5.023792s - Loss 0.086687 - Accuracy 0.864583 - Test Loss 0.759863 - Test Accuracy 0.592838\n",
      "Epoch 61/300 - 5.040059s - Loss 0.101898 - Accuracy 0.786458 - Test Loss 0.856619 - Test Accuracy 0.596375\n",
      "Epoch 62/300 - 5.031157s - Loss 0.063615 - Accuracy 0.869792 - Test Loss 0.853474 - Test Accuracy 0.599027\n",
      "Epoch 63/300 - 5.039455s - Loss 0.062083 - Accuracy 0.848958 - Test Loss 0.943155 - Test Accuracy 0.604332\n",
      "-----***-----\n",
      "0.6043324491600354\n",
      "Epoch 64/300 - 5.032955s - Loss 0.062521 - Accuracy 0.843750 - Test Loss 0.842079 - Test Accuracy 0.599912\n",
      "Epoch 65/300 - 5.028974s - Loss 0.079137 - Accuracy 0.848958 - Test Loss 0.974205 - Test Accuracy 0.586207\n",
      "Epoch 66/300 - 5.021039s - Loss 0.063287 - Accuracy 0.869792 - Test Loss 0.945453 - Test Accuracy 0.594164\n",
      "Epoch 67/300 - 5.028918s - Loss 0.087416 - Accuracy 0.869792 - Test Loss 1.011223 - Test Accuracy 0.599469\n",
      "Epoch 68/300 - 5.020443s - Loss 0.062129 - Accuracy 0.890625 - Test Loss 1.011284 - Test Accuracy 0.591070\n",
      "Epoch 69/300 - 5.039017s - Loss 0.059244 - Accuracy 0.885417 - Test Loss 0.987346 - Test Accuracy 0.599912\n",
      "Epoch 70/300 - 5.052627s - Loss 0.080903 - Accuracy 0.807292 - Test Loss 0.972785 - Test Accuracy 0.585765\n",
      "Epoch 71/300 - 5.027358s - Loss 0.052494 - Accuracy 0.906250 - Test Loss 1.191634 - Test Accuracy 0.587975\n",
      "Epoch 72/300 - 5.023113s - Loss 0.100226 - Accuracy 0.848958 - Test Loss 0.962684 - Test Accuracy 0.596375\n",
      "Epoch 73/300 - 5.029968s - Loss 0.077526 - Accuracy 0.828125 - Test Loss 1.115970 - Test Accuracy 0.591070\n",
      "Epoch 74/300 - 5.021007s - Loss 0.068008 - Accuracy 0.869792 - Test Loss 1.012452 - Test Accuracy 0.600796\n",
      "Epoch 75/300 - 5.034039s - Loss 0.064064 - Accuracy 0.864583 - Test Loss 1.092013 - Test Accuracy 0.591070\n",
      "Epoch 76/300 - 5.033806s - Loss 0.074077 - Accuracy 0.869792 - Test Loss 0.991226 - Test Accuracy 0.599469\n",
      "Epoch 77/300 - 5.032934s - Loss 0.050533 - Accuracy 0.916667 - Test Loss 0.999901 - Test Accuracy 0.591512\n",
      "Epoch 78/300 - 5.009307s - Loss 0.086218 - Accuracy 0.880208 - Test Loss 1.073198 - Test Accuracy 0.594607\n",
      "Epoch 79/300 - 5.035476s - Loss 0.062631 - Accuracy 0.864583 - Test Loss 1.079876 - Test Accuracy 0.599027\n",
      "Epoch 80/300 - 5.020221s - Loss 0.046683 - Accuracy 0.906250 - Test Loss 1.055581 - Test Accuracy 0.595933\n",
      "Epoch 81/300 - 5.028132s - Loss 0.049643 - Accuracy 0.911458 - Test Loss 1.030329 - Test Accuracy 0.592838\n",
      "Epoch 82/300 - 5.032313s - Loss 0.043144 - Accuracy 0.901042 - Test Loss 1.083468 - Test Accuracy 0.602122\n",
      "Epoch 83/300 - 5.056943s - Loss 0.070168 - Accuracy 0.880208 - Test Loss 1.117643 - Test Accuracy 0.598585\n",
      "Epoch 84/300 - 5.040173s - Loss 0.068003 - Accuracy 0.859375 - Test Loss 1.128962 - Test Accuracy 0.592838\n",
      "Epoch 85/300 - 5.031365s - Loss 0.065046 - Accuracy 0.895833 - Test Loss 1.238912 - Test Accuracy 0.595491\n",
      "Epoch 86/300 - 5.019100s - Loss 0.054333 - Accuracy 0.885417 - Test Loss 1.169905 - Test Accuracy 0.600354\n",
      "Epoch 87/300 - 5.027515s - Loss 0.071270 - Accuracy 0.854167 - Test Loss 1.063310 - Test Accuracy 0.600354\n",
      "Epoch 88/300 - 5.024097s - Loss 0.063383 - Accuracy 0.911458 - Test Loss 1.154215 - Test Accuracy 0.595049\n",
      "Epoch 89/300 - 5.028145s - Loss 0.041099 - Accuracy 0.921875 - Test Loss 1.188930 - Test Accuracy 0.597259\n",
      "Epoch 90/300 - 5.032532s - Loss 0.053625 - Accuracy 0.901042 - Test Loss 1.174878 - Test Accuracy 0.607427\n",
      "-----***-----\n",
      "0.6074270557029178\n",
      "Epoch 91/300 - 5.046845s - Loss 0.068406 - Accuracy 0.895833 - Test Loss 1.209031 - Test Accuracy 0.592396\n",
      "Epoch 92/300 - 5.019959s - Loss 0.063450 - Accuracy 0.859375 - Test Loss 1.153045 - Test Accuracy 0.592838\n",
      "Epoch 93/300 - 5.029810s - Loss 0.051561 - Accuracy 0.895833 - Test Loss 1.072150 - Test Accuracy 0.593280\n",
      "Epoch 94/300 - 5.025485s - Loss 0.071280 - Accuracy 0.854167 - Test Loss 1.225529 - Test Accuracy 0.604332\n",
      "Epoch 95/300 - 5.065578s - Loss 0.045182 - Accuracy 0.906250 - Test Loss 1.380187 - Test Accuracy 0.602122\n",
      "Epoch 96/300 - 5.029045s - Loss 0.051768 - Accuracy 0.880208 - Test Loss 1.320701 - Test Accuracy 0.597259\n",
      "Epoch 97/300 - 5.017116s - Loss 0.057218 - Accuracy 0.916667 - Test Loss 1.302427 - Test Accuracy 0.595491\n",
      "Epoch 98/300 - 5.042710s - Loss 0.038746 - Accuracy 0.937500 - Test Loss 1.277296 - Test Accuracy 0.601238\n",
      "Epoch 99/300 - 5.059725s - Loss 0.037054 - Accuracy 0.937500 - Test Loss 1.522555 - Test Accuracy 0.592838\n",
      "Epoch 100/300 - 5.049187s - Loss 0.038745 - Accuracy 0.906250 - Test Loss 1.440425 - Test Accuracy 0.599027\n",
      "Epoch 101/300 - 5.062747s - Loss 0.084754 - Accuracy 0.885417 - Test Loss 1.259528 - Test Accuracy 0.596817\n",
      "Epoch 102/300 - 5.019547s - Loss 0.065629 - Accuracy 0.895833 - Test Loss 1.325880 - Test Accuracy 0.591070\n",
      "Epoch 103/300 - 5.029916s - Loss 0.037813 - Accuracy 0.895833 - Test Loss 1.285696 - Test Accuracy 0.589744\n",
      "Epoch 104/300 - 5.018517s - Loss 0.081913 - Accuracy 0.859375 - Test Loss 1.369987 - Test Accuracy 0.593280\n",
      "Epoch 105/300 - 5.013345s - Loss 0.065401 - Accuracy 0.901042 - Test Loss 1.381237 - Test Accuracy 0.595491\n",
      "Epoch 106/300 - 5.019749s - Loss 0.068555 - Accuracy 0.833333 - Test Loss 1.276623 - Test Accuracy 0.594607\n",
      "Epoch 107/300 - 5.030438s - Loss 0.061818 - Accuracy 0.890625 - Test Loss 1.407193 - Test Accuracy 0.601680\n",
      "Epoch 108/300 - 5.024295s - Loss 0.064870 - Accuracy 0.906250 - Test Loss 1.385351 - Test Accuracy 0.591954\n",
      "Epoch 109/300 - 5.029083s - Loss 0.065449 - Accuracy 0.880208 - Test Loss 1.345879 - Test Accuracy 0.594164\n",
      "Epoch 110/300 - 5.041977s - Loss 0.078815 - Accuracy 0.869792 - Test Loss 1.413732 - Test Accuracy 0.593722\n",
      "Epoch 111/300 - 5.033654s - Loss 0.070363 - Accuracy 0.864583 - Test Loss 1.371507 - Test Accuracy 0.594164\n",
      "Epoch 112/300 - 5.047583s - Loss 0.041198 - Accuracy 0.885417 - Test Loss 1.414190 - Test Accuracy 0.590186\n",
      "Epoch 113/300 - 5.024830s - Loss 0.042747 - Accuracy 0.901042 - Test Loss 1.501584 - Test Accuracy 0.596375\n",
      "Epoch 114/300 - 5.289541s - Loss 0.064751 - Accuracy 0.932292 - Test Loss 1.472020 - Test Accuracy 0.589302\n",
      "Epoch 115/300 - 5.058897s - Loss 0.034049 - Accuracy 0.927083 - Test Loss 1.423708 - Test Accuracy 0.589744\n",
      "Epoch 116/300 - 5.080942s - Loss 0.053885 - Accuracy 0.875000 - Test Loss 1.377924 - Test Accuracy 0.591954\n",
      "Epoch 117/300 - 5.044635s - Loss 0.065130 - Accuracy 0.859375 - Test Loss 1.447086 - Test Accuracy 0.592396\n",
      "Epoch 118/300 - 5.077790s - Loss 0.083112 - Accuracy 0.869792 - Test Loss 1.356050 - Test Accuracy 0.596817\n",
      "Epoch 119/300 - 5.046912s - Loss 0.120217 - Accuracy 0.864583 - Test Loss 1.454099 - Test Accuracy 0.595491\n",
      "Epoch 120/300 - 5.046277s - Loss 0.066619 - Accuracy 0.906250 - Test Loss 1.492985 - Test Accuracy 0.594607\n",
      "Epoch 121/300 - 5.027524s - Loss 0.113519 - Accuracy 0.822917 - Test Loss 1.426530 - Test Accuracy 0.594164\n",
      "Epoch 122/300 - 5.043104s - Loss 0.072500 - Accuracy 0.875000 - Test Loss 1.545699 - Test Accuracy 0.592838\n",
      "Epoch 123/300 - 5.044445s - Loss 0.066610 - Accuracy 0.906250 - Test Loss 1.690065 - Test Accuracy 0.593280\n",
      "Epoch 124/300 - 5.042567s - Loss 0.064392 - Accuracy 0.901042 - Test Loss 1.494101 - Test Accuracy 0.595049\n",
      "Epoch 125/300 - 5.039541s - Loss 0.069019 - Accuracy 0.859375 - Test Loss 1.597359 - Test Accuracy 0.593722\n",
      "Epoch 126/300 - 5.035940s - Loss 0.066954 - Accuracy 0.859375 - Test Loss 1.503912 - Test Accuracy 0.598143\n",
      "Epoch 127/300 - 5.037805s - Loss 0.046236 - Accuracy 0.906250 - Test Loss 1.583057 - Test Accuracy 0.594164\n",
      "Epoch 128/300 - 5.035559s - Loss 0.046727 - Accuracy 0.901042 - Test Loss 1.543915 - Test Accuracy 0.598143\n",
      "Epoch 129/300 - 5.034978s - Loss 0.045431 - Accuracy 0.906250 - Test Loss 1.461127 - Test Accuracy 0.594164\n",
      "Epoch 130/300 - 5.055102s - Loss 0.124404 - Accuracy 0.822917 - Test Loss 1.475424 - Test Accuracy 0.595933\n",
      "Epoch 131/300 - 5.025297s - Loss 0.028756 - Accuracy 0.911458 - Test Loss 1.585780 - Test Accuracy 0.591070\n",
      "Epoch 132/300 - 5.043194s - Loss 0.038872 - Accuracy 0.921875 - Test Loss 1.555089 - Test Accuracy 0.595933\n",
      "Epoch 133/300 - 5.025506s - Loss 0.078531 - Accuracy 0.901042 - Test Loss 1.733357 - Test Accuracy 0.591512\n",
      "Epoch 134/300 - 5.038161s - Loss 0.053397 - Accuracy 0.947917 - Test Loss 1.844228 - Test Accuracy 0.595933\n",
      "Epoch 135/300 - 5.033095s - Loss 0.053920 - Accuracy 0.895833 - Test Loss 1.733088 - Test Accuracy 0.587533\n",
      "Epoch 136/300 - 5.029563s - Loss 0.043691 - Accuracy 0.947917 - Test Loss 1.576175 - Test Accuracy 0.597259\n",
      "Epoch 137/300 - 5.032389s - Loss 0.028322 - Accuracy 0.937500 - Test Loss 1.555688 - Test Accuracy 0.602564\n",
      "Epoch 138/300 - 5.041327s - Loss 0.077781 - Accuracy 0.875000 - Test Loss 1.491332 - Test Accuracy 0.595049\n",
      "Epoch 139/300 - 5.039064s - Loss 0.064773 - Accuracy 0.932292 - Test Loss 1.753327 - Test Accuracy 0.595491\n",
      "Epoch 140/300 - 5.037634s - Loss 0.029322 - Accuracy 0.937500 - Test Loss 1.658131 - Test Accuracy 0.597259\n",
      "Epoch 141/300 - 5.039407s - Loss 0.076802 - Accuracy 0.843750 - Test Loss 1.596795 - Test Accuracy 0.600354\n",
      "Epoch 142/300 - 5.039157s - Loss 0.041628 - Accuracy 0.895833 - Test Loss 1.708353 - Test Accuracy 0.592396\n",
      "Epoch 143/300 - 5.036182s - Loss 0.073552 - Accuracy 0.927083 - Test Loss 1.837110 - Test Accuracy 0.594164\n",
      "Epoch 144/300 - 5.032064s - Loss 0.021384 - Accuracy 0.953125 - Test Loss 1.930145 - Test Accuracy 0.590186\n",
      "Epoch 145/300 - 5.031872s - Loss 0.044187 - Accuracy 0.895833 - Test Loss 1.884982 - Test Accuracy 0.592396\n",
      "Epoch 146/300 - 5.034591s - Loss 0.051784 - Accuracy 0.880208 - Test Loss 1.799250 - Test Accuracy 0.591512\n",
      "Epoch 147/300 - 5.027958s - Loss 0.096549 - Accuracy 0.885417 - Test Loss 2.033747 - Test Accuracy 0.592396\n",
      "Epoch 148/300 - 5.039895s - Loss 0.044409 - Accuracy 0.885417 - Test Loss 1.702740 - Test Accuracy 0.605659\n",
      "Epoch 149/300 - 5.031331s - Loss 0.069639 - Accuracy 0.927083 - Test Loss 1.944447 - Test Accuracy 0.591070\n",
      "Epoch 150/300 - 5.035650s - Loss 0.044624 - Accuracy 0.906250 - Test Loss 1.930422 - Test Accuracy 0.596817\n",
      "Epoch 151/300 - 5.029242s - Loss 0.048076 - Accuracy 0.885417 - Test Loss 1.875108 - Test Accuracy 0.591512\n",
      "Epoch 152/300 - 5.040001s - Loss 0.069525 - Accuracy 0.875000 - Test Loss 1.738333 - Test Accuracy 0.599027\n",
      "Epoch 153/300 - 5.027273s - Loss 0.114885 - Accuracy 0.901042 - Test Loss 1.866028 - Test Accuracy 0.596375\n",
      "Epoch 154/300 - 5.030033s - Loss 0.047754 - Accuracy 0.901042 - Test Loss 1.804442 - Test Accuracy 0.591954\n",
      "Epoch 155/300 - 5.029131s - Loss 0.054226 - Accuracy 0.906250 - Test Loss 1.840051 - Test Accuracy 0.591512\n",
      "Epoch 156/300 - 5.029497s - Loss 0.063771 - Accuracy 0.880208 - Test Loss 1.835607 - Test Accuracy 0.594164\n",
      "Epoch 157/300 - 5.029029s - Loss 0.066374 - Accuracy 0.901042 - Test Loss 2.078282 - Test Accuracy 0.592838\n",
      "Epoch 158/300 - 5.034677s - Loss 0.048994 - Accuracy 0.890625 - Test Loss 2.041862 - Test Accuracy 0.591954\n",
      "Epoch 159/300 - 5.024621s - Loss 0.029906 - Accuracy 0.937500 - Test Loss 1.875545 - Test Accuracy 0.589744\n",
      "Epoch 160/300 - 5.038815s - Loss 0.022710 - Accuracy 0.916667 - Test Loss 1.674349 - Test Accuracy 0.591070\n",
      "Epoch 161/300 - 5.048293s - Loss 0.048543 - Accuracy 0.937500 - Test Loss 2.168444 - Test Accuracy 0.589744\n",
      "Epoch 162/300 - 5.040693s - Loss 0.064127 - Accuracy 0.880208 - Test Loss 2.142421 - Test Accuracy 0.587975\n",
      "Epoch 163/300 - 5.039541s - Loss 0.068104 - Accuracy 0.890625 - Test Loss 2.313356 - Test Accuracy 0.582228\n",
      "Epoch 164/300 - 5.064204s - Loss 0.099811 - Accuracy 0.885417 - Test Loss 1.904070 - Test Accuracy 0.587975\n",
      "Epoch 165/300 - 5.032924s - Loss 0.039304 - Accuracy 0.932292 - Test Loss 1.974634 - Test Accuracy 0.585323\n",
      "Epoch 166/300 - 5.043510s - Loss 0.097228 - Accuracy 0.864583 - Test Loss 1.822637 - Test Accuracy 0.593280\n",
      "Epoch 167/300 - 5.036498s - Loss 0.047458 - Accuracy 0.932292 - Test Loss 1.886734 - Test Accuracy 0.591070\n",
      "Epoch 168/300 - 5.054730s - Loss 0.110670 - Accuracy 0.880208 - Test Loss 1.991646 - Test Accuracy 0.589744\n",
      "Epoch 169/300 - 5.035453s - Loss 0.049185 - Accuracy 0.911458 - Test Loss 2.003344 - Test Accuracy 0.592838\n",
      "Epoch 170/300 - 5.051346s - Loss 0.075327 - Accuracy 0.890625 - Test Loss 1.948702 - Test Accuracy 0.597701\n",
      "Epoch 171/300 - 5.046839s - Loss 0.056154 - Accuracy 0.895833 - Test Loss 1.975989 - Test Accuracy 0.583112\n",
      "Epoch 172/300 - 5.050152s - Loss 0.088852 - Accuracy 0.906250 - Test Loss 2.117570 - Test Accuracy 0.599912\n",
      "Epoch 173/300 - 5.022902s - Loss 0.064525 - Accuracy 0.906250 - Test Loss 1.998127 - Test Accuracy 0.588417\n",
      "Epoch 174/300 - 5.034486s - Loss 0.069910 - Accuracy 0.854167 - Test Loss 1.955386 - Test Accuracy 0.587533\n",
      "Epoch 175/300 - 5.039890s - Loss 0.060934 - Accuracy 0.875000 - Test Loss 1.959714 - Test Accuracy 0.590186\n",
      "Epoch 176/300 - 5.046607s - Loss 0.041184 - Accuracy 0.875000 - Test Loss 1.899042 - Test Accuracy 0.593280\n",
      "Epoch 177/300 - 5.032995s - Loss 0.041024 - Accuracy 0.932292 - Test Loss 1.866723 - Test Accuracy 0.595933\n",
      "Epoch 178/300 - 5.046176s - Loss 0.083707 - Accuracy 0.869792 - Test Loss 2.007757 - Test Accuracy 0.594607\n",
      "Epoch 179/300 - 5.026810s - Loss 0.089945 - Accuracy 0.890625 - Test Loss 1.901482 - Test Accuracy 0.601680\n",
      "Epoch 180/300 - 5.033915s - Loss 0.094718 - Accuracy 0.869792 - Test Loss 2.049683 - Test Accuracy 0.599027\n",
      "Epoch 181/300 - 5.034122s - Loss 0.074324 - Accuracy 0.890625 - Test Loss 1.948108 - Test Accuracy 0.598585\n",
      "Epoch 182/300 - 5.027743s - Loss 0.056660 - Accuracy 0.901042 - Test Loss 1.836527 - Test Accuracy 0.591070\n",
      "Epoch 183/300 - 5.028164s - Loss 0.067780 - Accuracy 0.911458 - Test Loss 2.083545 - Test Accuracy 0.594164\n",
      "Epoch 184/300 - 5.048754s - Loss 0.094871 - Accuracy 0.890625 - Test Loss 1.951602 - Test Accuracy 0.588417\n",
      "Epoch 185/300 - 5.038795s - Loss 0.087365 - Accuracy 0.927083 - Test Loss 2.346021 - Test Accuracy 0.592838\n",
      "Epoch 186/300 - 5.038021s - Loss 0.060983 - Accuracy 0.901042 - Test Loss 2.095417 - Test Accuracy 0.597701\n",
      "Epoch 187/300 - 5.044309s - Loss 0.036701 - Accuracy 0.916667 - Test Loss 2.024972 - Test Accuracy 0.590186\n",
      "Epoch 188/300 - 5.047159s - Loss 0.039019 - Accuracy 0.921875 - Test Loss 2.174170 - Test Accuracy 0.595491\n",
      "Epoch 189/300 - 5.037901s - Loss 0.057311 - Accuracy 0.958333 - Test Loss 2.316599 - Test Accuracy 0.596817\n",
      "Epoch 190/300 - 5.052384s - Loss 0.080885 - Accuracy 0.895833 - Test Loss 2.251586 - Test Accuracy 0.590628\n",
      "Epoch 191/300 - 5.039328s - Loss 0.093436 - Accuracy 0.901042 - Test Loss 2.259050 - Test Accuracy 0.586649\n",
      "Epoch 192/300 - 5.051360s - Loss 0.047359 - Accuracy 0.942708 - Test Loss 2.257934 - Test Accuracy 0.589302\n",
      "Epoch 193/300 - 5.024283s - Loss 0.056579 - Accuracy 0.885417 - Test Loss 1.958488 - Test Accuracy 0.592838\n",
      "Epoch 194/300 - 5.046040s - Loss 0.058119 - Accuracy 0.906250 - Test Loss 2.209615 - Test Accuracy 0.596817\n",
      "Epoch 195/300 - 5.032684s - Loss 0.071439 - Accuracy 0.885417 - Test Loss 2.313290 - Test Accuracy 0.590628\n",
      "Epoch 196/300 - 5.120070s - Loss 0.045902 - Accuracy 0.901042 - Test Loss 2.158628 - Test Accuracy 0.600796\n",
      "Epoch 197/300 - 5.052495s - Loss 0.038967 - Accuracy 0.937500 - Test Loss 2.410358 - Test Accuracy 0.594607\n",
      "Epoch 198/300 - 5.070038s - Loss 0.043714 - Accuracy 0.901042 - Test Loss 2.362652 - Test Accuracy 0.599912\n",
      "Epoch 199/300 - 5.041069s - Loss 0.102681 - Accuracy 0.895833 - Test Loss 2.406237 - Test Accuracy 0.600796\n",
      "Epoch 200/300 - 5.040035s - Loss 0.051549 - Accuracy 0.916667 - Test Loss 2.684926 - Test Accuracy 0.588417\n",
      "Epoch 201/300 - 5.046318s - Loss 0.055792 - Accuracy 0.901042 - Test Loss 2.235497 - Test Accuracy 0.594607\n",
      "Epoch 202/300 - 5.050021s - Loss 0.046635 - Accuracy 0.927083 - Test Loss 2.189466 - Test Accuracy 0.599469\n",
      "Epoch 203/300 - 5.025468s - Loss 0.063634 - Accuracy 0.921875 - Test Loss 2.316795 - Test Accuracy 0.596817\n",
      "Epoch 204/300 - 5.029712s - Loss 0.043200 - Accuracy 0.953125 - Test Loss 2.543488 - Test Accuracy 0.593722\n",
      "Epoch 205/300 - 5.030135s - Loss 0.034537 - Accuracy 0.942708 - Test Loss 2.541715 - Test Accuracy 0.597259\n",
      "Epoch 206/300 - 5.057691s - Loss 0.043122 - Accuracy 0.937500 - Test Loss 2.312163 - Test Accuracy 0.595491\n",
      "Epoch 207/300 - 5.038382s - Loss 0.037402 - Accuracy 0.895833 - Test Loss 2.360273 - Test Accuracy 0.599027\n",
      "Epoch 208/300 - 5.044236s - Loss 0.035507 - Accuracy 0.942708 - Test Loss 2.517435 - Test Accuracy 0.598585\n",
      "Epoch 209/300 - 5.032956s - Loss 0.029816 - Accuracy 0.942708 - Test Loss 2.192877 - Test Accuracy 0.590628\n",
      "Epoch 210/300 - 5.054801s - Loss 0.092033 - Accuracy 0.895833 - Test Loss 2.381785 - Test Accuracy 0.592838\n",
      "Epoch 211/300 - 5.035367s - Loss 0.083553 - Accuracy 0.932292 - Test Loss 2.218492 - Test Accuracy 0.595933\n",
      "Epoch 212/300 - 5.089785s - Loss 0.049865 - Accuracy 0.911458 - Test Loss 2.433509 - Test Accuracy 0.592838\n",
      "Epoch 213/300 - 5.046099s - Loss 0.056174 - Accuracy 0.901042 - Test Loss 2.390526 - Test Accuracy 0.592396\n",
      "Epoch 214/300 - 5.057878s - Loss 0.059676 - Accuracy 0.911458 - Test Loss 2.441064 - Test Accuracy 0.595933\n",
      "Epoch 215/300 - 5.026439s - Loss 0.057557 - Accuracy 0.921875 - Test Loss 2.477974 - Test Accuracy 0.591954\n",
      "Epoch 216/300 - 5.036650s - Loss 0.044970 - Accuracy 0.895833 - Test Loss 2.505269 - Test Accuracy 0.595933\n",
      "Epoch 217/300 - 5.031548s - Loss 0.070426 - Accuracy 0.937500 - Test Loss 2.534236 - Test Accuracy 0.594607\n",
      "Epoch 218/300 - 5.050717s - Loss 0.084127 - Accuracy 0.885417 - Test Loss 2.374055 - Test Accuracy 0.594164\n",
      "Epoch 219/300 - 5.032321s - Loss 0.037774 - Accuracy 0.932292 - Test Loss 2.376339 - Test Accuracy 0.599027\n",
      "Epoch 220/300 - 5.049088s - Loss 0.036131 - Accuracy 0.911458 - Test Loss 2.338683 - Test Accuracy 0.600796\n",
      "Epoch 221/300 - 5.041511s - Loss 0.041225 - Accuracy 0.901042 - Test Loss 2.468705 - Test Accuracy 0.588859\n",
      "Epoch 222/300 - 5.041159s - Loss 0.071831 - Accuracy 0.911458 - Test Loss 2.489395 - Test Accuracy 0.596375\n",
      "Epoch 223/300 - 5.028009s - Loss 0.080372 - Accuracy 0.890625 - Test Loss 2.253126 - Test Accuracy 0.593280\n",
      "Epoch 224/300 - 5.021538s - Loss 0.049464 - Accuracy 0.921875 - Test Loss 2.519166 - Test Accuracy 0.591512\n",
      "Epoch 225/300 - 5.040730s - Loss 0.048473 - Accuracy 0.921875 - Test Loss 2.510258 - Test Accuracy 0.597259\n",
      "Epoch 226/300 - 5.030442s - Loss 0.039654 - Accuracy 0.927083 - Test Loss 2.533694 - Test Accuracy 0.591954\n",
      "Epoch 227/300 - 5.037377s - Loss 0.042895 - Accuracy 0.937500 - Test Loss 2.591473 - Test Accuracy 0.597259\n",
      "Epoch 228/300 - 5.033915s - Loss 0.054508 - Accuracy 0.895833 - Test Loss 2.417480 - Test Accuracy 0.591070\n",
      "Epoch 229/300 - 5.047760s - Loss 0.036367 - Accuracy 0.916667 - Test Loss 2.510807 - Test Accuracy 0.597259\n",
      "Epoch 230/300 - 5.035531s - Loss 0.083350 - Accuracy 0.838542 - Test Loss 2.410883 - Test Accuracy 0.605659\n",
      "Epoch 231/300 - 5.050903s - Loss 0.085763 - Accuracy 0.906250 - Test Loss 2.296704 - Test Accuracy 0.595049\n",
      "Epoch 232/300 - 5.037023s - Loss 0.047756 - Accuracy 0.927083 - Test Loss 2.461914 - Test Accuracy 0.596817\n",
      "Epoch 233/300 - 5.048799s - Loss 0.051352 - Accuracy 0.927083 - Test Loss 2.426850 - Test Accuracy 0.595491\n",
      "Epoch 234/300 - 5.034363s - Loss 0.074556 - Accuracy 0.901042 - Test Loss 2.591315 - Test Accuracy 0.588417\n",
      "Epoch 235/300 - 5.068569s - Loss 0.030331 - Accuracy 0.942708 - Test Loss 2.555372 - Test Accuracy 0.600354\n",
      "Epoch 236/300 - 5.024024s - Loss 0.054549 - Accuracy 0.942708 - Test Loss 2.675627 - Test Accuracy 0.592396\n",
      "Epoch 237/300 - 5.084147s - Loss 0.065450 - Accuracy 0.885417 - Test Loss 2.581317 - Test Accuracy 0.598585\n",
      "Epoch 238/300 - 5.028148s - Loss 0.023535 - Accuracy 0.937500 - Test Loss 2.455530 - Test Accuracy 0.599469\n",
      "Epoch 239/300 - 5.060571s - Loss 0.048030 - Accuracy 0.927083 - Test Loss 2.455294 - Test Accuracy 0.601238\n",
      "Epoch 240/300 - 5.023648s - Loss 0.070511 - Accuracy 0.880208 - Test Loss 2.203179 - Test Accuracy 0.592396\n",
      "Epoch 241/300 - 5.057350s - Loss 0.059321 - Accuracy 0.927083 - Test Loss 2.691875 - Test Accuracy 0.594607\n",
      "Epoch 242/300 - 5.040474s - Loss 0.047327 - Accuracy 0.906250 - Test Loss 2.650369 - Test Accuracy 0.595491\n",
      "Epoch 243/300 - 5.054952s - Loss 0.056576 - Accuracy 0.921875 - Test Loss 2.440313 - Test Accuracy 0.599027\n",
      "Epoch 244/300 - 5.032368s - Loss 0.059304 - Accuracy 0.927083 - Test Loss 2.264024 - Test Accuracy 0.606543\n",
      "Epoch 245/300 - 5.049410s - Loss 0.028357 - Accuracy 0.947917 - Test Loss 2.474970 - Test Accuracy 0.603890\n",
      "Epoch 246/300 - 5.028870s - Loss 0.020185 - Accuracy 0.963542 - Test Loss 2.574047 - Test Accuracy 0.592838\n",
      "Epoch 247/300 - 5.031058s - Loss 0.085143 - Accuracy 0.906250 - Test Loss 2.430601 - Test Accuracy 0.606543\n",
      "Epoch 248/300 - 5.020978s - Loss 0.048460 - Accuracy 0.906250 - Test Loss 2.560127 - Test Accuracy 0.596817\n",
      "Epoch 249/300 - 5.023840s - Loss 0.042580 - Accuracy 0.947917 - Test Loss 2.693817 - Test Accuracy 0.594164\n",
      "Epoch 250/300 - 5.032127s - Loss 0.045932 - Accuracy 0.947917 - Test Loss 2.714824 - Test Accuracy 0.603006\n",
      "Epoch 251/300 - 5.041323s - Loss 0.036487 - Accuracy 0.916667 - Test Loss 2.482337 - Test Accuracy 0.595049\n",
      "Epoch 252/300 - 5.031158s - Loss 0.065297 - Accuracy 0.911458 - Test Loss 2.496236 - Test Accuracy 0.598585\n",
      "Epoch 253/300 - 5.060626s - Loss 0.076562 - Accuracy 0.932292 - Test Loss 2.734182 - Test Accuracy 0.599027\n",
      "Epoch 254/300 - 5.040689s - Loss 0.054903 - Accuracy 0.911458 - Test Loss 2.645260 - Test Accuracy 0.595049\n",
      "Epoch 255/300 - 5.041894s - Loss 0.040839 - Accuracy 0.942708 - Test Loss 2.553123 - Test Accuracy 0.603890\n",
      "Epoch 256/300 - 5.033783s - Loss 0.062778 - Accuracy 0.958333 - Test Loss 2.617176 - Test Accuracy 0.590186\n",
      "Epoch 257/300 - 5.053288s - Loss 0.033433 - Accuracy 0.942708 - Test Loss 2.589762 - Test Accuracy 0.600354\n",
      "Epoch 258/300 - 5.036675s - Loss 0.031403 - Accuracy 0.937500 - Test Loss 2.417852 - Test Accuracy 0.602564\n",
      "Epoch 259/300 - 5.042497s - Loss 0.018610 - Accuracy 0.958333 - Test Loss 2.946013 - Test Accuracy 0.592396\n",
      "Epoch 260/300 - 5.036778s - Loss 0.047393 - Accuracy 0.942708 - Test Loss 2.947004 - Test Accuracy 0.599912\n",
      "Epoch 261/300 - 5.054015s - Loss 0.037135 - Accuracy 0.932292 - Test Loss 2.633432 - Test Accuracy 0.597259\n",
      "Epoch 262/300 - 5.031170s - Loss 0.029241 - Accuracy 0.947917 - Test Loss 2.654032 - Test Accuracy 0.599469\n",
      "Epoch 263/300 - 5.041606s - Loss 0.067291 - Accuracy 0.927083 - Test Loss 2.841030 - Test Accuracy 0.605217\n",
      "Epoch 264/300 - 5.036369s - Loss 0.048519 - Accuracy 0.911458 - Test Loss 2.684678 - Test Accuracy 0.594607\n",
      "Epoch 265/300 - 5.053409s - Loss 0.066928 - Accuracy 0.885417 - Test Loss 2.536963 - Test Accuracy 0.598585\n",
      "Epoch 266/300 - 5.062542s - Loss 0.035639 - Accuracy 0.963542 - Test Loss 2.630073 - Test Accuracy 0.599912\n",
      "Epoch 267/300 - 5.050158s - Loss 0.026492 - Accuracy 0.968750 - Test Loss 3.160762 - Test Accuracy 0.584439\n",
      "Epoch 268/300 - 5.031444s - Loss 0.052353 - Accuracy 0.942708 - Test Loss 2.701833 - Test Accuracy 0.595049\n",
      "Epoch 269/300 - 5.047525s - Loss 0.056796 - Accuracy 0.901042 - Test Loss 3.093938 - Test Accuracy 0.594607\n",
      "Epoch 270/300 - 5.044933s - Loss 0.022569 - Accuracy 0.968750 - Test Loss 2.860304 - Test Accuracy 0.590186\n",
      "Epoch 271/300 - 5.039183s - Loss 0.054679 - Accuracy 0.937500 - Test Loss 2.523135 - Test Accuracy 0.600354\n",
      "Epoch 272/300 - 5.055941s - Loss 0.012175 - Accuracy 0.973958 - Test Loss 2.874479 - Test Accuracy 0.594607\n",
      "Epoch 273/300 - 5.040731s - Loss 0.063763 - Accuracy 0.916667 - Test Loss 2.267273 - Test Accuracy 0.598143\n",
      "Epoch 274/300 - 5.061005s - Loss 0.084111 - Accuracy 0.932292 - Test Loss 2.540245 - Test Accuracy 0.598143\n",
      "Epoch 275/300 - 5.051481s - Loss 0.020848 - Accuracy 0.963542 - Test Loss 3.013570 - Test Accuracy 0.601238\n",
      "Epoch 276/300 - 5.028219s - Loss 0.022011 - Accuracy 0.968750 - Test Loss 3.099044 - Test Accuracy 0.595049\n",
      "Epoch 277/300 - 5.044313s - Loss 0.065555 - Accuracy 0.921875 - Test Loss 2.542423 - Test Accuracy 0.597259\n",
      "Epoch 278/300 - 5.046714s - Loss 0.057655 - Accuracy 0.911458 - Test Loss 2.671749 - Test Accuracy 0.596375\n",
      "Epoch 279/300 - 5.064790s - Loss 0.017543 - Accuracy 0.958333 - Test Loss 2.489084 - Test Accuracy 0.603006\n",
      "Epoch 280/300 - 5.055106s - Loss 0.029339 - Accuracy 0.958333 - Test Loss 2.904854 - Test Accuracy 0.593280\n",
      "Epoch 281/300 - 5.048877s - Loss 0.036598 - Accuracy 0.937500 - Test Loss 2.582542 - Test Accuracy 0.595933\n",
      "Epoch 282/300 - 5.028053s - Loss 0.054324 - Accuracy 0.937500 - Test Loss 3.060390 - Test Accuracy 0.593280\n",
      "Epoch 283/300 - 5.063970s - Loss 0.089189 - Accuracy 0.906250 - Test Loss 2.808022 - Test Accuracy 0.596375\n",
      "Epoch 284/300 - 5.040031s - Loss 0.033350 - Accuracy 0.937500 - Test Loss 2.669937 - Test Accuracy 0.592396\n",
      "Epoch 285/300 - 5.060892s - Loss 0.028935 - Accuracy 0.921875 - Test Loss 2.767883 - Test Accuracy 0.602122\n",
      "Epoch 286/300 - 5.044926s - Loss 0.026242 - Accuracy 0.947917 - Test Loss 2.799148 - Test Accuracy 0.599027\n",
      "Epoch 287/300 - 5.048021s - Loss 0.020313 - Accuracy 0.942708 - Test Loss 2.827922 - Test Accuracy 0.596817\n",
      "Epoch 288/300 - 5.020707s - Loss 0.050874 - Accuracy 0.927083 - Test Loss 3.360222 - Test Accuracy 0.594607\n",
      "Epoch 289/300 - 5.048709s - Loss 0.024053 - Accuracy 0.963542 - Test Loss 2.832946 - Test Accuracy 0.593722\n",
      "Epoch 290/300 - 5.042417s - Loss 0.040917 - Accuracy 0.916667 - Test Loss 2.690398 - Test Accuracy 0.594164\n",
      "Epoch 291/300 - 5.059572s - Loss 0.033068 - Accuracy 0.947917 - Test Loss 3.201006 - Test Accuracy 0.594164\n",
      "Epoch 292/300 - 5.042840s - Loss 0.031286 - Accuracy 0.953125 - Test Loss 3.193893 - Test Accuracy 0.595049\n",
      "Epoch 293/300 - 5.121544s - Loss 0.025684 - Accuracy 0.942708 - Test Loss 3.118750 - Test Accuracy 0.596375\n",
      "Epoch 294/300 - 5.061857s - Loss 0.045348 - Accuracy 0.953125 - Test Loss 2.759165 - Test Accuracy 0.597259\n",
      "Epoch 295/300 - 5.041023s - Loss 0.020492 - Accuracy 0.958333 - Test Loss 2.579657 - Test Accuracy 0.603448\n",
      "Epoch 296/300 - 5.033374s - Loss 0.018413 - Accuracy 0.963542 - Test Loss 3.024017 - Test Accuracy 0.593280\n",
      "Epoch 297/300 - 5.051854s - Loss 0.034652 - Accuracy 0.916667 - Test Loss 2.628989 - Test Accuracy 0.596817\n",
      "Epoch 298/300 - 5.042969s - Loss 0.045635 - Accuracy 0.921875 - Test Loss 2.721614 - Test Accuracy 0.590186\n",
      "Epoch 299/300 - 5.054841s - Loss 0.017571 - Accuracy 0.968750 - Test Loss 2.841433 - Test Accuracy 0.593722\n",
      "repeat_0 [0.6074270557029178]\n",
      "{'repeat_0': {'0': {'precision': 0.4, 'recall': 0.09803921568627451, 'f1-score': 0.15748031496062992, 'support': 102}, '1': {'precision': 0.4936708860759494, 'recall': 0.2932330827067669, 'f1-score': 0.36792452830188677, 'support': 133}, '2': {'precision': 0.22580645161290322, 'recall': 0.0603448275862069, 'f1-score': 0.09523809523809525, 'support': 116}, '3': {'precision': 0.25, 'recall': 0.1276595744680851, 'f1-score': 0.16901408450704225, 'support': 94}, '4': {'precision': 0.32075471698113206, 'recall': 0.1588785046728972, 'f1-score': 0.2125, 'support': 107}, '5': {'precision': 0.4716981132075472, 'recall': 0.22727272727272727, 'f1-score': 0.30674846625766866, 'support': 110}, '6': {'precision': 0.2222222222222222, 'recall': 0.06060606060606061, 'f1-score': 0.09523809523809523, 'support': 99}, '7': {'precision': 0.38235294117647056, 'recall': 0.23636363636363636, 'f1-score': 0.29213483146067415, 'support': 110}, '8': {'precision': 0.45901639344262296, 'recall': 0.25225225225225223, 'f1-score': 0.32558139534883723, 'support': 111}, 'micro avg': {'precision': 0.38202247191011235, 'recall': 0.17311608961303462, 'f1-score': 0.23826208829712683, 'support': 982}, 'macro avg': {'precision': 0.35839130274653863, 'recall': 0.16829443129054525, 'f1-score': 0.22465109014588103, 'support': 982}, 'weighted avg': {'precision': 0.36391951792927935, 'recall': 0.17311608961303462, 'f1-score': 0.23025933446306138, 'support': 982}, 'samples avg': {'precision': 0.06903919834954317, 'recall': 0.07515473032714412, 'f1-score': 0.07102858826996758, 'support': 982}}}\n",
      "{'repeat_0': {'0': {'precision': 0.4, 'recall': 0.09803921568627451, 'f1-score': 0.15748031496062992, 'support': 102}, '1': {'precision': 0.4936708860759494, 'recall': 0.2932330827067669, 'f1-score': 0.36792452830188677, 'support': 133}, '2': {'precision': 0.22580645161290322, 'recall': 0.0603448275862069, 'f1-score': 0.09523809523809525, 'support': 116}, '3': {'precision': 0.25, 'recall': 0.1276595744680851, 'f1-score': 0.16901408450704225, 'support': 94}, '4': {'precision': 0.32075471698113206, 'recall': 0.1588785046728972, 'f1-score': 0.2125, 'support': 107}, '5': {'precision': 0.4716981132075472, 'recall': 0.22727272727272727, 'f1-score': 0.30674846625766866, 'support': 110}, '6': {'precision': 0.2222222222222222, 'recall': 0.06060606060606061, 'f1-score': 0.09523809523809523, 'support': 99}, '7': {'precision': 0.38235294117647056, 'recall': 0.23636363636363636, 'f1-score': 0.29213483146067415, 'support': 110}, '8': {'precision': 0.45901639344262296, 'recall': 0.25225225225225223, 'f1-score': 0.32558139534883723, 'support': 111}, 'micro avg': {'precision': 0.38202247191011235, 'recall': 0.17311608961303462, 'f1-score': 0.23826208829712683, 'support': 982}, 'macro avg': {'precision': 0.35839130274653863, 'recall': 0.16829443129054525, 'f1-score': 0.22465109014588103, 'support': 982}, 'weighted avg': {'precision': 0.36391951792927935, 'recall': 0.17311608961303462, 'f1-score': 0.23025933446306138, 'support': 982}, 'samples avg': {'precision': 0.06903919834954317, 'recall': 0.07515473032714412, 'f1-score': 0.07102858826996758, 'support': 982}}, 'accuracy': {'avg': 0.6074270557029178, 'std': 0.0}, 'time_train': {'avg': 1512.7022624015808, 'std': 0.0}, 'time_test': {'avg': 0.47435736656188965, 'std': 0.0}, 'model': 'THAT', 'task': 'activity', 'data': {'num_users': ['0', '1', '2', '3', '4', '5'], 'wifi_band': ['2.4'], 'environment': ['classroom'], 'length': 3000}, 'nn': {'lr': 0.001, 'epoch': 300, 'batch_size': 64, 'threshold': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()           \n",
    "torch.cuda.empty_cache()  \n",
    "torch.cuda.ipc_collect()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "[file]          run.py\n",
    "[description]   run WiFi-based models and optionally save a multiclass confusion matrix\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# from preset import preset, name_run\n",
    "# from load_data import load_data_x, load_data_y, encode_data_y\n",
    "# from lstm import run_lstm, LSTMM\n",
    "# from bilstm import run_bilstm, BiLSTMM\n",
    "# from that import run_that, THAT\n",
    "# from resnet import run_resnet, ResNet18Model\n",
    "# from strf import run_strf  # if you have the ST-RF implementation\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model\",   default=preset[\"model\"],  type=str)\n",
    "    parser.add_argument(\"--task\",    default=preset[\"task\"],   type=str)\n",
    "    parser.add_argument(\"--repeat\",  default=preset[\"repeat\"], type=int)\n",
    "    parser.add_argument(\"--save_cm\", action=\"store_true\",\n",
    "                        help=\"Save a multiclass confusion matrix of the best model to PDF\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def save_multiclass_confusion_matrix(model, data_loader, device, pdf_path, num_classes):\n",
    "    \"\"\"\n",
    "    Given a model that outputs one-hot logits for a multiclass task,\n",
    "    convert to predicted classes via argmax, then plot and save a\n",
    "    num_classes × num_classes confusion matrix to pdf_path.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            # predicted class is index of max logit\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            trues = torch.argmax(yb, dim=1).cpu().numpy()\n",
    "            y_pred.extend(preds.tolist())\n",
    "            y_true.extend(trues.tolist())\n",
    "\n",
    "    labels = list(range(num_classes))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    disp.plot(ax=ax, xticks_rotation=\"vertical\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "def run():\n",
    "    args       = parse_args()\n",
    "    var_model  = args.model\n",
    "    var_task   = args.task\n",
    "    var_repeat = args.repeat\n",
    "\n",
    "    # --- Load and encode the data ---\n",
    "    data_pd_y = load_data_y(\n",
    "        preset[\"path\"][\"data_y\"],\n",
    "        var_environment=preset[\"data\"][\"environment\"],\n",
    "        var_wifi_band=preset[\"data\"][\"wifi_band\"],\n",
    "        var_num_users=preset[\"data\"][\"num_users\"]\n",
    "    )\n",
    "    labels = data_pd_y[\"label\"].tolist()\n",
    "    data_x = load_data_x(preset[\"path\"][\"data_x\"], labels)\n",
    "    data_y = encode_data_y(data_pd_y, var_task)\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(\n",
    "        data_x, data_y, test_size=0.2, shuffle=True, random_state=39\n",
    "    )\n",
    "\n",
    "    # --- Select which model runner to use ---\n",
    "    if var_model == \"ST-RF\":\n",
    "        from strf import run_strf\n",
    "        run_model = run_strf\n",
    "    elif var_model == \"LSTM\":\n",
    "        run_model = run_lstm\n",
    "    elif var_model == \"bi-LSTM\":\n",
    "        run_model = run_bilstm\n",
    "    elif var_model == \"THAT\":\n",
    "        run_model = run_that\n",
    "    elif var_model == \"ResNet18\":\n",
    "        run_model = run_resnet\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {var_model}\")\n",
    "\n",
    "    # --- Train and evaluate ---\n",
    "    print(f\"Running model: {var_model}\")\n",
    "    result = run_model(train_x, train_y, test_x, test_y, var_repeat)\n",
    "    result[\"model\"] = var_model\n",
    "    result[\"task\"]  = var_task\n",
    "    result[\"data\"]  = preset[\"data\"]\n",
    "    result[\"nn\"]    = preset[\"nn\"]\n",
    "    print(result)\n",
    "\n",
    "    # --- Save results to JSON ---\n",
    "    # with open(preset[\"path\"][\"save\"], \"w\") as f:\n",
    "    #     json.dump(result, f, indent=4)\n",
    "\n",
    "    # # --- Optionally save a multiclass confusion matrix ---\n",
    "    # # if args.save_cm:\n",
    "    # if Confusion_matrix == 1:\n",
    "    #     # 1) completely release GPU memory used for training\n",
    "    #     del run_model                      # if 'model' from training is still in scope\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     torch.cuda.ipc_collect()\n",
    "    \n",
    "    #     # 2) reshape input only if the network is sequence‑based\n",
    "    #     if var_model in (\"LSTM\", \"bi-LSTM\", \"THAT\"):\n",
    "    #         test_x_cm = test_x.reshape(test_x.shape[0], test_x.shape[1], -1)\n",
    "    #     else:                           # ResNet18, ST‑RF\n",
    "    #         test_x_cm = test_x\n",
    "    \n",
    "    #     # 3) build the *same* architecture on CPU and load its weights\n",
    "    #     device_cm = torch.device(\"cpu\")\n",
    "    #     if var_model == \"LSTM\":\n",
    "    #         model_cm = LSTMM(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     elif var_model == \"bi-LSTM\":\n",
    "    #         model_cm = BiLSTMM(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     elif var_model == \"THAT\":\n",
    "    #         model_cm = THAT(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     elif var_model == \"ResNet18\":\n",
    "    #         model_cm = ResNet18Model(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Confusion matrix not supported for {var_model}\")\n",
    "    \n",
    "    #     # best_path = f\"/kaggle/working/{name_run}_best_model.pt\"\n",
    "    #     # model_cm.load_state_dict(torch.load(best_path, map_location=device_cm))\n",
    "    #     # model_cm.eval()\n",
    "    \n",
    "    #     # 4) DataLoader on CPU with a safe batch size\n",
    "    #     test_ds = TensorDataset(torch.from_numpy(test_x_cm).float(),\n",
    "    #                             torch.from_numpy(test_y).float())\n",
    "    #     test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "    \n",
    "    #     # 5) save the confusion matrix PDF\n",
    "    #     num_classes = test_y.shape[1]\n",
    "    #     pdf_name = f\"{name_run}_confusion_matrix.pdf\"\n",
    "    #     save_multiclass_confusion_matrix(model_cm,test_loader,device_cm,pdf_name,num_classes)\n",
    "    #     print(f\"✅ Saved confusion matrix (classes 0–{num_classes-1}) to {pdf_name}\")\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"start\")\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aab7e8",
   "metadata": {
    "papermill": {
     "duration": 0.018775,
     "end_time": "2025-12-28T21:28:58.689292",
     "exception": false,
     "start_time": "2025-12-28T21:28:58.670517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 12: Few-shot Learning\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cde432d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T21:28:58.728138Z",
     "iopub.status.busy": "2025-12-28T21:28:58.727431Z",
     "iopub.status.idle": "2025-12-28T21:28:58.733265Z",
     "shell.execute_reply": "2025-12-28T21:28:58.732725Z"
    },
    "papermill": {
     "duration": 0.026587,
     "end_time": "2025-12-28T21:28:58.734371",
     "exception": false,
     "start_time": "2025-12-28T21:28:58.707784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import torch\n",
    "# import shutil\n",
    "# import json\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# gc.collect()           \n",
    "# torch.cuda.empty_cache()  \n",
    "# torch.cuda.ipc_collect()\n",
    "\n",
    "# # ---------- helper: save multiclass confusion matrix ------------------\n",
    "# def save_multiclass_confusion_matrix(model, data_loader, pdf_path, num_classes):\n",
    "#     \"\"\"\n",
    "#     Forward‑pass on CPU, collect predictions, and write an N×N confusion matrix\n",
    "#     to a single‑page PDF (pdf_path).\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     y_true, y_pred = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in data_loader:\n",
    "#             logits = model(xb.cpu())                       # ensure CPU\n",
    "#             preds  = torch.argmax(logits, dim=1).numpy()\n",
    "#             trues  = torch.argmax(yb, dim=1).numpy()\n",
    "#             y_pred.extend(preds.tolist())\n",
    "#             y_true.extend(trues.tolist())\n",
    "\n",
    "#     labels = list(range(num_classes))\n",
    "#     cm  = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "#     disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "#     fig, ax = plt.subplots(figsize=(8, 8))\n",
    "#     disp.plot(ax=ax, xticks_rotation=\"vertical\")\n",
    "#     ax.set_title(\"Few‑shot Confusion Matrix\")\n",
    "#     with PdfPages(pdf_path) as pdf:\n",
    "#         pdf.savefig(fig)\n",
    "#     plt.close(fig)\n",
    "\n",
    "# # -------------------- pick run_* function ------------------------------\n",
    "# if preset[\"model\"] == \"ST-RF\":\n",
    "#     run_model = run_strf\n",
    "# elif preset[\"model\"] == \"LSTM\":\n",
    "#     run_model = run_lstm\n",
    "# elif preset[\"model\"] == \"bi-LSTM\":\n",
    "#     run_model = run_bilstm\n",
    "# elif preset[\"model\"] == \"THAT\":\n",
    "#     run_model = run_that\n",
    "# elif preset[\"model\"] == \"ResNet18\":\n",
    "#     run_model = run_resnet\n",
    "# else:\n",
    "#     raise ValueError(f\"No few‑shot implementation for {preset['model']}.\")\n",
    "\n",
    "# # ------------------------ load / split data ----------------------------\n",
    "# data_pd_y = load_data_y(preset[\"path\"][\"data_y\"],\n",
    "#                         var_environment=[dest_env],\n",
    "#                         var_wifi_band=preset[\"data\"][\"wifi_band\"],\n",
    "#                         var_num_users=preset[\"data\"][\"num_users\"])\n",
    "\n",
    "# labels_list = data_pd_y[\"label\"].tolist()\n",
    "# data_x = load_data_x(preset[\"path\"][\"data_x\"], labels_list)\n",
    "# data_y = encode_data_y(data_pd_y, preset[\"task\"])\n",
    "\n",
    "# train_x, test_x, train_y, test_y = train_test_split(\n",
    "#     data_x, data_y, test_size=0.2, shuffle=True, random_state=39)\n",
    "\n",
    "# # Few-shot sample size\n",
    "# train_x = train_x[:few_shot_num_samples]\n",
    "# train_y = train_y[:few_shot_num_samples]\n",
    "\n",
    "# # ----------------------- few‑shot training -----------------------------\n",
    "# original_epochs = preset[\"nn\"][\"epoch\"]\n",
    "# preset[\"nn\"][\"epoch\"] = few_shot_epochs\n",
    "\n",
    "# # Load the best model weights\n",
    "# best_model_path = f\"{name_run}_best_model.pt\"\n",
    "\n",
    "# # Initialize the model \n",
    "# if preset[\"model\"] == \"LSTM\":\n",
    "#     model = LSTMM(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "#     # print('train_y_[0].shape:', train_y[0].shape)\n",
    "#     # print('train_x_[0].shape:', train_x[0].reshape(train_x[0].shape[0], -1).shape)\n",
    "# elif preset[\"model\"] == \"bi-LSTM\":\n",
    "#     model = BiLSTMM(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "# elif preset[\"model\"] == \"THAT\":\n",
    "#     model = THAT(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "# elif preset[\"model\"] == \"ResNet18\":\n",
    "#     model = ResNet18Model(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "# else:\n",
    "#     raise ValueError(f\"Model {preset['model']} not supported!\")\n",
    "\n",
    "# # Load the weights into the model\n",
    "# model.load_state_dict(torch.load(best_model_path, map_location=\"cpu\"))\n",
    "# model = model.to('cuda')\n",
    "\n",
    "# # Fine-tune the model on few-shot data (note: `run_model` should now return only the result)\n",
    "# result = run_model(train_x, train_y, test_x, test_y, var_repeat=1, init_model=model)\n",
    "# print(result)\n",
    "\n",
    "# # --------------------- save few‑shot checkpoints -----------------------\n",
    "# # After fine-tuning, save the model\n",
    "# torch.save(model.state_dict(), f\"{name_run}_fewshot_final_model.pt\")\n",
    "# torch.save(model.state_dict(), f\"{name_run}_fewshot_best_model.pt\")\n",
    "\n",
    "# # ------------------- confusion matrix on CPU ---------------------------\n",
    "# if Confusion_matrix == 1 and preset[\"model\"] != \"ST-RF\":\n",
    "\n",
    "#     # reshape for sequence models\n",
    "#     test_x_rs = (test_x.reshape(test_x.shape[0], test_x.shape[1], -1)\n",
    "#                  if preset[\"model\"] in (\"LSTM\", \"bi-LSTM\", \"THAT\") else test_x)\n",
    "\n",
    "#     # instantiate identical architecture on CPU\n",
    "#     if preset[\"model\"] == \"LSTM\":\n",
    "#         model_cpu = LSTMM(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "#     elif preset[\"model\"] == \"bi-LSTM\":\n",
    "#         model_cpu = BiLSTMM(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "#     elif preset[\"model\"] == \"THAT\":\n",
    "#         model_cpu = THAT(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "#     else:  # ResNet18\n",
    "#         model_cpu = ResNet18Model(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "\n",
    "#     # load weights\n",
    "#     model_cpu.load_state_dict(torch.load(f\"{name_run}_fewshot_best_model.pt\", map_location=\"cpu\"))\n",
    "\n",
    "#     # CPU DataLoader with a safe batch size\n",
    "#     test_ds = TensorDataset(torch.from_numpy(test_x_rs).float(),\n",
    "#                             torch.from_numpy(test_y).float())\n",
    "#     test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "#     pdf_name = f\"{name_run}_fewshot_confusion_matrix.pdf\"\n",
    "#     num_classes = test_y.shape[1]\n",
    "#     save_multiclass_confusion_matrix(model_cpu, test_loader, pdf_name, num_classes)\n",
    "#     print(f\"✅ Saved few‑shot confusion matrix (classes 0–{num_classes-1}) to {pdf_name}\")\n",
    "\n",
    "# # ----------------------- restore & persist -----------------------------\n",
    "# preset[\"nn\"][\"epoch\"] = original_epochs\n",
    "\n",
    "# # Save the final result to JSON\n",
    "# with open(\"result_fewshot.json\", \"w\") as f:\n",
    "#     json.dump(result, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "638c0b62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T21:28:58.772729Z",
     "iopub.status.busy": "2025-12-28T21:28:58.772505Z",
     "iopub.status.idle": "2025-12-28T21:28:58.782240Z",
     "shell.execute_reply": "2025-12-28T21:28:58.781541Z"
    },
    "papermill": {
     "duration": 0.030503,
     "end_time": "2025-12-28T21:28:58.783404",
     "exception": false,
     "start_time": "2025-12-28T21:28:58.752901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import scipy.io as scio\n",
    "# import time\n",
    "# import torch\n",
    "# import gc\n",
    "# from numpy.linalg import svd\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "# from copy import deepcopy\n",
    "# import json\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# import torch._dynamo\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # --- تنظیمات سیستمی ---\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# # --------------------------\n",
    "# # 1. تنظیمات (Configuration)\n",
    "# # --------------------------\n",
    "# preset = {\n",
    "#     \"model\": \"THAT\",          \n",
    "#     \"task\": \"activity\",       \n",
    "#     \"repeat\": 1,\n",
    "#     \"path\": {\n",
    "#         \"data_x\": \"/kaggle/input/wimans/wifi_csi/amp\",   \n",
    "#         \"data_y\": \"/kaggle/input/wimans/annotation.csv\", \n",
    "#     },\n",
    "#     \"data\": {\n",
    "#         \"num_users\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"],  \n",
    "#         \"wifi_band\": [\"2.4\"],                         \n",
    "#         \"environment\": [\"classroom\"],                 \n",
    "#         \"length\": 3000,\n",
    "        \n",
    "#         # 1.0 = 100% data (Full run) | 0.1 = 10% data (Quick test)\n",
    "#         \"subset_ratio\": 0.5,  \n",
    "#     },\n",
    "#     \"nn\": {\n",
    "#         \"lr\": 1e-3,           \n",
    "#         \"epoch\": 80,          \n",
    "#         \"batch_size\": 32,    \n",
    "#         \"threshold\": 0.5,\n",
    "#         \"patience\": 5,        \n",
    "#         \"factor\": 0.5,        \n",
    "#         \"min_lr\": 1e-6        \n",
    "#     },\n",
    "#     \"encoding\": {\n",
    "#         \"activity\": {\n",
    "#             \"nan\":      [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#             \"nothing\":  [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#             \"walk\":     [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "#             \"rotation\": [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "#             \"jump\":     [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "#             \"wave\":     [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "#             \"lie_down\": [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "#             \"pick_up\":  [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "#             \"sit_down\": [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "#             \"stand_up\": [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "#         },\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# # --------------------------\n",
    "# # 2. توابع RPCA و لود دیتا\n",
    "# # --------------------------\n",
    "# def soft_threshold(x, epsilon):\n",
    "#     return np.maximum(np.abs(x) - epsilon, 0) * np.sign(x)\n",
    "\n",
    "# def robust_pca(M, max_iter=10, tol=1e-4):\n",
    "#     n1, n2 = M.shape\n",
    "#     lambda_param = 1 / np.sqrt(max(n1, n2))\n",
    "#     Y = M / np.maximum(np.linalg.norm(M, 2), np.linalg.norm(M, np.inf) / lambda_param)\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     mu = 1.25 / np.linalg.norm(M, 2)\n",
    "#     rho = 1.5\n",
    "#     for i in range(max_iter):\n",
    "#         temp_L = M - S + (1/mu) * Y\n",
    "#         U, Sigma, Vt = svd(temp_L, full_matrices=False)\n",
    "#         Sigma_thresh = soft_threshold(Sigma, 1/mu)\n",
    "#         L_new = np.dot(U * Sigma_thresh, Vt)\n",
    "#         temp_S = M - L_new + (1/mu) * Y\n",
    "#         S_new = soft_threshold(temp_S, lambda_param/mu)\n",
    "#         error = np.linalg.norm(M - L_new - S_new, 'fro') / np.linalg.norm(M, 'fro')\n",
    "#         L = L_new; S = S_new\n",
    "#         if error < tol: break\n",
    "#         Y = Y + mu * (M - L - S)\n",
    "#         mu = min(mu * rho, 1e7)\n",
    "#     return L, S\n",
    "\n",
    "# def load_data_y(var_path_data_y, var_environment=None, var_wifi_band=None, var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None: data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None: data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None: data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list, use_rpca=True):\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     mode_str = \"WITH RPCA\" if use_rpca else \"RAW DATA (No RPCA)\"\n",
    "#     print(f\"Loading {len(var_path_list)} samples - Mode: {mode_str}...\")\n",
    "#     for i, var_path in enumerate(var_path_list):\n",
    "#         if i % 100 == 0 and i > 0: print(f\"Processing {i}/{len(var_path_list)}...\")\n",
    "#         data_csi = np.load(var_path) \n",
    "#         data_csi_2d = data_csi.reshape(data_csi.shape[0], -1)\n",
    "#         target_len = preset[\"data\"][\"length\"]\n",
    "#         current_len = data_csi_2d.shape[0]\n",
    "#         var_pad_length = target_len - current_len\n",
    "#         if var_pad_length > 0: data_csi_pad = np.pad(data_csi_2d, ((0, var_pad_length), (0, 0)), mode='constant')\n",
    "#         else: data_csi_pad = data_csi_2d[:target_len, :]\n",
    "#         if use_rpca:\n",
    "#             L, S = robust_pca(data_csi_pad)\n",
    "#             final_sample = np.concatenate([L, S], axis=1) \n",
    "#         else:\n",
    "#             final_sample = data_csi_pad\n",
    "#         data_x.append(final_sample)\n",
    "#     data_x = np.array(data_x)\n",
    "#     return data_x\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"activity\": return encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     return encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     cols = [f\"user_{i}_activity\" for i in range(1, 7)]\n",
    "#     data = data_pd_y[cols].to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[y] for y in sample] for sample in data])\n",
    "\n",
    "# # --------------------------\n",
    "# # 3. مدل THAT\n",
    "# # --------------------------\n",
    "# class Gaussian_Position(torch.nn.Module):\n",
    "#     def __init__(self, var_dim_feature, var_dim_time, var_num_gaussian=10):\n",
    "#         super(Gaussian_Position, self).__init__()\n",
    "#         self.var_embedding = torch.nn.Parameter(torch.zeros([var_num_gaussian, var_dim_feature]), requires_grad=True)\n",
    "#         torch.nn.init.xavier_uniform_(self.var_embedding)\n",
    "#         self.var_position = torch.nn.Parameter(torch.arange(0.0, var_dim_time).unsqueeze(1).repeat(1, var_num_gaussian), requires_grad=False)\n",
    "#         self.var_mu = torch.nn.Parameter(torch.arange(0.0, var_dim_time, var_dim_time/var_num_gaussian).unsqueeze(0), requires_grad=True)\n",
    "#         self.var_sigma = torch.nn.Parameter(torch.tensor([50.0] * var_num_gaussian).unsqueeze(0), requires_grad=True)\n",
    "#     def forward(self, var_input):\n",
    "#         var_pdf = - (self.var_position - self.var_mu)**2 / (2 * self.var_sigma**2) - torch.log(self.var_sigma)\n",
    "#         var_pdf = torch.softmax(var_pdf, dim=-1)\n",
    "#         return var_input + torch.matmul(var_pdf, self.var_embedding).unsqueeze(0)\n",
    "\n",
    "# class Encoder(torch.nn.Module):\n",
    "#     def __init__(self, var_dim_feature, var_num_head=10, var_size_cnn=[1, 3, 5]):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.layer_norm_0 = torch.nn.LayerNorm(var_dim_feature, eps=1e-6)\n",
    "#         self.layer_attention = torch.nn.MultiheadAttention(var_dim_feature, var_num_head, batch_first=True)\n",
    "#         self.layer_dropout_0 = torch.nn.Dropout(0.1)\n",
    "#         self.layer_norm_1 = torch.nn.LayerNorm(var_dim_feature, 1e-6)\n",
    "#         self.layer_cnn = torch.nn.ModuleList([torch.nn.Sequential(torch.nn.Conv1d(var_dim_feature, var_dim_feature, s, padding=\"same\"), torch.nn.BatchNorm1d(var_dim_feature), torch.nn.Dropout(0.1), torch.nn.LeakyReLU()) for s in var_size_cnn])\n",
    "#         self.layer_dropout_1 = torch.nn.Dropout(0.1)\n",
    "#     def forward(self, var_input):\n",
    "#         var_t = self.layer_norm_0(var_input)\n",
    "#         var_t, _ = self.layer_attention(var_t, var_t, var_t)\n",
    "#         var_t = self.layer_dropout_0(var_t) + var_input\n",
    "#         var_s = self.layer_norm_1(var_t).permute(0, 2, 1)\n",
    "#         var_c = torch.stack([l(var_s) for l in self.layer_cnn], dim=0)\n",
    "#         var_s = self.layer_dropout_1((torch.sum(var_c, dim=0) / len(self.layer_cnn)).permute(0, 2, 1))\n",
    "#         return var_s + var_t\n",
    "\n",
    "# class THAT(torch.nn.Module):\n",
    "#     def __init__(self, var_x_shape, var_y_shape):\n",
    "#         super(THAT, self).__init__()\n",
    "#         var_dim_feature, var_dim_time = var_x_shape[-1], var_x_shape[-2]\n",
    "#         var_dim_output = var_y_shape[-1]\n",
    "#         self.layer_left_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "#         self.layer_left_gaussian = Gaussian_Position(var_dim_feature, var_dim_time // 20)\n",
    "#         self.layer_left_encoder = torch.nn.ModuleList([Encoder(var_dim_feature, 10, [1, 3, 5]) for _ in range(4)])\n",
    "#         self.layer_left_norm = torch.nn.LayerNorm(var_dim_feature, eps=1e-6)\n",
    "#         self.layer_left_cnn = torch.nn.ModuleList([torch.nn.Conv1d(var_dim_feature, 128, k) for k in [8, 16]])\n",
    "#         self.layer_left_dropout = torch.nn.Dropout(0.5)\n",
    "#         var_dim_right = var_dim_time // 20\n",
    "#         self.layer_right_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "#         self.layer_right_encoder = torch.nn.ModuleList([Encoder(var_dim_right, 10, [1, 2, 3])])\n",
    "#         self.layer_right_norm = torch.nn.LayerNorm(var_dim_right, eps=1e-6)\n",
    "#         self.layer_right_cnn = torch.nn.ModuleList([torch.nn.Conv1d(var_dim_right, 16, k) for k in [2, 4]])\n",
    "#         self.layer_right_dropout = torch.nn.Dropout(0.5)\n",
    "#         self.layer_leakyrelu = torch.nn.LeakyReLU()\n",
    "#         self.layer_output = torch.nn.Linear(256 + 32, var_dim_output)\n",
    "#     def forward(self, var_input):\n",
    "#         v_l = self.layer_left_gaussian(self.layer_left_pooling(var_input.permute(0, 2, 1)).permute(0, 2, 1))\n",
    "#         for l in self.layer_left_encoder: v_l = l(v_l)\n",
    "#         v_l = self.layer_left_norm(v_l).permute(0, 2, 1)\n",
    "#         v_l = torch.cat([torch.sum(self.layer_leakyrelu(cnn(v_l)), dim=-1) for cnn in self.layer_left_cnn], dim=-1)\n",
    "#         v_l = self.layer_left_dropout(v_l)\n",
    "#         v_r = self.layer_right_pooling(var_input.permute(0, 2, 1))\n",
    "#         for l in self.layer_right_encoder: v_r = l(v_r)\n",
    "#         v_r = self.layer_right_norm(v_r).permute(0, 2, 1)\n",
    "#         v_r = torch.cat([torch.sum(self.layer_leakyrelu(cnn(v_r)), dim=-1) for cnn in self.layer_right_cnn], dim=-1)\n",
    "#         v_r = self.layer_right_dropout(v_r)\n",
    "#         return self.layer_output(torch.cat([v_l, v_r], dim=-1))\n",
    "\n",
    "# # --------------------------\n",
    "# # 4. Training Loop\n",
    "# # --------------------------\n",
    "# def train(model, optimizer, loss_fn, train_loader, test_loader, threshold, epochs, device, model_path):\n",
    "#     best_acc = -1.0\n",
    "#     best_w = deepcopy(model.state_dict())\n",
    "    \n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#         optimizer, mode='max', factor=preset[\"nn\"][\"factor\"], patience=preset[\"nn\"][\"patience\"],\n",
    "#         min_lr=preset[\"nn\"][\"min_lr\"], verbose=True\n",
    "#     )\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         t0 = time.time()\n",
    "#         model.train()\n",
    "        \n",
    "#         # --- [MODIFIED] Using requested variable names ---\n",
    "#         for data_batch_x, data_batch_y in train_loader:\n",
    "#             data_batch_x = data_batch_x.to(device)\n",
    "#             data_batch_y = data_batch_y.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             predict_train_y = model(data_batch_x)\n",
    "            \n",
    "#             # --- [REQUESTED LINE] ---\n",
    "#             loss_value = loss_fn(predict_train_y, data_batch_y.reshape(data_batch_y.shape[0], -1).float())\n",
    "            \n",
    "#             loss_value.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             tx, ty = next(iter(test_loader))\n",
    "#             tx, ty = tx.to(device), ty.to(device)\n",
    "#             pred_t = model(tx)\n",
    "            \n",
    "#             p_cls = (torch.sigmoid(pred_t) > threshold).float().cpu().numpy()\n",
    "#             t_cls = ty.cpu().numpy()\n",
    "#             acc = accuracy_score(t_cls.reshape(-1, t_cls.shape[-1]), p_cls.reshape(-1, t_cls.shape[-1]))\n",
    "            \n",
    "#         scheduler.step(acc)\n",
    "#         current_lr = optimizer.param_groups[0]['lr']\n",
    "#         print(f\"Ep {epoch+1}/{epochs} | LR: {current_lr:.6f} | L_tr: {loss_value.item():.4f} | Acc: {acc:.4f}\")\n",
    "        \n",
    "#         if acc > best_acc:\n",
    "#             best_acc = acc\n",
    "#             best_w = deepcopy(model.state_dict())\n",
    "            \n",
    "#     torch.save(best_w, model_path)\n",
    "#     return best_w\n",
    "\n",
    "# def save_multiclass_confusion_matrix(model, data_loader, device, pdf_path, num_classes, title_text):\n",
    "#     model.eval()\n",
    "#     y_true, y_pred = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in data_loader:\n",
    "#             xb = xb.to(device)\n",
    "#             logits = model(xb) \n",
    "#             logits = logits.reshape(-1, num_classes) \n",
    "#             yb = yb.reshape(-1, num_classes)        \n",
    "#             y_pred.extend(torch.argmax(logits, dim=1).cpu().numpy().tolist())\n",
    "#             y_true.extend(torch.argmax(yb, dim=1).cpu().numpy().tolist())\n",
    "    \n",
    "#     labels = list(range(num_classes))\n",
    "#     cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "#     disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "#     fig, ax = plt.subplots(figsize=(12, 12))\n",
    "#     disp.plot(ax=ax, xticks_rotation=\"vertical\", cmap='Blues')\n",
    "#     ax.set_title(title_text)\n",
    "#     with PdfPages(pdf_path) as pdf: pdf.savefig(fig)\n",
    "#     plt.close(fig)\n",
    "\n",
    "# # --------------------------\n",
    "# # 5. اجرا\n",
    "# # --------------------------\n",
    "# def run_experiment(scenario_name, use_rpca):\n",
    "#     print(f\"\\n################################################\")\n",
    "#     print(f\"STARTING SCENARIO: {scenario_name}\")\n",
    "#     print(f\"################################################\")\n",
    "    \n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     current_run_name = f\"{preset['model']}_{preset['task']}_{scenario_name}\"\n",
    "#     model_save_path = f\"{current_run_name}_best_model.pt\"\n",
    "#     json_save_path = f\"result_{current_run_name}.json\"\n",
    "#     pdf_save_path = f\"Confusion_{current_run_name}.pdf\"\n",
    "    \n",
    "#     # 1. Load Labels\n",
    "#     data_pd_y = load_data_y(preset[\"path\"][\"data_y\"], preset[\"data\"][\"environment\"], preset[\"data\"][\"wifi_band\"], preset[\"data\"][\"num_users\"])\n",
    "    \n",
    "#     # Apply Subset Ratio\n",
    "#     subset_ratio = preset[\"data\"][\"subset_ratio\"]\n",
    "#     if subset_ratio < 1.0:\n",
    "#         data_pd_y = data_pd_y.sample(frac=subset_ratio, random_state=42).reset_index(drop=True)\n",
    "#         print(f\"*** DEBUG MODE: Using {subset_ratio*100}% of data ({len(data_pd_y)} samples) ***\")\n",
    "    \n",
    "#     # 2. Load X\n",
    "#     data_x = load_data_x(preset[\"path\"][\"data_x\"], data_pd_y[\"label\"].tolist(), use_rpca=use_rpca)\n",
    "#     data_y = encode_data_y(data_pd_y, preset[\"task\"])\n",
    "    \n",
    "#     # 3. Split\n",
    "#     train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2, shuffle=True, random_state=39)\n",
    "#     train_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "#     test_ds = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "#     train_loader = DataLoader(train_ds, batch_size=preset[\"nn\"][\"batch_size\"], shuffle=True)\n",
    "#     test_loader = DataLoader(test_ds, batch_size=len(test_ds), shuffle=False)\n",
    "    \n",
    "#     result = {\"accuracy\": []}\n",
    "    \n",
    "#     for r in range(preset[\"repeat\"]):\n",
    "#         print(f\"--- Repeat {r+1}/{preset['repeat']} ---\")\n",
    "#         torch.random.manual_seed(r + 39)\n",
    "        \n",
    "#         model = THAT(train_x[0].shape, train_y[0].reshape(-1).shape).to(device)\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=preset[\"nn\"][\"lr\"])\n",
    "#         loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "#         best_w = train(model, optimizer, loss_fn, train_loader, test_loader, \n",
    "#                        preset[\"nn\"][\"threshold\"], preset[\"nn\"][\"epoch\"], device, model_save_path)\n",
    "        \n",
    "#         model.load_state_dict(best_w)\n",
    "#         with torch.no_grad():\n",
    "#             preds = model(torch.from_numpy(test_x).to(device))\n",
    "#             preds_reshaped = (torch.sigmoid(preds) > preset[\"nn\"][\"threshold\"]).float().cpu().numpy().reshape(-1, 9)\n",
    "#             targets_reshaped = test_y.reshape(-1, 9)\n",
    "#             acc = accuracy_score(targets_reshaped, preds_reshaped)\n",
    "#             result[\"accuracy\"].append(acc)\n",
    "            \n",
    "#     print(f\"Final Accuracy ({scenario_name}): {np.mean(result['accuracy']):.4f}\")\n",
    "#     with open(json_save_path, \"w\") as f: json.dump(result, f, indent=4)\n",
    "    \n",
    "#     print(\"Generating Confusion Matrix...\")\n",
    "#     model_cm = THAT(test_x[0].shape, test_y[0].reshape(-1).shape).to(\"cpu\")\n",
    "#     model_cm.load_state_dict(torch.load(model_save_path, map_location=\"cpu\"))\n",
    "#     cm_loader = DataLoader(TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y)), batch_size=32)\n",
    "#     num_classes = test_y.shape[2] \n",
    "#     title = f\"Confusion Matrix: {scenario_name} (Acc: {np.mean(result['accuracy']):.2f} - {subset_ratio*100}% Data)\"\n",
    "#     save_multiclass_confusion_matrix(model_cm, cm_loader, \"cpu\", pdf_save_path, num_classes, title)\n",
    "    \n",
    "#     del model, model_cm, train_x, test_x, data_x\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     print(f\"Done with {scenario_name}.\\n\")\n",
    "\n",
    "# def run():\n",
    "#     scenarios = [\n",
    "#         (\"RPCA\", True),\n",
    "#         (\"RAW\", False)\n",
    "#     ]\n",
    "#     for name, rpca_flag in scenarios:\n",
    "#         run_experiment(name, rpca_flag)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48689aef",
   "metadata": {
    "papermill": {
     "duration": 0.018451,
     "end_time": "2025-12-28T21:28:58.820604",
     "exception": false,
     "start_time": "2025-12-28T21:28:58.802153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4451316,
     "sourceId": 7638081,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7503373,
     "sourceId": 11934698,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3853.015989,
   "end_time": "2025-12-28T21:29:01.147175",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-28T20:24:48.131186",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
