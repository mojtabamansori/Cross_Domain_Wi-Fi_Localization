{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70b4f9f",
   "metadata": {
    "papermill": {
     "duration": 0.008353,
     "end_time": "2025-12-28T20:24:52.121474",
     "exception": false,
     "start_time": "2025-12-28T20:24:52.113121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 1: Library Imports\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c125f98e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:24:52.136549Z",
     "iopub.status.busy": "2025-12-28T20:24:52.136270Z",
     "iopub.status.idle": "2025-12-28T20:25:02.153109Z",
     "shell.execute_reply": "2025-12-28T20:25:02.152549Z"
    },
    "papermill": {
     "duration": 10.025896,
     "end_time": "2025-12-28T20:25:02.154528",
     "exception": false,
     "start_time": "2025-12-28T20:24:52.128632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 1: Library Imports\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as scio\n",
    "import time\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch._dynamo\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# from ptflops import get_model_complexity_info\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from copy import deepcopy\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9434c1f1",
   "metadata": {
    "papermill": {
     "duration": 0.006949,
     "end_time": "2025-12-28T20:25:02.168977",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.162028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 2: preset.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f0639e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.183495Z",
     "iopub.status.busy": "2025-12-28T20:25:02.182950Z",
     "iopub.status.idle": "2025-12-28T20:25:02.192200Z",
     "shell.execute_reply": "2025-12-28T20:25:02.191613Z"
    },
    "papermill": {
     "duration": 0.017788,
     "end_time": "2025-12-28T20:25:02.193372",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.175584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "few=empty_room,100,5,m=THAT,t=activity,epoch=300,batch=64,environment=['classroom']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "[file]          preset.py\n",
    "[description]   default settings of WiFi-based models\n",
    "\"\"\"\n",
    "minidata_set = 1\n",
    "preset = {\n",
    "    # define model\n",
    "    \"model\": \"THAT\",  # \"ST-RF\", \"MLP\", \"LSTM\", \"CNN-1D\", \"CNN-2D\", \"CLSTM\", \"ABLSTM\", \"THAT\", \"bi-LSTM\", \"ResNet18\"\n",
    "    # define task\n",
    "    \"task\": \"activity\",  # \"identity\", \"activity\", \"location\", \"count\"\n",
    "    # number of repeated experiments\n",
    "    \"repeat\": 1,\n",
    "    # path of data\n",
    "    \"path\": {\n",
    "        # \"data_x\": \"/kaggle/input/wimans/wifi_csi/mat\",   # directory of CSI amplitude files \n",
    "        \"data_x\": \"/kaggle/input/wimans/wifi_csi/amp\",   # directory of CSI amplitude files \n",
    "        \"data_y\": \"/kaggle/input/wimans/annotation.csv\", # path of annotation file\n",
    "        \"save\": \"result_lstm_epoch=80_batchsize=32_envs=empty_room_wifiband=2.4.json\"               # path to save results\n",
    "    },\n",
    "    # data selection for experiments\n",
    "    \"data\": {\n",
    "        \"num_users\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"],  # select number(s) of users\n",
    "        \"wifi_band\": [\"2.4\"],                         # select WiFi band(s)\n",
    "        \"environment\": [\"classroom\"],                 # select environment(s) [\"classroom\"], [\"meeting_room\"], [\"empty_room\"]\n",
    "        \"length\": 3000,                               # default length of CSI\n",
    "    },\n",
    "    # hyperparameters of models\n",
    "    \"nn\": {\n",
    "        \"lr\": 1e-3,           # learning rate\n",
    "        \"epoch\": 300,         # number of epochs\n",
    "        \"batch_size\": 64,    # batch size\n",
    "        \"threshold\": 0.5,     # threshold to binarize sigmoid outputs\n",
    "    },\n",
    "    # encoding of activities and locations\n",
    "    \"encoding\": {\n",
    "        \"activity\": {  # encoding of different activities\n",
    "            \"nan\":      [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"nothing\":  [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"walk\":     [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"rotation\": [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            \"jump\":     [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            \"wave\":     [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "            \"lie_down\": [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "            \"pick_up\":  [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "            \"sit_down\": [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "            \"stand_up\": [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        },\n",
    "        \"location\": {  # encoding of different locations\n",
    "            \"nan\":  [0, 0, 0, 0, 0],\n",
    "            \"a\":    [1, 0, 0, 0, 0],\n",
    "            \"b\":    [0, 1, 0, 0, 0],\n",
    "            \"c\":    [0, 0, 1, 0, 0],\n",
    "            \"d\":    [0, 0, 0, 1, 0],\n",
    "            \"e\":    [0, 0, 0, 0, 1],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Few-shot parameters (manually set)\n",
    "dest_env = \"empty_room\"       # Destination environment[\"classroom\"], [\"meeting_room\"], [\"empty_room\"]\n",
    "few_shot_epochs = 100         # Number of epochs for few-shot training\n",
    "few_shot_num_samples = 5     # Number of samples to use from the destination test data\n",
    "\n",
    "Confusion_matrix = 1\n",
    "\n",
    "name_run = \"few={},{},{},m={},t={},epoch={},batch={},environment={}\".format(dest_env, few_shot_epochs, few_shot_num_samples, preset[\"model\"], preset[\"task\"], preset[\"nn\"][\"epoch\"], preset[\"nn\"][\"batch_size\"], preset[\"data\"][\"environment\"])\n",
    "print(name_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1568d411",
   "metadata": {
    "papermill": {
     "duration": 0.006591,
     "end_time": "2025-12-28T20:25:02.206942",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.200351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 3: load_data.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208c3ce3",
   "metadata": {
    "papermill": {
     "duration": 0.006614,
     "end_time": "2025-12-28T20:25:02.220596",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.213982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "raw + sparse\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "face16e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.235280Z",
     "iopub.status.busy": "2025-12-28T20:25:02.235054Z",
     "iopub.status.idle": "2025-12-28T20:25:02.260644Z",
     "shell.execute_reply": "2025-12-28T20:25:02.259939Z"
    },
    "papermill": {
     "duration": 0.03447,
     "end_time": "2025-12-28T20:25:02.261795",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.227325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 3: load_data.py  (AMP + RPCA sparse, and RAW + alpha*SPARSE)\n",
    "# =========================\n",
    "\"\"\"\n",
    "[file]          load_data.py\n",
    "[description]   load annotation file and CSI amplitude, and encode labels\n",
    "                (Test mode) X_input = X_raw + alpha * X_sparse\n",
    "                where X_sparse is RPCA sparse component computed per (Tx,Rx) link\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd  # Cell1 already imports, but safe here too\n",
    "\n",
    "# =========================================================\n",
    "# NEW TEST SETTINGS\n",
    "# =========================================================\n",
    "# \"raw\"            : use amplitude as-is\n",
    "# \"sparse\"         : use RPCA sparse component only\n",
    "# \"raw_plus_sparse\": raw + alpha*sparse  (your requested test)\n",
    "CSI_INPUT_MODE = \"raw_plus_sparse\"   # <-- \"raw\" / \"sparse\" / \"raw_plus_sparse\"\n",
    "\n",
    "# weight of sparse when adding to raw\n",
    "SPARSE_ALPHA = 3.0\n",
    "\n",
    "# keep amplitudes non-negative after addition (recommended)\n",
    "CLAMP_NONNEG = True\n",
    "\n",
    "# RPCA settings (IALM)\n",
    "RPCA_MAX_ITER = 60\n",
    "RPCA_TOL      = 1e-5\n",
    "RPCA_RHO      = 1.5\n",
    "RPCA_MU_INIT  = None\n",
    "\n",
    "# lambda options you asked before\n",
    "# \"classic\": 1/sqrt(max(m,n))\n",
    "# \"median\" : 1/sqrt(max(m,n)) * median(abs(M))\n",
    "# \"scaled\" : 1.2/sqrt(max(m,n))\n",
    "RPCA_LAMBDA_MODE = \"median\"   # <-- \"classic\" / \"median\" / \"scaled\"\n",
    "\n",
    "# cache for speed\n",
    "CACHE_ENABLED = True\n",
    "CACHE_ROOT = \"/kaggle/working/csi_cache_amp_raw_plus_sparse\"\n",
    "\n",
    "# debug print once to confirm amplitude/phase\n",
    "_DEBUG_PRINT_ONCE = True\n",
    "\n",
    "# expected antenna/subcarrier dims (WiMANS typical)\n",
    "TX = 3\n",
    "RX = 3\n",
    "SC = 30\n",
    "# =========================================================\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RPCA helpers (IALM)\n",
    "# =========================\n",
    "def _compute_lambda(M: np.ndarray) -> float:\n",
    "    m, n = M.shape\n",
    "    base = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "    if RPCA_LAMBDA_MODE == \"classic\":\n",
    "        return base\n",
    "\n",
    "    if RPCA_LAMBDA_MODE == \"scaled\":\n",
    "        return 1.2 * base\n",
    "\n",
    "    if RPCA_LAMBDA_MODE == \"median\":\n",
    "        med = np.median(np.abs(M))\n",
    "        med = float(med) if med > 1e-12 else 1e-12\n",
    "        return base * med\n",
    "\n",
    "    raise ValueError(f\"Unknown RPCA_LAMBDA_MODE: {RPCA_LAMBDA_MODE}\")\n",
    "\n",
    "\n",
    "def _soft_threshold(X: np.ndarray, tau: float) -> np.ndarray:\n",
    "    return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "\n",
    "def _svt(X: np.ndarray, tau: float) -> np.ndarray:\n",
    "    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    s_thr = np.maximum(s - tau, 0.0)\n",
    "    if np.all(s_thr == 0):\n",
    "        return np.zeros_like(X)\n",
    "    return (U * s_thr) @ Vt\n",
    "\n",
    "\n",
    "def _rpca_ialm(M: np.ndarray,\n",
    "              max_iter: int = 60,\n",
    "              tol: float = 1e-5,\n",
    "              rho: float = 1.5,\n",
    "              mu: float | None = None):\n",
    "    \"\"\"\n",
    "    Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "    Decompose: M = L + S\n",
    "    Returns: L, S (float32)\n",
    "    \"\"\"\n",
    "    M = M.astype(np.float64, copy=False)\n",
    "    lam = _compute_lambda(M)\n",
    "\n",
    "    if mu is None:\n",
    "        # spectral norm approx via top singular value\n",
    "        s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "        mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "    L = np.zeros_like(M)\n",
    "    S = np.zeros_like(M)\n",
    "    Y = np.zeros_like(M)\n",
    "\n",
    "    normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "        S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "        R = M - L - S\n",
    "        Y = Y + mu * R\n",
    "\n",
    "        err = np.linalg.norm(R, ord=\"fro\") / normM\n",
    "        if err < tol:\n",
    "            break\n",
    "\n",
    "        mu *= rho\n",
    "\n",
    "    return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Data shape helpers\n",
    "# =========================\n",
    "def _ensure_shape_4d_amp(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Expect amp npy to be either:\n",
    "      - (T, 3, 3, 30)\n",
    "      - (T, 270)  -> reshape to (T,3,3,30)\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim == 4:\n",
    "        return x\n",
    "    if x.ndim == 2 and x.shape[1] == TX * RX * SC:\n",
    "        T = x.shape[0]\n",
    "        return x.reshape(T, TX, RX, SC)\n",
    "    raise ValueError(f\"Unexpected AMP shape: {x.shape} (expected (T,{TX},{RX},{SC}) or (T,{TX*RX*SC}))\")\n",
    "\n",
    "\n",
    "def _debug_print_once(label: str, arr: np.ndarray):\n",
    "    global _DEBUG_PRINT_ONCE\n",
    "    if not _DEBUG_PRINT_ONCE:\n",
    "        return\n",
    "    _DEBUG_PRINT_ONCE = False\n",
    "    print(f\"\\n[DEBUG] First loaded AMP sample: {label}\")\n",
    "    print(f\"[DEBUG] shape={arr.shape}, dtype={arr.dtype}, complex={np.iscomplexobj(arr)}\")\n",
    "    if np.iscomplexobj(arr):\n",
    "        print(\"[DEBUG] ==> WARNING: This looks complex; amp folder usually should be real amplitude.\")\n",
    "    else:\n",
    "        print(\"[DEBUG] ==> Input is REAL -> amplitude-only (no true phase).\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def _cache_path(label: str, mode: str) -> str:\n",
    "    return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "def _make_sparse_component_pairwise(x4: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    x4: (T,3,3,30) real amplitude\n",
    "    Run RPCA on each (Tx,Rx) link separately:\n",
    "      M = (T,30) for each link, compute S, place back.\n",
    "    Return: S_out same shape as x4 (float32)\n",
    "    \"\"\"\n",
    "    x4 = x4.astype(np.float32, copy=False)\n",
    "    T = x4.shape[0]\n",
    "    S_out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "    for tx in range(TX):\n",
    "        for rx in range(RX):\n",
    "            M = x4[:, tx, rx, :]  # (T,30)\n",
    "            _, S = _rpca_ialm(\n",
    "                M,\n",
    "                max_iter=RPCA_MAX_ITER,\n",
    "                tol=RPCA_TOL,\n",
    "                rho=RPCA_RHO,\n",
    "                mu=RPCA_MU_INIT\n",
    "            )\n",
    "            S_out[:, tx, rx, :] = S\n",
    "\n",
    "    return S_out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Public API (used by run.py)\n",
    "# =========================\n",
    "def load_data_y(var_path_data_y,\n",
    "                var_environment=None,\n",
    "                var_wifi_band=None,\n",
    "                var_num_users=None):\n",
    "    \"\"\"\n",
    "    Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "    \"\"\"\n",
    "    data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "    if var_environment is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "    if var_wifi_band is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "    if var_num_users is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "    return data_pd_y\n",
    "\n",
    "\n",
    "def load_data_x(var_path_data_x, var_label_list):\n",
    "    \"\"\"\n",
    "    Load CSI amplitude (*.npy) files based on a label list.\n",
    "\n",
    "    Modes:\n",
    "      - raw:            x_out = x_raw\n",
    "      - sparse:         x_out = S\n",
    "      - raw_plus_sparse:x_out = x_raw + alpha*S\n",
    "    \"\"\"\n",
    "    var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "    data_x = []\n",
    "    target_len = preset[\"data\"][\"length\"]\n",
    "\n",
    "    for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "        x_raw = np.load(var_path)\n",
    "        x_raw = _ensure_shape_4d_amp(x_raw).astype(np.float32, copy=False)\n",
    "\n",
    "        _debug_print_once(var_label, x_raw)\n",
    "\n",
    "        mode = CSI_INPUT_MODE\n",
    "\n",
    "        if mode == \"raw\":\n",
    "            x_out = x_raw\n",
    "\n",
    "        else:\n",
    "            if CACHE_ENABLED:\n",
    "                os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "                p = _cache_path(var_label, mode)\n",
    "                if os.path.exists(p):\n",
    "                    x_out = np.load(p).astype(np.float32, copy=False)\n",
    "                else:\n",
    "                    S = _make_sparse_component_pairwise(x_raw)\n",
    "                    if mode == \"sparse\":\n",
    "                        x_out = S\n",
    "                    elif mode == \"raw_plus_sparse\":\n",
    "                        x_out = x_raw + (SPARSE_ALPHA * S)\n",
    "                        if CLAMP_NONNEG:\n",
    "                            x_out = np.maximum(x_out, 0.0)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown CSI_INPUT_MODE={mode}\")\n",
    "\n",
    "                    np.save(p, x_out.astype(np.float32))\n",
    "            else:\n",
    "                S = _make_sparse_component_pairwise(x_raw)\n",
    "                if mode == \"sparse\":\n",
    "                    x_out = S\n",
    "                elif mode == \"raw_plus_sparse\":\n",
    "                    x_out = x_raw + (SPARSE_ALPHA * S)\n",
    "                    if CLAMP_NONNEG:\n",
    "                        x_out = np.maximum(x_out, 0.0)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown CSI_INPUT_MODE={mode}\")\n",
    "\n",
    "        # trim/pad to target_len (same behavior as your existing code expects)\n",
    "        if x_out.shape[0] > target_len:\n",
    "            x_out = x_out[-target_len:, :, :, :]\n",
    "\n",
    "        pad_len = target_len - x_out.shape[0]\n",
    "        if pad_len > 0:\n",
    "            x_out = np.pad(x_out, ((pad_len, 0), (0, 0), (0, 0), (0, 0)))\n",
    "\n",
    "        data_x.append(x_out)\n",
    "\n",
    "    return np.array(data_x)\n",
    "\n",
    "\n",
    "def encode_data_y(data_pd_y, var_task):\n",
    "    \"\"\"\n",
    "    Encode labels according to specific task.\n",
    "    \"\"\"\n",
    "    if var_task == \"identity\":\n",
    "        data_y = encode_identity(data_pd_y)\n",
    "    elif var_task == \"activity\":\n",
    "        data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "    elif var_task == \"location\":\n",
    "        data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "    elif var_task == \"count\":\n",
    "        data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "    return data_y\n",
    "\n",
    "\n",
    "def encode_identity(data_pd_y):\n",
    "    \"\"\"\n",
    "    Onehot encoding for identity labels.\n",
    "    \"\"\"\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "                                    \"user_3_location\", \"user_4_location\",\n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "    data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "    data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "    return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "def encode_activity(data_pd_y, var_encoding):\n",
    "    \"\"\"\n",
    "    Onehot encoding for activity labels.\n",
    "    \"\"\"\n",
    "    data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "                                    \"user_3_activity\", \"user_4_activity\",\n",
    "                                    \"user_5_activity\", \"user_6_activity\"]]\n",
    "    data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "    return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "def encode_location(data_pd_y, var_encoding):\n",
    "    \"\"\"\n",
    "    Onehot encoding for location labels.\n",
    "    \"\"\"\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "                                    \"user_3_location\", \"user_4_location\",\n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "    return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "def encode_count(data_pd_y, var_encoding):\n",
    "    \"\"\"\n",
    "    Onehot encoding for count labels.\n",
    "    \"\"\"\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "                                    \"user_3_location\", \"user_4_location\",\n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "    data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "    data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "    data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "    count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68758269",
   "metadata": {
    "papermill": {
     "duration": 0.006733,
     "end_time": "2025-12-28T20:25:02.275673",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.268940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "last stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab7d23a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.290590Z",
     "iopub.status.busy": "2025-12-28T20:25:02.290362Z",
     "iopub.status.idle": "2025-12-28T20:25:02.298308Z",
     "shell.execute_reply": "2025-12-28T20:25:02.297815Z"
    },
    "papermill": {
     "duration": 0.016683,
     "end_time": "2025-12-28T20:25:02.299305",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.282622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI (from .mat), and encode labels\n",
    "# \"\"\"\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import scipy.io as scio\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from numpy.fft import ifft\n",
    "\n",
    "# # =========================================================\n",
    "# #  Settings\n",
    "# # =========================================================\n",
    "# # \"raw\"     : abs(raw complex CSI)  -> float\n",
    "# # \"lowrank\" : abs(L) after RPCA     -> float\n",
    "# # \"sparse\"  : abs(S) after RPCA     -> float\n",
    "# CSI_INPUT_MODE = \"sparse\"   # raw / lowrank / sparse\n",
    "\n",
    "# # expected dimensions\n",
    "# TX = 3\n",
    "# RX = 3\n",
    "# SC = 30\n",
    "\n",
    "# # optional IFFT (frequency -> delay)\n",
    "# USE_IFFT = True\n",
    "\n",
    "# # RPCA iterations (برای شروع کم بگذار؛ اگر لازم شد بیشتر کن)\n",
    "# RPCA_MAX_ITER = 50\n",
    "\n",
    "# # lambda mode (اختیاری)\n",
    "# # \"classic\" : 1/sqrt(max(m,n))\n",
    "# # \"median\"  : 1/sqrt(max(m,n)) * median(abs(M))\n",
    "# # \"scaled\"  : 1.2/sqrt(max(m,n))\n",
    "# RPCA_LAMBDA_MODE = \"classic\"\n",
    "\n",
    "# # cache (very important for speed)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT = \"/kaggle/working/csi_cache_mat_pipeline\"\n",
    "\n",
    "# # print once to verify input is amplitude or complex/phase\n",
    "# _DEBUG_PRINT_ONCE = True\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# # =========================================================\n",
    "# # Minimal R_pca implementation (no external dependency)\n",
    "# # Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "# # =========================================================\n",
    "# class R_pca:\n",
    "#     def __init__(self, D):\n",
    "#         self.D = np.asarray(D, dtype=np.float64)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _shrink(M, tau):\n",
    "#         return np.sign(M) * np.maximum(np.abs(M) - tau, 0.0)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _svt(M, tau):\n",
    "#         U, s, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "#         s = np.maximum(s - tau, 0.0)\n",
    "#         if np.all(s == 0):\n",
    "#             return np.zeros_like(M)\n",
    "#         return (U * s) @ Vt\n",
    "\n",
    "#     def _lambda(self, D):\n",
    "#         m, n = D.shape\n",
    "#         base = 1.0 / np.sqrt(max(m, n))\n",
    "#         if RPCA_LAMBDA_MODE == \"classic\":\n",
    "#             return base\n",
    "#         if RPCA_LAMBDA_MODE == \"scaled\":\n",
    "#             return 1.2 * base\n",
    "#         if RPCA_LAMBDA_MODE == \"median\":\n",
    "#             med = np.median(np.abs(D))\n",
    "#             med = float(med) if med > 1e-12 else 1e-12\n",
    "#             return base * med\n",
    "#         raise ValueError(f\"Unknown RPCA_LAMBDA_MODE={RPCA_LAMBDA_MODE}\")\n",
    "\n",
    "#     def fit(self, max_iter=200, tol=1e-6, rho=1.5, mu=None):\n",
    "#         \"\"\"\n",
    "#         Returns L, S such that D ≈ L + S\n",
    "#         \"\"\"\n",
    "#         D = self.D\n",
    "#         m, n = D.shape\n",
    "#         lam = self._lambda(D)\n",
    "\n",
    "#         # auto mu\n",
    "#         if mu is None:\n",
    "#             s0 = np.linalg.svd(D, compute_uv=False, full_matrices=False)[0] if D.size else 1.0\n",
    "#             mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#         L = np.zeros_like(D)\n",
    "#         S = np.zeros_like(D)\n",
    "#         Y = np.zeros_like(D)\n",
    "\n",
    "#         normD = np.linalg.norm(D, ord=\"fro\") + 1e-12\n",
    "\n",
    "#         for _ in range(max_iter):\n",
    "#             L = self._svt(D - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#             S = self._shrink(D - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#             R = D - L - S\n",
    "#             Y = Y + mu * R\n",
    "\n",
    "#             if (np.linalg.norm(R, ord=\"fro\") / normD) < tol:\n",
    "#                 break\n",
    "\n",
    "#             mu *= rho\n",
    "\n",
    "#         return L.astype(np.float64), S.astype(np.float64)\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 1) Phase calibration (sanitize) - vectorized\n",
    "# # --------------------------------------------------\n",
    "# def phase_sanitize_matrix(X):\n",
    "#     \"\"\"\n",
    "#     X: complex matrix (N_subcarriers, T)\n",
    "#     \"\"\"\n",
    "#     phase = np.unwrap(np.angle(X), axis=0)  # (N,T)\n",
    "#     N, T = phase.shape\n",
    "#     k = np.arange(N, dtype=np.float64)[:, None]  # (N,1)\n",
    "\n",
    "#     A = np.concatenate([k, np.ones((N, 1), dtype=np.float64)], axis=1)  # (N,2)\n",
    "#     pinvA = np.linalg.pinv(A)  # (2,N)\n",
    "#     coeff = pinvA @ phase      # (2,T)\n",
    "#     a = coeff[0:1, :]\n",
    "#     b = coeff[1:2, :]\n",
    "\n",
    "#     phase_corr = phase - (k @ a + b)\n",
    "#     return np.abs(X) * np.exp(1j * phase_corr)\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 2) Preprocess CSI matrix\n",
    "# # --------------------------------------------------\n",
    "# def preprocess_csi(X):\n",
    "#     \"\"\"\n",
    "#     X : complex CSI matrix (N_subcarriers, T)\n",
    "#     \"\"\"\n",
    "#     X_corr = phase_sanitize_matrix(X).astype(np.complex128, copy=False)\n",
    "#     fro = np.linalg.norm(X_corr, \"fro\")\n",
    "#     if fro > 0:\n",
    "#         X_corr /= fro\n",
    "#     return X_corr\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 3) Optional IFFT\n",
    "# # --------------------------------------------------\n",
    "# def csi_to_cir(X):\n",
    "#     return ifft(X, axis=0)\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 4) RPCA on complex matrix (real & imag separately)\n",
    "# # --------------------------------------------------\n",
    "# def rpca_complex(X, max_iter=200):\n",
    "#     Xr = np.real(X)\n",
    "#     Xi = np.imag(X)\n",
    "\n",
    "#     rpca_r = R_pca(Xr)\n",
    "#     Lr, Sr = rpca_r.fit(max_iter=max_iter)\n",
    "\n",
    "#     rpca_i = R_pca(Xi)\n",
    "#     Li, Si = rpca_i.fit(max_iter=max_iter)\n",
    "\n",
    "#     L = Lr + 1j * Li\n",
    "#     S = Sr + 1j * Si\n",
    "#     return L, S\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 5) Full pipeline\n",
    "# # --------------------------------------------------\n",
    "# def csi_lowrank_sparse_pipeline(X, use_ifft=True, max_iter=200):\n",
    "#     Xp = preprocess_csi(X)\n",
    "#     if use_ifft:\n",
    "#         Xp = csi_to_cir(Xp)\n",
    "#     L, S = rpca_complex(Xp, max_iter=max_iter)\n",
    "#     return L, S\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # MAT loader -> (T,3,3,30) complex\n",
    "# # --------------------------------------------------\n",
    "# def load_csi_from_mat(mat_path):\n",
    "#     m = scio.loadmat(mat_path)\n",
    "#     if \"trace\" not in m:\n",
    "#         raise KeyError(f\"'trace' not found in {mat_path}\")\n",
    "\n",
    "#     trace = m[\"trace\"]  # (T,1) object array\n",
    "#     T = trace.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.complex128)\n",
    "\n",
    "#     for t in range(T):\n",
    "#         out[t] = trace[t, 0][\"csi\"][0, 0]  # (3,3,30) complex\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def _debug_print_once(label, csi_4d):\n",
    "#     global _DEBUG_PRINT_ONCE\n",
    "#     if not _DEBUG_PRINT_ONCE:\n",
    "#         return\n",
    "#     _DEBUG_PRINT_ONCE = False\n",
    "\n",
    "#     is_cplx = np.iscomplexobj(csi_4d)\n",
    "#     print(f\"\\n[DEBUG] First MAT sample: {label}\")\n",
    "#     print(f\"[DEBUG] shape={csi_4d.shape}, dtype={csi_4d.dtype}, complex={is_cplx}\")\n",
    "\n",
    "#     x0 = csi_4d[0, 0, 0, :]\n",
    "#     if is_cplx:\n",
    "#         ang = np.angle(x0)\n",
    "#         print(f\"[DEBUG] abs range:  min={np.min(np.abs(x0)):.6f}, max={np.max(np.abs(x0)):.6f}\")\n",
    "#         print(f\"[DEBUG] phase stats: mean={np.mean(ang):.6f}, std={np.std(ang):.6f}\")\n",
    "#         print(\"[DEBUG] ==> Input is COMPLEX (phase exists).\")\n",
    "#     else:\n",
    "#         print(f\"[DEBUG] value range: min={np.min(x0):.6f}, max={np.max(x0):.6f}\")\n",
    "#         print(\"[DEBUG] ==> Input is REAL (likely amplitude-only).\")\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _apply_pipeline_pairwise_9links(csi_4d_complex, label):\n",
    "#     \"\"\"\n",
    "#     csi_4d_complex: (T,3,3,30) complex\n",
    "#     Run pipeline on each link separately: (30,T) -> RPCA -> abs -> back to (T,3,3,30) float32\n",
    "#     \"\"\"\n",
    "#     _debug_print_once(label, csi_4d_complex)\n",
    "\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return np.abs(csi_4d_complex).astype(np.float32)\n",
    "\n",
    "#     mode = CSI_INPUT_MODE  # lowrank / sparse\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             return np.load(p).astype(np.float32, copy=False)\n",
    "\n",
    "#     T = csi_4d_complex.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "#     for tx in range(TX):\n",
    "#         for rx in range(RX):\n",
    "#             X = csi_4d_complex[:, tx, rx, :].T  # (30,T) complex\n",
    "#             L, S = csi_lowrank_sparse_pipeline(X, use_ifft=USE_IFFT, max_iter=RPCA_MAX_ITER)\n",
    "#             Y = L if mode == \"lowrank\" else S\n",
    "#             out[:, tx, rx, :] = np.abs(Y.T).astype(np.float32)\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out)\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # Existing label loading/encoding\n",
    "# # --------------------------------------------------\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None,\n",
    "#                 var_wifi_band=None,\n",
    "#                 var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     var_path_data_x should point to MAT directory, e.g. /kaggle/input/wimans/wifi_csi/mat\n",
    "#     Each label corresponds to <label>.mat\n",
    "#     \"\"\"\n",
    "#     data_x = []\n",
    "#     target_len = preset[\"data\"][\"length\"]\n",
    "\n",
    "#     for label in var_label_list:\n",
    "#         mat_path = os.path.join(var_path_data_x, label + \".mat\")\n",
    "#         if not os.path.exists(mat_path):\n",
    "#             raise FileNotFoundError(f\"MAT file not found: {mat_path}\")\n",
    "\n",
    "#         csi_complex = load_csi_from_mat(mat_path)  # (T,3,3,30) complex\n",
    "\n",
    "#         # if longer than target_len, keep last target_len frames\n",
    "#         if csi_complex.shape[0] > target_len:\n",
    "#             csi_complex = csi_complex[-target_len:, :, :, :]\n",
    "\n",
    "#         csi_feat = _apply_pipeline_pairwise_9links(csi_complex, label)\n",
    "\n",
    "#         # pad if shorter\n",
    "#         pad_len = target_len - csi_feat.shape[0]\n",
    "#         if pad_len > 0:\n",
    "#             csi_feat = np.pad(csi_feat, ((pad_len, 0), (0, 0), (0, 0), (0, 0)))\n",
    "\n",
    "#         data_x.append(csi_feat)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y)\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "#                                     \"user_3_activity\", \"user_4_activity\",\n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13828b1b",
   "metadata": {
    "papermill": {
     "duration": 0.0066,
     "end_time": "2025-12-28T20:25:02.312816",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.306216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "rpca 30 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c39fd07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.327820Z",
     "iopub.status.busy": "2025-12-28T20:25:02.327625Z",
     "iopub.status.idle": "2025-12-28T20:25:02.333800Z",
     "shell.execute_reply": "2025-12-28T20:25:02.333269Z"
    },
    "papermill": {
     "duration": 0.015377,
     "end_time": "2025-12-28T20:25:02.334850",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.319473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # =========================================================\n",
    "# # انتخاب نوع دیتای ورودی مدل:\n",
    "# # \"raw\"     : amp خام\n",
    "# # \"lowrank\" : خروجی Low-rank (L) از RPCA\n",
    "# # \"sparse\"  : خروجی Sparse (S) از RPCA\n",
    "# CSI_INPUT_MODE = \"lowrank\"   # <-- raw / lowrank / sparse\n",
    "\n",
    "# # ابعاد مورد انتظار CSI\n",
    "# TX = 3\n",
    "# RX = 3\n",
    "# SC = 30\n",
    "\n",
    "# # RPCA (IALM) تنظیمات سریع‌تر\n",
    "# RPCA_MAX_ITER = 60\n",
    "# RPCA_TOL      = 1e-5\n",
    "# RPCA_RHO      = 1.5\n",
    "# RPCA_MU_INIT  = None\n",
    "\n",
    "# # ✅ انتخاب لامبدا:\n",
    "# # \"median\" : lam = 1/sqrt(max(m,n)) * median(abs(M))\n",
    "# # \"scaled\" : lam = 1.2/sqrt(max(m,n))\n",
    "# RPCA_LAMBDA_MODE = \"median\"  # <-- \"median\" یا \"scaled\"\n",
    "\n",
    "# # Cache (خیلی مهم برای سرعت)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT    = \"/kaggle/working/csi_cache_pairwise\"\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _soft_threshold(X, tau):\n",
    "#     return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "# def _svt(X, tau):\n",
    "#     # Singular Value Thresholding\n",
    "#     U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "#     s = np.maximum(s - tau, 0.0)\n",
    "#     if np.all(s == 0):\n",
    "#         return np.zeros_like(X)\n",
    "#     return (U * s) @ Vt\n",
    "\n",
    "# def _compute_lambda(M, mode):\n",
    "#     \"\"\"\n",
    "#     mode:\n",
    "#       - 'median': 1/sqrt(max(m,n)) * median(abs(M))\n",
    "#       - 'scaled': 1.2/sqrt(max(m,n))\n",
    "#     \"\"\"\n",
    "#     m, n = M.shape\n",
    "#     base = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "#     if mode == \"median\":\n",
    "#         med = np.median(np.abs(M))\n",
    "#         # اگر med خیلی کوچک بود برای پایداری:\n",
    "#         med = float(med) if med > 1e-12 else 1e-12\n",
    "#         return base * med\n",
    "\n",
    "#     if mode == \"scaled\":\n",
    "#         return 1.2 * base\n",
    "\n",
    "#     raise ValueError(f\"Unknown RPCA_LAMBDA_MODE: {mode}. Use 'median' or 'scaled'.\")\n",
    "\n",
    "\n",
    "# def _rpca_ialm(M, mu=None, rho=1.5, max_iter=60, tol=1e-5):\n",
    "#     \"\"\"\n",
    "#     Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "#     Decompose: M = L + S\n",
    "#     M shape: (T, SC) = (3000, 30)\n",
    "#     \"\"\"\n",
    "#     M = M.astype(np.float64, copy=False)\n",
    "#     m, n = M.shape\n",
    "\n",
    "#     # ✅ lambda طبق انتخاب کاربر\n",
    "#     lam = _compute_lambda(M, RPCA_LAMBDA_MODE)\n",
    "\n",
    "#     # mu خودکار یا دستی\n",
    "#     if mu is None:\n",
    "#         s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "#         mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     Y = np.zeros_like(M)\n",
    "\n",
    "#     normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#         S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#         R = M - L - S\n",
    "#         Y = Y + mu * R\n",
    "\n",
    "#         if (np.linalg.norm(R, ord=\"fro\") / normM) < tol:\n",
    "#             break\n",
    "\n",
    "#         mu *= rho\n",
    "\n",
    "#     return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# def _ensure_shape_4d(data_csi):\n",
    "#     \"\"\"\n",
    "#     Ensure CSI shape is (T, TX, RX, SC).\n",
    "#     If input is (T, 270) we reshape to (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     if data_csi.ndim == 4:\n",
    "#         return data_csi\n",
    "\n",
    "#     if data_csi.ndim == 2 and data_csi.shape[1] == TX * RX * SC:\n",
    "#         T = data_csi.shape[0]\n",
    "#         return data_csi.reshape(T, TX, RX, SC)\n",
    "\n",
    "#     raise ValueError(\n",
    "#         f\"Unexpected CSI shape {data_csi.shape}. Expected (T,{TX},{RX},{SC}) or (T,{TX*RX*SC}).\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _rpca_pairwise_9links(data_csi_4d):\n",
    "#     \"\"\"\n",
    "#     data_csi_4d: (T, TX, RX, SC)\n",
    "#     Run RPCA on each (tx,rx) separately on matrix (T, SC) and reassemble.\n",
    "#     Output shape stays (T, TX, RX, SC).\n",
    "#     \"\"\"\n",
    "#     T = data_csi_4d.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "#     for tx in range(TX):\n",
    "#         for rx in range(RX):\n",
    "#             M = data_csi_4d[:, tx, rx, :]  # (T,30)\n",
    "#             L, S = _rpca_ialm(\n",
    "#                 M,\n",
    "#                 mu=RPCA_MU_INIT,\n",
    "#                 rho=RPCA_RHO,\n",
    "#                 max_iter=RPCA_MAX_ITER,\n",
    "#                 tol=RPCA_TOL\n",
    "#             )\n",
    "#             out[:, tx, rx, :] = L if CSI_INPUT_MODE == \"lowrank\" else S\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def _apply_mode_with_cache(data_csi, label):\n",
    "#     \"\"\"\n",
    "#     Apply raw/lowrank/sparse. For lowrank/sparse use pairwise RPCA (9 links of 3000x30).\n",
    "#     Keeps the original 4D shape (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     data_csi = _ensure_shape_4d(np.asarray(data_csi, dtype=np.float32))\n",
    "\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     mode = CSI_INPUT_MODE\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             cached = np.load(p)\n",
    "#             return _ensure_shape_4d(cached).astype(np.float32, copy=False)\n",
    "\n",
    "#     out = _rpca_pairwise_9links(data_csi)\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out.astype(np.float32))\n",
    "\n",
    "#     return out.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None,\n",
    "#                 var_wifi_band=None,\n",
    "#                 var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ RPCA روی 9 لینک جدا (3000x30) و بعد اتصال\n",
    "#         data_csi = _apply_mode_with_cache(data_csi, var_label)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "#                                     \"user_3_activity\", \"user_4_activity\",\n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "866d6bb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.349439Z",
     "iopub.status.busy": "2025-12-28T20:25:02.349225Z",
     "iopub.status.idle": "2025-12-28T20:25:02.355737Z",
     "shell.execute_reply": "2025-12-28T20:25:02.355210Z"
    },
    "papermill": {
     "duration": 0.015099,
     "end_time": "2025-12-28T20:25:02.356749",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.341650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # =========================================================\n",
    "# # انتخاب نوع دیتای ورودی مدل:\n",
    "# # \"raw\"     : amp خام\n",
    "# # \"lowrank\" : خروجی Low-rank (L) از RPCA\n",
    "# # \"sparse\"  : خروجی Sparse (S) از RPCA\n",
    "# CSI_INPUT_MODE = \"sparse\"   # <-- raw / lowrank / sparse\n",
    "\n",
    "# # ابعاد مورد انتظار CSI\n",
    "# TX = 3\n",
    "# RX = 3\n",
    "# SC = 30\n",
    "\n",
    "# # RPCA (IALM) تنظیمات سریع‌تر\n",
    "# RPCA_MAX_ITER = 60\n",
    "# RPCA_TOL      = 1e-5\n",
    "# RPCA_RHO      = 1.5\n",
    "# RPCA_MU_INIT  = None\n",
    "# RPCA_LAMBDA   = None     # None -> 1/sqrt(max(m,n))\n",
    "\n",
    "# # Cache (خیلی مهم برای سرعت)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT    = \"/kaggle/working/csi_cache_pairwise\"  # خروجی‌ها اینجا ذخیره میشن\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _soft_threshold(X, tau):\n",
    "#     return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "# def _svt(X, tau):\n",
    "#     # Singular Value Thresholding\n",
    "#     U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "#     s = np.maximum(s - tau, 0.0)\n",
    "#     if np.all(s == 0):\n",
    "#         return np.zeros_like(X)\n",
    "#     return (U * s) @ Vt\n",
    "\n",
    "# def _rpca_ialm(M, lam=None, mu=None, rho=1.5, max_iter=60, tol=1e-5):\n",
    "#     \"\"\"\n",
    "#     Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "#     Decompose: M = L + S\n",
    "#     M shape: (T, SC) = (3000, 30)\n",
    "#     \"\"\"\n",
    "#     M = M.astype(np.float64, copy=False)\n",
    "#     m, n = M.shape\n",
    "\n",
    "#     if lam is None:\n",
    "#         lam = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "#     if mu is None:\n",
    "#         # top singular value as spectral norm approximation\n",
    "#         s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "#         mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     Y = np.zeros_like(M)\n",
    "\n",
    "#     normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#         S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#         R = M - L - S\n",
    "#         Y = Y + mu * R\n",
    "\n",
    "#         if (np.linalg.norm(R, ord=\"fro\") / normM) < tol:\n",
    "#             break\n",
    "\n",
    "#         mu *= rho\n",
    "\n",
    "#     return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# def _ensure_shape_4d(data_csi):\n",
    "#     \"\"\"\n",
    "#     Ensure CSI shape is (T, TX, RX, SC).\n",
    "#     If input is (T, 270) we reshape to (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     if data_csi.ndim == 4:\n",
    "#         return data_csi\n",
    "\n",
    "#     if data_csi.ndim == 2 and data_csi.shape[1] == TX * RX * SC:\n",
    "#         T = data_csi.shape[0]\n",
    "#         return data_csi.reshape(T, TX, RX, SC)\n",
    "\n",
    "#     raise ValueError(\n",
    "#         f\"Unexpected CSI shape {data_csi.shape}. Expected (T,{TX},{RX},{SC}) or (T,{TX*RX*SC}).\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     # mode: \"lowrank\" or \"sparse\"\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _rpca_pairwise_9links(data_csi_4d):\n",
    "#     \"\"\"\n",
    "#     data_csi_4d: (T, TX, RX, SC)\n",
    "#     Run RPCA on each (tx,rx) separately on matrix (T, SC) and reassemble.\n",
    "#     Output shape stays (T, TX, RX, SC).\n",
    "#     \"\"\"\n",
    "#     T = data_csi_4d.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "#     # 9 times RPCA: for each tx-rx link\n",
    "#     for tx in range(TX):\n",
    "#         for rx in range(RX):\n",
    "#             M = data_csi_4d[:, tx, rx, :]  # (T, SC) => (3000,30)\n",
    "#             L, S = _rpca_ialm(\n",
    "#                 M,\n",
    "#                 lam=RPCA_LAMBDA,\n",
    "#                 mu=RPCA_MU_INIT,\n",
    "#                 rho=RPCA_RHO,\n",
    "#                 max_iter=RPCA_MAX_ITER,\n",
    "#                 tol=RPCA_TOL\n",
    "#             )\n",
    "#             if CSI_INPUT_MODE == \"lowrank\":\n",
    "#                 out[:, tx, rx, :] = L\n",
    "#             else:  # \"sparse\"\n",
    "#                 out[:, tx, rx, :] = S\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def _apply_mode_with_cache(data_csi, label):\n",
    "#     \"\"\"\n",
    "#     Apply raw/lowrank/sparse. For lowrank/sparse use pairwise RPCA (9 links of 3000x30).\n",
    "#     Keeps the original 4D shape (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     data_csi = _ensure_shape_4d(np.asarray(data_csi, dtype=np.float32))\n",
    "\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     mode = CSI_INPUT_MODE\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             cached = np.load(p)\n",
    "#             return _ensure_shape_4d(cached).astype(np.float32, copy=False)\n",
    "\n",
    "#     out = _rpca_pairwise_9links(data_csi)\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out.astype(np.float32))\n",
    "\n",
    "#     return out.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     \"\"\"\n",
    "#     Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "#     \"\"\"\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     Load CSI amplitude (*.npy) files based on a label list.\n",
    "#     \"\"\"\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ NEW: RPCA روی 9 لینک جداگانه (3000x30) و بعد چسباندن\n",
    "#         data_csi = _apply_mode_with_cache(data_csi, var_label)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     \"\"\"\n",
    "#     Encode labels according to specific task.\n",
    "#     \"\"\"\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     return data_identity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for activity labels.\n",
    "#     \"\"\"\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "#                                     \"user_3_activity\", \"user_4_activity\", \n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "#     return data_activity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for location labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "#     return data_location_onehot_y\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1)\n",
    "#     count_data = count_data.reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     count_data_onehot = encoder.fit_transform(count_data).astype(\"int8\")\n",
    "#     return count_data_onehot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab35a3",
   "metadata": {
    "papermill": {
     "duration": 0.006894,
     "end_time": "2025-12-28T20:25:02.370674",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.363780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "rpca\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edae57a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.385647Z",
     "iopub.status.busy": "2025-12-28T20:25:02.385312Z",
     "iopub.status.idle": "2025-12-28T20:25:02.391018Z",
     "shell.execute_reply": "2025-12-28T20:25:02.390369Z"
    },
    "papermill": {
     "duration": 0.014468,
     "end_time": "2025-12-28T20:25:02.392035",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.377567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # =========================================================\n",
    "# # انتخاب ورودی مدل:\n",
    "# # \"raw\"     : همون amp خام\n",
    "# # \"lowrank\" : مؤلفه Low-rank از RPCA (L)\n",
    "# # \"sparse\"  : مؤلفه Sparse از RPCA (S)\n",
    "# CSI_INPUT_MODE = \"lowrank\"   # <-- raw / lowrank / sparse\n",
    "\n",
    "# # RPCA (IALM) تنظیمات سریع‌تر\n",
    "# RPCA_MAX_ITER = 80\n",
    "# RPCA_TOL      = 1e-5\n",
    "# RPCA_RHO      = 1.5\n",
    "# RPCA_MU_INIT  = None\n",
    "# RPCA_LAMBDA   = None     # None -> 1/sqrt(max(m,n))\n",
    "\n",
    "# # Cache (خیلی مهم برای سرعت)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT    = \"/kaggle/working/csi_cache\"  # خروجی‌ها اینجا ذخیره میشن\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _soft_threshold(X, tau):\n",
    "#     return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "# def _svt(X, tau):\n",
    "#     U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "#     s = np.maximum(s - tau, 0.0)\n",
    "#     if np.all(s == 0):\n",
    "#         return np.zeros_like(X)\n",
    "#     return (U * s) @ Vt\n",
    "\n",
    "# def _rpca_ialm(M, lam=None, mu=None, rho=1.5, max_iter=80, tol=1e-5):\n",
    "#     M = M.astype(np.float64, copy=False)\n",
    "#     m, n = M.shape\n",
    "\n",
    "#     if lam is None:\n",
    "#         lam = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "#     if mu is None:\n",
    "#         s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "#         mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     Y = np.zeros_like(M)\n",
    "\n",
    "#     normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#         S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#         R = M - L - S\n",
    "#         Y = Y + mu * R\n",
    "\n",
    "#         if (np.linalg.norm(R, ord=\"fro\") / normM) < tol:\n",
    "#             break\n",
    "#         mu *= rho\n",
    "\n",
    "#     return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# def _rpca_keep_shape(X):\n",
    "#     \"\"\"RPCA روی (T,F) و بازگرداندن دقیقاً به shape اولیه\"\"\"\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "#     if X.ndim == 1:\n",
    "#         M = X[:, None]\n",
    "#         L, S = _rpca_ialm(M, RPCA_LAMBDA, RPCA_MU_INIT, RPCA_RHO, RPCA_MAX_ITER, RPCA_TOL)\n",
    "#         return L[:, 0], S[:, 0]\n",
    "\n",
    "#     T = X.shape[0]\n",
    "#     F = int(np.prod(X.shape[1:]))\n",
    "#     M = X.reshape(T, F)\n",
    "\n",
    "#     L, S = _rpca_ialm(M, RPCA_LAMBDA, RPCA_MU_INIT, RPCA_RHO, RPCA_MAX_ITER, RPCA_TOL)\n",
    "#     return L.reshape(X.shape), S.reshape(X.shape)\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     # mode: \"lowrank\" or \"sparse\"\n",
    "#     # فایل خروجی: /kaggle/working/csi_cache/lowrank/<label>.npy\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _apply_mode_with_cache(data_csi, label):\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     # lowrank یا sparse\n",
    "#     mode = CSI_INPUT_MODE\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             return np.load(p).astype(np.float32, copy=False)\n",
    "\n",
    "#     L, S = _rpca_keep_shape(data_csi)\n",
    "#     out = L if mode == \"lowrank\" else S\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out.astype(np.float32))\n",
    "\n",
    "#     return out.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ RPCA lowrank/sparse بدون تغییر shape + با cache\n",
    "#         data_csi = _apply_mode_with_cache(data_csi, var_label)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "#                                     \"user_3_activity\", \"user_4_activity\",\n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e87ee",
   "metadata": {
    "papermill": {
     "duration": 0.006798,
     "end_time": "2025-12-28T20:25:02.405790",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.398992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "svd\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b2b4b46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.420757Z",
     "iopub.status.busy": "2025-12-28T20:25:02.420544Z",
     "iopub.status.idle": "2025-12-28T20:25:02.427542Z",
     "shell.execute_reply": "2025-12-28T20:25:02.427037Z"
    },
    "papermill": {
     "duration": 0.01591,
     "end_time": "2025-12-28T20:25:02.428602",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.412692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # Note: All necessary libraries (os, numpy, pandas, etc.) are imported in Cell 1.\n",
    "# # from preset import preset   --> preset is already defined in Cell 2.\n",
    "\n",
    "# # =========================================================\n",
    "# # 🔧 NEW: Choose CSI representation mode here (ONLY EDIT THIS)\n",
    "# # ---------------------------------------------------------\n",
    "# # \"raw\"     : use original CSI amplitude as-is\n",
    "# # \"lowrank\" : use low-rank approximation (SVD)\n",
    "# # \"sparse\"  : keep only large-magnitude entries (dense array with many zeros)\n",
    "# CSI_INPUT_MODE = \"sparse\"     # <-- set to: \"raw\" / \"lowrank\" / \"sparse\"\n",
    "\n",
    "# # Low-rank settings\n",
    "# LOW_RANK_ENERGY = 0.95     # keep enough singular values to preserve this energy\n",
    "# LOW_RANK_RANK   = None     # if set to an int (e.g., 10), it overrides ENERGY\n",
    "\n",
    "# # Sparse settings\n",
    "# SPARSE_KEEP_RATIO = 0.10   # keep top 10% magnitudes (globally per sample)\n",
    "# SPARSE_MIN_ABS    = None   # if set (e.g., 0.5), keeps |x|>=threshold instead of keep_ratio\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _low_rank_approx_keep_shape(X, rank=None, energy=0.95):\n",
    "#     \"\"\"\n",
    "#     Low-rank approximation using SVD while preserving the original shape.\n",
    "#     Works for 1D/2D/ND by flattening all non-time dims into features.\n",
    "#     Assumes first axis is time.\n",
    "#     \"\"\"\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "\n",
    "#     if X.ndim == 1:\n",
    "#         M = X[:, None]  # (T,1)\n",
    "#         U, S, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "#         if rank is None:\n",
    "#             s2 = S**2\n",
    "#             cum = np.cumsum(s2) / (np.sum(s2) + 1e-12)\n",
    "#             rank = int(np.searchsorted(cum, energy) + 1)\n",
    "#         rank = max(1, min(rank, S.shape[0]))\n",
    "#         M_lr = (U[:, :rank] * S[:rank]) @ Vt[:rank, :]\n",
    "#         return M_lr[:, 0].astype(np.float32)\n",
    "\n",
    "#     # ND: reshape to (T, F)\n",
    "#     T = X.shape[0]\n",
    "#     F = int(np.prod(X.shape[1:]))\n",
    "#     M = X.reshape(T, F)\n",
    "\n",
    "#     U, S, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "\n",
    "#     if rank is None:\n",
    "#         s2 = S**2\n",
    "#         cum = np.cumsum(s2) / (np.sum(s2) + 1e-12)\n",
    "#         rank = int(np.searchsorted(cum, energy) + 1)\n",
    "\n",
    "#     rank = max(1, min(rank, S.shape[0]))\n",
    "#     M_lr = (U[:, :rank] * S[:rank]) @ Vt[:rank, :]\n",
    "\n",
    "#     return M_lr.reshape(X.shape).astype(np.float32)\n",
    "\n",
    "\n",
    "# def _to_sparse_dense_keep_shape(X, keep_ratio=0.10, min_abs=None):\n",
    "#     \"\"\"\n",
    "#     Makes X sparse-in-content (many zeros) but keeps it as a dense numpy array\n",
    "#     so the rest of the pipeline (np.save/np.load/pad/model) doesn't change.\n",
    "#     Keeps the same shape.\n",
    "#     \"\"\"\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "#     flat = X.ravel()\n",
    "#     if flat.size == 0:\n",
    "#         return X.astype(np.float32)\n",
    "\n",
    "#     absflat = np.abs(flat)\n",
    "\n",
    "#     if min_abs is not None:\n",
    "#         thr = float(min_abs)\n",
    "#         mask = absflat >= thr\n",
    "#     else:\n",
    "#         k = int(np.ceil(keep_ratio * flat.size))\n",
    "#         k = max(1, min(k, flat.size))\n",
    "#         if k == flat.size:\n",
    "#             mask = np.ones_like(absflat, dtype=bool)\n",
    "#         else:\n",
    "#             thr = np.partition(absflat, -k)[-k]\n",
    "#             mask = absflat >= thr\n",
    "\n",
    "#     out = np.zeros_like(flat, dtype=np.float32)\n",
    "#     out[mask] = flat[mask]\n",
    "#     return out.reshape(X.shape).astype(np.float32)\n",
    "\n",
    "\n",
    "# def _apply_csi_mode(data_csi):\n",
    "#     \"\"\"\n",
    "#     Apply selected CSI_INPUT_MODE to a single sample array.\n",
    "#     \"\"\"\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     elif CSI_INPUT_MODE == \"lowrank\":\n",
    "#         return _low_rank_approx_keep_shape(\n",
    "#             data_csi,\n",
    "#             rank=LOW_RANK_RANK,\n",
    "#             energy=LOW_RANK_ENERGY\n",
    "#         )\n",
    "\n",
    "#     elif CSI_INPUT_MODE == \"sparse\":\n",
    "#         return _to_sparse_dense_keep_shape(\n",
    "#             data_csi,\n",
    "#             keep_ratio=SPARSE_KEEP_RATIO,\n",
    "#             min_abs=SPARSE_MIN_ABS\n",
    "#         )\n",
    "\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown CSI_INPUT_MODE: {CSI_INPUT_MODE}. Use 'raw', 'lowrank', or 'sparse'.\")\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     \"\"\"\n",
    "#     Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "#     \"\"\"\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     Load CSI amplitude (*.npy) files based on a label list.\n",
    "#     \"\"\"\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_path in var_path_list:\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ NEW: convert input CSI according to selected mode (raw/lowrank/sparse)\n",
    "#         data_csi = _apply_csi_mode(data_csi)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "#     data_x = np.array(data_x)\n",
    "#     return data_x\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     \"\"\"\n",
    "#     Encode labels according to specific task.\n",
    "#     \"\"\"\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     return data_identity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for activity labels.\n",
    "#     \"\"\"\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "#                                     \"user_3_activity\", \"user_4_activity\", \n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "#     return data_activity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for location labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "#     return data_location_onehot_y\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     print(\"data_identity_onehot_y\",data_identity_onehot_y.shape)\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1)\n",
    "#     print(\"count_data\",count_data.shape)\n",
    "#     count_data = count_data.reshape(-1, 1)  # shape = (11286, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)  \n",
    "#     count_data_onehot = encoder.fit_transform(count_data)\n",
    "#     print(count_data_onehot.shape)  \n",
    "#     count_data_onehot = count_data_onehot.astype(\"int8\")\n",
    "\n",
    "#     return count_data_onehot\n",
    "\n",
    "\n",
    "# # Test functions (optional)\n",
    "# def test_load_data_y():\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"classroom\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=[\"1\", \"2\", \"3\"]).describe())\n",
    "\n",
    "# def test_load_data_x():\n",
    "#     data_pd_y = load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=None)\n",
    "#     var_label_list = data_pd_y[\"label\"].to_list()\n",
    "#     data_x = load_data_x(preset[\"path\"][\"data_x\"], var_label_list)\n",
    "#     print(data_x.shape)\n",
    "\n",
    "# def test_encode_identity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_identity_onehot_y = encode_identity(data_pd_y)\n",
    "#     print(data_identity_onehot_y.shape)\n",
    "#     print(data_identity_onehot_y[2000])\n",
    "\n",
    "# def test_encode_activity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_activity_onehot_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     print(data_activity_onehot_y.shape)\n",
    "#     print(data_activity_onehot_y[1560])\n",
    "\n",
    "# def test_encode_location():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_location_onehot_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     print(data_location_onehot_y.shape)\n",
    "#     print(data_location_onehot_y[1560])\n",
    "\n",
    "# def test_encode_count():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_count_onehot_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     print(data_count_onehot_y.shape)\n",
    "#     print(data_count_onehot_y[20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8746eb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.443491Z",
     "iopub.status.busy": "2025-12-28T20:25:02.443272Z",
     "iopub.status.idle": "2025-12-28T20:25:02.448660Z",
     "shell.execute_reply": "2025-12-28T20:25:02.448167Z"
    },
    "papermill": {
     "duration": 0.014155,
     "end_time": "2025-12-28T20:25:02.449767",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.435612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # Note: All necessary libraries (os, numpy, pandas, etc.) are imported in Cell 1.\n",
    "# # from preset import preset   --> preset is already defined in Cell 2.\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     \"\"\"\n",
    "#     Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "#     \"\"\"\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     Load CSI amplitude (*.npy) files based on a label list.\n",
    "#     \"\"\"\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_path in var_path_list:\n",
    "#         data_csi = np.load(var_path)\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "#     data_x = np.array(data_x)\n",
    "#     return data_x\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     \"\"\"\n",
    "#     Encode labels according to specific task.\n",
    "#     \"\"\"\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     return data_identity_onehot_y\n",
    "\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for activity labels.\n",
    "#     \"\"\"\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "#                                     \"user_3_activity\", \"user_4_activity\", \n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "#     return data_activity_onehot_y\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for location labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "#     return data_location_onehot_y\n",
    "\n",
    "# # Test functions (optional)\n",
    "# def test_load_data_y():\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"classroom\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=[\"1\", \"2\", \"3\"]).describe())\n",
    "\n",
    "# def test_load_data_x():\n",
    "#     data_pd_y = load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=None)\n",
    "#     var_label_list = data_pd_y[\"label\"].to_list()\n",
    "#     data_x = load_data_x(preset[\"path\"][\"data_x\"], var_label_list)\n",
    "#     print(data_x.shape)\n",
    "\n",
    "# def test_encode_identity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_identity_onehot_y = encode_identity(data_pd_y)\n",
    "#     print(data_identity_onehot_y.shape)\n",
    "#     print(data_identity_onehot_y[2000])\n",
    "\n",
    "# def test_encode_activity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_activity_onehot_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     print(data_activity_onehot_y.shape)\n",
    "#     print(data_activity_onehot_y[1560])\n",
    "\n",
    "# def test_encode_location():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_location_onehot_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     print(data_location_onehot_y.shape)\n",
    "#     print(data_location_onehot_y[1560])\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     print(\"data_identity_onehot_y\",data_identity_onehot_y.shape)\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1)\n",
    "#     print(\"count_data\",count_data.shape)\n",
    "#     count_data = count_data.reshape(-1, 1)  # shape = (11286, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)  \n",
    "#     count_data_onehot = encoder.fit_transform(count_data)\n",
    "#     print(count_data_onehot.shape)  \n",
    "#     count_data_onehot = count_data_onehot.astype(\"int8\")\n",
    "\n",
    "#     return count_data_onehot\n",
    "\n",
    "\n",
    "# def test_encode_count():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_count_onehot_y = encode_count(data_pd_y)\n",
    "#     print(data_count_onehot_y.shape)\n",
    "#     print(data_count_onehot_y[20])\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     test_encode_count()\n",
    "# #     test_load_data_y()\n",
    "# #     test_load_data_x()\n",
    "# #     test_encode_identity()\n",
    "# #     test_encode_activity()\n",
    "# #     test_encode_location()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44c02f4",
   "metadata": {
    "papermill": {
     "duration": 0.006924,
     "end_time": "2025-12-28T20:25:02.463732",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.456808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 4: preprocess.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34cd126f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.478645Z",
     "iopub.status.busy": "2025-12-28T20:25:02.478415Z",
     "iopub.status.idle": "2025-12-28T20:25:02.492772Z",
     "shell.execute_reply": "2025-12-28T20:25:02.492228Z"
    },
    "papermill": {
     "duration": 0.023302,
     "end_time": "2025-12-28T20:25:02.493825",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.470523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          preprocess.py\n",
    "[description]   preprocess WiFi CSI data\n",
    "\"\"\"\n",
    "\n",
    "# All necessary libraries are already imported in Cell 1.\n",
    "\n",
    "# def mat_to_amp(data_mat):\n",
    "#     \"\"\"\n",
    "#     Calculate amplitude of raw WiFi CSI data.\n",
    "#     \"\"\"\n",
    "#     var_length = data_mat[\"trace\"].shape[0]\n",
    "#     data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "#     data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "#     return data_csi_amp\n",
    "\n",
    "def extract_csi_amp(var_dir_mat, var_dir_amp):\n",
    "    \"\"\"\n",
    "    Read raw WiFi CSI (*.mat) files, calculate CSI amplitude, and save as (*.npy).\n",
    "    \"\"\"\n",
    "    var_path_mat = os.listdir(var_dir_mat)\n",
    "    for var_c, var_path in enumerate(var_path_mat):\n",
    "        data_mat = scio.loadmat(os.path.join(var_dir_mat, var_path))\n",
    "        data_csi_amp = mat_to_amp(data_mat)\n",
    "        # print(var_c, data_csi_amp.shape)\n",
    "        var_path_save = os.path.join(var_dir_amp, var_path.replace(\".mat\", \".npy\"))\n",
    "        with open(var_path_save, \"wb\") as var_file:\n",
    "            np.save(var_file, data_csi_amp)\n",
    "\n",
    "\n",
    "\n",
    "# # تنظیمات low-rank (بدون تغییر ورودی mat_to_amp)\n",
    "# LOW_RANK_ENERGY = 0.95   # مثلاً 95% انرژی\n",
    "# LOW_RANK_RANK = None     # اگر عدد بذاری (مثلاً 5)، به جای ENERGY از rank ثابت استفاده میشه\n",
    "\n",
    "# def _low_rank_approx(X, rank=None, energy=0.95):\n",
    "#     X = np.asarray(X)\n",
    "\n",
    "#     was_1d = (X.ndim == 1)\n",
    "#     if was_1d:\n",
    "#         X = X[:, None]\n",
    "\n",
    "#     U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "#     if rank is None:\n",
    "#         s2 = S**2\n",
    "#         cum = np.cumsum(s2) / (np.sum(s2) + 1e-12)\n",
    "#         rank = int(np.searchsorted(cum, energy) + 1)\n",
    "\n",
    "#     rank = max(1, min(rank, S.shape[0]))\n",
    "#     X_lr = (U[:, :rank] * S[:rank]) @ Vt[:rank, :]\n",
    "\n",
    "#     if was_1d:\n",
    "#         X_lr = X_lr[:, 0]\n",
    "\n",
    "#     return X_lr.astype(np.float32)\n",
    "\n",
    "# def mat_to_amp(data_mat):\n",
    "#     \"\"\"\n",
    "#     Calculate amplitude of raw WiFi CSI data, then return its low-rank approximation.\n",
    "#     (ورودی تابع تغییر نکرده)\n",
    "#     \"\"\"\n",
    "#     var_length = data_mat[\"trace\"].shape[0]\n",
    "#     data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "#     data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "#     # خروجی low-rank با همان ابعاد\n",
    "#     data_csi_amp_lr = _low_rank_approx(\n",
    "#         data_csi_amp,\n",
    "#         rank=LOW_RANK_RANK,\n",
    "#         energy=LOW_RANK_ENERGY\n",
    "#     )\n",
    "#     return data_csi_amp_lr\n",
    "\n",
    "\n",
    "\n",
    "# # تنظیمات sparsity (بدون تغییر ورودی mat_to_amp)\n",
    "# SPARSE_KEEP_RATIO = 0.10   # مثلا فقط 10% بزرگترین مقادیر نگه داشته بشن\n",
    "# SPARSE_MIN_ABS = None      # اگر عدد بذاری (مثلا 0.5)، به جای keep_ratio آستانه ثابت میشه\n",
    "\n",
    "# def _to_sparse(X, keep_ratio=0.10, min_abs=None):\n",
    "#     \"\"\"\n",
    "#     Convert X to a sparse representation by keeping only large-magnitude entries.\n",
    "#     Returns:\n",
    "#       - scipy.sparse.csr_matrix if SciPy is available\n",
    "#       - otherwise returns a dense array with many zeros (still \"sparse\" in content)\n",
    "#     \"\"\"\n",
    "#     X = np.asarray(X)\n",
    "#     flat = X.ravel()\n",
    "#     absflat = np.abs(flat)\n",
    "\n",
    "#     if flat.size == 0:\n",
    "#         return X.astype(np.float32)\n",
    "\n",
    "#     # انتخاب آستانه\n",
    "#     if min_abs is not None:\n",
    "#         thr = float(min_abs)\n",
    "#         mask = absflat >= thr\n",
    "#     else:\n",
    "#         k = int(np.ceil(keep_ratio * flat.size))\n",
    "#         k = max(1, min(k, flat.size))\n",
    "#         if k == flat.size:\n",
    "#             mask = np.ones_like(absflat, dtype=bool)\n",
    "#         else:\n",
    "#             thr = np.partition(absflat, -k)[-k]  # kth largest magnitude\n",
    "#             mask = absflat >= thr\n",
    "\n",
    "#     idx = np.nonzero(mask)[0]\n",
    "#     data = flat[idx].astype(np.float32)\n",
    "\n",
    "#     # اگر SciPy هست: sparse واقعی بساز\n",
    "#     try:\n",
    "#         # معمولاً تو Cell1 یا از قبل import شده؛ اگر هم نشده باشه اینجا تلاش می‌کنه.\n",
    "#         import scipy.sparse as sp\n",
    "\n",
    "#         if X.ndim == 1:\n",
    "#             rows = idx\n",
    "#             cols = np.zeros_like(rows)\n",
    "#             shape = (X.shape[0], 1)\n",
    "#         else:\n",
    "#             rows, cols = np.unravel_index(idx, X.shape)\n",
    "#             shape = X.shape\n",
    "\n",
    "#         return sp.coo_matrix((data, (rows, cols)), shape=shape).tocsr()\n",
    "\n",
    "#     except Exception:\n",
    "#         # fallback: آرایه‌ی dense با صفرهای زیاد\n",
    "#         out = np.zeros_like(flat, dtype=np.float32)\n",
    "#         out[idx] = data\n",
    "#         return out.reshape(X.shape)\n",
    "\n",
    "def mat_to_amp(data_mat):\n",
    "    \"\"\"\n",
    "    Calculate amplitude of raw WiFi CSI data, then return its sparse version.\n",
    "    (ورودی تابع تغییر نکرده)\n",
    "    \"\"\"\n",
    "    var_length = data_mat[\"trace\"].shape[0]\n",
    "    data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "    data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "    # خروجی sparse (CSR اگر SciPy باشد)\n",
    "    return _to_sparse(data_csi_amp, keep_ratio=SPARSE_KEEP_RATIO, min_abs=SPARSE_MIN_ABS)\n",
    "\n",
    "# تنظیمات RPCA (می‌تونی عوضشون کنی)\n",
    "RPCA_MAX_ITER = 500\n",
    "RPCA_TOL = 1e-7\n",
    "RPCA_RHO = 1.5\n",
    "RPCA_MU_INIT = None     # None یعنی خودکار\n",
    "RPCA_LAMBDA = None      # None یعنی 1/sqrt(max(m,n))\n",
    "\n",
    "def _soft_threshold(X, tau):\n",
    "    return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "def _svt(X, tau):\n",
    "    # Singular Value Thresholding\n",
    "    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    s_thr = np.maximum(s - tau, 0.0)\n",
    "    # اگر همه صفر شد، سریع برگرد\n",
    "    if np.all(s_thr == 0):\n",
    "        return np.zeros_like(X)\n",
    "    return (U * s_thr) @ Vt\n",
    "\n",
    "def _rpca_ialm(M, lam=None, mu=None, rho=1.5, max_iter=500, tol=1e-7):\n",
    "    \"\"\"\n",
    "    Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "    Decompose: M = L + S\n",
    "    Returns: L, S (same shape as M)\n",
    "    \"\"\"\n",
    "    M = M.astype(np.float64, copy=False)\n",
    "    m, n = M.shape\n",
    "\n",
    "    if lam is None:\n",
    "        lam = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "    # mu پیشنهادی (خودکار)\n",
    "    if mu is None:\n",
    "        # ||M||_2 تقریباً بزرگ‌ترین singular value است\n",
    "        norm2 = np.linalg.svd(M, compute_uv=False)[0] if M.size else 1.0\n",
    "        mu = 1.25 / (norm2 + 1e-12)\n",
    "\n",
    "    L = np.zeros_like(M)\n",
    "    S = np.zeros_like(M)\n",
    "    Y = np.zeros_like(M)\n",
    "\n",
    "    normM = np.linalg.norm(M, ord='fro') + 1e-12\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # L update\n",
    "        L = _svt(M - S + (1.0/mu)*Y, 1.0/mu)\n",
    "\n",
    "        # S update (sparse)\n",
    "        S = _soft_threshold(M - L + (1.0/mu)*Y, lam/mu)\n",
    "\n",
    "        # dual update\n",
    "        R = M - L - S\n",
    "        Y = Y + mu * R\n",
    "\n",
    "        # stop\n",
    "        err = np.linalg.norm(R, ord='fro') / normM\n",
    "        if err < tol:\n",
    "            break\n",
    "\n",
    "        mu *= rho\n",
    "\n",
    "    return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "# def mat_to_amp(data_mat):\n",
    "#     \"\"\"\n",
    "#     Calculate amplitude of raw WiFi CSI data, then return RPCA sparse component S.\n",
    "#     (ورودی تابع تغییر نکرده)\n",
    "#     \"\"\"\n",
    "#     var_length = data_mat[\"trace\"].shape[0]\n",
    "#     data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "#     data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "#     was_1d = (data_csi_amp.ndim == 1)\n",
    "#     M = data_csi_amp[:, None] if was_1d else data_csi_amp\n",
    "\n",
    "#     _, S = _rpca_ialm(\n",
    "#         M,\n",
    "#         lam=RPCA_LAMBDA,\n",
    "#         mu=RPCA_MU_INIT,\n",
    "#         rho=RPCA_RHO,\n",
    "#         max_iter=RPCA_MAX_ITER,\n",
    "#         tol=RPCA_TOL\n",
    "#     )\n",
    "\n",
    "#     if was_1d:\n",
    "#         S = S[:, 0]\n",
    "\n",
    "#     return S\n",
    "\n",
    "def mat_to_amp(data_mat):\n",
    "    \"\"\"\n",
    "    Calculate amplitude of raw WiFi CSI data, then return RPCA low-rank component L.\n",
    "    (ورودی تابع تغییر نکرده)\n",
    "    \"\"\"\n",
    "    var_length = data_mat[\"trace\"].shape[0]\n",
    "    data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "    data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "    was_1d = (data_csi_amp.ndim == 1)\n",
    "    M = data_csi_amp[:, None] if was_1d else data_csi_amp\n",
    "\n",
    "    L, _ = _rpca_ialm(\n",
    "        M,\n",
    "        lam=RPCA_LAMBDA,\n",
    "        mu=RPCA_MU_INIT,\n",
    "        rho=RPCA_RHO,\n",
    "        max_iter=RPCA_MAX_ITER,\n",
    "        tol=RPCA_TOL\n",
    "    )\n",
    "\n",
    "    if was_1d:\n",
    "        L = L[:, 0]\n",
    "\n",
    "    return L\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parse arguments from input.\n",
    "    \"\"\"\n",
    "    var_args = argparse.ArgumentParser()\n",
    "    var_args.add_argument(\"--dir_mat\", default=\"/kaggle/input/wimans/wifi_csi/mat\", type=str)\n",
    "    var_args.add_argument(\"--dir_amp\", default=\"/kaggle/input/wimans/wifi_csi/amp\", type=str)\n",
    "    return var_args.parse_args()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     var_args = parse_args()\n",
    "#     extract_csi_amp(var_dir_mat=var_args.dir_mat, var_dir_amp=var_args.dir_amp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a42f2",
   "metadata": {
    "papermill": {
     "duration": 0.006904,
     "end_time": "2025-12-28T20:25:02.507871",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.500967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 5: that.py (WiFi-based Model THAT)\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dcf56b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.522741Z",
     "iopub.status.busy": "2025-12-28T20:25:02.522523Z",
     "iopub.status.idle": "2025-12-28T20:25:02.546658Z",
     "shell.execute_reply": "2025-12-28T20:25:02.546170Z"
    },
    "papermill": {
     "duration": 0.032882,
     "end_time": "2025-12-28T20:25:02.547673",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.514791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          that.py\n",
    "[description]   implement and evaluate WiFi-based model THAT\n",
    "                https://github.com/windofshadow/THAT\n",
    "\"\"\"\n",
    "\n",
    "# All necessary libraries are imported in Cell 1.\n",
    "# from train import train   --> Defined in Cell 6.\n",
    "# from preset import preset --> Defined in Cell 2.\n",
    "\n",
    "class Gaussian_Position(torch.nn.Module):\n",
    "    def __init__(self, var_dim_feature, var_dim_time, var_num_gaussian=10):\n",
    "        super(Gaussian_Position, self).__init__()\n",
    "        var_embedding = torch.zeros([var_num_gaussian, var_dim_feature], dtype=torch.float)\n",
    "        self.var_embedding = torch.nn.Parameter(var_embedding, requires_grad=True)\n",
    "        torch.nn.init.xavier_uniform_(self.var_embedding)\n",
    "        var_position = torch.arange(0.0, var_dim_time).unsqueeze(1).repeat(1, var_num_gaussian)\n",
    "        self.var_position = torch.nn.Parameter(var_position, requires_grad=False)\n",
    "        var_mu = torch.arange(0.0, var_dim_time, var_dim_time/var_num_gaussian).unsqueeze(0)\n",
    "        self.var_mu = torch.nn.Parameter(var_mu, requires_grad=True)\n",
    "        var_sigma = torch.tensor([50.0] * var_num_gaussian).unsqueeze(0)\n",
    "        self.var_sigma = torch.nn.Parameter(var_sigma, requires_grad=True)\n",
    "\n",
    "    def calculate_pdf(self, var_position, var_mu, var_sigma):\n",
    "        var_pdf = var_position - var_mu\n",
    "        var_pdf = - var_pdf * var_pdf\n",
    "        var_pdf = var_pdf / var_sigma / var_sigma / 2\n",
    "        var_pdf = var_pdf - torch.log(var_sigma)\n",
    "        return var_pdf\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        var_pdf = self.calculate_pdf(self.var_position, self.var_mu, self.var_sigma)\n",
    "        var_pdf = torch.softmax(var_pdf, dim=-1)\n",
    "        var_position_encoding = torch.matmul(var_pdf, self.var_embedding)\n",
    "        var_output = var_input + var_position_encoding.unsqueeze(0)\n",
    "        return var_output\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, var_dim_feature, var_num_head=10, var_size_cnn=[1, 3, 5]):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layer_norm_0 = torch.nn.LayerNorm(var_dim_feature, eps=1e-6)\n",
    "        self.layer_attention = torch.nn.MultiheadAttention(var_dim_feature, var_num_head, batch_first=True)\n",
    "        self.layer_dropout_0 = torch.nn.Dropout(0.1)\n",
    "        self.layer_norm_1 = torch.nn.LayerNorm(var_dim_feature, 1e-6)\n",
    "        layer_cnn = []\n",
    "        for var_size in var_size_cnn:\n",
    "            layer = torch.nn.Sequential(\n",
    "                torch.nn.Conv1d(var_dim_feature, var_dim_feature, var_size, padding=\"same\"),\n",
    "                torch.nn.BatchNorm1d(var_dim_feature),\n",
    "                torch.nn.Dropout(0.1),\n",
    "                torch.nn.LeakyReLU()\n",
    "            )\n",
    "            layer_cnn.append(layer)\n",
    "        self.layer_cnn = torch.nn.ModuleList(layer_cnn)\n",
    "        self.layer_dropout_1 = torch.nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        var_t = var_input\n",
    "        var_t = self.layer_norm_0(var_t)\n",
    "        var_t, _ = self.layer_attention(var_t, var_t, var_t)\n",
    "        var_t = self.layer_dropout_0(var_t)\n",
    "        var_t = var_t + var_input\n",
    "        var_s = self.layer_norm_1(var_t)\n",
    "        var_s = torch.permute(var_s, (0, 2, 1))\n",
    "        var_c = torch.stack([layer(var_s) for layer in self.layer_cnn], dim=0)\n",
    "        var_s = torch.sum(var_c, dim=0) / len(self.layer_cnn)\n",
    "        var_s = self.layer_dropout_1(var_s)\n",
    "        var_s = torch.permute(var_s, (0, 2, 1))\n",
    "        var_output = var_s + var_t\n",
    "        return var_output\n",
    "\n",
    "class THAT(torch.nn.Module):\n",
    "    def __init__(self, var_x_shape, var_y_shape):\n",
    "        super(THAT, self).__init__()\n",
    "        var_dim_feature = var_x_shape[-1]\n",
    "        var_dim_time = var_x_shape[-2]\n",
    "        var_dim_output = var_y_shape[-1]\n",
    "        # Left branch\n",
    "        self.layer_left_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "        self.layer_left_gaussian = Gaussian_Position(var_dim_feature, var_dim_time // 20)\n",
    "        var_num_left = 4\n",
    "        var_dim_left = var_dim_feature\n",
    "        self.layer_left_encoder = torch.nn.ModuleList([\n",
    "            Encoder(var_dim_feature=var_dim_left, var_num_head=10, var_size_cnn=[1, 3, 5])\n",
    "            for _ in range(var_num_left)\n",
    "        ])\n",
    "        self.layer_left_norm = torch.nn.LayerNorm(var_dim_left, eps=1e-6)\n",
    "        self.layer_left_cnn_0 = torch.nn.Conv1d(in_channels=var_dim_left, out_channels=128, kernel_size=8)\n",
    "        self.layer_left_cnn_1 = torch.nn.Conv1d(in_channels=var_dim_left, out_channels=128, kernel_size=16)\n",
    "        self.layer_left_dropout = torch.nn.Dropout(0.5)\n",
    "        # Right branch\n",
    "        self.layer_right_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "        var_num_right = 1\n",
    "        var_dim_right = var_dim_time // 20\n",
    "        self.layer_right_encoder = torch.nn.ModuleList([\n",
    "            Encoder(var_dim_feature=var_dim_right, var_num_head=10, var_size_cnn=[1, 2, 3])\n",
    "            for _ in range(var_num_right)\n",
    "        ])\n",
    "        self.layer_right_norm = torch.nn.LayerNorm(var_dim_right, eps=1e-6)\n",
    "        self.layer_right_cnn_0 = torch.nn.Conv1d(in_channels=var_dim_right, out_channels=16, kernel_size=2)\n",
    "        self.layer_right_cnn_1 = torch.nn.Conv1d(in_channels=var_dim_right, out_channels=16, kernel_size=4)\n",
    "        self.layer_right_dropout = torch.nn.Dropout(0.5)\n",
    "        self.layer_leakyrelu = torch.nn.LeakyReLU()\n",
    "        self.layer_output = torch.nn.Linear(256 + 32, var_dim_output)\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        var_t = var_input  # shape: (batch_size, time_steps, features)\n",
    "        # Left branch\n",
    "        var_left = torch.permute(var_t, (0, 2, 1))\n",
    "        var_left = self.layer_left_pooling(var_left)\n",
    "        var_left = torch.permute(var_left, (0, 2, 1))\n",
    "        var_left = self.layer_left_gaussian(var_left)\n",
    "        for layer in self.layer_left_encoder:\n",
    "            var_left = layer(var_left)\n",
    "        var_left = self.layer_left_norm(var_left)\n",
    "        var_left = torch.permute(var_left, (0, 2, 1))\n",
    "        var_left_0 = self.layer_leakyrelu(self.layer_left_cnn_0(var_left))\n",
    "        var_left_1 = self.layer_leakyrelu(self.layer_left_cnn_1(var_left))\n",
    "        var_left_0 = torch.sum(var_left_0, dim=-1)\n",
    "        var_left_1 = torch.sum(var_left_1, dim=-1)\n",
    "        var_left = torch.concat([var_left_0, var_left_1], dim=-1)\n",
    "        var_left = self.layer_left_dropout(var_left)\n",
    "        # Right branch\n",
    "        var_right = torch.permute(var_t, (0, 2, 1))\n",
    "        var_right = self.layer_right_pooling(var_right)\n",
    "        for layer in self.layer_right_encoder:\n",
    "            var_right = layer(var_right)\n",
    "        var_right = self.layer_right_norm(var_right)\n",
    "        var_right = torch.permute(var_right, (0, 2, 1))\n",
    "        var_right_0 = self.layer_leakyrelu(self.layer_right_cnn_0(var_right))\n",
    "        var_right_1 = self.layer_leakyrelu(self.layer_right_cnn_1(var_right))\n",
    "        var_right_0 = torch.sum(var_right_0, dim=-1)\n",
    "        var_right_1 = torch.sum(var_right_1, dim=-1)\n",
    "        var_right = torch.concat([var_right_0, var_right_1], dim=-1)\n",
    "        var_right = self.layer_right_dropout(var_right)\n",
    "        # Concatenate branches\n",
    "        var_t = torch.concat([var_left, var_right], dim=-1)\n",
    "        var_output = self.layer_output(var_t)\n",
    "        return var_output\n",
    "\n",
    "def run_that(data_train_x, data_train_y, data_test_x, data_test_y, var_repeat=10, init_model=None):\n",
    "    \"\"\"\n",
    "    Run WiFi-based model THAT.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data_train_x = data_train_x.reshape(data_train_x.shape[0], data_train_x.shape[1], -1)\n",
    "    data_test_x = data_test_x.reshape(data_test_x.shape[0], data_test_x.shape[1], -1)\n",
    "    var_x_shape, var_y_shape = data_train_x[0].shape, data_train_y[0].reshape(-1).shape\n",
    "    data_train_set = TensorDataset(torch.from_numpy(data_train_x), torch.from_numpy(data_train_y))\n",
    "    data_test_set = TensorDataset(torch.from_numpy(data_test_x), torch.from_numpy(data_test_y))\n",
    "    \n",
    "    result = {}\n",
    "    result_accuracy = []\n",
    "    result_time_train = []\n",
    "    result_time_test = []\n",
    "    \n",
    "    # var_macs, var_params = get_model_complexity_info(THAT(var_x_shape, var_y_shape), var_x_shape, as_strings=False)\n",
    "    # print(\"Parameters:\", var_params, \"- FLOPs:\", var_macs * 2)\n",
    "    \n",
    "    for var_r in range(var_repeat):\n",
    "        print(\"Repeat\", var_r)\n",
    "        torch.random.manual_seed(var_r + 39)\n",
    "        if init_model is not None:\n",
    "            model_that = init_model\n",
    "            lr2 = preset[\"nn\"][\"lr\"] /10\n",
    "        else:\n",
    "            model_that = THAT(var_x_shape, var_y_shape).to(device)\n",
    "            lr2 = preset[\"nn\"][\"lr\"]\n",
    "\n",
    "        optimizer = torch.optim.Adam(model_that.parameters(), lr=lr2, weight_decay=0)\n",
    "        loss = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([4] * var_y_shape[-1]).to(device))\n",
    "        var_time_0 = time.time()\n",
    "        \n",
    "        # Train\n",
    "        var_best_weight = train(model=model_that, optimizer=optimizer, loss=loss, \n",
    "                                  data_train_set=data_train_set, data_test_set=data_test_set,\n",
    "                                  var_threshold=preset[\"nn\"][\"threshold\"],\n",
    "                                  var_batch_size=preset[\"nn\"][\"batch_size\"],\n",
    "                                  var_epochs=preset[\"nn\"][\"epoch\"],\n",
    "                                  device=device)\n",
    "        var_time_1 = time.time()\n",
    "        \n",
    "        # Test\n",
    "        model_that.load_state_dict(var_best_weight)\n",
    "        with torch.no_grad():\n",
    "            predict_test_y = model_that(torch.from_numpy(data_test_x).to(device))\n",
    "        predict_test_y = (torch.sigmoid(predict_test_y) > preset[\"nn\"][\"threshold\"]).float()\n",
    "        predict_test_y = predict_test_y.detach().cpu().numpy()\n",
    "        var_time_2 = time.time()\n",
    "        \n",
    "        # Evaluate\n",
    "        data_test_y_c = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "        predict_test_y_c = predict_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "        result_acc = accuracy_score(data_test_y_c.astype(int), predict_test_y_c.astype(int))\n",
    "        result_dict = classification_report(data_test_y_c, predict_test_y_c, digits=6, zero_division=0, output_dict=True)\n",
    "        result[\"repeat_\" + str(var_r)] = result_dict\n",
    "        result_accuracy.append(result_acc)\n",
    "        result_time_train.append(var_time_1 - var_time_0)\n",
    "        result_time_test.append(var_time_2 - var_time_1)\n",
    "        print(\"repeat_\" + str(var_r), result_accuracy)\n",
    "        print(result)\n",
    "    \n",
    "    result[\"accuracy\"] = {\"avg\": np.mean(result_accuracy), \"std\": np.std(result_accuracy)}\n",
    "    result[\"time_train\"] = {\"avg\": np.mean(result_time_train), \"std\": np.std(result_time_train)}\n",
    "    result[\"time_test\"] = {\"avg\": np.mean(result_time_test), \"std\": np.std(result_time_test)}\n",
    "    # result[\"complexity\"] = {\"parameter\": var_params, \"flops\": var_macs * 2}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f84612",
   "metadata": {
    "papermill": {
     "duration": 0.006825,
     "end_time": "2025-12-28T20:25:02.561693",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.554868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c43593f",
   "metadata": {
    "papermill": {
     "duration": 0.006916,
     "end_time": "2025-12-28T20:25:02.575633",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.568717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell7: for RESNET18 Model\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0120094b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.590837Z",
     "iopub.status.busy": "2025-12-28T20:25:02.590622Z",
     "iopub.status.idle": "2025-12-28T20:25:02.597112Z",
     "shell.execute_reply": "2025-12-28T20:25:02.596422Z"
    },
    "papermill": {
     "duration": 0.015697,
     "end_time": "2025-12-28T20:25:02.598378",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.582681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# import torch._dynamo\n",
    "# torch._dynamo.config.suppress_errors = True\n",
    "# import time\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "# import numpy as np\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# import torchvision.models as models\n",
    "# from copy import deepcopy\n",
    "\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "# torch._dynamo.config.cache_size_limit = 65536\n",
    "\n",
    "# # فرض می‌کنیم preset قبلاً تعریف شده باشه\n",
    "# # preset = { \"nn\": {\"lr\": 1e-3, \"epoch\": 10, \"batch_size\": 4, \"threshold\": 0.5}, ... }\n",
    "\n",
    "# class ResNet18Model(torch.nn.Module):\n",
    "#     def __init__(self, var_x_shape, var_y_shape):\n",
    "#         super(ResNet18Model, self).__init__()\n",
    "#         model_resnet = models.resnet18(weights=None)\n",
    "#         model_resnet.conv1 = torch.nn.Conv2d(1, 64, 7, 3, 2, bias=False)\n",
    "#         in_features_fc = model_resnet.fc.in_features  # معمولاً 512\n",
    "#         out_features_fc = var_y_shape[-1]\n",
    "#         model_resnet.fc = torch.nn.Linear(in_features_fc, out_features_fc)\n",
    "#         self.resnet = model_resnet\n",
    "\n",
    "#     def forward(self, var_input):\n",
    "#         var_input = var_input.reshape(var_input.size(0), 1, 3000, 270)\n",
    "#         return self.resnet(var_input)\n",
    "\n",
    "# def run_resnet(data_train_x, data_train_y, data_test_x, data_test_y, var_repeat=10, init_model=None):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     var_x_shape = data_train_x[0].shape\n",
    "#     var_y_shape = data_train_y[0].reshape(-1).shape\n",
    "\n",
    "#     # تغییر شکل داده‌ها روی CPU\n",
    "#     data_train_x = data_train_x.reshape(data_train_x.shape[0], 1, data_train_x.shape[1],\n",
    "#                                         data_train_x.shape[2]*data_train_x.shape[3]*data_train_x.shape[4])\n",
    "#     data_test_x  = data_test_x.reshape(data_test_x.shape[0], 1, data_test_x.shape[1],\n",
    "#                                        data_test_x.shape[2]*data_test_x.shape[3]*data_test_x.shape[4])\n",
    "    \n",
    "#     # دیتاست‌ها روی CPU\n",
    "#     data_train_set = TensorDataset(torch.from_numpy(data_train_x).float(),\n",
    "#                                    torch.from_numpy(data_train_y).float())\n",
    "#     data_test_set  = TensorDataset(torch.from_numpy(data_test_x).float(),\n",
    "#                                    torch.from_numpy(data_test_y).float())\n",
    "    \n",
    "#     result = {}\n",
    "#     result_accuracy = []\n",
    "#     result_time_train = []\n",
    "#     result_time_test = []\n",
    "    \n",
    "#     for var_r in range(var_repeat):\n",
    "#         print(\"Repeat\", var_r)\n",
    "#         torch.random.manual_seed(var_r + 39)\n",
    "        \n",
    "#         # ساخت مدل و انتقال به GPU\n",
    "#         if init_model is not None:\n",
    "#             model_resnet = init_model\n",
    "#             lr2 = preset[\"nn\"][\"lr\"] /10\n",
    "            \n",
    "#         else:\n",
    "#             model_resnet = ResNet18Model(var_x_shape, var_y_shape).to(device)\n",
    "#             lr2 = preset[\"nn\"][\"lr\"]\n",
    "\n",
    "#         optimizer = torch.optim.Adam(model_resnet.parameters(), lr=lr2, weight_decay=0)\n",
    "#         loss_func = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([6] * var_y_shape[-1]).to(device))\n",
    "        \n",
    "#         # تابع آموزش داخلی؛ دیتا روی CPU باقی می‌مونه و فقط هنگام محاسبه batch به GPU میره\n",
    "#         def train_inner():\n",
    "#             train_loader = DataLoader(data_train_set, preset[\"nn\"][\"batch_size\"], shuffle=True, pin_memory=False)\n",
    "#             test_loader = DataLoader(data_test_set, preset[\"nn\"][\"batch_size\"], shuffle=False, pin_memory=False)\n",
    "#             best_accuracy = 0\n",
    "#             best_weight = None\n",
    "            \n",
    "#             for epoch in range(preset[\"nn\"][\"epoch\"]):\n",
    "#                 t0 = time.time()\n",
    "#                 model_resnet.train()\n",
    "#                 # متغیرهای مربوط به آخرین batch آموزش\n",
    "#                 last_train_loss = None\n",
    "#                 last_train_acc = None\n",
    "#                 for batch in train_loader:\n",
    "#                     batch_x, batch_y = batch\n",
    "#                     batch_x = batch_x.to(device)\n",
    "#                     batch_y = batch_y.to(device)\n",
    "#                     outputs = model_resnet(batch_x)\n",
    "#                     loss_val = loss_func(outputs, batch_y.reshape(batch_y.shape[0], -1).float())\n",
    "#                     optimizer.zero_grad()\n",
    "#                     loss_val.backward()\n",
    "#                     optimizer.step()\n",
    "#                     last_train_loss = loss_val.item()\n",
    "#                     # محاسبه دقت آخرین batch آموزش\n",
    "#                     train_preds = (torch.sigmoid(outputs) > preset[\"nn\"][\"threshold\"]).float()\n",
    "#                     last_train_acc = accuracy_score(batch_y.reshape(batch_y.shape[0], -1).detach().cpu().numpy().astype(int),\n",
    "#                                                     train_preds.detach().cpu().numpy().astype(int))\n",
    "                \n",
    "#                 # ارزیابی روی دیتاست تست به صورت batch به batch\n",
    "#                 model_resnet.eval()\n",
    "#                 all_preds = []\n",
    "#                 all_labels = []\n",
    "#                 test_loss_val = None\n",
    "#                 with torch.no_grad():\n",
    "#                     for t_batch in test_loader:\n",
    "#                         t_x, t_y = t_batch\n",
    "#                         t_x = t_x.to(device)\n",
    "#                         outputs_test = model_resnet(t_x)\n",
    "#                         outputs_test = (torch.sigmoid(outputs_test) > preset[\"nn\"][\"threshold\"]).float()\n",
    "#                         all_preds.append(outputs_test.detach().cpu().numpy())\n",
    "#                         all_labels.append(t_y.cpu().numpy())  # اینجا تغییر دادیم\n",
    "#                 preds_cat = np.vstack(all_preds)\n",
    "#                 labels_cat = np.vstack(all_labels)\n",
    "#                 print(\"preds_cat\",preds_cat.shape)\n",
    "#                 # تبدیل به شکل (n, 6, 5)\n",
    "                \n",
    "#                 # preds_cat = preds_cat.reshape(-1, 6, 5)\n",
    "#                 # labels_cat = labels_cat.reshape(-1, 6, 5)\n",
    "\n",
    "#                 preds_cat = preds_cat.reshape(-1, 6)\n",
    "#                 labels_cat = labels_cat.reshape(-1, 6)\n",
    "                \n",
    "#                 # برای محاسبه دقت، مسطح می‌کنیم\n",
    "#                 test_acc = accuracy_score(labels_cat.reshape(labels_cat.shape[0], -1).astype(int),\n",
    "#                                           preds_cat.reshape(preds_cat.shape[0], -1).astype(int))\n",
    "#                 epoch_time = time.time() - t0\n",
    "#                 print(f\"Epoch {epoch}/{preset['nn']['epoch']} - \"\n",
    "#                       f\"Train Loss: {(last_train_loss if last_train_loss is not None else 0.0):.6f}, \"\n",
    "#                       f\"Train Acc: {(last_train_acc if last_train_acc is not None else 0.0):.6f}, \"\n",
    "#                       f\"Test Loss: {(test_loss_val if test_loss_val is not None else 0.0):.6f}, \"\n",
    "#                       f\"Test Acc: {(test_acc if test_acc is not None else 0.0):.6f} - \"\n",
    "#                       f\"Time: {epoch_time:.4f}s\")\n",
    "\n",
    "#                 if test_acc > best_accuracy:\n",
    "#                     best_accuracy = test_acc\n",
    "#                     print('-----***-----')\n",
    "#                     print(best_accuracy)\n",
    "#                     best_weight = deepcopy(model_resnet.state_dict())\n",
    "#             return best_weight\n",
    "        \n",
    "#         t0_run = time.time()\n",
    "#         best_weight = train_inner()\n",
    "#         t1_run = time.time()\n",
    "        \n",
    "#         torch.save(model_resnet.state_dict(), f\"{name_run}_model_final.pt\")\n",
    "#         model_resnet.load_state_dict(best_weight)\n",
    "#         torch.save(model_resnet.state_dict(), f\"{name_run}_best_model.pt\")\n",
    "\n",
    "#         # bad age niaz bod load koni\n",
    "#         # model_resnet = ResNet18Model(var_x_shape, var_y_shape).to(device)\n",
    "#         # model_resnet.load_state_dict(torch.load(\"resnet_model_repeat0.pt\"))\n",
    "#         # model_resnet.eval()\n",
    "\n",
    "        \n",
    "#         # ارزیابی نهایی مدل روی دیتاست تست (استفاده از batchهای کوچک)\n",
    "#         model_resnet.eval()\n",
    "#         all_preds = []\n",
    "#         test_loader_final = DataLoader(data_test_set, preset[\"nn\"][\"batch_size\"], shuffle=False, pin_memory=False)\n",
    "#         with torch.no_grad():\n",
    "#             for batch in test_loader_final:\n",
    "#                 batch_x, _ = batch\n",
    "#                 batch_x = batch_x.to(device)\n",
    "#                 all_preds.append(model_resnet(batch_x))\n",
    "#         preds_all = torch.cat(all_preds, dim=0)\n",
    "#         preds_final = (torch.sigmoid(preds_all) > preset[\"nn\"][\"threshold\"]).float().detach().cpu().numpy()\n",
    "#         t2_run = time.time()\n",
    "        \n",
    "#         data_test_y_np = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "#         preds_final = preds_final.reshape(-1, data_test_y.shape[-1])\n",
    "#         acc_final = accuracy_score(data_test_y_np.astype(int), preds_final.astype(int))\n",
    "#         result[f\"repeat_{var_r}\"] = {\"accuracy\": acc_final}\n",
    "#         result_accuracy.append(acc_final)\n",
    "#         result_time_train.append(t1_run - t0_run)\n",
    "#         result_time_test.append(t2_run - t1_run)\n",
    "#         print(\"Repeat\", var_r, \"Final Test Accuracy:\", acc_final)\n",
    "    \n",
    "#     result[\"accuracy\"] = {\"avg\": np.mean(result_accuracy), \"std\": np.std(result_accuracy)}\n",
    "#     result[\"time_train\"] = {\"avg\": np.mean(result_time_train), \"std\": np.std(result_time_train)}\n",
    "#     result[\"time_test\"] = {\"avg\": np.mean(result_time_test), \"std\": np.std(result_time_test)}\n",
    "#     return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0608a7d4",
   "metadata": {
    "papermill": {
     "duration": 0.006785,
     "end_time": "2025-12-28T20:25:02.612317",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.605532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 9: train.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e61bc66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:02.627199Z",
     "iopub.status.busy": "2025-12-28T20:25:02.627004Z",
     "iopub.status.idle": "2025-12-28T20:25:03.150329Z",
     "shell.execute_reply": "2025-12-28T20:25:03.149790Z"
    },
    "papermill": {
     "duration": 0.532678,
     "end_time": "2025-12-28T20:25:03.151871",
     "exception": false,
     "start_time": "2025-12-28T20:25:02.619193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          train.py\n",
    "[description]   function to train WiFi-based models\n",
    "\"\"\"\n",
    "\n",
    "# All necessary libraries are imported in Cell 1.\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch._dynamo.config.cache_size_limit = 65536\n",
    "\n",
    "def train(model, optimizer, loss, data_train_set, data_test_set, var_threshold, var_batch_size, var_epochs, device):\n",
    "    \"\"\"\n",
    "    Generic training function for WiFi-based models.\n",
    "    \"\"\"\n",
    "    # دیتا رو روی CPU نگه می‌داریم (pin_memory=False)\n",
    "    data_train_loader = DataLoader(data_train_set, var_batch_size, shuffle=True, pin_memory=False)\n",
    "    data_test_loader = DataLoader(data_test_set, batch_size=len(data_test_set), shuffle=False, pin_memory=False)\n",
    "    \n",
    "    var_best_accuracy = -1.0\n",
    "    var_best_weight   = deepcopy(model.state_dict())\n",
    "    \n",
    "    \n",
    "    for var_epoch in range(var_epochs):\n",
    "        var_time_e0 = time.time()\n",
    "        model.train()\n",
    "        for data_batch in data_train_loader:\n",
    "            data_batch_x, data_batch_y = data_batch\n",
    "            # انتقال موقتی داده به GPU فقط برای forward pass\n",
    "            data_batch_x = data_batch_x.to(device)\n",
    "            data_batch_y = data_batch_y.to(device)\n",
    "            predict_train_y = model(data_batch_x)\n",
    "            var_loss_train = loss(predict_train_y, data_batch_y.reshape(data_batch_y.shape[0], -1).float())\n",
    "            optimizer.zero_grad()\n",
    "            var_loss_train.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # محاسبه دقت روی آخرین batch و انتقال نتایج به CPU\n",
    "        predict_train_y = (torch.sigmoid(predict_train_y) > var_threshold).float()\n",
    "        data_batch_y = data_batch_y.detach().cpu().numpy()\n",
    "        predict_train_y = predict_train_y.detach().cpu().numpy()\n",
    "        \n",
    "        predict_train_y = predict_train_y.reshape(-1, data_batch_y.shape[-1])\n",
    "        data_batch_y = data_batch_y.reshape(-1, data_batch_y.shape[-1])\n",
    "        var_accuracy_train = accuracy_score(data_batch_y.astype(int), predict_train_y.astype(int))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            data_test_x, data_test_y = next(iter(data_test_loader))\n",
    "            # انتقال موقتی دیتا تست به GPU برای محاسبات\n",
    "            data_test_x = data_test_x.to(device)\n",
    "            data_test_y = data_test_y.to(device)\n",
    "            \n",
    "            predict_test_y = model(data_test_x)\n",
    "            var_loss_test = loss(predict_test_y, data_test_y.reshape(data_test_y.shape[0], -1).float())\n",
    "            \n",
    "            predict_test_y = (torch.sigmoid(predict_test_y) > var_threshold).float()\n",
    "            \n",
    "            # انتقال نتایج به CPU برای ارزیابی\n",
    "            data_test_y = data_test_y.detach().cpu().numpy()\n",
    "            predict_test_y = predict_test_y.detach().cpu().numpy()\n",
    "            \n",
    "            predict_test_y = predict_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            data_test_y = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            var_accuracy_test = accuracy_score(data_test_y.astype(int), predict_test_y.astype(int))\n",
    "        \n",
    "        print(f\"Epoch {var_epoch}/{var_epochs}\",\n",
    "              \"- %.6fs\"%(time.time() - var_time_e0),\n",
    "              \"- Loss %.6f\"%var_loss_train.cpu(),\n",
    "              \"- Accuracy %.6f\"%var_accuracy_train,\n",
    "              \"- Test Loss %.6f\"%var_loss_test.cpu(),\n",
    "              \"- Test Accuracy %.6f\"%var_accuracy_test)\n",
    "            \n",
    "        if var_accuracy_test > var_best_accuracy:\n",
    "            var_best_accuracy = var_accuracy_test\n",
    "            print('-----***-----')\n",
    "            print(var_best_accuracy)\n",
    "            var_best_weight = deepcopy(model.state_dict())\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{name_run}_model_final.pt\")\n",
    "    torch.save(var_best_weight, f\"{name_run}_best_model.pt\")\n",
    "\n",
    "    \n",
    "    return var_best_weight\n",
    "\n",
    "\n",
    "\n",
    "# === importsِ لازم را یک‌بار بالای فایل اضافه کن ===\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- تابع کمکی ----------\n",
    "def save_confusion_matrix(model, data_loader, threshold, device, pdf_path):\n",
    "    \"\"\"\n",
    "    Runs the model on `data_loader`, builds a confusion matrix and writes it to `pdf_path`.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "\n",
    "            preds = (torch.sigmoid(logits) > threshold).float().cpu().numpy().ravel()\n",
    "            yb    = yb.cpu().numpy().ravel()\n",
    "\n",
    "            y_true.extend(yb)\n",
    "            y_pred.extend(preds)\n",
    "\n",
    "    cm  = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    ConfusionMatrixDisplay(cm).plot(ax=ax)\n",
    "    ax.set_title(\"Confusion Matrix – Test\")\n",
    "\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "# ---------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02a42f7",
   "metadata": {
    "papermill": {
     "duration": 0.006999,
     "end_time": "2025-12-28T20:25:03.166572",
     "exception": false,
     "start_time": "2025-12-28T20:25:03.159573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 11: run.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c8f286d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T20:25:03.181627Z",
     "iopub.status.busy": "2025-12-28T20:25:03.181231Z",
     "iopub.status.idle": "2025-12-28T21:29:58.237844Z",
     "shell.execute_reply": "2025-12-28T21:29:58.236922Z"
    },
    "papermill": {
     "duration": 3895.065858,
     "end_time": "2025-12-28T21:29:58.239307",
     "exception": false,
     "start_time": "2025-12-28T20:25:03.173449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "\n",
      "[DEBUG] First loaded AMP sample: act_1_1\n",
      "[DEBUG] shape=(2835, 3, 3, 30), dtype=float32, complex=False\n",
      "[DEBUG] ==> Input is REAL -> amplitude-only (no true phase).\n",
      "\n",
      "Running model: THAT\n",
      "Repeat 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py:370: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1036.)\n",
      "  return F.conv1d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/300 - 6.180839s - Loss 2.284149 - Accuracy 0.104167 - Test Loss 1.235589 - Test Accuracy 0.313882\n",
      "-----***-----\n",
      "0.3138815207780725\n",
      "Epoch 1/300 - 5.021005s - Loss 0.906538 - Accuracy 0.276042 - Test Loss 0.687572 - Test Accuracy 0.546419\n",
      "-----***-----\n",
      "0.5464190981432361\n",
      "Epoch 2/300 - 5.041729s - Loss 0.788830 - Accuracy 0.317708 - Test Loss 0.549631 - Test Accuracy 0.551282\n",
      "-----***-----\n",
      "0.5512820512820513\n",
      "Epoch 3/300 - 5.020846s - Loss 0.635518 - Accuracy 0.338542 - Test Loss 0.513322 - Test Accuracy 0.542440\n",
      "Epoch 4/300 - 5.039014s - Loss 0.578383 - Accuracy 0.447917 - Test Loss 0.511168 - Test Accuracy 0.551724\n",
      "-----***-----\n",
      "0.5517241379310345\n",
      "Epoch 5/300 - 5.034047s - Loss 0.567837 - Accuracy 0.406250 - Test Loss 0.487986 - Test Accuracy 0.557913\n",
      "-----***-----\n",
      "0.5579133510167993\n",
      "Epoch 6/300 - 5.020698s - Loss 0.494784 - Accuracy 0.421875 - Test Loss 0.476386 - Test Accuracy 0.548630\n",
      "Epoch 7/300 - 5.032372s - Loss 0.544081 - Accuracy 0.453125 - Test Loss 0.481919 - Test Accuracy 0.549072\n",
      "Epoch 8/300 - 5.032421s - Loss 0.597454 - Accuracy 0.390625 - Test Loss 0.495687 - Test Accuracy 0.560124\n",
      "-----***-----\n",
      "0.5601237842617153\n",
      "Epoch 9/300 - 5.032967s - Loss 0.442911 - Accuracy 0.489583 - Test Loss 0.475754 - Test Accuracy 0.527851\n",
      "Epoch 10/300 - 5.059016s - Loss 0.474363 - Accuracy 0.494792 - Test Loss 0.482053 - Test Accuracy 0.518126\n",
      "Epoch 11/300 - 5.024608s - Loss 0.447980 - Accuracy 0.510417 - Test Loss 0.470529 - Test Accuracy 0.570734\n",
      "-----***-----\n",
      "0.5707338638373121\n",
      "Epoch 12/300 - 5.024841s - Loss 0.490812 - Accuracy 0.395833 - Test Loss 0.478146 - Test Accuracy 0.547303\n",
      "Epoch 13/300 - 5.026487s - Loss 0.404882 - Accuracy 0.536458 - Test Loss 0.470123 - Test Accuracy 0.565871\n",
      "Epoch 14/300 - 5.016721s - Loss 0.403868 - Accuracy 0.453125 - Test Loss 0.447363 - Test Accuracy 0.566755\n",
      "Epoch 15/300 - 5.015901s - Loss 0.430160 - Accuracy 0.479167 - Test Loss 0.456483 - Test Accuracy 0.573828\n",
      "-----***-----\n",
      "0.5738284703801945\n",
      "Epoch 16/300 - 5.033180s - Loss 0.380195 - Accuracy 0.473958 - Test Loss 0.450051 - Test Accuracy 0.560124\n",
      "Epoch 17/300 - 5.007358s - Loss 0.387912 - Accuracy 0.442708 - Test Loss 0.455739 - Test Accuracy 0.567639\n",
      "Epoch 18/300 - 5.027785s - Loss 0.393847 - Accuracy 0.500000 - Test Loss 0.444214 - Test Accuracy 0.552166\n",
      "Epoch 19/300 - 5.015730s - Loss 0.283385 - Accuracy 0.656250 - Test Loss 0.449269 - Test Accuracy 0.549072\n",
      "Epoch 20/300 - 5.020921s - Loss 0.347617 - Accuracy 0.489583 - Test Loss 0.459860 - Test Accuracy 0.561008\n",
      "Epoch 21/300 - 5.016328s - Loss 0.268255 - Accuracy 0.635417 - Test Loss 0.460312 - Test Accuracy 0.550840\n",
      "Epoch 22/300 - 5.021416s - Loss 0.275658 - Accuracy 0.661458 - Test Loss 0.474483 - Test Accuracy 0.576039\n",
      "-----***-----\n",
      "0.5760389036251106\n",
      "Epoch 23/300 - 5.015542s - Loss 0.256435 - Accuracy 0.572917 - Test Loss 0.477573 - Test Accuracy 0.566755\n",
      "Epoch 24/300 - 5.050096s - Loss 0.228345 - Accuracy 0.635417 - Test Loss 0.470067 - Test Accuracy 0.568081\n",
      "Epoch 25/300 - 5.006336s - Loss 0.232570 - Accuracy 0.598958 - Test Loss 0.481164 - Test Accuracy 0.562334\n",
      "Epoch 26/300 - 5.022938s - Loss 0.228501 - Accuracy 0.666667 - Test Loss 0.481834 - Test Accuracy 0.574713\n",
      "Epoch 27/300 - 5.022358s - Loss 0.259000 - Accuracy 0.666667 - Test Loss 0.498890 - Test Accuracy 0.579576\n",
      "-----***-----\n",
      "0.5795755968169761\n",
      "Epoch 28/300 - 5.034886s - Loss 0.223525 - Accuracy 0.656250 - Test Loss 0.506560 - Test Accuracy 0.589744\n",
      "-----***-----\n",
      "0.5897435897435898\n",
      "Epoch 29/300 - 5.024982s - Loss 0.182358 - Accuracy 0.687500 - Test Loss 0.489341 - Test Accuracy 0.567639\n",
      "Epoch 30/300 - 5.040260s - Loss 0.191718 - Accuracy 0.656250 - Test Loss 0.531994 - Test Accuracy 0.578249\n",
      "Epoch 31/300 - 5.033964s - Loss 0.163641 - Accuracy 0.765625 - Test Loss 0.538144 - Test Accuracy 0.587975\n",
      "Epoch 32/300 - 5.024202s - Loss 0.167286 - Accuracy 0.744792 - Test Loss 0.556217 - Test Accuracy 0.590628\n",
      "-----***-----\n",
      "0.5906277630415562\n",
      "Epoch 33/300 - 5.036653s - Loss 0.176356 - Accuracy 0.713542 - Test Loss 0.561860 - Test Accuracy 0.580460\n",
      "Epoch 34/300 - 5.045715s - Loss 0.160042 - Accuracy 0.692708 - Test Loss 0.567692 - Test Accuracy 0.583112\n",
      "Epoch 35/300 - 5.023207s - Loss 0.170923 - Accuracy 0.708333 - Test Loss 0.547881 - Test Accuracy 0.599469\n",
      "-----***-----\n",
      "0.5994694960212201\n",
      "Epoch 36/300 - 5.037182s - Loss 0.133502 - Accuracy 0.729167 - Test Loss 0.601516 - Test Accuracy 0.583996\n",
      "Epoch 37/300 - 5.023559s - Loss 0.161733 - Accuracy 0.765625 - Test Loss 0.588623 - Test Accuracy 0.593280\n",
      "Epoch 38/300 - 5.056235s - Loss 0.110096 - Accuracy 0.822917 - Test Loss 0.665272 - Test Accuracy 0.576923\n",
      "Epoch 39/300 - 5.034933s - Loss 0.139266 - Accuracy 0.791667 - Test Loss 0.625225 - Test Accuracy 0.587975\n",
      "Epoch 40/300 - 5.020050s - Loss 0.157282 - Accuracy 0.723958 - Test Loss 0.651358 - Test Accuracy 0.595491\n",
      "Epoch 41/300 - 5.025234s - Loss 0.126187 - Accuracy 0.776042 - Test Loss 0.656108 - Test Accuracy 0.591954\n",
      "Epoch 42/300 - 5.015588s - Loss 0.137504 - Accuracy 0.765625 - Test Loss 0.670940 - Test Accuracy 0.590628\n",
      "Epoch 43/300 - 5.026434s - Loss 0.117055 - Accuracy 0.812500 - Test Loss 0.697339 - Test Accuracy 0.601238\n",
      "-----***-----\n",
      "0.601237842617153\n",
      "Epoch 44/300 - 5.046293s - Loss 0.072700 - Accuracy 0.833333 - Test Loss 0.726484 - Test Accuracy 0.593280\n",
      "Epoch 45/300 - 5.044358s - Loss 0.089024 - Accuracy 0.828125 - Test Loss 0.782729 - Test Accuracy 0.590186\n",
      "Epoch 46/300 - 5.018720s - Loss 0.107008 - Accuracy 0.817708 - Test Loss 0.744168 - Test Accuracy 0.592838\n",
      "Epoch 47/300 - 5.045669s - Loss 0.091633 - Accuracy 0.843750 - Test Loss 0.791302 - Test Accuracy 0.584881\n",
      "Epoch 48/300 - 5.028515s - Loss 0.110145 - Accuracy 0.838542 - Test Loss 0.836638 - Test Accuracy 0.597701\n",
      "Epoch 49/300 - 5.029951s - Loss 0.085865 - Accuracy 0.864583 - Test Loss 0.801472 - Test Accuracy 0.593280\n",
      "Epoch 50/300 - 5.018383s - Loss 0.087246 - Accuracy 0.812500 - Test Loss 0.796397 - Test Accuracy 0.595049\n",
      "Epoch 51/300 - 5.025951s - Loss 0.089917 - Accuracy 0.848958 - Test Loss 0.852174 - Test Accuracy 0.592396\n",
      "Epoch 52/300 - 5.036264s - Loss 0.097548 - Accuracy 0.843750 - Test Loss 0.826143 - Test Accuracy 0.599027\n",
      "Epoch 53/300 - 5.043634s - Loss 0.068167 - Accuracy 0.833333 - Test Loss 0.875788 - Test Accuracy 0.594164\n",
      "Epoch 54/300 - 5.019166s - Loss 0.079956 - Accuracy 0.848958 - Test Loss 0.945506 - Test Accuracy 0.590628\n",
      "Epoch 55/300 - 5.056367s - Loss 0.102123 - Accuracy 0.848958 - Test Loss 0.900758 - Test Accuracy 0.591954\n",
      "Epoch 56/300 - 5.027164s - Loss 0.086472 - Accuracy 0.859375 - Test Loss 0.916414 - Test Accuracy 0.592396\n",
      "Epoch 57/300 - 5.052721s - Loss 0.092589 - Accuracy 0.817708 - Test Loss 1.006464 - Test Accuracy 0.595933\n",
      "Epoch 58/300 - 5.031566s - Loss 0.058475 - Accuracy 0.901042 - Test Loss 0.987382 - Test Accuracy 0.598143\n",
      "Epoch 59/300 - 5.076792s - Loss 0.056605 - Accuracy 0.859375 - Test Loss 0.890056 - Test Accuracy 0.598143\n",
      "Epoch 60/300 - 5.026333s - Loss 0.083878 - Accuracy 0.880208 - Test Loss 0.904666 - Test Accuracy 0.590186\n",
      "Epoch 61/300 - 5.067306s - Loss 0.071663 - Accuracy 0.854167 - Test Loss 0.967315 - Test Accuracy 0.596817\n",
      "Epoch 62/300 - 5.028852s - Loss 0.067684 - Accuracy 0.869792 - Test Loss 1.011376 - Test Accuracy 0.595933\n",
      "Epoch 63/300 - 5.053748s - Loss 0.044235 - Accuracy 0.927083 - Test Loss 0.989677 - Test Accuracy 0.599027\n",
      "Epoch 64/300 - 5.027103s - Loss 0.053951 - Accuracy 0.859375 - Test Loss 0.981529 - Test Accuracy 0.598143\n",
      "Epoch 65/300 - 5.027705s - Loss 0.072171 - Accuracy 0.885417 - Test Loss 0.954244 - Test Accuracy 0.599912\n",
      "Epoch 66/300 - 5.030535s - Loss 0.057214 - Accuracy 0.906250 - Test Loss 1.027141 - Test Accuracy 0.596817\n",
      "Epoch 67/300 - 5.029537s - Loss 0.067715 - Accuracy 0.854167 - Test Loss 1.052077 - Test Accuracy 0.605217\n",
      "-----***-----\n",
      "0.6052166224580018\n",
      "Epoch 68/300 - 5.049188s - Loss 0.070974 - Accuracy 0.859375 - Test Loss 0.884572 - Test Accuracy 0.593722\n",
      "Epoch 69/300 - 5.032673s - Loss 0.059940 - Accuracy 0.875000 - Test Loss 1.071483 - Test Accuracy 0.592838\n",
      "Epoch 70/300 - 5.035799s - Loss 0.058031 - Accuracy 0.890625 - Test Loss 0.992197 - Test Accuracy 0.603448\n",
      "Epoch 71/300 - 5.041817s - Loss 0.074100 - Accuracy 0.895833 - Test Loss 1.210711 - Test Accuracy 0.594164\n",
      "Epoch 72/300 - 5.033147s - Loss 0.050598 - Accuracy 0.895833 - Test Loss 1.248723 - Test Accuracy 0.597259\n",
      "Epoch 73/300 - 5.049877s - Loss 0.043779 - Accuracy 0.895833 - Test Loss 1.227186 - Test Accuracy 0.595049\n",
      "Epoch 74/300 - 5.029658s - Loss 0.038988 - Accuracy 0.901042 - Test Loss 1.176826 - Test Accuracy 0.595491\n",
      "Epoch 75/300 - 5.023436s - Loss 0.055394 - Accuracy 0.906250 - Test Loss 1.158395 - Test Accuracy 0.597701\n",
      "Epoch 76/300 - 5.027377s - Loss 0.061862 - Accuracy 0.869792 - Test Loss 1.085778 - Test Accuracy 0.604775\n",
      "Epoch 77/300 - 5.027494s - Loss 0.037597 - Accuracy 0.942708 - Test Loss 1.202516 - Test Accuracy 0.595933\n",
      "Epoch 78/300 - 5.027536s - Loss 0.044370 - Accuracy 0.937500 - Test Loss 1.278216 - Test Accuracy 0.600354\n",
      "Epoch 79/300 - 5.043008s - Loss 0.054306 - Accuracy 0.864583 - Test Loss 1.182134 - Test Accuracy 0.601238\n",
      "Epoch 80/300 - 5.085350s - Loss 0.066024 - Accuracy 0.901042 - Test Loss 1.134541 - Test Accuracy 0.599469\n",
      "Epoch 81/300 - 5.043146s - Loss 0.044956 - Accuracy 0.906250 - Test Loss 1.121669 - Test Accuracy 0.597701\n",
      "Epoch 82/300 - 5.051203s - Loss 0.086012 - Accuracy 0.885417 - Test Loss 1.171878 - Test Accuracy 0.597701\n",
      "Epoch 83/300 - 5.048392s - Loss 0.043072 - Accuracy 0.916667 - Test Loss 1.300631 - Test Accuracy 0.599469\n",
      "Epoch 84/300 - 5.055666s - Loss 0.066680 - Accuracy 0.890625 - Test Loss 1.207370 - Test Accuracy 0.599912\n",
      "Epoch 85/300 - 5.047239s - Loss 0.048407 - Accuracy 0.901042 - Test Loss 1.258807 - Test Accuracy 0.599027\n",
      "Epoch 86/300 - 5.035783s - Loss 0.039518 - Accuracy 0.942708 - Test Loss 1.244922 - Test Accuracy 0.596817\n",
      "Epoch 87/300 - 5.030419s - Loss 0.065399 - Accuracy 0.854167 - Test Loss 1.198329 - Test Accuracy 0.595933\n",
      "Epoch 88/300 - 5.023253s - Loss 0.067507 - Accuracy 0.885417 - Test Loss 1.322266 - Test Accuracy 0.596817\n",
      "Epoch 89/300 - 5.034170s - Loss 0.038765 - Accuracy 0.906250 - Test Loss 1.374961 - Test Accuracy 0.593280\n",
      "Epoch 90/300 - 5.026861s - Loss 0.050678 - Accuracy 0.890625 - Test Loss 1.283470 - Test Accuracy 0.597701\n",
      "Epoch 91/300 - 5.040539s - Loss 0.054998 - Accuracy 0.906250 - Test Loss 1.176142 - Test Accuracy 0.599027\n",
      "Epoch 92/300 - 5.031070s - Loss 0.046517 - Accuracy 0.911458 - Test Loss 1.351202 - Test Accuracy 0.603890\n",
      "Epoch 93/300 - 5.041628s - Loss 0.057635 - Accuracy 0.901042 - Test Loss 1.197191 - Test Accuracy 0.598585\n",
      "Epoch 94/300 - 5.032324s - Loss 0.035660 - Accuracy 0.906250 - Test Loss 1.471666 - Test Accuracy 0.599912\n",
      "Epoch 95/300 - 5.040429s - Loss 0.044719 - Accuracy 0.901042 - Test Loss 1.482970 - Test Accuracy 0.598585\n",
      "Epoch 96/300 - 5.059766s - Loss 0.035260 - Accuracy 0.911458 - Test Loss 1.401466 - Test Accuracy 0.591954\n",
      "Epoch 97/300 - 5.050026s - Loss 0.065319 - Accuracy 0.906250 - Test Loss 1.365546 - Test Accuracy 0.596817\n",
      "Epoch 98/300 - 5.030441s - Loss 0.031529 - Accuracy 0.963542 - Test Loss 1.471115 - Test Accuracy 0.597259\n",
      "Epoch 99/300 - 5.032092s - Loss 0.046797 - Accuracy 0.932292 - Test Loss 1.469481 - Test Accuracy 0.595049\n",
      "Epoch 100/300 - 5.025217s - Loss 0.037092 - Accuracy 0.953125 - Test Loss 1.520414 - Test Accuracy 0.595933\n",
      "Epoch 101/300 - 5.038260s - Loss 0.051569 - Accuracy 0.880208 - Test Loss 1.330297 - Test Accuracy 0.591512\n",
      "Epoch 102/300 - 5.030682s - Loss 0.048725 - Accuracy 0.921875 - Test Loss 1.398156 - Test Accuracy 0.588859\n",
      "Epoch 103/300 - 5.036983s - Loss 0.056198 - Accuracy 0.880208 - Test Loss 1.287788 - Test Accuracy 0.603890\n",
      "Epoch 104/300 - 5.032617s - Loss 0.034308 - Accuracy 0.911458 - Test Loss 1.454479 - Test Accuracy 0.594164\n",
      "Epoch 105/300 - 5.062131s - Loss 0.062027 - Accuracy 0.916667 - Test Loss 1.645337 - Test Accuracy 0.588859\n",
      "Epoch 106/300 - 5.053017s - Loss 0.038545 - Accuracy 0.921875 - Test Loss 1.425248 - Test Accuracy 0.594164\n",
      "Epoch 107/300 - 5.082958s - Loss 0.043945 - Accuracy 0.875000 - Test Loss 1.377842 - Test Accuracy 0.597701\n",
      "Epoch 108/300 - 5.038337s - Loss 0.077714 - Accuracy 0.880208 - Test Loss 1.590106 - Test Accuracy 0.598143\n",
      "Epoch 109/300 - 5.060166s - Loss 0.039708 - Accuracy 0.921875 - Test Loss 1.434165 - Test Accuracy 0.605217\n",
      "Epoch 110/300 - 5.037023s - Loss 0.061867 - Accuracy 0.895833 - Test Loss 1.507197 - Test Accuracy 0.599469\n",
      "Epoch 111/300 - 5.053780s - Loss 0.049763 - Accuracy 0.953125 - Test Loss 1.409653 - Test Accuracy 0.598143\n",
      "Epoch 112/300 - 5.073225s - Loss 0.039362 - Accuracy 0.911458 - Test Loss 1.481141 - Test Accuracy 0.599027\n",
      "Epoch 113/300 - 5.078464s - Loss 0.032436 - Accuracy 0.921875 - Test Loss 1.666776 - Test Accuracy 0.594607\n",
      "Epoch 114/300 - 5.042795s - Loss 0.046683 - Accuracy 0.901042 - Test Loss 1.385682 - Test Accuracy 0.593722\n",
      "Epoch 115/300 - 5.050446s - Loss 0.043829 - Accuracy 0.916667 - Test Loss 1.582422 - Test Accuracy 0.595933\n",
      "Epoch 116/300 - 5.035357s - Loss 0.051874 - Accuracy 0.875000 - Test Loss 1.586601 - Test Accuracy 0.594607\n",
      "Epoch 117/300 - 5.273682s - Loss 0.033844 - Accuracy 0.921875 - Test Loss 1.431116 - Test Accuracy 0.584881\n",
      "Epoch 118/300 - 5.027715s - Loss 0.055863 - Accuracy 0.890625 - Test Loss 1.446934 - Test Accuracy 0.600796\n",
      "Epoch 119/300 - 5.073648s - Loss 0.066946 - Accuracy 0.890625 - Test Loss 1.501111 - Test Accuracy 0.593280\n",
      "Epoch 120/300 - 5.054070s - Loss 0.049401 - Accuracy 0.911458 - Test Loss 1.238559 - Test Accuracy 0.588417\n",
      "Epoch 121/300 - 5.075881s - Loss 0.043327 - Accuracy 0.890625 - Test Loss 1.591691 - Test Accuracy 0.587533\n",
      "Epoch 122/300 - 5.070915s - Loss 0.047591 - Accuracy 0.895833 - Test Loss 1.540870 - Test Accuracy 0.585323\n",
      "Epoch 123/300 - 5.057394s - Loss 0.054366 - Accuracy 0.932292 - Test Loss 1.797899 - Test Accuracy 0.585765\n",
      "Epoch 124/300 - 5.018999s - Loss 0.079442 - Accuracy 0.890625 - Test Loss 1.533205 - Test Accuracy 0.601238\n",
      "Epoch 125/300 - 5.041419s - Loss 0.075134 - Accuracy 0.848958 - Test Loss 1.523864 - Test Accuracy 0.583996\n",
      "Epoch 126/300 - 5.049084s - Loss 0.055011 - Accuracy 0.875000 - Test Loss 1.422247 - Test Accuracy 0.589744\n",
      "Epoch 127/300 - 5.057771s - Loss 0.042189 - Accuracy 0.927083 - Test Loss 1.533194 - Test Accuracy 0.598585\n",
      "Epoch 128/300 - 5.046011s - Loss 0.070487 - Accuracy 0.911458 - Test Loss 1.665202 - Test Accuracy 0.593280\n",
      "Epoch 129/300 - 5.070429s - Loss 0.016919 - Accuracy 0.963542 - Test Loss 1.856178 - Test Accuracy 0.590186\n",
      "Epoch 130/300 - 5.066784s - Loss 0.061455 - Accuracy 0.885417 - Test Loss 1.531862 - Test Accuracy 0.594164\n",
      "Epoch 131/300 - 5.051550s - Loss 0.047714 - Accuracy 0.901042 - Test Loss 1.690735 - Test Accuracy 0.592838\n",
      "Epoch 132/300 - 5.036414s - Loss 0.071725 - Accuracy 0.838542 - Test Loss 1.458589 - Test Accuracy 0.594607\n",
      "Epoch 133/300 - 5.041376s - Loss 0.052232 - Accuracy 0.906250 - Test Loss 1.555292 - Test Accuracy 0.587533\n",
      "Epoch 134/300 - 5.027082s - Loss 0.044638 - Accuracy 0.916667 - Test Loss 1.733893 - Test Accuracy 0.594164\n",
      "Epoch 135/300 - 5.042717s - Loss 0.052668 - Accuracy 0.911458 - Test Loss 1.660103 - Test Accuracy 0.594607\n",
      "Epoch 136/300 - 5.030107s - Loss 0.043065 - Accuracy 0.911458 - Test Loss 1.617427 - Test Accuracy 0.602122\n",
      "Epoch 137/300 - 5.034685s - Loss 0.051631 - Accuracy 0.906250 - Test Loss 1.589648 - Test Accuracy 0.589302\n",
      "Epoch 138/300 - 5.035697s - Loss 0.060074 - Accuracy 0.921875 - Test Loss 1.649357 - Test Accuracy 0.591954\n",
      "Epoch 139/300 - 5.030123s - Loss 0.099886 - Accuracy 0.848958 - Test Loss 1.743972 - Test Accuracy 0.588417\n",
      "Epoch 140/300 - 5.031363s - Loss 0.031814 - Accuracy 0.932292 - Test Loss 1.573847 - Test Accuracy 0.587975\n",
      "Epoch 141/300 - 5.051382s - Loss 0.052023 - Accuracy 0.932292 - Test Loss 1.815621 - Test Accuracy 0.588417\n",
      "Epoch 142/300 - 5.042444s - Loss 0.060815 - Accuracy 0.875000 - Test Loss 1.575942 - Test Accuracy 0.591070\n",
      "Epoch 143/300 - 5.069951s - Loss 0.099539 - Accuracy 0.890625 - Test Loss 1.909701 - Test Accuracy 0.583554\n",
      "Epoch 144/300 - 5.080142s - Loss 0.047499 - Accuracy 0.921875 - Test Loss 1.824005 - Test Accuracy 0.591070\n",
      "Epoch 145/300 - 5.043230s - Loss 0.046258 - Accuracy 0.901042 - Test Loss 1.928573 - Test Accuracy 0.591070\n",
      "Epoch 146/300 - 5.039268s - Loss 0.040136 - Accuracy 0.916667 - Test Loss 1.843772 - Test Accuracy 0.592838\n",
      "Epoch 147/300 - 5.043475s - Loss 0.043928 - Accuracy 0.921875 - Test Loss 2.206507 - Test Accuracy 0.585323\n",
      "Epoch 148/300 - 5.050248s - Loss 0.041510 - Accuracy 0.916667 - Test Loss 1.919907 - Test Accuracy 0.590628\n",
      "Epoch 149/300 - 5.062341s - Loss 0.053190 - Accuracy 0.906250 - Test Loss 1.955137 - Test Accuracy 0.593722\n",
      "Epoch 150/300 - 5.045529s - Loss 0.036769 - Accuracy 0.916667 - Test Loss 1.811861 - Test Accuracy 0.594164\n",
      "Epoch 151/300 - 5.053406s - Loss 0.041776 - Accuracy 0.911458 - Test Loss 1.951708 - Test Accuracy 0.591070\n",
      "Epoch 152/300 - 5.042752s - Loss 0.030204 - Accuracy 0.942708 - Test Loss 2.081264 - Test Accuracy 0.586649\n",
      "Epoch 153/300 - 5.036666s - Loss 0.083893 - Accuracy 0.937500 - Test Loss 1.986395 - Test Accuracy 0.584439\n",
      "Epoch 154/300 - 5.045696s - Loss 0.049058 - Accuracy 0.911458 - Test Loss 2.085635 - Test Accuracy 0.584881\n",
      "Epoch 155/300 - 5.038957s - Loss 0.032351 - Accuracy 0.942708 - Test Loss 1.903384 - Test Accuracy 0.587091\n",
      "Epoch 156/300 - 5.041629s - Loss 0.025382 - Accuracy 0.942708 - Test Loss 1.727143 - Test Accuracy 0.594164\n",
      "Epoch 157/300 - 5.049729s - Loss 0.047243 - Accuracy 0.916667 - Test Loss 2.045202 - Test Accuracy 0.590186\n",
      "Epoch 158/300 - 5.058558s - Loss 0.056009 - Accuracy 0.906250 - Test Loss 2.050860 - Test Accuracy 0.594607\n",
      "Epoch 159/300 - 5.027045s - Loss 0.099579 - Accuracy 0.927083 - Test Loss 2.067102 - Test Accuracy 0.595049\n",
      "Epoch 160/300 - 5.047610s - Loss 0.063998 - Accuracy 0.921875 - Test Loss 1.771158 - Test Accuracy 0.578691\n",
      "Epoch 161/300 - 5.049330s - Loss 0.038356 - Accuracy 0.921875 - Test Loss 2.172003 - Test Accuracy 0.588859\n",
      "Epoch 162/300 - 5.069375s - Loss 0.055391 - Accuracy 0.895833 - Test Loss 2.141995 - Test Accuracy 0.592838\n",
      "Epoch 163/300 - 5.037549s - Loss 0.107854 - Accuracy 0.885417 - Test Loss 2.093647 - Test Accuracy 0.592838\n",
      "Epoch 164/300 - 5.081954s - Loss 0.077661 - Accuracy 0.890625 - Test Loss 1.969942 - Test Accuracy 0.587091\n",
      "Epoch 165/300 - 5.043489s - Loss 0.063726 - Accuracy 0.927083 - Test Loss 2.259126 - Test Accuracy 0.581786\n",
      "Epoch 166/300 - 5.080756s - Loss 0.048115 - Accuracy 0.901042 - Test Loss 2.052523 - Test Accuracy 0.594164\n",
      "Epoch 167/300 - 5.052570s - Loss 0.065309 - Accuracy 0.921875 - Test Loss 2.249070 - Test Accuracy 0.588859\n",
      "Epoch 168/300 - 5.065176s - Loss 0.091704 - Accuracy 0.942708 - Test Loss 2.227736 - Test Accuracy 0.592838\n",
      "Epoch 169/300 - 5.056753s - Loss 0.070996 - Accuracy 0.895833 - Test Loss 2.016024 - Test Accuracy 0.595049\n",
      "Epoch 170/300 - 5.043861s - Loss 0.081652 - Accuracy 0.854167 - Test Loss 2.024318 - Test Accuracy 0.596817\n",
      "Epoch 171/300 - 5.042864s - Loss 0.053864 - Accuracy 0.916667 - Test Loss 2.429137 - Test Accuracy 0.591512\n",
      "Epoch 172/300 - 5.038583s - Loss 0.074532 - Accuracy 0.843750 - Test Loss 1.923273 - Test Accuracy 0.596817\n",
      "Epoch 173/300 - 5.043777s - Loss 0.035592 - Accuracy 0.942708 - Test Loss 2.133858 - Test Accuracy 0.598143\n",
      "Epoch 174/300 - 5.039257s - Loss 0.054427 - Accuracy 0.921875 - Test Loss 2.127445 - Test Accuracy 0.594607\n",
      "Epoch 175/300 - 5.036454s - Loss 0.034170 - Accuracy 0.937500 - Test Loss 2.071211 - Test Accuracy 0.595933\n",
      "Epoch 176/300 - 5.082547s - Loss 0.057699 - Accuracy 0.890625 - Test Loss 2.257768 - Test Accuracy 0.596375\n",
      "Epoch 177/300 - 5.035842s - Loss 0.051853 - Accuracy 0.885417 - Test Loss 2.023751 - Test Accuracy 0.593280\n",
      "Epoch 178/300 - 5.050254s - Loss 0.065862 - Accuracy 0.921875 - Test Loss 2.206496 - Test Accuracy 0.599027\n",
      "Epoch 179/300 - 5.030788s - Loss 0.081545 - Accuracy 0.932292 - Test Loss 2.454619 - Test Accuracy 0.591512\n",
      "Epoch 180/300 - 5.041367s - Loss 0.057452 - Accuracy 0.901042 - Test Loss 2.107841 - Test Accuracy 0.591954\n",
      "Epoch 181/300 - 5.051629s - Loss 0.058749 - Accuracy 0.895833 - Test Loss 2.615354 - Test Accuracy 0.592396\n",
      "Epoch 182/300 - 5.062588s - Loss 0.049289 - Accuracy 0.916667 - Test Loss 2.130934 - Test Accuracy 0.591512\n",
      "Epoch 183/300 - 5.041816s - Loss 0.043912 - Accuracy 0.921875 - Test Loss 2.375197 - Test Accuracy 0.584881\n",
      "Epoch 184/300 - 5.064965s - Loss 0.061315 - Accuracy 0.906250 - Test Loss 2.415143 - Test Accuracy 0.589302\n",
      "Epoch 185/300 - 5.041404s - Loss 0.075690 - Accuracy 0.895833 - Test Loss 2.391667 - Test Accuracy 0.593280\n",
      "Epoch 186/300 - 5.071644s - Loss 0.076280 - Accuracy 0.906250 - Test Loss 2.348819 - Test Accuracy 0.590628\n",
      "Epoch 187/300 - 5.057415s - Loss 0.040400 - Accuracy 0.906250 - Test Loss 2.091176 - Test Accuracy 0.589744\n",
      "Epoch 188/300 - 5.046902s - Loss 0.055647 - Accuracy 0.906250 - Test Loss 2.502061 - Test Accuracy 0.589744\n",
      "Epoch 189/300 - 5.057458s - Loss 0.045246 - Accuracy 0.916667 - Test Loss 2.216420 - Test Accuracy 0.600796\n",
      "Epoch 190/300 - 5.065707s - Loss 0.049356 - Accuracy 0.906250 - Test Loss 2.391999 - Test Accuracy 0.592396\n",
      "Epoch 191/300 - 5.039948s - Loss 0.045968 - Accuracy 0.932292 - Test Loss 2.489720 - Test Accuracy 0.593722\n",
      "Epoch 192/300 - 5.066550s - Loss 0.025788 - Accuracy 0.942708 - Test Loss 2.523473 - Test Accuracy 0.591070\n",
      "Epoch 193/300 - 5.046939s - Loss 0.053275 - Accuracy 0.895833 - Test Loss 2.711566 - Test Accuracy 0.596817\n",
      "Epoch 194/300 - 5.051236s - Loss 0.026593 - Accuracy 0.968750 - Test Loss 2.369927 - Test Accuracy 0.594164\n",
      "Epoch 195/300 - 5.034343s - Loss 0.056822 - Accuracy 0.932292 - Test Loss 2.723739 - Test Accuracy 0.595049\n",
      "Epoch 196/300 - 5.042122s - Loss 0.038379 - Accuracy 0.927083 - Test Loss 2.530856 - Test Accuracy 0.587975\n",
      "Epoch 197/300 - 5.075883s - Loss 0.072911 - Accuracy 0.911458 - Test Loss 2.371454 - Test Accuracy 0.600796\n",
      "Epoch 198/300 - 5.052778s - Loss 0.076502 - Accuracy 0.854167 - Test Loss 2.211803 - Test Accuracy 0.591954\n",
      "Epoch 199/300 - 5.048669s - Loss 0.052234 - Accuracy 0.916667 - Test Loss 2.404105 - Test Accuracy 0.593280\n",
      "Epoch 200/300 - 5.066542s - Loss 0.039738 - Accuracy 0.942708 - Test Loss 2.485236 - Test Accuracy 0.593280\n",
      "Epoch 201/300 - 5.038786s - Loss 0.031044 - Accuracy 0.953125 - Test Loss 2.460213 - Test Accuracy 0.595491\n",
      "Epoch 202/300 - 5.056813s - Loss 0.078578 - Accuracy 0.937500 - Test Loss 2.630795 - Test Accuracy 0.595491\n",
      "Epoch 203/300 - 5.051734s - Loss 0.061138 - Accuracy 0.921875 - Test Loss 2.367233 - Test Accuracy 0.591070\n",
      "Epoch 204/300 - 5.062003s - Loss 0.044481 - Accuracy 0.932292 - Test Loss 2.364892 - Test Accuracy 0.598585\n",
      "Epoch 205/300 - 5.047291s - Loss 0.067859 - Accuracy 0.921875 - Test Loss 2.608240 - Test Accuracy 0.592838\n",
      "Epoch 206/300 - 5.036297s - Loss 0.073403 - Accuracy 0.895833 - Test Loss 2.279612 - Test Accuracy 0.591070\n",
      "Epoch 207/300 - 5.046252s - Loss 0.074348 - Accuracy 0.895833 - Test Loss 2.385074 - Test Accuracy 0.593722\n",
      "Epoch 208/300 - 5.074806s - Loss 0.048239 - Accuracy 0.921875 - Test Loss 2.622765 - Test Accuracy 0.592396\n",
      "Epoch 209/300 - 5.032899s - Loss 0.034706 - Accuracy 0.968750 - Test Loss 2.332723 - Test Accuracy 0.587533\n",
      "Epoch 210/300 - 5.051626s - Loss 0.060775 - Accuracy 0.890625 - Test Loss 2.793759 - Test Accuracy 0.587533\n",
      "Epoch 211/300 - 5.037421s - Loss 0.039026 - Accuracy 0.906250 - Test Loss 1.956575 - Test Accuracy 0.594607\n",
      "Epoch 212/300 - 5.053628s - Loss 0.041082 - Accuracy 0.942708 - Test Loss 2.672800 - Test Accuracy 0.599027\n",
      "Epoch 213/300 - 5.047483s - Loss 0.089132 - Accuracy 0.864583 - Test Loss 2.458734 - Test Accuracy 0.603448\n",
      "Epoch 214/300 - 5.044829s - Loss 0.046515 - Accuracy 0.901042 - Test Loss 2.557935 - Test Accuracy 0.590186\n",
      "Epoch 215/300 - 5.039131s - Loss 0.058179 - Accuracy 0.906250 - Test Loss 2.658073 - Test Accuracy 0.596817\n",
      "Epoch 216/300 - 5.049545s - Loss 0.066277 - Accuracy 0.880208 - Test Loss 2.650910 - Test Accuracy 0.593280\n",
      "Epoch 217/300 - 5.039249s - Loss 0.032587 - Accuracy 0.916667 - Test Loss 2.617711 - Test Accuracy 0.593280\n",
      "Epoch 218/300 - 5.043573s - Loss 0.049275 - Accuracy 0.906250 - Test Loss 2.533079 - Test Accuracy 0.593280\n",
      "Epoch 219/300 - 5.047239s - Loss 0.075640 - Accuracy 0.916667 - Test Loss 2.581108 - Test Accuracy 0.584881\n",
      "Epoch 220/300 - 5.071549s - Loss 0.082676 - Accuracy 0.890625 - Test Loss 2.719471 - Test Accuracy 0.596817\n",
      "Epoch 221/300 - 5.055387s - Loss 0.044857 - Accuracy 0.927083 - Test Loss 2.512052 - Test Accuracy 0.596375\n",
      "Epoch 222/300 - 5.069858s - Loss 0.074559 - Accuracy 0.895833 - Test Loss 2.415082 - Test Accuracy 0.596817\n",
      "Epoch 223/300 - 5.055293s - Loss 0.036697 - Accuracy 0.937500 - Test Loss 2.702806 - Test Accuracy 0.592396\n",
      "Epoch 224/300 - 5.094547s - Loss 0.047666 - Accuracy 0.932292 - Test Loss 2.793418 - Test Accuracy 0.592838\n",
      "Epoch 225/300 - 5.060596s - Loss 0.025239 - Accuracy 0.937500 - Test Loss 3.051510 - Test Accuracy 0.595049\n",
      "Epoch 226/300 - 5.068579s - Loss 0.083268 - Accuracy 0.875000 - Test Loss 2.526819 - Test Accuracy 0.592838\n",
      "Epoch 227/300 - 5.061919s - Loss 0.049863 - Accuracy 0.916667 - Test Loss 2.398633 - Test Accuracy 0.596817\n",
      "Epoch 228/300 - 5.071633s - Loss 0.081172 - Accuracy 0.921875 - Test Loss 3.152543 - Test Accuracy 0.585323\n",
      "Epoch 229/300 - 5.069285s - Loss 0.034398 - Accuracy 0.958333 - Test Loss 2.568254 - Test Accuracy 0.594164\n",
      "Epoch 230/300 - 5.055080s - Loss 0.063589 - Accuracy 0.916667 - Test Loss 2.510974 - Test Accuracy 0.589302\n",
      "Epoch 231/300 - 5.045793s - Loss 0.023865 - Accuracy 0.942708 - Test Loss 3.206058 - Test Accuracy 0.592838\n",
      "Epoch 232/300 - 5.052493s - Loss 0.043562 - Accuracy 0.953125 - Test Loss 2.667450 - Test Accuracy 0.588859\n",
      "Epoch 233/300 - 5.039991s - Loss 0.043577 - Accuracy 0.947917 - Test Loss 2.894833 - Test Accuracy 0.593280\n",
      "Epoch 234/300 - 5.047051s - Loss 0.048905 - Accuracy 0.916667 - Test Loss 2.382776 - Test Accuracy 0.588417\n",
      "Epoch 235/300 - 5.048689s - Loss 0.077822 - Accuracy 0.927083 - Test Loss 2.895783 - Test Accuracy 0.590628\n",
      "Epoch 236/300 - 5.075627s - Loss 0.046345 - Accuracy 0.953125 - Test Loss 2.882869 - Test Accuracy 0.595491\n",
      "Epoch 237/300 - 5.066857s - Loss 0.056509 - Accuracy 0.906250 - Test Loss 2.741862 - Test Accuracy 0.598143\n",
      "Epoch 238/300 - 5.077896s - Loss 0.047052 - Accuracy 0.942708 - Test Loss 2.834424 - Test Accuracy 0.594607\n",
      "Epoch 239/300 - 5.041078s - Loss 0.030064 - Accuracy 0.968750 - Test Loss 2.592164 - Test Accuracy 0.590628\n",
      "Epoch 240/300 - 5.060175s - Loss 0.030366 - Accuracy 0.932292 - Test Loss 2.487887 - Test Accuracy 0.592838\n",
      "Epoch 241/300 - 5.050627s - Loss 0.088437 - Accuracy 0.890625 - Test Loss 2.906243 - Test Accuracy 0.588859\n",
      "Epoch 242/300 - 5.063763s - Loss 0.026034 - Accuracy 0.942708 - Test Loss 2.774335 - Test Accuracy 0.587533\n",
      "Epoch 243/300 - 5.056313s - Loss 0.057942 - Accuracy 0.927083 - Test Loss 3.363241 - Test Accuracy 0.584881\n",
      "Epoch 244/300 - 5.076785s - Loss 0.043978 - Accuracy 0.921875 - Test Loss 2.738702 - Test Accuracy 0.588417\n",
      "Epoch 245/300 - 5.047581s - Loss 0.057465 - Accuracy 0.921875 - Test Loss 2.727658 - Test Accuracy 0.594164\n",
      "Epoch 246/300 - 5.052492s - Loss 0.032162 - Accuracy 0.932292 - Test Loss 2.757423 - Test Accuracy 0.595491\n",
      "Epoch 247/300 - 5.062800s - Loss 0.020834 - Accuracy 0.958333 - Test Loss 2.852622 - Test Accuracy 0.587533\n",
      "Epoch 248/300 - 5.042772s - Loss 0.040106 - Accuracy 0.932292 - Test Loss 2.880150 - Test Accuracy 0.588859\n",
      "Epoch 249/300 - 5.075370s - Loss 0.096754 - Accuracy 0.953125 - Test Loss 2.878053 - Test Accuracy 0.584439\n",
      "Epoch 250/300 - 5.051232s - Loss 0.073745 - Accuracy 0.906250 - Test Loss 3.093998 - Test Accuracy 0.587091\n",
      "Epoch 251/300 - 5.058010s - Loss 0.112937 - Accuracy 0.916667 - Test Loss 3.012494 - Test Accuracy 0.586649\n",
      "Epoch 252/300 - 5.054543s - Loss 0.088944 - Accuracy 0.916667 - Test Loss 2.768503 - Test Accuracy 0.592838\n",
      "Epoch 253/300 - 5.076508s - Loss 0.047259 - Accuracy 0.911458 - Test Loss 3.280571 - Test Accuracy 0.591512\n",
      "Epoch 254/300 - 5.048427s - Loss 0.030037 - Accuracy 0.958333 - Test Loss 2.841313 - Test Accuracy 0.590186\n",
      "Epoch 255/300 - 5.103597s - Loss 0.066343 - Accuracy 0.911458 - Test Loss 2.997087 - Test Accuracy 0.597701\n",
      "Epoch 256/300 - 5.060838s - Loss 0.035919 - Accuracy 0.942708 - Test Loss 3.183583 - Test Accuracy 0.593722\n",
      "Epoch 257/300 - 5.082335s - Loss 0.058563 - Accuracy 0.932292 - Test Loss 2.721389 - Test Accuracy 0.597259\n",
      "Epoch 258/300 - 5.055749s - Loss 0.074398 - Accuracy 0.942708 - Test Loss 2.949850 - Test Accuracy 0.594607\n",
      "Epoch 259/300 - 5.058586s - Loss 0.017267 - Accuracy 0.942708 - Test Loss 3.026678 - Test Accuracy 0.594164\n",
      "Epoch 260/300 - 5.067320s - Loss 0.044448 - Accuracy 0.958333 - Test Loss 2.761852 - Test Accuracy 0.599912\n",
      "Epoch 261/300 - 5.053440s - Loss 0.032966 - Accuracy 0.958333 - Test Loss 3.034938 - Test Accuracy 0.591512\n",
      "Epoch 262/300 - 5.057516s - Loss 0.057717 - Accuracy 0.937500 - Test Loss 3.045564 - Test Accuracy 0.594164\n",
      "Epoch 263/300 - 5.055093s - Loss 0.026447 - Accuracy 0.963542 - Test Loss 3.147928 - Test Accuracy 0.587533\n",
      "Epoch 264/300 - 5.045707s - Loss 0.071736 - Accuracy 0.953125 - Test Loss 2.967582 - Test Accuracy 0.587975\n",
      "Epoch 265/300 - 5.047457s - Loss 0.082631 - Accuracy 0.916667 - Test Loss 3.037836 - Test Accuracy 0.591512\n",
      "Epoch 266/300 - 5.033168s - Loss 0.047132 - Accuracy 0.958333 - Test Loss 3.155078 - Test Accuracy 0.587533\n",
      "Epoch 267/300 - 5.053774s - Loss 0.047813 - Accuracy 0.927083 - Test Loss 2.757291 - Test Accuracy 0.605217\n",
      "Epoch 268/300 - 5.046522s - Loss 0.033689 - Accuracy 0.963542 - Test Loss 3.195514 - Test Accuracy 0.592396\n",
      "Epoch 269/300 - 5.085255s - Loss 0.042298 - Accuracy 0.921875 - Test Loss 3.196481 - Test Accuracy 0.595933\n",
      "Epoch 270/300 - 5.041939s - Loss 0.050430 - Accuracy 0.911458 - Test Loss 2.669732 - Test Accuracy 0.595049\n",
      "Epoch 271/300 - 5.058280s - Loss 0.027185 - Accuracy 0.937500 - Test Loss 2.683717 - Test Accuracy 0.596375\n",
      "Epoch 272/300 - 5.066680s - Loss 0.040161 - Accuracy 0.942708 - Test Loss 3.263856 - Test Accuracy 0.593722\n",
      "Epoch 273/300 - 5.058440s - Loss 0.052822 - Accuracy 0.947917 - Test Loss 2.869901 - Test Accuracy 0.599469\n",
      "Epoch 274/300 - 5.056787s - Loss 0.048214 - Accuracy 0.958333 - Test Loss 2.550847 - Test Accuracy 0.587975\n",
      "Epoch 275/300 - 5.067442s - Loss 0.028815 - Accuracy 0.963542 - Test Loss 3.500315 - Test Accuracy 0.588417\n",
      "Epoch 276/300 - 5.056930s - Loss 0.027292 - Accuracy 0.953125 - Test Loss 2.990204 - Test Accuracy 0.594164\n",
      "Epoch 277/300 - 5.075130s - Loss 0.060442 - Accuracy 0.921875 - Test Loss 2.822098 - Test Accuracy 0.593280\n",
      "Epoch 278/300 - 5.049942s - Loss 0.038246 - Accuracy 0.947917 - Test Loss 3.301396 - Test Accuracy 0.593280\n",
      "Epoch 279/300 - 5.059912s - Loss 0.022889 - Accuracy 0.963542 - Test Loss 3.142240 - Test Accuracy 0.594607\n",
      "Epoch 280/300 - 5.056469s - Loss 0.025431 - Accuracy 0.963542 - Test Loss 3.026892 - Test Accuracy 0.592838\n",
      "Epoch 281/300 - 5.043939s - Loss 0.059151 - Accuracy 0.911458 - Test Loss 3.199763 - Test Accuracy 0.591512\n",
      "Epoch 282/300 - 5.041729s - Loss 0.046516 - Accuracy 0.932292 - Test Loss 3.055741 - Test Accuracy 0.591954\n",
      "Epoch 283/300 - 5.061529s - Loss 0.060078 - Accuracy 0.927083 - Test Loss 3.165616 - Test Accuracy 0.589744\n",
      "Epoch 284/300 - 5.043042s - Loss 0.031474 - Accuracy 0.953125 - Test Loss 3.421817 - Test Accuracy 0.595933\n",
      "Epoch 285/300 - 5.053088s - Loss 0.035983 - Accuracy 0.953125 - Test Loss 3.426510 - Test Accuracy 0.591512\n",
      "Epoch 286/300 - 5.083675s - Loss 0.014133 - Accuracy 0.958333 - Test Loss 3.214169 - Test Accuracy 0.595049\n",
      "Epoch 287/300 - 5.059079s - Loss 0.028532 - Accuracy 0.968750 - Test Loss 3.351134 - Test Accuracy 0.596375\n",
      "Epoch 288/300 - 5.100927s - Loss 0.054485 - Accuracy 0.942708 - Test Loss 2.918503 - Test Accuracy 0.598585\n",
      "Epoch 289/300 - 5.066880s - Loss 0.017093 - Accuracy 0.963542 - Test Loss 2.990016 - Test Accuracy 0.591512\n",
      "Epoch 290/300 - 5.085006s - Loss 0.037916 - Accuracy 0.947917 - Test Loss 3.502144 - Test Accuracy 0.594607\n",
      "Epoch 291/300 - 5.058638s - Loss 0.085100 - Accuracy 0.932292 - Test Loss 3.005982 - Test Accuracy 0.596817\n",
      "Epoch 292/300 - 5.052900s - Loss 0.017955 - Accuracy 0.968750 - Test Loss 3.722119 - Test Accuracy 0.588417\n",
      "Epoch 293/300 - 5.067034s - Loss 0.045197 - Accuracy 0.958333 - Test Loss 3.121410 - Test Accuracy 0.593722\n",
      "Epoch 294/300 - 5.050954s - Loss 0.079129 - Accuracy 0.921875 - Test Loss 3.277094 - Test Accuracy 0.595049\n",
      "Epoch 295/300 - 5.057886s - Loss 0.021793 - Accuracy 0.953125 - Test Loss 3.314425 - Test Accuracy 0.596817\n",
      "Epoch 296/300 - 5.049053s - Loss 0.072029 - Accuracy 0.937500 - Test Loss 2.876745 - Test Accuracy 0.600354\n",
      "Epoch 297/300 - 5.052057s - Loss 0.042211 - Accuracy 0.937500 - Test Loss 3.392406 - Test Accuracy 0.596375\n",
      "Epoch 298/300 - 5.044781s - Loss 0.051843 - Accuracy 0.942708 - Test Loss 3.145034 - Test Accuracy 0.594164\n",
      "Epoch 299/300 - 5.073849s - Loss 0.029671 - Accuracy 0.932292 - Test Loss 2.865103 - Test Accuracy 0.591512\n",
      "repeat_0 [0.6052166224580018]\n",
      "{'repeat_0': {'0': {'precision': 0.5238095238095238, 'recall': 0.10784313725490197, 'f1-score': 0.1788617886178862, 'support': 102}, '1': {'precision': 0.4696969696969697, 'recall': 0.23308270676691728, 'f1-score': 0.31155778894472363, 'support': 133}, '2': {'precision': 0.2926829268292683, 'recall': 0.10344827586206896, 'f1-score': 0.15286624203821655, 'support': 116}, '3': {'precision': 0.3888888888888889, 'recall': 0.14893617021276595, 'f1-score': 0.21538461538461537, 'support': 94}, '4': {'precision': 0.425, 'recall': 0.1588785046728972, 'f1-score': 0.2312925170068027, 'support': 107}, '5': {'precision': 0.4146341463414634, 'recall': 0.15454545454545454, 'f1-score': 0.2251655629139073, 'support': 110}, '6': {'precision': 0.08823529411764706, 'recall': 0.030303030303030304, 'f1-score': 0.04511278195488722, 'support': 99}, '7': {'precision': 0.42857142857142855, 'recall': 0.16363636363636364, 'f1-score': 0.23684210526315788, 'support': 110}, '8': {'precision': 0.5238095238095238, 'recall': 0.1981981981981982, 'f1-score': 0.28758169934640526, 'support': 111}, 'micro avg': {'precision': 0.39944903581267216, 'recall': 0.14765784114052954, 'f1-score': 0.21561338289962825, 'support': 982}, 'macro avg': {'precision': 0.3950365224516348, 'recall': 0.14431909349473312, 'f1-score': 0.20940723349673357, 'support': 982}, 'weighted avg': {'precision': 0.39868717715574487, 'recall': 0.14765784114052954, 'f1-score': 0.21345886267968378, 'support': 982}, 'samples avg': {'precision': 0.0592396109637489, 'recall': 0.0641025641025641, 'f1-score': 0.060786914235190094, 'support': 982}}}\n",
      "{'repeat_0': {'0': {'precision': 0.5238095238095238, 'recall': 0.10784313725490197, 'f1-score': 0.1788617886178862, 'support': 102}, '1': {'precision': 0.4696969696969697, 'recall': 0.23308270676691728, 'f1-score': 0.31155778894472363, 'support': 133}, '2': {'precision': 0.2926829268292683, 'recall': 0.10344827586206896, 'f1-score': 0.15286624203821655, 'support': 116}, '3': {'precision': 0.3888888888888889, 'recall': 0.14893617021276595, 'f1-score': 0.21538461538461537, 'support': 94}, '4': {'precision': 0.425, 'recall': 0.1588785046728972, 'f1-score': 0.2312925170068027, 'support': 107}, '5': {'precision': 0.4146341463414634, 'recall': 0.15454545454545454, 'f1-score': 0.2251655629139073, 'support': 110}, '6': {'precision': 0.08823529411764706, 'recall': 0.030303030303030304, 'f1-score': 0.04511278195488722, 'support': 99}, '7': {'precision': 0.42857142857142855, 'recall': 0.16363636363636364, 'f1-score': 0.23684210526315788, 'support': 110}, '8': {'precision': 0.5238095238095238, 'recall': 0.1981981981981982, 'f1-score': 0.28758169934640526, 'support': 111}, 'micro avg': {'precision': 0.39944903581267216, 'recall': 0.14765784114052954, 'f1-score': 0.21561338289962825, 'support': 982}, 'macro avg': {'precision': 0.3950365224516348, 'recall': 0.14431909349473312, 'f1-score': 0.20940723349673357, 'support': 982}, 'weighted avg': {'precision': 0.39868717715574487, 'recall': 0.14765784114052954, 'f1-score': 0.21345886267968378, 'support': 982}, 'samples avg': {'precision': 0.0592396109637489, 'recall': 0.0641025641025641, 'f1-score': 0.060786914235190094, 'support': 982}}, 'accuracy': {'avg': 0.6052166224580018, 'std': 0.0}, 'time_train': {'avg': 1515.8096594810486, 'std': 0.0}, 'time_test': {'avg': 0.4770057201385498, 'std': 0.0}, 'model': 'THAT', 'task': 'activity', 'data': {'num_users': ['0', '1', '2', '3', '4', '5'], 'wifi_band': ['2.4'], 'environment': ['classroom'], 'length': 3000}, 'nn': {'lr': 0.001, 'epoch': 300, 'batch_size': 64, 'threshold': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()           \n",
    "torch.cuda.empty_cache()  \n",
    "torch.cuda.ipc_collect()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "[file]          run.py\n",
    "[description]   run WiFi-based models and optionally save a multiclass confusion matrix\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# from preset import preset, name_run\n",
    "# from load_data import load_data_x, load_data_y, encode_data_y\n",
    "# from lstm import run_lstm, LSTMM\n",
    "# from bilstm import run_bilstm, BiLSTMM\n",
    "# from that import run_that, THAT\n",
    "# from resnet import run_resnet, ResNet18Model\n",
    "# from strf import run_strf  # if you have the ST-RF implementation\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model\",   default=preset[\"model\"],  type=str)\n",
    "    parser.add_argument(\"--task\",    default=preset[\"task\"],   type=str)\n",
    "    parser.add_argument(\"--repeat\",  default=preset[\"repeat\"], type=int)\n",
    "    parser.add_argument(\"--save_cm\", action=\"store_true\",\n",
    "                        help=\"Save a multiclass confusion matrix of the best model to PDF\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def save_multiclass_confusion_matrix(model, data_loader, device, pdf_path, num_classes):\n",
    "    \"\"\"\n",
    "    Given a model that outputs one-hot logits for a multiclass task,\n",
    "    convert to predicted classes via argmax, then plot and save a\n",
    "    num_classes × num_classes confusion matrix to pdf_path.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            # predicted class is index of max logit\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            trues = torch.argmax(yb, dim=1).cpu().numpy()\n",
    "            y_pred.extend(preds.tolist())\n",
    "            y_true.extend(trues.tolist())\n",
    "\n",
    "    labels = list(range(num_classes))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    disp.plot(ax=ax, xticks_rotation=\"vertical\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "def run():\n",
    "    args       = parse_args()\n",
    "    var_model  = args.model\n",
    "    var_task   = args.task\n",
    "    var_repeat = args.repeat\n",
    "\n",
    "    # --- Load and encode the data ---\n",
    "    data_pd_y = load_data_y(\n",
    "        preset[\"path\"][\"data_y\"],\n",
    "        var_environment=preset[\"data\"][\"environment\"],\n",
    "        var_wifi_band=preset[\"data\"][\"wifi_band\"],\n",
    "        var_num_users=preset[\"data\"][\"num_users\"]\n",
    "    )\n",
    "    labels = data_pd_y[\"label\"].tolist()\n",
    "    data_x = load_data_x(preset[\"path\"][\"data_x\"], labels)\n",
    "    data_y = encode_data_y(data_pd_y, var_task)\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(\n",
    "        data_x, data_y, test_size=0.2, shuffle=True, random_state=39\n",
    "    )\n",
    "\n",
    "    # --- Select which model runner to use ---\n",
    "    if var_model == \"ST-RF\":\n",
    "        from strf import run_strf\n",
    "        run_model = run_strf\n",
    "    elif var_model == \"LSTM\":\n",
    "        run_model = run_lstm\n",
    "    elif var_model == \"bi-LSTM\":\n",
    "        run_model = run_bilstm\n",
    "    elif var_model == \"THAT\":\n",
    "        run_model = run_that\n",
    "    elif var_model == \"ResNet18\":\n",
    "        run_model = run_resnet\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {var_model}\")\n",
    "\n",
    "    # --- Train and evaluate ---\n",
    "    print(f\"Running model: {var_model}\")\n",
    "    result = run_model(train_x, train_y, test_x, test_y, var_repeat)\n",
    "    result[\"model\"] = var_model\n",
    "    result[\"task\"]  = var_task\n",
    "    result[\"data\"]  = preset[\"data\"]\n",
    "    result[\"nn\"]    = preset[\"nn\"]\n",
    "    print(result)\n",
    "\n",
    "    # --- Save results to JSON ---\n",
    "    # with open(preset[\"path\"][\"save\"], \"w\") as f:\n",
    "    #     json.dump(result, f, indent=4)\n",
    "\n",
    "    # # --- Optionally save a multiclass confusion matrix ---\n",
    "    # # if args.save_cm:\n",
    "    # if Confusion_matrix == 1:\n",
    "    #     # 1) completely release GPU memory used for training\n",
    "    #     del run_model                      # if 'model' from training is still in scope\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     torch.cuda.ipc_collect()\n",
    "    \n",
    "    #     # 2) reshape input only if the network is sequence‑based\n",
    "    #     if var_model in (\"LSTM\", \"bi-LSTM\", \"THAT\"):\n",
    "    #         test_x_cm = test_x.reshape(test_x.shape[0], test_x.shape[1], -1)\n",
    "    #     else:                           # ResNet18, ST‑RF\n",
    "    #         test_x_cm = test_x\n",
    "    \n",
    "    #     # 3) build the *same* architecture on CPU and load its weights\n",
    "    #     device_cm = torch.device(\"cpu\")\n",
    "    #     if var_model == \"LSTM\":\n",
    "    #         model_cm = LSTMM(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     elif var_model == \"bi-LSTM\":\n",
    "    #         model_cm = BiLSTMM(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     elif var_model == \"THAT\":\n",
    "    #         model_cm = THAT(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     elif var_model == \"ResNet18\":\n",
    "    #         model_cm = ResNet18Model(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Confusion matrix not supported for {var_model}\")\n",
    "    \n",
    "    #     # best_path = f\"/kaggle/working/{name_run}_best_model.pt\"\n",
    "    #     # model_cm.load_state_dict(torch.load(best_path, map_location=device_cm))\n",
    "    #     # model_cm.eval()\n",
    "    \n",
    "    #     # 4) DataLoader on CPU with a safe batch size\n",
    "    #     test_ds = TensorDataset(torch.from_numpy(test_x_cm).float(),\n",
    "    #                             torch.from_numpy(test_y).float())\n",
    "    #     test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "    \n",
    "    #     # 5) save the confusion matrix PDF\n",
    "    #     num_classes = test_y.shape[1]\n",
    "    #     pdf_name = f\"{name_run}_confusion_matrix.pdf\"\n",
    "    #     save_multiclass_confusion_matrix(model_cm,test_loader,device_cm,pdf_name,num_classes)\n",
    "    #     print(f\"✅ Saved confusion matrix (classes 0–{num_classes-1}) to {pdf_name}\")\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"start\")\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bddc1e",
   "metadata": {
    "papermill": {
     "duration": 0.018372,
     "end_time": "2025-12-28T21:29:58.277262",
     "exception": false,
     "start_time": "2025-12-28T21:29:58.258890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 12: Few-shot Learning\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81ea6d5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T21:29:58.316261Z",
     "iopub.status.busy": "2025-12-28T21:29:58.315988Z",
     "iopub.status.idle": "2025-12-28T21:29:58.321602Z",
     "shell.execute_reply": "2025-12-28T21:29:58.320965Z"
    },
    "papermill": {
     "duration": 0.026799,
     "end_time": "2025-12-28T21:29:58.322744",
     "exception": false,
     "start_time": "2025-12-28T21:29:58.295945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import torch\n",
    "# import shutil\n",
    "# import json\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# gc.collect()           \n",
    "# torch.cuda.empty_cache()  \n",
    "# torch.cuda.ipc_collect()\n",
    "\n",
    "# # ---------- helper: save multiclass confusion matrix ------------------\n",
    "# def save_multiclass_confusion_matrix(model, data_loader, pdf_path, num_classes):\n",
    "#     \"\"\"\n",
    "#     Forward‑pass on CPU, collect predictions, and write an N×N confusion matrix\n",
    "#     to a single‑page PDF (pdf_path).\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     y_true, y_pred = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in data_loader:\n",
    "#             logits = model(xb.cpu())                       # ensure CPU\n",
    "#             preds  = torch.argmax(logits, dim=1).numpy()\n",
    "#             trues  = torch.argmax(yb, dim=1).numpy()\n",
    "#             y_pred.extend(preds.tolist())\n",
    "#             y_true.extend(trues.tolist())\n",
    "\n",
    "#     labels = list(range(num_classes))\n",
    "#     cm  = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "#     disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "#     fig, ax = plt.subplots(figsize=(8, 8))\n",
    "#     disp.plot(ax=ax, xticks_rotation=\"vertical\")\n",
    "#     ax.set_title(\"Few‑shot Confusion Matrix\")\n",
    "#     with PdfPages(pdf_path) as pdf:\n",
    "#         pdf.savefig(fig)\n",
    "#     plt.close(fig)\n",
    "\n",
    "# # -------------------- pick run_* function ------------------------------\n",
    "# if preset[\"model\"] == \"ST-RF\":\n",
    "#     run_model = run_strf\n",
    "# elif preset[\"model\"] == \"LSTM\":\n",
    "#     run_model = run_lstm\n",
    "# elif preset[\"model\"] == \"bi-LSTM\":\n",
    "#     run_model = run_bilstm\n",
    "# elif preset[\"model\"] == \"THAT\":\n",
    "#     run_model = run_that\n",
    "# elif preset[\"model\"] == \"ResNet18\":\n",
    "#     run_model = run_resnet\n",
    "# else:\n",
    "#     raise ValueError(f\"No few‑shot implementation for {preset['model']}.\")\n",
    "\n",
    "# # ------------------------ load / split data ----------------------------\n",
    "# data_pd_y = load_data_y(preset[\"path\"][\"data_y\"],\n",
    "#                         var_environment=[dest_env],\n",
    "#                         var_wifi_band=preset[\"data\"][\"wifi_band\"],\n",
    "#                         var_num_users=preset[\"data\"][\"num_users\"])\n",
    "\n",
    "# labels_list = data_pd_y[\"label\"].tolist()\n",
    "# data_x = load_data_x(preset[\"path\"][\"data_x\"], labels_list)\n",
    "# data_y = encode_data_y(data_pd_y, preset[\"task\"])\n",
    "\n",
    "# train_x, test_x, train_y, test_y = train_test_split(\n",
    "#     data_x, data_y, test_size=0.2, shuffle=True, random_state=39)\n",
    "\n",
    "# # Few-shot sample size\n",
    "# train_x = train_x[:few_shot_num_samples]\n",
    "# train_y = train_y[:few_shot_num_samples]\n",
    "\n",
    "# # ----------------------- few‑shot training -----------------------------\n",
    "# original_epochs = preset[\"nn\"][\"epoch\"]\n",
    "# preset[\"nn\"][\"epoch\"] = few_shot_epochs\n",
    "\n",
    "# # Load the best model weights\n",
    "# best_model_path = f\"{name_run}_best_model.pt\"\n",
    "\n",
    "# # Initialize the model \n",
    "# if preset[\"model\"] == \"LSTM\":\n",
    "#     model = LSTMM(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "#     # print('train_y_[0].shape:', train_y[0].shape)\n",
    "#     # print('train_x_[0].shape:', train_x[0].reshape(train_x[0].shape[0], -1).shape)\n",
    "# elif preset[\"model\"] == \"bi-LSTM\":\n",
    "#     model = BiLSTMM(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "# elif preset[\"model\"] == \"THAT\":\n",
    "#     model = THAT(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "# elif preset[\"model\"] == \"ResNet18\":\n",
    "#     model = ResNet18Model(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "# else:\n",
    "#     raise ValueError(f\"Model {preset['model']} not supported!\")\n",
    "\n",
    "# # Load the weights into the model\n",
    "# model.load_state_dict(torch.load(best_model_path, map_location=\"cpu\"))\n",
    "# model = model.to('cuda')\n",
    "\n",
    "# # Fine-tune the model on few-shot data (note: `run_model` should now return only the result)\n",
    "# result = run_model(train_x, train_y, test_x, test_y, var_repeat=1, init_model=model)\n",
    "# print(result)\n",
    "\n",
    "# # --------------------- save few‑shot checkpoints -----------------------\n",
    "# # After fine-tuning, save the model\n",
    "# torch.save(model.state_dict(), f\"{name_run}_fewshot_final_model.pt\")\n",
    "# torch.save(model.state_dict(), f\"{name_run}_fewshot_best_model.pt\")\n",
    "\n",
    "# # ------------------- confusion matrix on CPU ---------------------------\n",
    "# if Confusion_matrix == 1 and preset[\"model\"] != \"ST-RF\":\n",
    "\n",
    "#     # reshape for sequence models\n",
    "#     test_x_rs = (test_x.reshape(test_x.shape[0], test_x.shape[1], -1)\n",
    "#                  if preset[\"model\"] in (\"LSTM\", \"bi-LSTM\", \"THAT\") else test_x)\n",
    "\n",
    "#     # instantiate identical architecture on CPU\n",
    "#     if preset[\"model\"] == \"LSTM\":\n",
    "#         model_cpu = LSTMM(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "#     elif preset[\"model\"] == \"bi-LSTM\":\n",
    "#         model_cpu = BiLSTMM(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "#     elif preset[\"model\"] == \"THAT\":\n",
    "#         model_cpu = THAT(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "#     else:  # ResNet18\n",
    "#         model_cpu = ResNet18Model(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "\n",
    "#     # load weights\n",
    "#     model_cpu.load_state_dict(torch.load(f\"{name_run}_fewshot_best_model.pt\", map_location=\"cpu\"))\n",
    "\n",
    "#     # CPU DataLoader with a safe batch size\n",
    "#     test_ds = TensorDataset(torch.from_numpy(test_x_rs).float(),\n",
    "#                             torch.from_numpy(test_y).float())\n",
    "#     test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "#     pdf_name = f\"{name_run}_fewshot_confusion_matrix.pdf\"\n",
    "#     num_classes = test_y.shape[1]\n",
    "#     save_multiclass_confusion_matrix(model_cpu, test_loader, pdf_name, num_classes)\n",
    "#     print(f\"✅ Saved few‑shot confusion matrix (classes 0–{num_classes-1}) to {pdf_name}\")\n",
    "\n",
    "# # ----------------------- restore & persist -----------------------------\n",
    "# preset[\"nn\"][\"epoch\"] = original_epochs\n",
    "\n",
    "# # Save the final result to JSON\n",
    "# with open(\"result_fewshot.json\", \"w\") as f:\n",
    "#     json.dump(result, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7456fa37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T21:29:58.362309Z",
     "iopub.status.busy": "2025-12-28T21:29:58.362065Z",
     "iopub.status.idle": "2025-12-28T21:29:58.371942Z",
     "shell.execute_reply": "2025-12-28T21:29:58.371319Z"
    },
    "papermill": {
     "duration": 0.031247,
     "end_time": "2025-12-28T21:29:58.373026",
     "exception": false,
     "start_time": "2025-12-28T21:29:58.341779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import scipy.io as scio\n",
    "# import time\n",
    "# import torch\n",
    "# import gc\n",
    "# from numpy.linalg import svd\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "# from copy import deepcopy\n",
    "# import json\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# import torch._dynamo\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # --- تنظیمات سیستمی ---\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# # --------------------------\n",
    "# # 1. تنظیمات (Configuration)\n",
    "# # --------------------------\n",
    "# preset = {\n",
    "#     \"model\": \"THAT\",          \n",
    "#     \"task\": \"activity\",       \n",
    "#     \"repeat\": 1,\n",
    "#     \"path\": {\n",
    "#         \"data_x\": \"/kaggle/input/wimans/wifi_csi/amp\",   \n",
    "#         \"data_y\": \"/kaggle/input/wimans/annotation.csv\", \n",
    "#     },\n",
    "#     \"data\": {\n",
    "#         \"num_users\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"],  \n",
    "#         \"wifi_band\": [\"2.4\"],                         \n",
    "#         \"environment\": [\"classroom\"],                 \n",
    "#         \"length\": 3000,\n",
    "        \n",
    "#         # 1.0 = 100% data (Full run) | 0.1 = 10% data (Quick test)\n",
    "#         \"subset_ratio\": 0.5,  \n",
    "#     },\n",
    "#     \"nn\": {\n",
    "#         \"lr\": 1e-3,           \n",
    "#         \"epoch\": 80,          \n",
    "#         \"batch_size\": 32,    \n",
    "#         \"threshold\": 0.5,\n",
    "#         \"patience\": 5,        \n",
    "#         \"factor\": 0.5,        \n",
    "#         \"min_lr\": 1e-6        \n",
    "#     },\n",
    "#     \"encoding\": {\n",
    "#         \"activity\": {\n",
    "#             \"nan\":      [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#             \"nothing\":  [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#             \"walk\":     [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "#             \"rotation\": [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "#             \"jump\":     [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "#             \"wave\":     [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "#             \"lie_down\": [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "#             \"pick_up\":  [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "#             \"sit_down\": [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "#             \"stand_up\": [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "#         },\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# # --------------------------\n",
    "# # 2. توابع RPCA و لود دیتا\n",
    "# # --------------------------\n",
    "# def soft_threshold(x, epsilon):\n",
    "#     return np.maximum(np.abs(x) - epsilon, 0) * np.sign(x)\n",
    "\n",
    "# def robust_pca(M, max_iter=10, tol=1e-4):\n",
    "#     n1, n2 = M.shape\n",
    "#     lambda_param = 1 / np.sqrt(max(n1, n2))\n",
    "#     Y = M / np.maximum(np.linalg.norm(M, 2), np.linalg.norm(M, np.inf) / lambda_param)\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     mu = 1.25 / np.linalg.norm(M, 2)\n",
    "#     rho = 1.5\n",
    "#     for i in range(max_iter):\n",
    "#         temp_L = M - S + (1/mu) * Y\n",
    "#         U, Sigma, Vt = svd(temp_L, full_matrices=False)\n",
    "#         Sigma_thresh = soft_threshold(Sigma, 1/mu)\n",
    "#         L_new = np.dot(U * Sigma_thresh, Vt)\n",
    "#         temp_S = M - L_new + (1/mu) * Y\n",
    "#         S_new = soft_threshold(temp_S, lambda_param/mu)\n",
    "#         error = np.linalg.norm(M - L_new - S_new, 'fro') / np.linalg.norm(M, 'fro')\n",
    "#         L = L_new; S = S_new\n",
    "#         if error < tol: break\n",
    "#         Y = Y + mu * (M - L - S)\n",
    "#         mu = min(mu * rho, 1e7)\n",
    "#     return L, S\n",
    "\n",
    "# def load_data_y(var_path_data_y, var_environment=None, var_wifi_band=None, var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None: data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None: data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None: data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list, use_rpca=True):\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     mode_str = \"WITH RPCA\" if use_rpca else \"RAW DATA (No RPCA)\"\n",
    "#     print(f\"Loading {len(var_path_list)} samples - Mode: {mode_str}...\")\n",
    "#     for i, var_path in enumerate(var_path_list):\n",
    "#         if i % 100 == 0 and i > 0: print(f\"Processing {i}/{len(var_path_list)}...\")\n",
    "#         data_csi = np.load(var_path) \n",
    "#         data_csi_2d = data_csi.reshape(data_csi.shape[0], -1)\n",
    "#         target_len = preset[\"data\"][\"length\"]\n",
    "#         current_len = data_csi_2d.shape[0]\n",
    "#         var_pad_length = target_len - current_len\n",
    "#         if var_pad_length > 0: data_csi_pad = np.pad(data_csi_2d, ((0, var_pad_length), (0, 0)), mode='constant')\n",
    "#         else: data_csi_pad = data_csi_2d[:target_len, :]\n",
    "#         if use_rpca:\n",
    "#             L, S = robust_pca(data_csi_pad)\n",
    "#             final_sample = np.concatenate([L, S], axis=1) \n",
    "#         else:\n",
    "#             final_sample = data_csi_pad\n",
    "#         data_x.append(final_sample)\n",
    "#     data_x = np.array(data_x)\n",
    "#     return data_x\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"activity\": return encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     return encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     cols = [f\"user_{i}_activity\" for i in range(1, 7)]\n",
    "#     data = data_pd_y[cols].to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[y] for y in sample] for sample in data])\n",
    "\n",
    "# # --------------------------\n",
    "# # 3. مدل THAT\n",
    "# # --------------------------\n",
    "# class Gaussian_Position(torch.nn.Module):\n",
    "#     def __init__(self, var_dim_feature, var_dim_time, var_num_gaussian=10):\n",
    "#         super(Gaussian_Position, self).__init__()\n",
    "#         self.var_embedding = torch.nn.Parameter(torch.zeros([var_num_gaussian, var_dim_feature]), requires_grad=True)\n",
    "#         torch.nn.init.xavier_uniform_(self.var_embedding)\n",
    "#         self.var_position = torch.nn.Parameter(torch.arange(0.0, var_dim_time).unsqueeze(1).repeat(1, var_num_gaussian), requires_grad=False)\n",
    "#         self.var_mu = torch.nn.Parameter(torch.arange(0.0, var_dim_time, var_dim_time/var_num_gaussian).unsqueeze(0), requires_grad=True)\n",
    "#         self.var_sigma = torch.nn.Parameter(torch.tensor([50.0] * var_num_gaussian).unsqueeze(0), requires_grad=True)\n",
    "#     def forward(self, var_input):\n",
    "#         var_pdf = - (self.var_position - self.var_mu)**2 / (2 * self.var_sigma**2) - torch.log(self.var_sigma)\n",
    "#         var_pdf = torch.softmax(var_pdf, dim=-1)\n",
    "#         return var_input + torch.matmul(var_pdf, self.var_embedding).unsqueeze(0)\n",
    "\n",
    "# class Encoder(torch.nn.Module):\n",
    "#     def __init__(self, var_dim_feature, var_num_head=10, var_size_cnn=[1, 3, 5]):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.layer_norm_0 = torch.nn.LayerNorm(var_dim_feature, eps=1e-6)\n",
    "#         self.layer_attention = torch.nn.MultiheadAttention(var_dim_feature, var_num_head, batch_first=True)\n",
    "#         self.layer_dropout_0 = torch.nn.Dropout(0.1)\n",
    "#         self.layer_norm_1 = torch.nn.LayerNorm(var_dim_feature, 1e-6)\n",
    "#         self.layer_cnn = torch.nn.ModuleList([torch.nn.Sequential(torch.nn.Conv1d(var_dim_feature, var_dim_feature, s, padding=\"same\"), torch.nn.BatchNorm1d(var_dim_feature), torch.nn.Dropout(0.1), torch.nn.LeakyReLU()) for s in var_size_cnn])\n",
    "#         self.layer_dropout_1 = torch.nn.Dropout(0.1)\n",
    "#     def forward(self, var_input):\n",
    "#         var_t = self.layer_norm_0(var_input)\n",
    "#         var_t, _ = self.layer_attention(var_t, var_t, var_t)\n",
    "#         var_t = self.layer_dropout_0(var_t) + var_input\n",
    "#         var_s = self.layer_norm_1(var_t).permute(0, 2, 1)\n",
    "#         var_c = torch.stack([l(var_s) for l in self.layer_cnn], dim=0)\n",
    "#         var_s = self.layer_dropout_1((torch.sum(var_c, dim=0) / len(self.layer_cnn)).permute(0, 2, 1))\n",
    "#         return var_s + var_t\n",
    "\n",
    "# class THAT(torch.nn.Module):\n",
    "#     def __init__(self, var_x_shape, var_y_shape):\n",
    "#         super(THAT, self).__init__()\n",
    "#         var_dim_feature, var_dim_time = var_x_shape[-1], var_x_shape[-2]\n",
    "#         var_dim_output = var_y_shape[-1]\n",
    "#         self.layer_left_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "#         self.layer_left_gaussian = Gaussian_Position(var_dim_feature, var_dim_time // 20)\n",
    "#         self.layer_left_encoder = torch.nn.ModuleList([Encoder(var_dim_feature, 10, [1, 3, 5]) for _ in range(4)])\n",
    "#         self.layer_left_norm = torch.nn.LayerNorm(var_dim_feature, eps=1e-6)\n",
    "#         self.layer_left_cnn = torch.nn.ModuleList([torch.nn.Conv1d(var_dim_feature, 128, k) for k in [8, 16]])\n",
    "#         self.layer_left_dropout = torch.nn.Dropout(0.5)\n",
    "#         var_dim_right = var_dim_time // 20\n",
    "#         self.layer_right_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "#         self.layer_right_encoder = torch.nn.ModuleList([Encoder(var_dim_right, 10, [1, 2, 3])])\n",
    "#         self.layer_right_norm = torch.nn.LayerNorm(var_dim_right, eps=1e-6)\n",
    "#         self.layer_right_cnn = torch.nn.ModuleList([torch.nn.Conv1d(var_dim_right, 16, k) for k in [2, 4]])\n",
    "#         self.layer_right_dropout = torch.nn.Dropout(0.5)\n",
    "#         self.layer_leakyrelu = torch.nn.LeakyReLU()\n",
    "#         self.layer_output = torch.nn.Linear(256 + 32, var_dim_output)\n",
    "#     def forward(self, var_input):\n",
    "#         v_l = self.layer_left_gaussian(self.layer_left_pooling(var_input.permute(0, 2, 1)).permute(0, 2, 1))\n",
    "#         for l in self.layer_left_encoder: v_l = l(v_l)\n",
    "#         v_l = self.layer_left_norm(v_l).permute(0, 2, 1)\n",
    "#         v_l = torch.cat([torch.sum(self.layer_leakyrelu(cnn(v_l)), dim=-1) for cnn in self.layer_left_cnn], dim=-1)\n",
    "#         v_l = self.layer_left_dropout(v_l)\n",
    "#         v_r = self.layer_right_pooling(var_input.permute(0, 2, 1))\n",
    "#         for l in self.layer_right_encoder: v_r = l(v_r)\n",
    "#         v_r = self.layer_right_norm(v_r).permute(0, 2, 1)\n",
    "#         v_r = torch.cat([torch.sum(self.layer_leakyrelu(cnn(v_r)), dim=-1) for cnn in self.layer_right_cnn], dim=-1)\n",
    "#         v_r = self.layer_right_dropout(v_r)\n",
    "#         return self.layer_output(torch.cat([v_l, v_r], dim=-1))\n",
    "\n",
    "# # --------------------------\n",
    "# # 4. Training Loop\n",
    "# # --------------------------\n",
    "# def train(model, optimizer, loss_fn, train_loader, test_loader, threshold, epochs, device, model_path):\n",
    "#     best_acc = -1.0\n",
    "#     best_w = deepcopy(model.state_dict())\n",
    "    \n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#         optimizer, mode='max', factor=preset[\"nn\"][\"factor\"], patience=preset[\"nn\"][\"patience\"],\n",
    "#         min_lr=preset[\"nn\"][\"min_lr\"], verbose=True\n",
    "#     )\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         t0 = time.time()\n",
    "#         model.train()\n",
    "        \n",
    "#         # --- [MODIFIED] Using requested variable names ---\n",
    "#         for data_batch_x, data_batch_y in train_loader:\n",
    "#             data_batch_x = data_batch_x.to(device)\n",
    "#             data_batch_y = data_batch_y.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             predict_train_y = model(data_batch_x)\n",
    "            \n",
    "#             # --- [REQUESTED LINE] ---\n",
    "#             loss_value = loss_fn(predict_train_y, data_batch_y.reshape(data_batch_y.shape[0], -1).float())\n",
    "            \n",
    "#             loss_value.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             tx, ty = next(iter(test_loader))\n",
    "#             tx, ty = tx.to(device), ty.to(device)\n",
    "#             pred_t = model(tx)\n",
    "            \n",
    "#             p_cls = (torch.sigmoid(pred_t) > threshold).float().cpu().numpy()\n",
    "#             t_cls = ty.cpu().numpy()\n",
    "#             acc = accuracy_score(t_cls.reshape(-1, t_cls.shape[-1]), p_cls.reshape(-1, t_cls.shape[-1]))\n",
    "            \n",
    "#         scheduler.step(acc)\n",
    "#         current_lr = optimizer.param_groups[0]['lr']\n",
    "#         print(f\"Ep {epoch+1}/{epochs} | LR: {current_lr:.6f} | L_tr: {loss_value.item():.4f} | Acc: {acc:.4f}\")\n",
    "        \n",
    "#         if acc > best_acc:\n",
    "#             best_acc = acc\n",
    "#             best_w = deepcopy(model.state_dict())\n",
    "            \n",
    "#     torch.save(best_w, model_path)\n",
    "#     return best_w\n",
    "\n",
    "# def save_multiclass_confusion_matrix(model, data_loader, device, pdf_path, num_classes, title_text):\n",
    "#     model.eval()\n",
    "#     y_true, y_pred = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in data_loader:\n",
    "#             xb = xb.to(device)\n",
    "#             logits = model(xb) \n",
    "#             logits = logits.reshape(-1, num_classes) \n",
    "#             yb = yb.reshape(-1, num_classes)        \n",
    "#             y_pred.extend(torch.argmax(logits, dim=1).cpu().numpy().tolist())\n",
    "#             y_true.extend(torch.argmax(yb, dim=1).cpu().numpy().tolist())\n",
    "    \n",
    "#     labels = list(range(num_classes))\n",
    "#     cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "#     disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "#     fig, ax = plt.subplots(figsize=(12, 12))\n",
    "#     disp.plot(ax=ax, xticks_rotation=\"vertical\", cmap='Blues')\n",
    "#     ax.set_title(title_text)\n",
    "#     with PdfPages(pdf_path) as pdf: pdf.savefig(fig)\n",
    "#     plt.close(fig)\n",
    "\n",
    "# # --------------------------\n",
    "# # 5. اجرا\n",
    "# # --------------------------\n",
    "# def run_experiment(scenario_name, use_rpca):\n",
    "#     print(f\"\\n################################################\")\n",
    "#     print(f\"STARTING SCENARIO: {scenario_name}\")\n",
    "#     print(f\"################################################\")\n",
    "    \n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     current_run_name = f\"{preset['model']}_{preset['task']}_{scenario_name}\"\n",
    "#     model_save_path = f\"{current_run_name}_best_model.pt\"\n",
    "#     json_save_path = f\"result_{current_run_name}.json\"\n",
    "#     pdf_save_path = f\"Confusion_{current_run_name}.pdf\"\n",
    "    \n",
    "#     # 1. Load Labels\n",
    "#     data_pd_y = load_data_y(preset[\"path\"][\"data_y\"], preset[\"data\"][\"environment\"], preset[\"data\"][\"wifi_band\"], preset[\"data\"][\"num_users\"])\n",
    "    \n",
    "#     # Apply Subset Ratio\n",
    "#     subset_ratio = preset[\"data\"][\"subset_ratio\"]\n",
    "#     if subset_ratio < 1.0:\n",
    "#         data_pd_y = data_pd_y.sample(frac=subset_ratio, random_state=42).reset_index(drop=True)\n",
    "#         print(f\"*** DEBUG MODE: Using {subset_ratio*100}% of data ({len(data_pd_y)} samples) ***\")\n",
    "    \n",
    "#     # 2. Load X\n",
    "#     data_x = load_data_x(preset[\"path\"][\"data_x\"], data_pd_y[\"label\"].tolist(), use_rpca=use_rpca)\n",
    "#     data_y = encode_data_y(data_pd_y, preset[\"task\"])\n",
    "    \n",
    "#     # 3. Split\n",
    "#     train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2, shuffle=True, random_state=39)\n",
    "#     train_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "#     test_ds = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "#     train_loader = DataLoader(train_ds, batch_size=preset[\"nn\"][\"batch_size\"], shuffle=True)\n",
    "#     test_loader = DataLoader(test_ds, batch_size=len(test_ds), shuffle=False)\n",
    "    \n",
    "#     result = {\"accuracy\": []}\n",
    "    \n",
    "#     for r in range(preset[\"repeat\"]):\n",
    "#         print(f\"--- Repeat {r+1}/{preset['repeat']} ---\")\n",
    "#         torch.random.manual_seed(r + 39)\n",
    "        \n",
    "#         model = THAT(train_x[0].shape, train_y[0].reshape(-1).shape).to(device)\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=preset[\"nn\"][\"lr\"])\n",
    "#         loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "#         best_w = train(model, optimizer, loss_fn, train_loader, test_loader, \n",
    "#                        preset[\"nn\"][\"threshold\"], preset[\"nn\"][\"epoch\"], device, model_save_path)\n",
    "        \n",
    "#         model.load_state_dict(best_w)\n",
    "#         with torch.no_grad():\n",
    "#             preds = model(torch.from_numpy(test_x).to(device))\n",
    "#             preds_reshaped = (torch.sigmoid(preds) > preset[\"nn\"][\"threshold\"]).float().cpu().numpy().reshape(-1, 9)\n",
    "#             targets_reshaped = test_y.reshape(-1, 9)\n",
    "#             acc = accuracy_score(targets_reshaped, preds_reshaped)\n",
    "#             result[\"accuracy\"].append(acc)\n",
    "            \n",
    "#     print(f\"Final Accuracy ({scenario_name}): {np.mean(result['accuracy']):.4f}\")\n",
    "#     with open(json_save_path, \"w\") as f: json.dump(result, f, indent=4)\n",
    "    \n",
    "#     print(\"Generating Confusion Matrix...\")\n",
    "#     model_cm = THAT(test_x[0].shape, test_y[0].reshape(-1).shape).to(\"cpu\")\n",
    "#     model_cm.load_state_dict(torch.load(model_save_path, map_location=\"cpu\"))\n",
    "#     cm_loader = DataLoader(TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y)), batch_size=32)\n",
    "#     num_classes = test_y.shape[2] \n",
    "#     title = f\"Confusion Matrix: {scenario_name} (Acc: {np.mean(result['accuracy']):.2f} - {subset_ratio*100}% Data)\"\n",
    "#     save_multiclass_confusion_matrix(model_cm, cm_loader, \"cpu\", pdf_save_path, num_classes, title)\n",
    "    \n",
    "#     del model, model_cm, train_x, test_x, data_x\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     print(f\"Done with {scenario_name}.\\n\")\n",
    "\n",
    "# def run():\n",
    "#     scenarios = [\n",
    "#         (\"RPCA\", True),\n",
    "#         (\"RAW\", False)\n",
    "#     ]\n",
    "#     for name, rpca_flag in scenarios:\n",
    "#         run_experiment(name, rpca_flag)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0ae154",
   "metadata": {
    "papermill": {
     "duration": 0.01844,
     "end_time": "2025-12-28T21:29:58.410825",
     "exception": false,
     "start_time": "2025-12-28T21:29:58.392385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4451316,
     "sourceId": 7638081,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7503373,
     "sourceId": 11934698,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3913.051561,
   "end_time": "2025-12-28T21:30:01.187970",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-28T20:24:48.136409",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
