{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95973f50",
   "metadata": {
    "papermill": {
     "duration": 0.008243,
     "end_time": "2025-12-29T07:33:44.492039",
     "exception": false,
     "start_time": "2025-12-29T07:33:44.483796",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 1: Library Imports\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c660d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T07:33:44.506758Z",
     "iopub.status.busy": "2025-12-29T07:33:44.506530Z",
     "iopub.status.idle": "2025-12-29T07:33:54.951609Z",
     "shell.execute_reply": "2025-12-29T07:33:54.950986Z"
    },
    "papermill": {
     "duration": 10.453936,
     "end_time": "2025-12-29T07:33:54.952964",
     "exception": false,
     "start_time": "2025-12-29T07:33:44.499028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 1: Library Imports\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as scio\n",
    "import time\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch._dynamo\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# from ptflops import get_model_complexity_info\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from copy import deepcopy\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f11472",
   "metadata": {
    "papermill": {
     "duration": 0.006742,
     "end_time": "2025-12-29T07:33:54.967174",
     "exception": false,
     "start_time": "2025-12-29T07:33:54.960432",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 2: preset.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09419230",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T07:33:54.981644Z",
     "iopub.status.busy": "2025-12-29T07:33:54.981326Z",
     "iopub.status.idle": "2025-12-29T07:33:54.990283Z",
     "shell.execute_reply": "2025-12-29T07:33:54.989631Z"
    },
    "papermill": {
     "duration": 0.017626,
     "end_time": "2025-12-29T07:33:54.991426",
     "exception": false,
     "start_time": "2025-12-29T07:33:54.973800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "few=empty_room,100,5,m=THAT,t=activity,epoch=300,batch=64,environment=['classroom']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "[file]          preset.py\n",
    "[description]   default settings of WiFi-based models\n",
    "\"\"\"\n",
    "minidata_set = 1\n",
    "preset = {\n",
    "    # define model\n",
    "    \"model\": \"THAT\",  # \"ST-RF\", \"MLP\", \"LSTM\", \"CNN-1D\", \"CNN-2D\", \"CLSTM\", \"ABLSTM\", \"THAT\", \"bi-LSTM\", \"ResNet18\"\n",
    "    # define task\n",
    "    \"task\": \"activity\",  # \"identity\", \"activity\", \"location\", \"count\"\n",
    "    # number of repeated experiments\n",
    "    \"repeat\": 1,\n",
    "    # path of data\n",
    "    \"path\": {\n",
    "        # \"data_x\": \"/kaggle/input/wimans/wifi_csi/mat\",   # directory of CSI amplitude files \n",
    "        \"data_x\": \"/kaggle/input/wimans/wifi_csi/amp\",   # directory of CSI amplitude files \n",
    "        \"data_y\": \"/kaggle/input/wimans/annotation.csv\", # path of annotation file\n",
    "        \"save\": \"result_lstm_epoch=80_batchsize=32_envs=empty_room_wifiband=2.4.json\"               # path to save results\n",
    "    },\n",
    "    # data selection for experiments\n",
    "    \"data\": {\n",
    "        \"num_users\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"],  # select number(s) of users\n",
    "        \"wifi_band\": [\"2.4\"],                         # select WiFi band(s)\n",
    "        \"environment\": [\"classroom\"],                 # select environment(s) [\"classroom\"], [\"meeting_room\"], [\"empty_room\"]\n",
    "        \"length\": 3000,                               # default length of CSI\n",
    "    },\n",
    "    # hyperparameters of models\n",
    "    \"nn\": {\n",
    "        \"lr\": 1e-3,           # learning rate\n",
    "        \"epoch\": 300,         # number of epochs\n",
    "        \"batch_size\": 64,    # batch size\n",
    "        \"threshold\": 0.5,     # threshold to binarize sigmoid outputs\n",
    "    },\n",
    "    # encoding of activities and locations\n",
    "    \"encoding\": {\n",
    "        \"activity\": {  # encoding of different activities\n",
    "            \"nan\":      [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"nothing\":  [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"walk\":     [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"rotation\": [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            \"jump\":     [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            \"wave\":     [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "            \"lie_down\": [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "            \"pick_up\":  [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "            \"sit_down\": [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "            \"stand_up\": [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        },\n",
    "        \"location\": {  # encoding of different locations\n",
    "            \"nan\":  [0, 0, 0, 0, 0],\n",
    "            \"a\":    [1, 0, 0, 0, 0],\n",
    "            \"b\":    [0, 1, 0, 0, 0],\n",
    "            \"c\":    [0, 0, 1, 0, 0],\n",
    "            \"d\":    [0, 0, 0, 1, 0],\n",
    "            \"e\":    [0, 0, 0, 0, 1],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Few-shot parameters (manually set)\n",
    "dest_env = \"empty_room\"       # Destination environment[\"classroom\"], [\"meeting_room\"], [\"empty_room\"]\n",
    "few_shot_epochs = 100         # Number of epochs for few-shot training\n",
    "few_shot_num_samples = 5     # Number of samples to use from the destination test data\n",
    "\n",
    "Confusion_matrix = 1\n",
    "\n",
    "name_run = \"few={},{},{},m={},t={},epoch={},batch={},environment={}\".format(dest_env, few_shot_epochs, few_shot_num_samples, preset[\"model\"], preset[\"task\"], preset[\"nn\"][\"epoch\"], preset[\"nn\"][\"batch_size\"], preset[\"data\"][\"environment\"])\n",
    "print(name_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c82e13",
   "metadata": {
    "papermill": {
     "duration": 0.00667,
     "end_time": "2025-12-29T07:33:55.005045",
     "exception": false,
     "start_time": "2025-12-29T07:33:54.998375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 3: load_data.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833d734c",
   "metadata": {
    "papermill": {
     "duration": 0.006676,
     "end_time": "2025-12-29T07:33:55.018431",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.011755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "raw + sparse\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08401ca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T07:33:55.033582Z",
     "iopub.status.busy": "2025-12-29T07:33:55.033369Z",
     "iopub.status.idle": "2025-12-29T07:33:55.059151Z",
     "shell.execute_reply": "2025-12-29T07:33:55.058667Z"
    },
    "papermill": {
     "duration": 0.034947,
     "end_time": "2025-12-29T07:33:55.060220",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.025273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 3: load_data.py  (AMP + RPCA sparse, and RAW + alpha*SPARSE)\n",
    "# =========================\n",
    "\"\"\"\n",
    "[file]          load_data.py\n",
    "[description]   load annotation file and CSI amplitude, and encode labels\n",
    "                (Test mode) X_input = X_raw + alpha * X_sparse\n",
    "                where X_sparse is RPCA sparse component computed per (Tx,Rx) link\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd  # Cell1 already imports, but safe here too\n",
    "\n",
    "# =========================================================\n",
    "# NEW TEST SETTINGS\n",
    "# =========================================================\n",
    "# \"raw\"            : use amplitude as-is\n",
    "# \"sparse\"         : use RPCA sparse component only\n",
    "# \"raw_plus_sparse\": raw + alpha*sparse  (your requested test)\n",
    "CSI_INPUT_MODE = \"raw\"   # <-- \"raw\" / \"sparse\" / \"raw_plus_sparse\"\n",
    "\n",
    "# weight of sparse when adding to raw\n",
    "SPARSE_ALPHA = 1.0\n",
    "\n",
    "# keep amplitudes non-negative after addition (recommended)\n",
    "CLAMP_NONNEG = True\n",
    "\n",
    "# RPCA settings (IALM)\n",
    "RPCA_MAX_ITER = 60\n",
    "RPCA_TOL      = 1e-5\n",
    "RPCA_RHO      = 1.5\n",
    "RPCA_MU_INIT  = None\n",
    "\n",
    "# lambda options you asked before\n",
    "# \"classic\": 1/sqrt(max(m,n))\n",
    "# \"median\" : 1/sqrt(max(m,n)) * median(abs(M))\n",
    "# \"scaled\" : 1.2/sqrt(max(m,n))\n",
    "RPCA_LAMBDA_MODE = \"median\"   # <-- \"classic\" / \"median\" / \"scaled\"\n",
    "\n",
    "# cache for speed\n",
    "CACHE_ENABLED = True\n",
    "CACHE_ROOT = \"/kaggle/working/csi_cache_amp_raw_plus_sparse\"\n",
    "\n",
    "# debug print once to confirm amplitude/phase\n",
    "_DEBUG_PRINT_ONCE = True\n",
    "\n",
    "# expected antenna/subcarrier dims (WiMANS typical)\n",
    "TX = 3\n",
    "RX = 3\n",
    "SC = 30\n",
    "# =========================================================\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RPCA helpers (IALM)\n",
    "# =========================\n",
    "def _compute_lambda(M: np.ndarray) -> float:\n",
    "    m, n = M.shape\n",
    "    base = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "    if RPCA_LAMBDA_MODE == \"classic\":\n",
    "        return base\n",
    "\n",
    "    if RPCA_LAMBDA_MODE == \"scaled\":\n",
    "        return 1.2 * base\n",
    "\n",
    "    if RPCA_LAMBDA_MODE == \"median\":\n",
    "        med = np.median(np.abs(M))\n",
    "        med = float(med) if med > 1e-12 else 1e-12\n",
    "        return base * med\n",
    "\n",
    "    raise ValueError(f\"Unknown RPCA_LAMBDA_MODE: {RPCA_LAMBDA_MODE}\")\n",
    "\n",
    "\n",
    "def _soft_threshold(X: np.ndarray, tau: float) -> np.ndarray:\n",
    "    return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "\n",
    "def _svt(X: np.ndarray, tau: float) -> np.ndarray:\n",
    "    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    s_thr = np.maximum(s - tau, 0.0)\n",
    "    if np.all(s_thr == 0):\n",
    "        return np.zeros_like(X)\n",
    "    return (U * s_thr) @ Vt\n",
    "\n",
    "\n",
    "def _rpca_ialm(M: np.ndarray,\n",
    "              max_iter: int = 60,\n",
    "              tol: float = 1e-5,\n",
    "              rho: float = 1.5,\n",
    "              mu: float | None = None):\n",
    "    \"\"\"\n",
    "    Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "    Decompose: M = L + S\n",
    "    Returns: L, S (float32)\n",
    "    \"\"\"\n",
    "    M = M.astype(np.float64, copy=False)\n",
    "    lam = _compute_lambda(M)\n",
    "\n",
    "    if mu is None:\n",
    "        # spectral norm approx via top singular value\n",
    "        s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "        mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "    L = np.zeros_like(M)\n",
    "    S = np.zeros_like(M)\n",
    "    Y = np.zeros_like(M)\n",
    "\n",
    "    normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "        S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "        R = M - L - S\n",
    "        Y = Y + mu * R\n",
    "\n",
    "        err = np.linalg.norm(R, ord=\"fro\") / normM\n",
    "        if err < tol:\n",
    "            break\n",
    "\n",
    "        mu *= rho\n",
    "\n",
    "    return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Data shape helpers\n",
    "# =========================\n",
    "def _ensure_shape_4d_amp(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Expect amp npy to be either:\n",
    "      - (T, 3, 3, 30)\n",
    "      - (T, 270)  -> reshape to (T,3,3,30)\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim == 4:\n",
    "        return x\n",
    "    if x.ndim == 2 and x.shape[1] == TX * RX * SC:\n",
    "        T = x.shape[0]\n",
    "        return x.reshape(T, TX, RX, SC)\n",
    "    raise ValueError(f\"Unexpected AMP shape: {x.shape} (expected (T,{TX},{RX},{SC}) or (T,{TX*RX*SC}))\")\n",
    "\n",
    "\n",
    "def _debug_print_once(label: str, arr: np.ndarray):\n",
    "    global _DEBUG_PRINT_ONCE\n",
    "    if not _DEBUG_PRINT_ONCE:\n",
    "        return\n",
    "    _DEBUG_PRINT_ONCE = False\n",
    "    print(f\"\\n[DEBUG] First loaded AMP sample: {label}\")\n",
    "    print(f\"[DEBUG] shape={arr.shape}, dtype={arr.dtype}, complex={np.iscomplexobj(arr)}\")\n",
    "    if np.iscomplexobj(arr):\n",
    "        print(\"[DEBUG] ==> WARNING: This looks complex; amp folder usually should be real amplitude.\")\n",
    "    else:\n",
    "        print(\"[DEBUG] ==> Input is REAL -> amplitude-only (no true phase).\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def _cache_path(label: str, mode: str) -> str:\n",
    "    return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "def _make_sparse_component_pairwise(x4: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    x4: (T,3,3,30) real amplitude\n",
    "    Run RPCA on each (Tx,Rx) link separately:\n",
    "      M = (T,30) for each link, compute S, place back.\n",
    "    Return: S_out same shape as x4 (float32)\n",
    "    \"\"\"\n",
    "    x4 = x4.astype(np.float32, copy=False)\n",
    "    T = x4.shape[0]\n",
    "    S_out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "    for tx in range(TX):\n",
    "        for rx in range(RX):\n",
    "            M = x4[:, tx, rx, :]  # (T,30)\n",
    "            _, S = _rpca_ialm(\n",
    "                M,\n",
    "                max_iter=RPCA_MAX_ITER,\n",
    "                tol=RPCA_TOL,\n",
    "                rho=RPCA_RHO,\n",
    "                mu=RPCA_MU_INIT\n",
    "            )\n",
    "            S_out[:, tx, rx, :] = S\n",
    "\n",
    "    return S_out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Public API (used by run.py)\n",
    "# =========================\n",
    "def load_data_y(var_path_data_y,\n",
    "                var_environment=None,\n",
    "                var_wifi_band=None,\n",
    "                var_num_users=None):\n",
    "    \"\"\"\n",
    "    Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "    \"\"\"\n",
    "    data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "    if var_environment is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "    if var_wifi_band is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "    if var_num_users is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "    return data_pd_y\n",
    "\n",
    "\n",
    "def load_data_x(var_path_data_x, var_label_list):\n",
    "    \"\"\"\n",
    "    Load CSI amplitude (*.npy) files based on a label list.\n",
    "\n",
    "    Modes:\n",
    "      - raw:            x_out = x_raw\n",
    "      - sparse:         x_out = S\n",
    "      - raw_plus_sparse:x_out = x_raw + alpha*S\n",
    "    \"\"\"\n",
    "    var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "    data_x = []\n",
    "    target_len = preset[\"data\"][\"length\"]\n",
    "\n",
    "    for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "        x_raw = np.load(var_path)\n",
    "        x_raw = _ensure_shape_4d_amp(x_raw).astype(np.float32, copy=False)\n",
    "\n",
    "        _debug_print_once(var_label, x_raw)\n",
    "\n",
    "        mode = CSI_INPUT_MODE\n",
    "\n",
    "        if mode == \"raw\":\n",
    "            x_out = x_raw\n",
    "\n",
    "        else:\n",
    "            if CACHE_ENABLED:\n",
    "                os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "                p = _cache_path(var_label, mode)\n",
    "                if os.path.exists(p):\n",
    "                    x_out = np.load(p).astype(np.float32, copy=False)\n",
    "                else:\n",
    "                    S = _make_sparse_component_pairwise(x_raw)\n",
    "                    if mode == \"sparse\":\n",
    "                        x_out = S\n",
    "                    elif mode == \"raw_plus_sparse\":\n",
    "                        x_out = x_raw + (SPARSE_ALPHA * S)\n",
    "                        if CLAMP_NONNEG:\n",
    "                            x_out = np.maximum(x_out, 0.0)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown CSI_INPUT_MODE={mode}\")\n",
    "\n",
    "                    np.save(p, x_out.astype(np.float32))\n",
    "            else:\n",
    "                S = _make_sparse_component_pairwise(x_raw)\n",
    "                if mode == \"sparse\":\n",
    "                    x_out = S\n",
    "                elif mode == \"raw_plus_sparse\":\n",
    "                    x_out = x_raw + (SPARSE_ALPHA * S)\n",
    "                    if CLAMP_NONNEG:\n",
    "                        x_out = np.maximum(x_out, 0.0)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown CSI_INPUT_MODE={mode}\")\n",
    "\n",
    "        # trim/pad to target_len (same behavior as your existing code expects)\n",
    "        if x_out.shape[0] > target_len:\n",
    "            x_out = x_out[-target_len:, :, :, :]\n",
    "\n",
    "        pad_len = target_len - x_out.shape[0]\n",
    "        if pad_len > 0:\n",
    "            x_out = np.pad(x_out, ((pad_len, 0), (0, 0), (0, 0), (0, 0)))\n",
    "\n",
    "        data_x.append(x_out)\n",
    "\n",
    "    return np.array(data_x)\n",
    "\n",
    "\n",
    "def encode_data_y(data_pd_y, var_task):\n",
    "    \"\"\"\n",
    "    Encode labels according to specific task.\n",
    "    \"\"\"\n",
    "    if var_task == \"identity\":\n",
    "        data_y = encode_identity(data_pd_y)\n",
    "    elif var_task == \"activity\":\n",
    "        data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "    elif var_task == \"location\":\n",
    "        data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "    elif var_task == \"count\":\n",
    "        data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "    return data_y\n",
    "\n",
    "\n",
    "def encode_identity(data_pd_y):\n",
    "    \"\"\"\n",
    "    Onehot encoding for identity labels.\n",
    "    \"\"\"\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "                                    \"user_3_location\", \"user_4_location\",\n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "    data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "    data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "    return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "def encode_activity(data_pd_y, var_encoding):\n",
    "    \"\"\"\n",
    "    Onehot encoding for activity labels.\n",
    "    \"\"\"\n",
    "    data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "                                    \"user_3_activity\", \"user_4_activity\",\n",
    "                                    \"user_5_activity\", \"user_6_activity\"]]\n",
    "    data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "    return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "def encode_location(data_pd_y, var_encoding):\n",
    "    \"\"\"\n",
    "    Onehot encoding for location labels.\n",
    "    \"\"\"\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "                                    \"user_3_location\", \"user_4_location\",\n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "    return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "def encode_count(data_pd_y, var_encoding):\n",
    "    \"\"\"\n",
    "    Onehot encoding for count labels.\n",
    "    \"\"\"\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "                                    \"user_3_location\", \"user_4_location\",\n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "    data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "    data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "    data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "    count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4131061",
   "metadata": {
    "papermill": {
     "duration": 0.00682,
     "end_time": "2025-12-29T07:33:55.073945",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.067125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "last stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16fe296e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T07:33:55.089184Z",
     "iopub.status.busy": "2025-12-29T07:33:55.088996Z",
     "iopub.status.idle": "2025-12-29T07:33:55.096945Z",
     "shell.execute_reply": "2025-12-29T07:33:55.096466Z"
    },
    "papermill": {
     "duration": 0.016822,
     "end_time": "2025-12-29T07:33:55.097878",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.081056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI (from .mat), and encode labels\n",
    "# \"\"\"\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import scipy.io as scio\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from numpy.fft import ifft\n",
    "\n",
    "# # =========================================================\n",
    "# #  Settings\n",
    "# # =========================================================\n",
    "# # \"raw\"     : abs(raw complex CSI)  -> float\n",
    "# # \"lowrank\" : abs(L) after RPCA     -> float\n",
    "# # \"sparse\"  : abs(S) after RPCA     -> float\n",
    "# CSI_INPUT_MODE = \"sparse\"   # raw / lowrank / sparse\n",
    "\n",
    "# # expected dimensions\n",
    "# TX = 3\n",
    "# RX = 3\n",
    "# SC = 30\n",
    "\n",
    "# # optional IFFT (frequency -> delay)\n",
    "# USE_IFFT = True\n",
    "\n",
    "# # RPCA iterations (برای شروع کم بگذار؛ اگر لازم شد بیشتر کن)\n",
    "# RPCA_MAX_ITER = 50\n",
    "\n",
    "# # lambda mode (اختیاری)\n",
    "# # \"classic\" : 1/sqrt(max(m,n))\n",
    "# # \"median\"  : 1/sqrt(max(m,n)) * median(abs(M))\n",
    "# # \"scaled\"  : 1.2/sqrt(max(m,n))\n",
    "# RPCA_LAMBDA_MODE = \"classic\"\n",
    "\n",
    "# # cache (very important for speed)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT = \"/kaggle/working/csi_cache_mat_pipeline\"\n",
    "\n",
    "# # print once to verify input is amplitude or complex/phase\n",
    "# _DEBUG_PRINT_ONCE = True\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# # =========================================================\n",
    "# # Minimal R_pca implementation (no external dependency)\n",
    "# # Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "# # =========================================================\n",
    "# class R_pca:\n",
    "#     def __init__(self, D):\n",
    "#         self.D = np.asarray(D, dtype=np.float64)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _shrink(M, tau):\n",
    "#         return np.sign(M) * np.maximum(np.abs(M) - tau, 0.0)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _svt(M, tau):\n",
    "#         U, s, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "#         s = np.maximum(s - tau, 0.0)\n",
    "#         if np.all(s == 0):\n",
    "#             return np.zeros_like(M)\n",
    "#         return (U * s) @ Vt\n",
    "\n",
    "#     def _lambda(self, D):\n",
    "#         m, n = D.shape\n",
    "#         base = 1.0 / np.sqrt(max(m, n))\n",
    "#         if RPCA_LAMBDA_MODE == \"classic\":\n",
    "#             return base\n",
    "#         if RPCA_LAMBDA_MODE == \"scaled\":\n",
    "#             return 1.2 * base\n",
    "#         if RPCA_LAMBDA_MODE == \"median\":\n",
    "#             med = np.median(np.abs(D))\n",
    "#             med = float(med) if med > 1e-12 else 1e-12\n",
    "#             return base * med\n",
    "#         raise ValueError(f\"Unknown RPCA_LAMBDA_MODE={RPCA_LAMBDA_MODE}\")\n",
    "\n",
    "#     def fit(self, max_iter=200, tol=1e-6, rho=1.5, mu=None):\n",
    "#         \"\"\"\n",
    "#         Returns L, S such that D ≈ L + S\n",
    "#         \"\"\"\n",
    "#         D = self.D\n",
    "#         m, n = D.shape\n",
    "#         lam = self._lambda(D)\n",
    "\n",
    "#         # auto mu\n",
    "#         if mu is None:\n",
    "#             s0 = np.linalg.svd(D, compute_uv=False, full_matrices=False)[0] if D.size else 1.0\n",
    "#             mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#         L = np.zeros_like(D)\n",
    "#         S = np.zeros_like(D)\n",
    "#         Y = np.zeros_like(D)\n",
    "\n",
    "#         normD = np.linalg.norm(D, ord=\"fro\") + 1e-12\n",
    "\n",
    "#         for _ in range(max_iter):\n",
    "#             L = self._svt(D - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#             S = self._shrink(D - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#             R = D - L - S\n",
    "#             Y = Y + mu * R\n",
    "\n",
    "#             if (np.linalg.norm(R, ord=\"fro\") / normD) < tol:\n",
    "#                 break\n",
    "\n",
    "#             mu *= rho\n",
    "\n",
    "#         return L.astype(np.float64), S.astype(np.float64)\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 1) Phase calibration (sanitize) - vectorized\n",
    "# # --------------------------------------------------\n",
    "# def phase_sanitize_matrix(X):\n",
    "#     \"\"\"\n",
    "#     X: complex matrix (N_subcarriers, T)\n",
    "#     \"\"\"\n",
    "#     phase = np.unwrap(np.angle(X), axis=0)  # (N,T)\n",
    "#     N, T = phase.shape\n",
    "#     k = np.arange(N, dtype=np.float64)[:, None]  # (N,1)\n",
    "\n",
    "#     A = np.concatenate([k, np.ones((N, 1), dtype=np.float64)], axis=1)  # (N,2)\n",
    "#     pinvA = np.linalg.pinv(A)  # (2,N)\n",
    "#     coeff = pinvA @ phase      # (2,T)\n",
    "#     a = coeff[0:1, :]\n",
    "#     b = coeff[1:2, :]\n",
    "\n",
    "#     phase_corr = phase - (k @ a + b)\n",
    "#     return np.abs(X) * np.exp(1j * phase_corr)\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 2) Preprocess CSI matrix\n",
    "# # --------------------------------------------------\n",
    "# def preprocess_csi(X):\n",
    "#     \"\"\"\n",
    "#     X : complex CSI matrix (N_subcarriers, T)\n",
    "#     \"\"\"\n",
    "#     X_corr = phase_sanitize_matrix(X).astype(np.complex128, copy=False)\n",
    "#     fro = np.linalg.norm(X_corr, \"fro\")\n",
    "#     if fro > 0:\n",
    "#         X_corr /= fro\n",
    "#     return X_corr\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 3) Optional IFFT\n",
    "# # --------------------------------------------------\n",
    "# def csi_to_cir(X):\n",
    "#     return ifft(X, axis=0)\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 4) RPCA on complex matrix (real & imag separately)\n",
    "# # --------------------------------------------------\n",
    "# def rpca_complex(X, max_iter=200):\n",
    "#     Xr = np.real(X)\n",
    "#     Xi = np.imag(X)\n",
    "\n",
    "#     rpca_r = R_pca(Xr)\n",
    "#     Lr, Sr = rpca_r.fit(max_iter=max_iter)\n",
    "\n",
    "#     rpca_i = R_pca(Xi)\n",
    "#     Li, Si = rpca_i.fit(max_iter=max_iter)\n",
    "\n",
    "#     L = Lr + 1j * Li\n",
    "#     S = Sr + 1j * Si\n",
    "#     return L, S\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 5) Full pipeline\n",
    "# # --------------------------------------------------\n",
    "# def csi_lowrank_sparse_pipeline(X, use_ifft=True, max_iter=200):\n",
    "#     Xp = preprocess_csi(X)\n",
    "#     if use_ifft:\n",
    "#         Xp = csi_to_cir(Xp)\n",
    "#     L, S = rpca_complex(Xp, max_iter=max_iter)\n",
    "#     return L, S\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # MAT loader -> (T,3,3,30) complex\n",
    "# # --------------------------------------------------\n",
    "# def load_csi_from_mat(mat_path):\n",
    "#     m = scio.loadmat(mat_path)\n",
    "#     if \"trace\" not in m:\n",
    "#         raise KeyError(f\"'trace' not found in {mat_path}\")\n",
    "\n",
    "#     trace = m[\"trace\"]  # (T,1) object array\n",
    "#     T = trace.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.complex128)\n",
    "\n",
    "#     for t in range(T):\n",
    "#         out[t] = trace[t, 0][\"csi\"][0, 0]  # (3,3,30) complex\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def _debug_print_once(label, csi_4d):\n",
    "#     global _DEBUG_PRINT_ONCE\n",
    "#     if not _DEBUG_PRINT_ONCE:\n",
    "#         return\n",
    "#     _DEBUG_PRINT_ONCE = False\n",
    "\n",
    "#     is_cplx = np.iscomplexobj(csi_4d)\n",
    "#     print(f\"\\n[DEBUG] First MAT sample: {label}\")\n",
    "#     print(f\"[DEBUG] shape={csi_4d.shape}, dtype={csi_4d.dtype}, complex={is_cplx}\")\n",
    "\n",
    "#     x0 = csi_4d[0, 0, 0, :]\n",
    "#     if is_cplx:\n",
    "#         ang = np.angle(x0)\n",
    "#         print(f\"[DEBUG] abs range:  min={np.min(np.abs(x0)):.6f}, max={np.max(np.abs(x0)):.6f}\")\n",
    "#         print(f\"[DEBUG] phase stats: mean={np.mean(ang):.6f}, std={np.std(ang):.6f}\")\n",
    "#         print(\"[DEBUG] ==> Input is COMPLEX (phase exists).\")\n",
    "#     else:\n",
    "#         print(f\"[DEBUG] value range: min={np.min(x0):.6f}, max={np.max(x0):.6f}\")\n",
    "#         print(\"[DEBUG] ==> Input is REAL (likely amplitude-only).\")\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _apply_pipeline_pairwise_9links(csi_4d_complex, label):\n",
    "#     \"\"\"\n",
    "#     csi_4d_complex: (T,3,3,30) complex\n",
    "#     Run pipeline on each link separately: (30,T) -> RPCA -> abs -> back to (T,3,3,30) float32\n",
    "#     \"\"\"\n",
    "#     _debug_print_once(label, csi_4d_complex)\n",
    "\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return np.abs(csi_4d_complex).astype(np.float32)\n",
    "\n",
    "#     mode = CSI_INPUT_MODE  # lowrank / sparse\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             return np.load(p).astype(np.float32, copy=False)\n",
    "\n",
    "#     T = csi_4d_complex.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "#     for tx in range(TX):\n",
    "#         for rx in range(RX):\n",
    "#             X = csi_4d_complex[:, tx, rx, :].T  # (30,T) complex\n",
    "#             L, S = csi_lowrank_sparse_pipeline(X, use_ifft=USE_IFFT, max_iter=RPCA_MAX_ITER)\n",
    "#             Y = L if mode == \"lowrank\" else S\n",
    "#             out[:, tx, rx, :] = np.abs(Y.T).astype(np.float32)\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out)\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # Existing label loading/encoding\n",
    "# # --------------------------------------------------\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None,\n",
    "#                 var_wifi_band=None,\n",
    "#                 var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     var_path_data_x should point to MAT directory, e.g. /kaggle/input/wimans/wifi_csi/mat\n",
    "#     Each label corresponds to <label>.mat\n",
    "#     \"\"\"\n",
    "#     data_x = []\n",
    "#     target_len = preset[\"data\"][\"length\"]\n",
    "\n",
    "#     for label in var_label_list:\n",
    "#         mat_path = os.path.join(var_path_data_x, label + \".mat\")\n",
    "#         if not os.path.exists(mat_path):\n",
    "#             raise FileNotFoundError(f\"MAT file not found: {mat_path}\")\n",
    "\n",
    "#         csi_complex = load_csi_from_mat(mat_path)  # (T,3,3,30) complex\n",
    "\n",
    "#         # if longer than target_len, keep last target_len frames\n",
    "#         if csi_complex.shape[0] > target_len:\n",
    "#             csi_complex = csi_complex[-target_len:, :, :, :]\n",
    "\n",
    "#         csi_feat = _apply_pipeline_pairwise_9links(csi_complex, label)\n",
    "\n",
    "#         # pad if shorter\n",
    "#         pad_len = target_len - csi_feat.shape[0]\n",
    "#         if pad_len > 0:\n",
    "#             csi_feat = np.pad(csi_feat, ((pad_len, 0), (0, 0), (0, 0), (0, 0)))\n",
    "\n",
    "#         data_x.append(csi_feat)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y)\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "#                                     \"user_3_activity\", \"user_4_activity\",\n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e83174",
   "metadata": {
    "papermill": {
     "duration": 0.006589,
     "end_time": "2025-12-29T07:33:55.111609",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.105020",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "rpca 30 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e81fc020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T07:33:55.126899Z",
     "iopub.status.busy": "2025-12-29T07:33:55.126685Z",
     "iopub.status.idle": "2025-12-29T07:33:55.133164Z",
     "shell.execute_reply": "2025-12-29T07:33:55.132517Z"
    },
    "papermill": {
     "duration": 0.015563,
     "end_time": "2025-12-29T07:33:55.134243",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.118680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # =========================================================\n",
    "# # انتخاب نوع دیتای ورودی مدل:\n",
    "# # \"raw\"     : amp خام\n",
    "# # \"lowrank\" : خروجی Low-rank (L) از RPCA\n",
    "# # \"sparse\"  : خروجی Sparse (S) از RPCA\n",
    "# CSI_INPUT_MODE = \"lowrank\"   # <-- raw / lowrank / sparse\n",
    "\n",
    "# # ابعاد مورد انتظار CSI\n",
    "# TX = 3\n",
    "# RX = 3\n",
    "# SC = 30\n",
    "\n",
    "# # RPCA (IALM) تنظیمات سریع‌تر\n",
    "# RPCA_MAX_ITER = 60\n",
    "# RPCA_TOL      = 1e-5\n",
    "# RPCA_RHO      = 1.5\n",
    "# RPCA_MU_INIT  = None\n",
    "\n",
    "# # ✅ انتخاب لامبدا:\n",
    "# # \"median\" : lam = 1/sqrt(max(m,n)) * median(abs(M))\n",
    "# # \"scaled\" : lam = 1.2/sqrt(max(m,n))\n",
    "# RPCA_LAMBDA_MODE = \"median\"  # <-- \"median\" یا \"scaled\"\n",
    "\n",
    "# # Cache (خیلی مهم برای سرعت)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT    = \"/kaggle/working/csi_cache_pairwise\"\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _soft_threshold(X, tau):\n",
    "#     return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "# def _svt(X, tau):\n",
    "#     # Singular Value Thresholding\n",
    "#     U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "#     s = np.maximum(s - tau, 0.0)\n",
    "#     if np.all(s == 0):\n",
    "#         return np.zeros_like(X)\n",
    "#     return (U * s) @ Vt\n",
    "\n",
    "# def _compute_lambda(M, mode):\n",
    "#     \"\"\"\n",
    "#     mode:\n",
    "#       - 'median': 1/sqrt(max(m,n)) * median(abs(M))\n",
    "#       - 'scaled': 1.2/sqrt(max(m,n))\n",
    "#     \"\"\"\n",
    "#     m, n = M.shape\n",
    "#     base = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "#     if mode == \"median\":\n",
    "#         med = np.median(np.abs(M))\n",
    "#         # اگر med خیلی کوچک بود برای پایداری:\n",
    "#         med = float(med) if med > 1e-12 else 1e-12\n",
    "#         return base * med\n",
    "\n",
    "#     if mode == \"scaled\":\n",
    "#         return 1.2 * base\n",
    "\n",
    "#     raise ValueError(f\"Unknown RPCA_LAMBDA_MODE: {mode}. Use 'median' or 'scaled'.\")\n",
    "\n",
    "\n",
    "# def _rpca_ialm(M, mu=None, rho=1.5, max_iter=60, tol=1e-5):\n",
    "#     \"\"\"\n",
    "#     Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "#     Decompose: M = L + S\n",
    "#     M shape: (T, SC) = (3000, 30)\n",
    "#     \"\"\"\n",
    "#     M = M.astype(np.float64, copy=False)\n",
    "#     m, n = M.shape\n",
    "\n",
    "#     # ✅ lambda طبق انتخاب کاربر\n",
    "#     lam = _compute_lambda(M, RPCA_LAMBDA_MODE)\n",
    "\n",
    "#     # mu خودکار یا دستی\n",
    "#     if mu is None:\n",
    "#         s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "#         mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     Y = np.zeros_like(M)\n",
    "\n",
    "#     normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#         S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#         R = M - L - S\n",
    "#         Y = Y + mu * R\n",
    "\n",
    "#         if (np.linalg.norm(R, ord=\"fro\") / normM) < tol:\n",
    "#             break\n",
    "\n",
    "#         mu *= rho\n",
    "\n",
    "#     return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# def _ensure_shape_4d(data_csi):\n",
    "#     \"\"\"\n",
    "#     Ensure CSI shape is (T, TX, RX, SC).\n",
    "#     If input is (T, 270) we reshape to (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     if data_csi.ndim == 4:\n",
    "#         return data_csi\n",
    "\n",
    "#     if data_csi.ndim == 2 and data_csi.shape[1] == TX * RX * SC:\n",
    "#         T = data_csi.shape[0]\n",
    "#         return data_csi.reshape(T, TX, RX, SC)\n",
    "\n",
    "#     raise ValueError(\n",
    "#         f\"Unexpected CSI shape {data_csi.shape}. Expected (T,{TX},{RX},{SC}) or (T,{TX*RX*SC}).\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _rpca_pairwise_9links(data_csi_4d):\n",
    "#     \"\"\"\n",
    "#     data_csi_4d: (T, TX, RX, SC)\n",
    "#     Run RPCA on each (tx,rx) separately on matrix (T, SC) and reassemble.\n",
    "#     Output shape stays (T, TX, RX, SC).\n",
    "#     \"\"\"\n",
    "#     T = data_csi_4d.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "#     for tx in range(TX):\n",
    "#         for rx in range(RX):\n",
    "#             M = data_csi_4d[:, tx, rx, :]  # (T,30)\n",
    "#             L, S = _rpca_ialm(\n",
    "#                 M,\n",
    "#                 mu=RPCA_MU_INIT,\n",
    "#                 rho=RPCA_RHO,\n",
    "#                 max_iter=RPCA_MAX_ITER,\n",
    "#                 tol=RPCA_TOL\n",
    "#             )\n",
    "#             out[:, tx, rx, :] = L if CSI_INPUT_MODE == \"lowrank\" else S\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def _apply_mode_with_cache(data_csi, label):\n",
    "#     \"\"\"\n",
    "#     Apply raw/lowrank/sparse. For lowrank/sparse use pairwise RPCA (9 links of 3000x30).\n",
    "#     Keeps the original 4D shape (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     data_csi = _ensure_shape_4d(np.asarray(data_csi, dtype=np.float32))\n",
    "\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     mode = CSI_INPUT_MODE\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             cached = np.load(p)\n",
    "#             return _ensure_shape_4d(cached).astype(np.float32, copy=False)\n",
    "\n",
    "#     out = _rpca_pairwise_9links(data_csi)\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out.astype(np.float32))\n",
    "\n",
    "#     return out.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None,\n",
    "#                 var_wifi_band=None,\n",
    "#                 var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ RPCA روی 9 لینک جدا (3000x30) و بعد اتصال\n",
    "#         data_csi = _apply_mode_with_cache(data_csi, var_label)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "#                                     \"user_3_activity\", \"user_4_activity\",\n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec11dba5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T07:33:55.149298Z",
     "iopub.status.busy": "2025-12-29T07:33:55.149070Z",
     "iopub.status.idle": "2025-12-29T07:33:55.155796Z",
     "shell.execute_reply": "2025-12-29T07:33:55.155140Z"
    },
    "papermill": {
     "duration": 0.01558,
     "end_time": "2025-12-29T07:33:55.156854",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.141274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # =========================================================\n",
    "# # انتخاب نوع دیتای ورودی مدل:\n",
    "# # \"raw\"     : amp خام\n",
    "# # \"lowrank\" : خروجی Low-rank (L) از RPCA\n",
    "# # \"sparse\"  : خروجی Sparse (S) از RPCA\n",
    "# CSI_INPUT_MODE = \"sparse\"   # <-- raw / lowrank / sparse\n",
    "\n",
    "# # ابعاد مورد انتظار CSI\n",
    "# TX = 3\n",
    "# RX = 3\n",
    "# SC = 30\n",
    "\n",
    "# # RPCA (IALM) تنظیمات سریع‌تر\n",
    "# RPCA_MAX_ITER = 60\n",
    "# RPCA_TOL      = 1e-5\n",
    "# RPCA_RHO      = 1.5\n",
    "# RPCA_MU_INIT  = None\n",
    "# RPCA_LAMBDA   = None     # None -> 1/sqrt(max(m,n))\n",
    "\n",
    "# # Cache (خیلی مهم برای سرعت)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT    = \"/kaggle/working/csi_cache_pairwise\"  # خروجی‌ها اینجا ذخیره میشن\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _soft_threshold(X, tau):\n",
    "#     return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "# def _svt(X, tau):\n",
    "#     # Singular Value Thresholding\n",
    "#     U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "#     s = np.maximum(s - tau, 0.0)\n",
    "#     if np.all(s == 0):\n",
    "#         return np.zeros_like(X)\n",
    "#     return (U * s) @ Vt\n",
    "\n",
    "# def _rpca_ialm(M, lam=None, mu=None, rho=1.5, max_iter=60, tol=1e-5):\n",
    "#     \"\"\"\n",
    "#     Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "#     Decompose: M = L + S\n",
    "#     M shape: (T, SC) = (3000, 30)\n",
    "#     \"\"\"\n",
    "#     M = M.astype(np.float64, copy=False)\n",
    "#     m, n = M.shape\n",
    "\n",
    "#     if lam is None:\n",
    "#         lam = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "#     if mu is None:\n",
    "#         # top singular value as spectral norm approximation\n",
    "#         s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "#         mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     Y = np.zeros_like(M)\n",
    "\n",
    "#     normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#         S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#         R = M - L - S\n",
    "#         Y = Y + mu * R\n",
    "\n",
    "#         if (np.linalg.norm(R, ord=\"fro\") / normM) < tol:\n",
    "#             break\n",
    "\n",
    "#         mu *= rho\n",
    "\n",
    "#     return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# def _ensure_shape_4d(data_csi):\n",
    "#     \"\"\"\n",
    "#     Ensure CSI shape is (T, TX, RX, SC).\n",
    "#     If input is (T, 270) we reshape to (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     if data_csi.ndim == 4:\n",
    "#         return data_csi\n",
    "\n",
    "#     if data_csi.ndim == 2 and data_csi.shape[1] == TX * RX * SC:\n",
    "#         T = data_csi.shape[0]\n",
    "#         return data_csi.reshape(T, TX, RX, SC)\n",
    "\n",
    "#     raise ValueError(\n",
    "#         f\"Unexpected CSI shape {data_csi.shape}. Expected (T,{TX},{RX},{SC}) or (T,{TX*RX*SC}).\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     # mode: \"lowrank\" or \"sparse\"\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _rpca_pairwise_9links(data_csi_4d):\n",
    "#     \"\"\"\n",
    "#     data_csi_4d: (T, TX, RX, SC)\n",
    "#     Run RPCA on each (tx,rx) separately on matrix (T, SC) and reassemble.\n",
    "#     Output shape stays (T, TX, RX, SC).\n",
    "#     \"\"\"\n",
    "#     T = data_csi_4d.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "#     # 9 times RPCA: for each tx-rx link\n",
    "#     for tx in range(TX):\n",
    "#         for rx in range(RX):\n",
    "#             M = data_csi_4d[:, tx, rx, :]  # (T, SC) => (3000,30)\n",
    "#             L, S = _rpca_ialm(\n",
    "#                 M,\n",
    "#                 lam=RPCA_LAMBDA,\n",
    "#                 mu=RPCA_MU_INIT,\n",
    "#                 rho=RPCA_RHO,\n",
    "#                 max_iter=RPCA_MAX_ITER,\n",
    "#                 tol=RPCA_TOL\n",
    "#             )\n",
    "#             if CSI_INPUT_MODE == \"lowrank\":\n",
    "#                 out[:, tx, rx, :] = L\n",
    "#             else:  # \"sparse\"\n",
    "#                 out[:, tx, rx, :] = S\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def _apply_mode_with_cache(data_csi, label):\n",
    "#     \"\"\"\n",
    "#     Apply raw/lowrank/sparse. For lowrank/sparse use pairwise RPCA (9 links of 3000x30).\n",
    "#     Keeps the original 4D shape (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     data_csi = _ensure_shape_4d(np.asarray(data_csi, dtype=np.float32))\n",
    "\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     mode = CSI_INPUT_MODE\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             cached = np.load(p)\n",
    "#             return _ensure_shape_4d(cached).astype(np.float32, copy=False)\n",
    "\n",
    "#     out = _rpca_pairwise_9links(data_csi)\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out.astype(np.float32))\n",
    "\n",
    "#     return out.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     \"\"\"\n",
    "#     Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "#     \"\"\"\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     Load CSI amplitude (*.npy) files based on a label list.\n",
    "#     \"\"\"\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ NEW: RPCA روی 9 لینک جداگانه (3000x30) و بعد چسباندن\n",
    "#         data_csi = _apply_mode_with_cache(data_csi, var_label)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     \"\"\"\n",
    "#     Encode labels according to specific task.\n",
    "#     \"\"\"\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     return data_identity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for activity labels.\n",
    "#     \"\"\"\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "#                                     \"user_3_activity\", \"user_4_activity\", \n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "#     return data_activity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for location labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "#     return data_location_onehot_y\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1)\n",
    "#     count_data = count_data.reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     count_data_onehot = encoder.fit_transform(count_data).astype(\"int8\")\n",
    "#     return count_data_onehot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a8ceb9",
   "metadata": {
    "papermill": {
     "duration": 0.008106,
     "end_time": "2025-12-29T07:33:55.172103",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.163997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "rpca\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0443b3fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T07:33:55.187061Z",
     "iopub.status.busy": "2025-12-29T07:33:55.186858Z",
     "iopub.status.idle": "2025-12-29T07:33:55.192236Z",
     "shell.execute_reply": "2025-12-29T07:33:55.191754Z"
    },
    "papermill": {
     "duration": 0.013829,
     "end_time": "2025-12-29T07:33:55.193233",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.179404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # =========================================================\n",
    "# # انتخاب ورودی مدل:\n",
    "# # \"raw\"     : همون amp خام\n",
    "# # \"lowrank\" : مؤلفه Low-rank از RPCA (L)\n",
    "# # \"sparse\"  : مؤلفه Sparse از RPCA (S)\n",
    "# CSI_INPUT_MODE = \"lowrank\"   # <-- raw / lowrank / sparse\n",
    "\n",
    "# # RPCA (IALM) تنظیمات سریع‌تر\n",
    "# RPCA_MAX_ITER = 80\n",
    "# RPCA_TOL      = 1e-5\n",
    "# RPCA_RHO      = 1.5\n",
    "# RPCA_MU_INIT  = None\n",
    "# RPCA_LAMBDA   = None     # None -> 1/sqrt(max(m,n))\n",
    "\n",
    "# # Cache (خیلی مهم برای سرعت)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT    = \"/kaggle/working/csi_cache\"  # خروجی‌ها اینجا ذخیره میشن\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _soft_threshold(X, tau):\n",
    "#     return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "# def _svt(X, tau):\n",
    "#     U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "#     s = np.maximum(s - tau, 0.0)\n",
    "#     if np.all(s == 0):\n",
    "#         return np.zeros_like(X)\n",
    "#     return (U * s) @ Vt\n",
    "\n",
    "# def _rpca_ialm(M, lam=None, mu=None, rho=1.5, max_iter=80, tol=1e-5):\n",
    "#     M = M.astype(np.float64, copy=False)\n",
    "#     m, n = M.shape\n",
    "\n",
    "#     if lam is None:\n",
    "#         lam = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "#     if mu is None:\n",
    "#         s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "#         mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     Y = np.zeros_like(M)\n",
    "\n",
    "#     normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#         S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#         R = M - L - S\n",
    "#         Y = Y + mu * R\n",
    "\n",
    "#         if (np.linalg.norm(R, ord=\"fro\") / normM) < tol:\n",
    "#             break\n",
    "#         mu *= rho\n",
    "\n",
    "#     return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# def _rpca_keep_shape(X):\n",
    "#     \"\"\"RPCA روی (T,F) و بازگرداندن دقیقاً به shape اولیه\"\"\"\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "#     if X.ndim == 1:\n",
    "#         M = X[:, None]\n",
    "#         L, S = _rpca_ialm(M, RPCA_LAMBDA, RPCA_MU_INIT, RPCA_RHO, RPCA_MAX_ITER, RPCA_TOL)\n",
    "#         return L[:, 0], S[:, 0]\n",
    "\n",
    "#     T = X.shape[0]\n",
    "#     F = int(np.prod(X.shape[1:]))\n",
    "#     M = X.reshape(T, F)\n",
    "\n",
    "#     L, S = _rpca_ialm(M, RPCA_LAMBDA, RPCA_MU_INIT, RPCA_RHO, RPCA_MAX_ITER, RPCA_TOL)\n",
    "#     return L.reshape(X.shape), S.reshape(X.shape)\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     # mode: \"lowrank\" or \"sparse\"\n",
    "#     # فایل خروجی: /kaggle/working/csi_cache/lowrank/<label>.npy\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _apply_mode_with_cache(data_csi, label):\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     # lowrank یا sparse\n",
    "#     mode = CSI_INPUT_MODE\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             return np.load(p).astype(np.float32, copy=False)\n",
    "\n",
    "#     L, S = _rpca_keep_shape(data_csi)\n",
    "#     out = L if mode == \"lowrank\" else S\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out.astype(np.float32))\n",
    "\n",
    "#     return out.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ RPCA lowrank/sparse بدون تغییر shape + با cache\n",
    "#         data_csi = _apply_mode_with_cache(data_csi, var_label)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "#                                     \"user_3_activity\", \"user_4_activity\",\n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d69001",
   "metadata": {
    "papermill": {
     "duration": 0.007261,
     "end_time": "2025-12-29T07:33:55.207539",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.200278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "svd\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3e642c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T07:33:55.222706Z",
     "iopub.status.busy": "2025-12-29T07:33:55.222479Z",
     "iopub.status.idle": "2025-12-29T07:33:55.229499Z",
     "shell.execute_reply": "2025-12-29T07:33:55.229016Z"
    },
    "papermill": {
     "duration": 0.016149,
     "end_time": "2025-12-29T07:33:55.230554",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.214405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # Note: All necessary libraries (os, numpy, pandas, etc.) are imported in Cell 1.\n",
    "# # from preset import preset   --> preset is already defined in Cell 2.\n",
    "\n",
    "# # =========================================================\n",
    "# # 🔧 NEW: Choose CSI representation mode here (ONLY EDIT THIS)\n",
    "# # ---------------------------------------------------------\n",
    "# # \"raw\"     : use original CSI amplitude as-is\n",
    "# # \"lowrank\" : use low-rank approximation (SVD)\n",
    "# # \"sparse\"  : keep only large-magnitude entries (dense array with many zeros)\n",
    "# CSI_INPUT_MODE = \"sparse\"     # <-- set to: \"raw\" / \"lowrank\" / \"sparse\"\n",
    "\n",
    "# # Low-rank settings\n",
    "# LOW_RANK_ENERGY = 0.95     # keep enough singular values to preserve this energy\n",
    "# LOW_RANK_RANK   = None     # if set to an int (e.g., 10), it overrides ENERGY\n",
    "\n",
    "# # Sparse settings\n",
    "# SPARSE_KEEP_RATIO = 0.10   # keep top 10% magnitudes (globally per sample)\n",
    "# SPARSE_MIN_ABS    = None   # if set (e.g., 0.5), keeps |x|>=threshold instead of keep_ratio\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _low_rank_approx_keep_shape(X, rank=None, energy=0.95):\n",
    "#     \"\"\"\n",
    "#     Low-rank approximation using SVD while preserving the original shape.\n",
    "#     Works for 1D/2D/ND by flattening all non-time dims into features.\n",
    "#     Assumes first axis is time.\n",
    "#     \"\"\"\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "\n",
    "#     if X.ndim == 1:\n",
    "#         M = X[:, None]  # (T,1)\n",
    "#         U, S, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "#         if rank is None:\n",
    "#             s2 = S**2\n",
    "#             cum = np.cumsum(s2) / (np.sum(s2) + 1e-12)\n",
    "#             rank = int(np.searchsorted(cum, energy) + 1)\n",
    "#         rank = max(1, min(rank, S.shape[0]))\n",
    "#         M_lr = (U[:, :rank] * S[:rank]) @ Vt[:rank, :]\n",
    "#         return M_lr[:, 0].astype(np.float32)\n",
    "\n",
    "#     # ND: reshape to (T, F)\n",
    "#     T = X.shape[0]\n",
    "#     F = int(np.prod(X.shape[1:]))\n",
    "#     M = X.reshape(T, F)\n",
    "\n",
    "#     U, S, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "\n",
    "#     if rank is None:\n",
    "#         s2 = S**2\n",
    "#         cum = np.cumsum(s2) / (np.sum(s2) + 1e-12)\n",
    "#         rank = int(np.searchsorted(cum, energy) + 1)\n",
    "\n",
    "#     rank = max(1, min(rank, S.shape[0]))\n",
    "#     M_lr = (U[:, :rank] * S[:rank]) @ Vt[:rank, :]\n",
    "\n",
    "#     return M_lr.reshape(X.shape).astype(np.float32)\n",
    "\n",
    "\n",
    "# def _to_sparse_dense_keep_shape(X, keep_ratio=0.10, min_abs=None):\n",
    "#     \"\"\"\n",
    "#     Makes X sparse-in-content (many zeros) but keeps it as a dense numpy array\n",
    "#     so the rest of the pipeline (np.save/np.load/pad/model) doesn't change.\n",
    "#     Keeps the same shape.\n",
    "#     \"\"\"\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "#     flat = X.ravel()\n",
    "#     if flat.size == 0:\n",
    "#         return X.astype(np.float32)\n",
    "\n",
    "#     absflat = np.abs(flat)\n",
    "\n",
    "#     if min_abs is not None:\n",
    "#         thr = float(min_abs)\n",
    "#         mask = absflat >= thr\n",
    "#     else:\n",
    "#         k = int(np.ceil(keep_ratio * flat.size))\n",
    "#         k = max(1, min(k, flat.size))\n",
    "#         if k == flat.size:\n",
    "#             mask = np.ones_like(absflat, dtype=bool)\n",
    "#         else:\n",
    "#             thr = np.partition(absflat, -k)[-k]\n",
    "#             mask = absflat >= thr\n",
    "\n",
    "#     out = np.zeros_like(flat, dtype=np.float32)\n",
    "#     out[mask] = flat[mask]\n",
    "#     return out.reshape(X.shape).astype(np.float32)\n",
    "\n",
    "\n",
    "# def _apply_csi_mode(data_csi):\n",
    "#     \"\"\"\n",
    "#     Apply selected CSI_INPUT_MODE to a single sample array.\n",
    "#     \"\"\"\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     elif CSI_INPUT_MODE == \"lowrank\":\n",
    "#         return _low_rank_approx_keep_shape(\n",
    "#             data_csi,\n",
    "#             rank=LOW_RANK_RANK,\n",
    "#             energy=LOW_RANK_ENERGY\n",
    "#         )\n",
    "\n",
    "#     elif CSI_INPUT_MODE == \"sparse\":\n",
    "#         return _to_sparse_dense_keep_shape(\n",
    "#             data_csi,\n",
    "#             keep_ratio=SPARSE_KEEP_RATIO,\n",
    "#             min_abs=SPARSE_MIN_ABS\n",
    "#         )\n",
    "\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown CSI_INPUT_MODE: {CSI_INPUT_MODE}. Use 'raw', 'lowrank', or 'sparse'.\")\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     \"\"\"\n",
    "#     Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "#     \"\"\"\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     Load CSI amplitude (*.npy) files based on a label list.\n",
    "#     \"\"\"\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_path in var_path_list:\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ NEW: convert input CSI according to selected mode (raw/lowrank/sparse)\n",
    "#         data_csi = _apply_csi_mode(data_csi)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "#     data_x = np.array(data_x)\n",
    "#     return data_x\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     \"\"\"\n",
    "#     Encode labels according to specific task.\n",
    "#     \"\"\"\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     return data_identity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for activity labels.\n",
    "#     \"\"\"\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "#                                     \"user_3_activity\", \"user_4_activity\", \n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "#     return data_activity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for location labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "#     return data_location_onehot_y\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     print(\"data_identity_onehot_y\",data_identity_onehot_y.shape)\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1)\n",
    "#     print(\"count_data\",count_data.shape)\n",
    "#     count_data = count_data.reshape(-1, 1)  # shape = (11286, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)  \n",
    "#     count_data_onehot = encoder.fit_transform(count_data)\n",
    "#     print(count_data_onehot.shape)  \n",
    "#     count_data_onehot = count_data_onehot.astype(\"int8\")\n",
    "\n",
    "#     return count_data_onehot\n",
    "\n",
    "\n",
    "# # Test functions (optional)\n",
    "# def test_load_data_y():\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"classroom\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=[\"1\", \"2\", \"3\"]).describe())\n",
    "\n",
    "# def test_load_data_x():\n",
    "#     data_pd_y = load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=None)\n",
    "#     var_label_list = data_pd_y[\"label\"].to_list()\n",
    "#     data_x = load_data_x(preset[\"path\"][\"data_x\"], var_label_list)\n",
    "#     print(data_x.shape)\n",
    "\n",
    "# def test_encode_identity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_identity_onehot_y = encode_identity(data_pd_y)\n",
    "#     print(data_identity_onehot_y.shape)\n",
    "#     print(data_identity_onehot_y[2000])\n",
    "\n",
    "# def test_encode_activity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_activity_onehot_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     print(data_activity_onehot_y.shape)\n",
    "#     print(data_activity_onehot_y[1560])\n",
    "\n",
    "# def test_encode_location():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_location_onehot_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     print(data_location_onehot_y.shape)\n",
    "#     print(data_location_onehot_y[1560])\n",
    "\n",
    "# def test_encode_count():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_count_onehot_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     print(data_count_onehot_y.shape)\n",
    "#     print(data_count_onehot_y[20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeaf3ed6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T07:33:55.245951Z",
     "iopub.status.busy": "2025-12-29T07:33:55.245735Z",
     "iopub.status.idle": "2025-12-29T07:33:55.250911Z",
     "shell.execute_reply": "2025-12-29T07:33:55.250443Z"
    },
    "papermill": {
     "duration": 0.014367,
     "end_time": "2025-12-29T07:33:55.252040",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.237673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # Note: All necessary libraries (os, numpy, pandas, etc.) are imported in Cell 1.\n",
    "# # from preset import preset   --> preset is already defined in Cell 2.\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     \"\"\"\n",
    "#     Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "#     \"\"\"\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     Load CSI amplitude (*.npy) files based on a label list.\n",
    "#     \"\"\"\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_path in var_path_list:\n",
    "#         data_csi = np.load(var_path)\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "#     data_x = np.array(data_x)\n",
    "#     return data_x\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     \"\"\"\n",
    "#     Encode labels according to specific task.\n",
    "#     \"\"\"\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     return data_identity_onehot_y\n",
    "\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for activity labels.\n",
    "#     \"\"\"\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "#                                     \"user_3_activity\", \"user_4_activity\", \n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "#     return data_activity_onehot_y\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for location labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "#     return data_location_onehot_y\n",
    "\n",
    "# # Test functions (optional)\n",
    "# def test_load_data_y():\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"classroom\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=[\"1\", \"2\", \"3\"]).describe())\n",
    "\n",
    "# def test_load_data_x():\n",
    "#     data_pd_y = load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=None)\n",
    "#     var_label_list = data_pd_y[\"label\"].to_list()\n",
    "#     data_x = load_data_x(preset[\"path\"][\"data_x\"], var_label_list)\n",
    "#     print(data_x.shape)\n",
    "\n",
    "# def test_encode_identity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_identity_onehot_y = encode_identity(data_pd_y)\n",
    "#     print(data_identity_onehot_y.shape)\n",
    "#     print(data_identity_onehot_y[2000])\n",
    "\n",
    "# def test_encode_activity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_activity_onehot_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     print(data_activity_onehot_y.shape)\n",
    "#     print(data_activity_onehot_y[1560])\n",
    "\n",
    "# def test_encode_location():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_location_onehot_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     print(data_location_onehot_y.shape)\n",
    "#     print(data_location_onehot_y[1560])\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     print(\"data_identity_onehot_y\",data_identity_onehot_y.shape)\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1)\n",
    "#     print(\"count_data\",count_data.shape)\n",
    "#     count_data = count_data.reshape(-1, 1)  # shape = (11286, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)  \n",
    "#     count_data_onehot = encoder.fit_transform(count_data)\n",
    "#     print(count_data_onehot.shape)  \n",
    "#     count_data_onehot = count_data_onehot.astype(\"int8\")\n",
    "\n",
    "#     return count_data_onehot\n",
    "\n",
    "\n",
    "# def test_encode_count():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_count_onehot_y = encode_count(data_pd_y)\n",
    "#     print(data_count_onehot_y.shape)\n",
    "#     print(data_count_onehot_y[20])\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     test_encode_count()\n",
    "# #     test_load_data_y()\n",
    "# #     test_load_data_x()\n",
    "# #     test_encode_identity()\n",
    "# #     test_encode_activity()\n",
    "# #     test_encode_location()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ef499",
   "metadata": {
    "papermill": {
     "duration": 0.006617,
     "end_time": "2025-12-29T07:33:55.265961",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.259344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 4: preprocess.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69c553e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T07:33:55.280685Z",
     "iopub.status.busy": "2025-12-29T07:33:55.280484Z",
     "iopub.status.idle": "2025-12-29T07:33:55.294159Z",
     "shell.execute_reply": "2025-12-29T07:33:55.293706Z"
    },
    "papermill": {
     "duration": 0.022359,
     "end_time": "2025-12-29T07:33:55.295124",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.272765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          preprocess.py\n",
    "[description]   preprocess WiFi CSI data\n",
    "\"\"\"\n",
    "\n",
    "# All necessary libraries are already imported in Cell 1.\n",
    "\n",
    "# def mat_to_amp(data_mat):\n",
    "#     \"\"\"\n",
    "#     Calculate amplitude of raw WiFi CSI data.\n",
    "#     \"\"\"\n",
    "#     var_length = data_mat[\"trace\"].shape[0]\n",
    "#     data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "#     data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "#     return data_csi_amp\n",
    "\n",
    "def extract_csi_amp(var_dir_mat, var_dir_amp):\n",
    "    \"\"\"\n",
    "    Read raw WiFi CSI (*.mat) files, calculate CSI amplitude, and save as (*.npy).\n",
    "    \"\"\"\n",
    "    var_path_mat = os.listdir(var_dir_mat)\n",
    "    for var_c, var_path in enumerate(var_path_mat):\n",
    "        data_mat = scio.loadmat(os.path.join(var_dir_mat, var_path))\n",
    "        data_csi_amp = mat_to_amp(data_mat)\n",
    "        # print(var_c, data_csi_amp.shape)\n",
    "        var_path_save = os.path.join(var_dir_amp, var_path.replace(\".mat\", \".npy\"))\n",
    "        with open(var_path_save, \"wb\") as var_file:\n",
    "            np.save(var_file, data_csi_amp)\n",
    "\n",
    "\n",
    "\n",
    "# # تنظیمات low-rank (بدون تغییر ورودی mat_to_amp)\n",
    "# LOW_RANK_ENERGY = 0.95   # مثلاً 95% انرژی\n",
    "# LOW_RANK_RANK = None     # اگر عدد بذاری (مثلاً 5)، به جای ENERGY از rank ثابت استفاده میشه\n",
    "\n",
    "# def _low_rank_approx(X, rank=None, energy=0.95):\n",
    "#     X = np.asarray(X)\n",
    "\n",
    "#     was_1d = (X.ndim == 1)\n",
    "#     if was_1d:\n",
    "#         X = X[:, None]\n",
    "\n",
    "#     U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "#     if rank is None:\n",
    "#         s2 = S**2\n",
    "#         cum = np.cumsum(s2) / (np.sum(s2) + 1e-12)\n",
    "#         rank = int(np.searchsorted(cum, energy) + 1)\n",
    "\n",
    "#     rank = max(1, min(rank, S.shape[0]))\n",
    "#     X_lr = (U[:, :rank] * S[:rank]) @ Vt[:rank, :]\n",
    "\n",
    "#     if was_1d:\n",
    "#         X_lr = X_lr[:, 0]\n",
    "\n",
    "#     return X_lr.astype(np.float32)\n",
    "\n",
    "# def mat_to_amp(data_mat):\n",
    "#     \"\"\"\n",
    "#     Calculate amplitude of raw WiFi CSI data, then return its low-rank approximation.\n",
    "#     (ورودی تابع تغییر نکرده)\n",
    "#     \"\"\"\n",
    "#     var_length = data_mat[\"trace\"].shape[0]\n",
    "#     data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "#     data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "#     # خروجی low-rank با همان ابعاد\n",
    "#     data_csi_amp_lr = _low_rank_approx(\n",
    "#         data_csi_amp,\n",
    "#         rank=LOW_RANK_RANK,\n",
    "#         energy=LOW_RANK_ENERGY\n",
    "#     )\n",
    "#     return data_csi_amp_lr\n",
    "\n",
    "\n",
    "\n",
    "# # تنظیمات sparsity (بدون تغییر ورودی mat_to_amp)\n",
    "# SPARSE_KEEP_RATIO = 0.10   # مثلا فقط 10% بزرگترین مقادیر نگه داشته بشن\n",
    "# SPARSE_MIN_ABS = None      # اگر عدد بذاری (مثلا 0.5)، به جای keep_ratio آستانه ثابت میشه\n",
    "\n",
    "# def _to_sparse(X, keep_ratio=0.10, min_abs=None):\n",
    "#     \"\"\"\n",
    "#     Convert X to a sparse representation by keeping only large-magnitude entries.\n",
    "#     Returns:\n",
    "#       - scipy.sparse.csr_matrix if SciPy is available\n",
    "#       - otherwise returns a dense array with many zeros (still \"sparse\" in content)\n",
    "#     \"\"\"\n",
    "#     X = np.asarray(X)\n",
    "#     flat = X.ravel()\n",
    "#     absflat = np.abs(flat)\n",
    "\n",
    "#     if flat.size == 0:\n",
    "#         return X.astype(np.float32)\n",
    "\n",
    "#     # انتخاب آستانه\n",
    "#     if min_abs is not None:\n",
    "#         thr = float(min_abs)\n",
    "#         mask = absflat >= thr\n",
    "#     else:\n",
    "#         k = int(np.ceil(keep_ratio * flat.size))\n",
    "#         k = max(1, min(k, flat.size))\n",
    "#         if k == flat.size:\n",
    "#             mask = np.ones_like(absflat, dtype=bool)\n",
    "#         else:\n",
    "#             thr = np.partition(absflat, -k)[-k]  # kth largest magnitude\n",
    "#             mask = absflat >= thr\n",
    "\n",
    "#     idx = np.nonzero(mask)[0]\n",
    "#     data = flat[idx].astype(np.float32)\n",
    "\n",
    "#     # اگر SciPy هست: sparse واقعی بساز\n",
    "#     try:\n",
    "#         # معمولاً تو Cell1 یا از قبل import شده؛ اگر هم نشده باشه اینجا تلاش می‌کنه.\n",
    "#         import scipy.sparse as sp\n",
    "\n",
    "#         if X.ndim == 1:\n",
    "#             rows = idx\n",
    "#             cols = np.zeros_like(rows)\n",
    "#             shape = (X.shape[0], 1)\n",
    "#         else:\n",
    "#             rows, cols = np.unravel_index(idx, X.shape)\n",
    "#             shape = X.shape\n",
    "\n",
    "#         return sp.coo_matrix((data, (rows, cols)), shape=shape).tocsr()\n",
    "\n",
    "#     except Exception:\n",
    "#         # fallback: آرایه‌ی dense با صفرهای زیاد\n",
    "#         out = np.zeros_like(flat, dtype=np.float32)\n",
    "#         out[idx] = data\n",
    "#         return out.reshape(X.shape)\n",
    "\n",
    "def mat_to_amp(data_mat):\n",
    "    \"\"\"\n",
    "    Calculate amplitude of raw WiFi CSI data, then return its sparse version.\n",
    "    (ورودی تابع تغییر نکرده)\n",
    "    \"\"\"\n",
    "    var_length = data_mat[\"trace\"].shape[0]\n",
    "    data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "    data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "    # خروجی sparse (CSR اگر SciPy باشد)\n",
    "    return _to_sparse(data_csi_amp, keep_ratio=SPARSE_KEEP_RATIO, min_abs=SPARSE_MIN_ABS)\n",
    "\n",
    "# تنظیمات RPCA (می‌تونی عوضشون کنی)\n",
    "RPCA_MAX_ITER = 500\n",
    "RPCA_TOL = 1e-7\n",
    "RPCA_RHO = 1.5\n",
    "RPCA_MU_INIT = None     # None یعنی خودکار\n",
    "RPCA_LAMBDA = None      # None یعنی 1/sqrt(max(m,n))\n",
    "\n",
    "def _soft_threshold(X, tau):\n",
    "    return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "def _svt(X, tau):\n",
    "    # Singular Value Thresholding\n",
    "    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    s_thr = np.maximum(s - tau, 0.0)\n",
    "    # اگر همه صفر شد، سریع برگرد\n",
    "    if np.all(s_thr == 0):\n",
    "        return np.zeros_like(X)\n",
    "    return (U * s_thr) @ Vt\n",
    "\n",
    "def _rpca_ialm(M, lam=None, mu=None, rho=1.5, max_iter=500, tol=1e-7):\n",
    "    \"\"\"\n",
    "    Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "    Decompose: M = L + S\n",
    "    Returns: L, S (same shape as M)\n",
    "    \"\"\"\n",
    "    M = M.astype(np.float64, copy=False)\n",
    "    m, n = M.shape\n",
    "\n",
    "    if lam is None:\n",
    "        lam = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "    # mu پیشنهادی (خودکار)\n",
    "    if mu is None:\n",
    "        # ||M||_2 تقریباً بزرگ‌ترین singular value است\n",
    "        norm2 = np.linalg.svd(M, compute_uv=False)[0] if M.size else 1.0\n",
    "        mu = 1.25 / (norm2 + 1e-12)\n",
    "\n",
    "    L = np.zeros_like(M)\n",
    "    S = np.zeros_like(M)\n",
    "    Y = np.zeros_like(M)\n",
    "\n",
    "    normM = np.linalg.norm(M, ord='fro') + 1e-12\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # L update\n",
    "        L = _svt(M - S + (1.0/mu)*Y, 1.0/mu)\n",
    "\n",
    "        # S update (sparse)\n",
    "        S = _soft_threshold(M - L + (1.0/mu)*Y, lam/mu)\n",
    "\n",
    "        # dual update\n",
    "        R = M - L - S\n",
    "        Y = Y + mu * R\n",
    "\n",
    "        # stop\n",
    "        err = np.linalg.norm(R, ord='fro') / normM\n",
    "        if err < tol:\n",
    "            break\n",
    "\n",
    "        mu *= rho\n",
    "\n",
    "    return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "# def mat_to_amp(data_mat):\n",
    "#     \"\"\"\n",
    "#     Calculate amplitude of raw WiFi CSI data, then return RPCA sparse component S.\n",
    "#     (ورودی تابع تغییر نکرده)\n",
    "#     \"\"\"\n",
    "#     var_length = data_mat[\"trace\"].shape[0]\n",
    "#     data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "#     data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "#     was_1d = (data_csi_amp.ndim == 1)\n",
    "#     M = data_csi_amp[:, None] if was_1d else data_csi_amp\n",
    "\n",
    "#     _, S = _rpca_ialm(\n",
    "#         M,\n",
    "#         lam=RPCA_LAMBDA,\n",
    "#         mu=RPCA_MU_INIT,\n",
    "#         rho=RPCA_RHO,\n",
    "#         max_iter=RPCA_MAX_ITER,\n",
    "#         tol=RPCA_TOL\n",
    "#     )\n",
    "\n",
    "#     if was_1d:\n",
    "#         S = S[:, 0]\n",
    "\n",
    "#     return S\n",
    "\n",
    "def mat_to_amp(data_mat):\n",
    "    \"\"\"\n",
    "    Calculate amplitude of raw WiFi CSI data, then return RPCA low-rank component L.\n",
    "    (ورودی تابع تغییر نکرده)\n",
    "    \"\"\"\n",
    "    var_length = data_mat[\"trace\"].shape[0]\n",
    "    data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "    data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "    was_1d = (data_csi_amp.ndim == 1)\n",
    "    M = data_csi_amp[:, None] if was_1d else data_csi_amp\n",
    "\n",
    "    L, _ = _rpca_ialm(\n",
    "        M,\n",
    "        lam=RPCA_LAMBDA,\n",
    "        mu=RPCA_MU_INIT,\n",
    "        rho=RPCA_RHO,\n",
    "        max_iter=RPCA_MAX_ITER,\n",
    "        tol=RPCA_TOL\n",
    "    )\n",
    "\n",
    "    if was_1d:\n",
    "        L = L[:, 0]\n",
    "\n",
    "    return L\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parse arguments from input.\n",
    "    \"\"\"\n",
    "    var_args = argparse.ArgumentParser()\n",
    "    var_args.add_argument(\"--dir_mat\", default=\"/kaggle/input/wimans/wifi_csi/mat\", type=str)\n",
    "    var_args.add_argument(\"--dir_amp\", default=\"/kaggle/input/wimans/wifi_csi/amp\", type=str)\n",
    "    return var_args.parse_args()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     var_args = parse_args()\n",
    "#     extract_csi_amp(var_dir_mat=var_args.dir_mat, var_dir_amp=var_args.dir_amp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4619b8df",
   "metadata": {
    "papermill": {
     "duration": 0.006627,
     "end_time": "2025-12-29T07:33:55.308718",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.302091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 5: that.py (WiFi-based Model THAT)\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "767c81af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T07:33:55.323565Z",
     "iopub.status.busy": "2025-12-29T07:33:55.323394Z",
     "iopub.status.idle": "2025-12-29T07:33:55.347354Z",
     "shell.execute_reply": "2025-12-29T07:33:55.346780Z"
    },
    "papermill": {
     "duration": 0.033101,
     "end_time": "2025-12-29T07:33:55.348659",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.315558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          that.py\n",
    "[description]   implement and evaluate WiFi-based model THAT\n",
    "                https://github.com/windofshadow/THAT\n",
    "\"\"\"\n",
    "\n",
    "# All necessary libraries are imported in Cell 1.\n",
    "# from train import train   --> Defined in Cell 6.\n",
    "# from preset import preset --> Defined in Cell 2.\n",
    "\n",
    "class Gaussian_Position(torch.nn.Module):\n",
    "    def __init__(self, var_dim_feature, var_dim_time, var_num_gaussian=10):\n",
    "        super(Gaussian_Position, self).__init__()\n",
    "        var_embedding = torch.zeros([var_num_gaussian, var_dim_feature], dtype=torch.float)\n",
    "        self.var_embedding = torch.nn.Parameter(var_embedding, requires_grad=True)\n",
    "        torch.nn.init.xavier_uniform_(self.var_embedding)\n",
    "        var_position = torch.arange(0.0, var_dim_time).unsqueeze(1).repeat(1, var_num_gaussian)\n",
    "        self.var_position = torch.nn.Parameter(var_position, requires_grad=False)\n",
    "        var_mu = torch.arange(0.0, var_dim_time, var_dim_time/var_num_gaussian).unsqueeze(0)\n",
    "        self.var_mu = torch.nn.Parameter(var_mu, requires_grad=True)\n",
    "        var_sigma = torch.tensor([50.0] * var_num_gaussian).unsqueeze(0)\n",
    "        self.var_sigma = torch.nn.Parameter(var_sigma, requires_grad=True)\n",
    "\n",
    "    def calculate_pdf(self, var_position, var_mu, var_sigma):\n",
    "        var_pdf = var_position - var_mu\n",
    "        var_pdf = - var_pdf * var_pdf\n",
    "        var_pdf = var_pdf / var_sigma / var_sigma / 2\n",
    "        var_pdf = var_pdf - torch.log(var_sigma)\n",
    "        return var_pdf\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        var_pdf = self.calculate_pdf(self.var_position, self.var_mu, self.var_sigma)\n",
    "        var_pdf = torch.softmax(var_pdf, dim=-1)\n",
    "        var_position_encoding = torch.matmul(var_pdf, self.var_embedding)\n",
    "        var_output = var_input + var_position_encoding.unsqueeze(0)\n",
    "        return var_output\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, var_dim_feature, var_num_head=10, var_size_cnn=[1, 3, 5]):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layer_norm_0 = torch.nn.LayerNorm(var_dim_feature, eps=1e-6)\n",
    "        self.layer_attention = torch.nn.MultiheadAttention(var_dim_feature, var_num_head, batch_first=True)\n",
    "        self.layer_dropout_0 = torch.nn.Dropout(0.1)\n",
    "        self.layer_norm_1 = torch.nn.LayerNorm(var_dim_feature, 1e-6)\n",
    "        layer_cnn = []\n",
    "        for var_size in var_size_cnn:\n",
    "            layer = torch.nn.Sequential(\n",
    "                torch.nn.Conv1d(var_dim_feature, var_dim_feature, var_size, padding=\"same\"),\n",
    "                torch.nn.BatchNorm1d(var_dim_feature),\n",
    "                torch.nn.Dropout(0.1),\n",
    "                torch.nn.LeakyReLU()\n",
    "            )\n",
    "            layer_cnn.append(layer)\n",
    "        self.layer_cnn = torch.nn.ModuleList(layer_cnn)\n",
    "        self.layer_dropout_1 = torch.nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        var_t = var_input\n",
    "        var_t = self.layer_norm_0(var_t)\n",
    "        var_t, _ = self.layer_attention(var_t, var_t, var_t)\n",
    "        var_t = self.layer_dropout_0(var_t)\n",
    "        var_t = var_t + var_input\n",
    "        var_s = self.layer_norm_1(var_t)\n",
    "        var_s = torch.permute(var_s, (0, 2, 1))\n",
    "        var_c = torch.stack([layer(var_s) for layer in self.layer_cnn], dim=0)\n",
    "        var_s = torch.sum(var_c, dim=0) / len(self.layer_cnn)\n",
    "        var_s = self.layer_dropout_1(var_s)\n",
    "        var_s = torch.permute(var_s, (0, 2, 1))\n",
    "        var_output = var_s + var_t\n",
    "        return var_output\n",
    "\n",
    "class THAT(torch.nn.Module):\n",
    "    def __init__(self, var_x_shape, var_y_shape):\n",
    "        super(THAT, self).__init__()\n",
    "        var_dim_feature = var_x_shape[-1]\n",
    "        var_dim_time = var_x_shape[-2]\n",
    "        var_dim_output = var_y_shape[-1]\n",
    "        # Left branch\n",
    "        self.layer_left_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "        self.layer_left_gaussian = Gaussian_Position(var_dim_feature, var_dim_time // 20)\n",
    "        var_num_left = 4\n",
    "        var_dim_left = var_dim_feature\n",
    "        self.layer_left_encoder = torch.nn.ModuleList([\n",
    "            Encoder(var_dim_feature=var_dim_left, var_num_head=10, var_size_cnn=[1, 3, 5])\n",
    "            for _ in range(var_num_left)\n",
    "        ])\n",
    "        self.layer_left_norm = torch.nn.LayerNorm(var_dim_left, eps=1e-6)\n",
    "        self.layer_left_cnn_0 = torch.nn.Conv1d(in_channels=var_dim_left, out_channels=128, kernel_size=8)\n",
    "        self.layer_left_cnn_1 = torch.nn.Conv1d(in_channels=var_dim_left, out_channels=128, kernel_size=16)\n",
    "        self.layer_left_dropout = torch.nn.Dropout(0.5)\n",
    "        # Right branch\n",
    "        self.layer_right_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "        var_num_right = 1\n",
    "        var_dim_right = var_dim_time // 20\n",
    "        self.layer_right_encoder = torch.nn.ModuleList([\n",
    "            Encoder(var_dim_feature=var_dim_right, var_num_head=10, var_size_cnn=[1, 2, 3])\n",
    "            for _ in range(var_num_right)\n",
    "        ])\n",
    "        self.layer_right_norm = torch.nn.LayerNorm(var_dim_right, eps=1e-6)\n",
    "        self.layer_right_cnn_0 = torch.nn.Conv1d(in_channels=var_dim_right, out_channels=16, kernel_size=2)\n",
    "        self.layer_right_cnn_1 = torch.nn.Conv1d(in_channels=var_dim_right, out_channels=16, kernel_size=4)\n",
    "        self.layer_right_dropout = torch.nn.Dropout(0.5)\n",
    "        self.layer_leakyrelu = torch.nn.LeakyReLU()\n",
    "        self.layer_output = torch.nn.Linear(256 + 32, var_dim_output)\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        var_t = var_input  # shape: (batch_size, time_steps, features)\n",
    "        # Left branch\n",
    "        var_left = torch.permute(var_t, (0, 2, 1))\n",
    "        var_left = self.layer_left_pooling(var_left)\n",
    "        var_left = torch.permute(var_left, (0, 2, 1))\n",
    "        var_left = self.layer_left_gaussian(var_left)\n",
    "        for layer in self.layer_left_encoder:\n",
    "            var_left = layer(var_left)\n",
    "        var_left = self.layer_left_norm(var_left)\n",
    "        var_left = torch.permute(var_left, (0, 2, 1))\n",
    "        var_left_0 = self.layer_leakyrelu(self.layer_left_cnn_0(var_left))\n",
    "        var_left_1 = self.layer_leakyrelu(self.layer_left_cnn_1(var_left))\n",
    "        var_left_0 = torch.sum(var_left_0, dim=-1)\n",
    "        var_left_1 = torch.sum(var_left_1, dim=-1)\n",
    "        var_left = torch.concat([var_left_0, var_left_1], dim=-1)\n",
    "        var_left = self.layer_left_dropout(var_left)\n",
    "        # Right branch\n",
    "        var_right = torch.permute(var_t, (0, 2, 1))\n",
    "        var_right = self.layer_right_pooling(var_right)\n",
    "        for layer in self.layer_right_encoder:\n",
    "            var_right = layer(var_right)\n",
    "        var_right = self.layer_right_norm(var_right)\n",
    "        var_right = torch.permute(var_right, (0, 2, 1))\n",
    "        var_right_0 = self.layer_leakyrelu(self.layer_right_cnn_0(var_right))\n",
    "        var_right_1 = self.layer_leakyrelu(self.layer_right_cnn_1(var_right))\n",
    "        var_right_0 = torch.sum(var_right_0, dim=-1)\n",
    "        var_right_1 = torch.sum(var_right_1, dim=-1)\n",
    "        var_right = torch.concat([var_right_0, var_right_1], dim=-1)\n",
    "        var_right = self.layer_right_dropout(var_right)\n",
    "        # Concatenate branches\n",
    "        var_t = torch.concat([var_left, var_right], dim=-1)\n",
    "        var_output = self.layer_output(var_t)\n",
    "        return var_output\n",
    "\n",
    "def run_that(data_train_x, data_train_y, data_test_x, data_test_y, var_repeat=10, init_model=None):\n",
    "    \"\"\"\n",
    "    Run WiFi-based model THAT.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data_train_x = data_train_x.reshape(data_train_x.shape[0], data_train_x.shape[1], -1)\n",
    "    data_test_x = data_test_x.reshape(data_test_x.shape[0], data_test_x.shape[1], -1)\n",
    "    var_x_shape, var_y_shape = data_train_x[0].shape, data_train_y[0].reshape(-1).shape\n",
    "    data_train_set = TensorDataset(torch.from_numpy(data_train_x), torch.from_numpy(data_train_y))\n",
    "    data_test_set = TensorDataset(torch.from_numpy(data_test_x), torch.from_numpy(data_test_y))\n",
    "    \n",
    "    result = {}\n",
    "    result_accuracy = []\n",
    "    result_time_train = []\n",
    "    result_time_test = []\n",
    "    \n",
    "    # var_macs, var_params = get_model_complexity_info(THAT(var_x_shape, var_y_shape), var_x_shape, as_strings=False)\n",
    "    # print(\"Parameters:\", var_params, \"- FLOPs:\", var_macs * 2)\n",
    "    \n",
    "    for var_r in range(var_repeat):\n",
    "        print(\"Repeat\", var_r)\n",
    "        torch.random.manual_seed(var_r + 39)\n",
    "        if init_model is not None:\n",
    "            model_that = init_model\n",
    "            lr2 = preset[\"nn\"][\"lr\"] /10\n",
    "        else:\n",
    "            model_that = THAT(var_x_shape, var_y_shape).to(device)\n",
    "            lr2 = preset[\"nn\"][\"lr\"]\n",
    "\n",
    "        optimizer = torch.optim.Adam(model_that.parameters(), lr=lr2, weight_decay=0)\n",
    "        loss = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([4] * var_y_shape[-1]).to(device))\n",
    "        var_time_0 = time.time()\n",
    "        \n",
    "        # Train\n",
    "        var_best_weight = train(model=model_that, optimizer=optimizer, loss=loss, \n",
    "                                  data_train_set=data_train_set, data_test_set=data_test_set,\n",
    "                                  var_threshold=preset[\"nn\"][\"threshold\"],\n",
    "                                  var_batch_size=preset[\"nn\"][\"batch_size\"],\n",
    "                                  var_epochs=preset[\"nn\"][\"epoch\"],\n",
    "                                  device=device)\n",
    "        var_time_1 = time.time()\n",
    "        \n",
    "        # Test\n",
    "        model_that.load_state_dict(var_best_weight)\n",
    "        with torch.no_grad():\n",
    "            predict_test_y = model_that(torch.from_numpy(data_test_x).to(device))\n",
    "        predict_test_y = (torch.sigmoid(predict_test_y) > preset[\"nn\"][\"threshold\"]).float()\n",
    "        predict_test_y = predict_test_y.detach().cpu().numpy()\n",
    "        var_time_2 = time.time()\n",
    "        \n",
    "        # Evaluate\n",
    "        data_test_y_c = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "        predict_test_y_c = predict_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "        result_acc = accuracy_score(data_test_y_c.astype(int), predict_test_y_c.astype(int))\n",
    "        result_dict = classification_report(data_test_y_c, predict_test_y_c, digits=6, zero_division=0, output_dict=True)\n",
    "        result[\"repeat_\" + str(var_r)] = result_dict\n",
    "        result_accuracy.append(result_acc)\n",
    "        result_time_train.append(var_time_1 - var_time_0)\n",
    "        result_time_test.append(var_time_2 - var_time_1)\n",
    "        print(\"repeat_\" + str(var_r), result_accuracy)\n",
    "        print(result)\n",
    "    \n",
    "    result[\"accuracy\"] = {\"avg\": np.mean(result_accuracy), \"std\": np.std(result_accuracy)}\n",
    "    result[\"time_train\"] = {\"avg\": np.mean(result_time_train), \"std\": np.std(result_time_train)}\n",
    "    result[\"time_test\"] = {\"avg\": np.mean(result_time_test), \"std\": np.std(result_time_test)}\n",
    "    # result[\"complexity\"] = {\"parameter\": var_params, \"flops\": var_macs * 2}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701dd232",
   "metadata": {
    "papermill": {
     "duration": 0.007006,
     "end_time": "2025-12-29T07:33:55.365650",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.358644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf4216f5",
   "metadata": {
    "papermill": {
     "duration": 0.008172,
     "end_time": "2025-12-29T07:33:55.381098",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.372926",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell7: for RESNET18 Model\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72c2e3a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T07:33:55.397150Z",
     "iopub.status.busy": "2025-12-29T07:33:55.396924Z",
     "iopub.status.idle": "2025-12-29T07:33:55.403699Z",
     "shell.execute_reply": "2025-12-29T07:33:55.403025Z"
    },
    "papermill": {
     "duration": 0.01599,
     "end_time": "2025-12-29T07:33:55.404785",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.388795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# import torch._dynamo\n",
    "# torch._dynamo.config.suppress_errors = True\n",
    "# import time\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "# import numpy as np\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# import torchvision.models as models\n",
    "# from copy import deepcopy\n",
    "\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "# torch._dynamo.config.cache_size_limit = 65536\n",
    "\n",
    "# # فرض می‌کنیم preset قبلاً تعریف شده باشه\n",
    "# # preset = { \"nn\": {\"lr\": 1e-3, \"epoch\": 10, \"batch_size\": 4, \"threshold\": 0.5}, ... }\n",
    "\n",
    "# class ResNet18Model(torch.nn.Module):\n",
    "#     def __init__(self, var_x_shape, var_y_shape):\n",
    "#         super(ResNet18Model, self).__init__()\n",
    "#         model_resnet = models.resnet18(weights=None)\n",
    "#         model_resnet.conv1 = torch.nn.Conv2d(1, 64, 7, 3, 2, bias=False)\n",
    "#         in_features_fc = model_resnet.fc.in_features  # معمولاً 512\n",
    "#         out_features_fc = var_y_shape[-1]\n",
    "#         model_resnet.fc = torch.nn.Linear(in_features_fc, out_features_fc)\n",
    "#         self.resnet = model_resnet\n",
    "\n",
    "#     def forward(self, var_input):\n",
    "#         var_input = var_input.reshape(var_input.size(0), 1, 3000, 270)\n",
    "#         return self.resnet(var_input)\n",
    "\n",
    "# def run_resnet(data_train_x, data_train_y, data_test_x, data_test_y, var_repeat=10, init_model=None):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     var_x_shape = data_train_x[0].shape\n",
    "#     var_y_shape = data_train_y[0].reshape(-1).shape\n",
    "\n",
    "#     # تغییر شکل داده‌ها روی CPU\n",
    "#     data_train_x = data_train_x.reshape(data_train_x.shape[0], 1, data_train_x.shape[1],\n",
    "#                                         data_train_x.shape[2]*data_train_x.shape[3]*data_train_x.shape[4])\n",
    "#     data_test_x  = data_test_x.reshape(data_test_x.shape[0], 1, data_test_x.shape[1],\n",
    "#                                        data_test_x.shape[2]*data_test_x.shape[3]*data_test_x.shape[4])\n",
    "    \n",
    "#     # دیتاست‌ها روی CPU\n",
    "#     data_train_set = TensorDataset(torch.from_numpy(data_train_x).float(),\n",
    "#                                    torch.from_numpy(data_train_y).float())\n",
    "#     data_test_set  = TensorDataset(torch.from_numpy(data_test_x).float(),\n",
    "#                                    torch.from_numpy(data_test_y).float())\n",
    "    \n",
    "#     result = {}\n",
    "#     result_accuracy = []\n",
    "#     result_time_train = []\n",
    "#     result_time_test = []\n",
    "    \n",
    "#     for var_r in range(var_repeat):\n",
    "#         print(\"Repeat\", var_r)\n",
    "#         torch.random.manual_seed(var_r + 39)\n",
    "        \n",
    "#         # ساخت مدل و انتقال به GPU\n",
    "#         if init_model is not None:\n",
    "#             model_resnet = init_model\n",
    "#             lr2 = preset[\"nn\"][\"lr\"] /10\n",
    "            \n",
    "#         else:\n",
    "#             model_resnet = ResNet18Model(var_x_shape, var_y_shape).to(device)\n",
    "#             lr2 = preset[\"nn\"][\"lr\"]\n",
    "\n",
    "#         optimizer = torch.optim.Adam(model_resnet.parameters(), lr=lr2, weight_decay=0)\n",
    "#         loss_func = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([6] * var_y_shape[-1]).to(device))\n",
    "        \n",
    "#         # تابع آموزش داخلی؛ دیتا روی CPU باقی می‌مونه و فقط هنگام محاسبه batch به GPU میره\n",
    "#         def train_inner():\n",
    "#             train_loader = DataLoader(data_train_set, preset[\"nn\"][\"batch_size\"], shuffle=True, pin_memory=False)\n",
    "#             test_loader = DataLoader(data_test_set, preset[\"nn\"][\"batch_size\"], shuffle=False, pin_memory=False)\n",
    "#             best_accuracy = 0\n",
    "#             best_weight = None\n",
    "            \n",
    "#             for epoch in range(preset[\"nn\"][\"epoch\"]):\n",
    "#                 t0 = time.time()\n",
    "#                 model_resnet.train()\n",
    "#                 # متغیرهای مربوط به آخرین batch آموزش\n",
    "#                 last_train_loss = None\n",
    "#                 last_train_acc = None\n",
    "#                 for batch in train_loader:\n",
    "#                     batch_x, batch_y = batch\n",
    "#                     batch_x = batch_x.to(device)\n",
    "#                     batch_y = batch_y.to(device)\n",
    "#                     outputs = model_resnet(batch_x)\n",
    "#                     loss_val = loss_func(outputs, batch_y.reshape(batch_y.shape[0], -1).float())\n",
    "#                     optimizer.zero_grad()\n",
    "#                     loss_val.backward()\n",
    "#                     optimizer.step()\n",
    "#                     last_train_loss = loss_val.item()\n",
    "#                     # محاسبه دقت آخرین batch آموزش\n",
    "#                     train_preds = (torch.sigmoid(outputs) > preset[\"nn\"][\"threshold\"]).float()\n",
    "#                     last_train_acc = accuracy_score(batch_y.reshape(batch_y.shape[0], -1).detach().cpu().numpy().astype(int),\n",
    "#                                                     train_preds.detach().cpu().numpy().astype(int))\n",
    "                \n",
    "#                 # ارزیابی روی دیتاست تست به صورت batch به batch\n",
    "#                 model_resnet.eval()\n",
    "#                 all_preds = []\n",
    "#                 all_labels = []\n",
    "#                 test_loss_val = None\n",
    "#                 with torch.no_grad():\n",
    "#                     for t_batch in test_loader:\n",
    "#                         t_x, t_y = t_batch\n",
    "#                         t_x = t_x.to(device)\n",
    "#                         outputs_test = model_resnet(t_x)\n",
    "#                         outputs_test = (torch.sigmoid(outputs_test) > preset[\"nn\"][\"threshold\"]).float()\n",
    "#                         all_preds.append(outputs_test.detach().cpu().numpy())\n",
    "#                         all_labels.append(t_y.cpu().numpy())  # اینجا تغییر دادیم\n",
    "#                 preds_cat = np.vstack(all_preds)\n",
    "#                 labels_cat = np.vstack(all_labels)\n",
    "#                 print(\"preds_cat\",preds_cat.shape)\n",
    "#                 # تبدیل به شکل (n, 6, 5)\n",
    "                \n",
    "#                 # preds_cat = preds_cat.reshape(-1, 6, 5)\n",
    "#                 # labels_cat = labels_cat.reshape(-1, 6, 5)\n",
    "\n",
    "#                 preds_cat = preds_cat.reshape(-1, 6)\n",
    "#                 labels_cat = labels_cat.reshape(-1, 6)\n",
    "                \n",
    "#                 # برای محاسبه دقت، مسطح می‌کنیم\n",
    "#                 test_acc = accuracy_score(labels_cat.reshape(labels_cat.shape[0], -1).astype(int),\n",
    "#                                           preds_cat.reshape(preds_cat.shape[0], -1).astype(int))\n",
    "#                 epoch_time = time.time() - t0\n",
    "#                 print(f\"Epoch {epoch}/{preset['nn']['epoch']} - \"\n",
    "#                       f\"Train Loss: {(last_train_loss if last_train_loss is not None else 0.0):.6f}, \"\n",
    "#                       f\"Train Acc: {(last_train_acc if last_train_acc is not None else 0.0):.6f}, \"\n",
    "#                       f\"Test Loss: {(test_loss_val if test_loss_val is not None else 0.0):.6f}, \"\n",
    "#                       f\"Test Acc: {(test_acc if test_acc is not None else 0.0):.6f} - \"\n",
    "#                       f\"Time: {epoch_time:.4f}s\")\n",
    "\n",
    "#                 if test_acc > best_accuracy:\n",
    "#                     best_accuracy = test_acc\n",
    "#                     print('-----***-----')\n",
    "#                     print(best_accuracy)\n",
    "#                     best_weight = deepcopy(model_resnet.state_dict())\n",
    "#             return best_weight\n",
    "        \n",
    "#         t0_run = time.time()\n",
    "#         best_weight = train_inner()\n",
    "#         t1_run = time.time()\n",
    "        \n",
    "#         torch.save(model_resnet.state_dict(), f\"{name_run}_model_final.pt\")\n",
    "#         model_resnet.load_state_dict(best_weight)\n",
    "#         torch.save(model_resnet.state_dict(), f\"{name_run}_best_model.pt\")\n",
    "\n",
    "#         # bad age niaz bod load koni\n",
    "#         # model_resnet = ResNet18Model(var_x_shape, var_y_shape).to(device)\n",
    "#         # model_resnet.load_state_dict(torch.load(\"resnet_model_repeat0.pt\"))\n",
    "#         # model_resnet.eval()\n",
    "\n",
    "        \n",
    "#         # ارزیابی نهایی مدل روی دیتاست تست (استفاده از batchهای کوچک)\n",
    "#         model_resnet.eval()\n",
    "#         all_preds = []\n",
    "#         test_loader_final = DataLoader(data_test_set, preset[\"nn\"][\"batch_size\"], shuffle=False, pin_memory=False)\n",
    "#         with torch.no_grad():\n",
    "#             for batch in test_loader_final:\n",
    "#                 batch_x, _ = batch\n",
    "#                 batch_x = batch_x.to(device)\n",
    "#                 all_preds.append(model_resnet(batch_x))\n",
    "#         preds_all = torch.cat(all_preds, dim=0)\n",
    "#         preds_final = (torch.sigmoid(preds_all) > preset[\"nn\"][\"threshold\"]).float().detach().cpu().numpy()\n",
    "#         t2_run = time.time()\n",
    "        \n",
    "#         data_test_y_np = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "#         preds_final = preds_final.reshape(-1, data_test_y.shape[-1])\n",
    "#         acc_final = accuracy_score(data_test_y_np.astype(int), preds_final.astype(int))\n",
    "#         result[f\"repeat_{var_r}\"] = {\"accuracy\": acc_final}\n",
    "#         result_accuracy.append(acc_final)\n",
    "#         result_time_train.append(t1_run - t0_run)\n",
    "#         result_time_test.append(t2_run - t1_run)\n",
    "#         print(\"Repeat\", var_r, \"Final Test Accuracy:\", acc_final)\n",
    "    \n",
    "#     result[\"accuracy\"] = {\"avg\": np.mean(result_accuracy), \"std\": np.std(result_accuracy)}\n",
    "#     result[\"time_train\"] = {\"avg\": np.mean(result_time_train), \"std\": np.std(result_time_train)}\n",
    "#     result[\"time_test\"] = {\"avg\": np.mean(result_time_test), \"std\": np.std(result_time_test)}\n",
    "#     return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499fd743",
   "metadata": {
    "papermill": {
     "duration": 0.006743,
     "end_time": "2025-12-29T07:33:55.418636",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.411893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 9: train.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a9a0376",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T07:33:55.432938Z",
     "iopub.status.busy": "2025-12-29T07:33:55.432765Z",
     "iopub.status.idle": "2025-12-29T07:33:56.123603Z",
     "shell.execute_reply": "2025-12-29T07:33:56.122994Z"
    },
    "papermill": {
     "duration": 0.69956,
     "end_time": "2025-12-29T07:33:56.124947",
     "exception": false,
     "start_time": "2025-12-29T07:33:55.425387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          train.py\n",
    "[description]   function to train WiFi-based models\n",
    "\"\"\"\n",
    "\n",
    "# All necessary libraries are imported in Cell 1.\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch._dynamo.config.cache_size_limit = 65536\n",
    "\n",
    "def train(model, optimizer, loss, data_train_set, data_test_set, var_threshold, var_batch_size, var_epochs, device):\n",
    "    \"\"\"\n",
    "    Generic training function for WiFi-based models.\n",
    "    \"\"\"\n",
    "    # دیتا رو روی CPU نگه می‌داریم (pin_memory=False)\n",
    "    data_train_loader = DataLoader(data_train_set, var_batch_size, shuffle=True, pin_memory=False)\n",
    "    data_test_loader = DataLoader(data_test_set, batch_size=len(data_test_set), shuffle=False, pin_memory=False)\n",
    "    \n",
    "    var_best_accuracy = -1.0\n",
    "    var_best_weight   = deepcopy(model.state_dict())\n",
    "    \n",
    "    \n",
    "    for var_epoch in range(var_epochs):\n",
    "        var_time_e0 = time.time()\n",
    "        model.train()\n",
    "        for data_batch in data_train_loader:\n",
    "            data_batch_x, data_batch_y = data_batch\n",
    "            # انتقال موقتی داده به GPU فقط برای forward pass\n",
    "            data_batch_x = data_batch_x.to(device)\n",
    "            data_batch_y = data_batch_y.to(device)\n",
    "            predict_train_y = model(data_batch_x)\n",
    "            var_loss_train = loss(predict_train_y, data_batch_y.reshape(data_batch_y.shape[0], -1).float())\n",
    "            optimizer.zero_grad()\n",
    "            var_loss_train.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # محاسبه دقت روی آخرین batch و انتقال نتایج به CPU\n",
    "        predict_train_y = (torch.sigmoid(predict_train_y) > var_threshold).float()\n",
    "        data_batch_y = data_batch_y.detach().cpu().numpy()\n",
    "        predict_train_y = predict_train_y.detach().cpu().numpy()\n",
    "        \n",
    "        predict_train_y = predict_train_y.reshape(-1, data_batch_y.shape[-1])\n",
    "        data_batch_y = data_batch_y.reshape(-1, data_batch_y.shape[-1])\n",
    "        var_accuracy_train = accuracy_score(data_batch_y.astype(int), predict_train_y.astype(int))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            data_test_x, data_test_y = next(iter(data_test_loader))\n",
    "            # انتقال موقتی دیتا تست به GPU برای محاسبات\n",
    "            data_test_x = data_test_x.to(device)\n",
    "            data_test_y = data_test_y.to(device)\n",
    "            \n",
    "            predict_test_y = model(data_test_x)\n",
    "            var_loss_test = loss(predict_test_y, data_test_y.reshape(data_test_y.shape[0], -1).float())\n",
    "            \n",
    "            predict_test_y = (torch.sigmoid(predict_test_y) > var_threshold).float()\n",
    "            \n",
    "            # انتقال نتایج به CPU برای ارزیابی\n",
    "            data_test_y = data_test_y.detach().cpu().numpy()\n",
    "            predict_test_y = predict_test_y.detach().cpu().numpy()\n",
    "            \n",
    "            predict_test_y = predict_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            data_test_y = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            var_accuracy_test = accuracy_score(data_test_y.astype(int), predict_test_y.astype(int))\n",
    "        \n",
    "        print(f\"Epoch {var_epoch}/{var_epochs}\",\n",
    "              \"- %.6fs\"%(time.time() - var_time_e0),\n",
    "              \"- Loss %.6f\"%var_loss_train.cpu(),\n",
    "              \"- Accuracy %.6f\"%var_accuracy_train,\n",
    "              \"- Test Loss %.6f\"%var_loss_test.cpu(),\n",
    "              \"- Test Accuracy %.6f\"%var_accuracy_test)\n",
    "            \n",
    "        if var_accuracy_test > var_best_accuracy:\n",
    "            var_best_accuracy = var_accuracy_test\n",
    "            print('-----***-----')\n",
    "            print(var_best_accuracy)\n",
    "            var_best_weight = deepcopy(model.state_dict())\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{name_run}_model_final.pt\")\n",
    "    torch.save(var_best_weight, f\"{name_run}_best_model.pt\")\n",
    "\n",
    "    \n",
    "    return var_best_weight\n",
    "\n",
    "\n",
    "\n",
    "# === importsِ لازم را یک‌بار بالای فایل اضافه کن ===\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- تابع کمکی ----------\n",
    "def save_confusion_matrix(model, data_loader, threshold, device, pdf_path):\n",
    "    \"\"\"\n",
    "    Runs the model on `data_loader`, builds a confusion matrix and writes it to `pdf_path`.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "\n",
    "            preds = (torch.sigmoid(logits) > threshold).float().cpu().numpy().ravel()\n",
    "            yb    = yb.cpu().numpy().ravel()\n",
    "\n",
    "            y_true.extend(yb)\n",
    "            y_pred.extend(preds)\n",
    "\n",
    "    cm  = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    ConfusionMatrixDisplay(cm).plot(ax=ax)\n",
    "    ax.set_title(\"Confusion Matrix – Test\")\n",
    "\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "# ---------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b2984c",
   "metadata": {
    "papermill": {
     "duration": 0.006996,
     "end_time": "2025-12-29T07:33:56.139535",
     "exception": false,
     "start_time": "2025-12-29T07:33:56.132539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 11: run.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c9624b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T07:33:56.154363Z",
     "iopub.status.busy": "2025-12-29T07:33:56.154015Z",
     "iopub.status.idle": "2025-12-29T07:59:59.241688Z",
     "shell.execute_reply": "2025-12-29T07:59:59.240861Z"
    },
    "papermill": {
     "duration": 1563.096554,
     "end_time": "2025-12-29T07:59:59.242995",
     "exception": false,
     "start_time": "2025-12-29T07:33:56.146441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "\n",
      "[DEBUG] First loaded AMP sample: act_1_1\n",
      "[DEBUG] shape=(2835, 3, 3, 30), dtype=float32, complex=False\n",
      "[DEBUG] ==> Input is REAL -> amplitude-only (no true phase).\n",
      "\n",
      "Running model: THAT\n",
      "Repeat 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py:370: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1036.)\n",
      "  return F.conv1d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/300 - 6.190278s - Loss 2.379945 - Accuracy 0.098958 - Test Loss 1.244064 - Test Accuracy 0.319187\n",
      "-----***-----\n",
      "0.3191865605658709\n",
      "Epoch 1/300 - 5.047727s - Loss 0.894863 - Accuracy 0.276042 - Test Loss 0.685072 - Test Accuracy 0.555703\n",
      "-----***-----\n",
      "0.5557029177718833\n",
      "Epoch 2/300 - 5.023291s - Loss 0.821817 - Accuracy 0.312500 - Test Loss 0.581442 - Test Accuracy 0.552608\n",
      "Epoch 3/300 - 5.105255s - Loss 0.640806 - Accuracy 0.307292 - Test Loss 0.553315 - Test Accuracy 0.523431\n",
      "Epoch 4/300 - 5.018135s - Loss 0.660342 - Accuracy 0.421875 - Test Loss 0.519708 - Test Accuracy 0.552608\n",
      "Epoch 5/300 - 5.043402s - Loss 0.602087 - Accuracy 0.348958 - Test Loss 0.525365 - Test Accuracy 0.558798\n",
      "-----***-----\n",
      "0.5587975243147657\n",
      "Epoch 6/300 - 5.020546s - Loss 0.571069 - Accuracy 0.421875 - Test Loss 0.514553 - Test Accuracy 0.565429\n",
      "-----***-----\n",
      "0.5654288240495137\n",
      "Epoch 7/300 - 5.026696s - Loss 0.548618 - Accuracy 0.432292 - Test Loss 0.488771 - Test Accuracy 0.550840\n",
      "Epoch 8/300 - 5.010974s - Loss 0.646316 - Accuracy 0.385417 - Test Loss 0.509613 - Test Accuracy 0.550840\n",
      "Epoch 9/300 - 5.048738s - Loss 0.502895 - Accuracy 0.453125 - Test Loss 0.506898 - Test Accuracy 0.548187\n",
      "Epoch 10/300 - 5.045128s - Loss 0.504504 - Accuracy 0.401042 - Test Loss 0.478214 - Test Accuracy 0.539346\n",
      "Epoch 11/300 - 5.055726s - Loss 0.485591 - Accuracy 0.552083 - Test Loss 0.485910 - Test Accuracy 0.568523\n",
      "-----***-----\n",
      "0.5685234305923961\n",
      "Epoch 12/300 - 5.025156s - Loss 0.581248 - Accuracy 0.322917 - Test Loss 0.472601 - Test Accuracy 0.533599\n",
      "Epoch 13/300 - 5.029197s - Loss 0.424907 - Accuracy 0.510417 - Test Loss 0.473691 - Test Accuracy 0.565429\n",
      "Epoch 14/300 - 5.023947s - Loss 0.438190 - Accuracy 0.489583 - Test Loss 0.461524 - Test Accuracy 0.545535\n",
      "Epoch 15/300 - 5.046325s - Loss 0.507558 - Accuracy 0.458333 - Test Loss 0.448606 - Test Accuracy 0.551282\n",
      "Epoch 16/300 - 5.012686s - Loss 0.455286 - Accuracy 0.479167 - Test Loss 0.448812 - Test Accuracy 0.559240\n",
      "Epoch 17/300 - 5.032837s - Loss 0.491239 - Accuracy 0.375000 - Test Loss 0.441381 - Test Accuracy 0.555261\n",
      "Epoch 18/300 - 5.026822s - Loss 0.428624 - Accuracy 0.484375 - Test Loss 0.446188 - Test Accuracy 0.533156\n",
      "Epoch 19/300 - 5.035602s - Loss 0.325297 - Accuracy 0.619792 - Test Loss 0.452286 - Test Accuracy 0.544209\n",
      "Epoch 20/300 - 5.020622s - Loss 0.436165 - Accuracy 0.468750 - Test Loss 0.456540 - Test Accuracy 0.563660\n",
      "Epoch 21/300 - 5.034610s - Loss 0.359439 - Accuracy 0.572917 - Test Loss 0.451109 - Test Accuracy 0.540230\n",
      "Epoch 22/300 - 5.025152s - Loss 0.369760 - Accuracy 0.578125 - Test Loss 0.451521 - Test Accuracy 0.554819\n",
      "Epoch 23/300 - 5.045445s - Loss 0.304672 - Accuracy 0.583333 - Test Loss 0.446807 - Test Accuracy 0.572060\n",
      "-----***-----\n",
      "0.5720601237842617\n",
      "Epoch 24/300 - 5.022466s - Loss 0.293836 - Accuracy 0.588542 - Test Loss 0.449514 - Test Accuracy 0.557029\n",
      "Epoch 25/300 - 5.035345s - Loss 0.334684 - Accuracy 0.510417 - Test Loss 0.442888 - Test Accuracy 0.550840\n",
      "Epoch 26/300 - 5.030736s - Loss 0.319518 - Accuracy 0.536458 - Test Loss 0.445700 - Test Accuracy 0.548187\n",
      "Epoch 27/300 - 5.031601s - Loss 0.316171 - Accuracy 0.588542 - Test Loss 0.455423 - Test Accuracy 0.556587\n",
      "Epoch 28/300 - 5.043762s - Loss 0.299572 - Accuracy 0.604167 - Test Loss 0.453770 - Test Accuracy 0.568523\n",
      "Epoch 29/300 - 5.052904s - Loss 0.293803 - Accuracy 0.572917 - Test Loss 0.451445 - Test Accuracy 0.557913\n",
      "Epoch 30/300 - 5.025728s - Loss 0.258521 - Accuracy 0.604167 - Test Loss 0.465348 - Test Accuracy 0.576039\n",
      "-----***-----\n",
      "0.5760389036251106\n",
      "Epoch 31/300 - 5.034724s - Loss 0.228862 - Accuracy 0.666667 - Test Loss 0.488696 - Test Accuracy 0.563660\n",
      "Epoch 32/300 - 5.021979s - Loss 0.214996 - Accuracy 0.614583 - Test Loss 0.521705 - Test Accuracy 0.579134\n",
      "-----***-----\n",
      "0.579133510167993\n",
      "Epoch 33/300 - 5.030888s - Loss 0.210109 - Accuracy 0.666667 - Test Loss 0.502446 - Test Accuracy 0.571618\n",
      "Epoch 34/300 - 5.018142s - Loss 0.254349 - Accuracy 0.562500 - Test Loss 0.501733 - Test Accuracy 0.568523\n",
      "Epoch 35/300 - 5.045293s - Loss 0.242814 - Accuracy 0.671875 - Test Loss 0.492323 - Test Accuracy 0.574271\n",
      "Epoch 36/300 - 5.023154s - Loss 0.155731 - Accuracy 0.744792 - Test Loss 0.552035 - Test Accuracy 0.593722\n",
      "-----***-----\n",
      "0.5937223695844386\n",
      "Epoch 37/300 - 5.022920s - Loss 0.248748 - Accuracy 0.677083 - Test Loss 0.516885 - Test Accuracy 0.561892\n",
      "Epoch 38/300 - 5.026156s - Loss 0.192493 - Accuracy 0.723958 - Test Loss 0.535889 - Test Accuracy 0.570734\n",
      "Epoch 39/300 - 5.052413s - Loss 0.162104 - Accuracy 0.744792 - Test Loss 0.536887 - Test Accuracy 0.580018\n",
      "Epoch 40/300 - 5.024708s - Loss 0.180845 - Accuracy 0.729167 - Test Loss 0.533441 - Test Accuracy 0.579576\n",
      "Epoch 41/300 - 5.031637s - Loss 0.171439 - Accuracy 0.713542 - Test Loss 0.576913 - Test Accuracy 0.580018\n",
      "Epoch 42/300 - 5.027567s - Loss 0.157299 - Accuracy 0.677083 - Test Loss 0.585859 - Test Accuracy 0.578691\n",
      "Epoch 43/300 - 5.046836s - Loss 0.138120 - Accuracy 0.765625 - Test Loss 0.595373 - Test Accuracy 0.586649\n",
      "Epoch 44/300 - 5.020326s - Loss 0.142159 - Accuracy 0.708333 - Test Loss 0.601960 - Test Accuracy 0.584439\n",
      "Epoch 45/300 - 5.045015s - Loss 0.134851 - Accuracy 0.734375 - Test Loss 0.610791 - Test Accuracy 0.587091\n",
      "Epoch 46/300 - 5.026711s - Loss 0.148067 - Accuracy 0.713542 - Test Loss 0.687966 - Test Accuracy 0.591954\n",
      "Epoch 47/300 - 5.025647s - Loss 0.163279 - Accuracy 0.729167 - Test Loss 0.679021 - Test Accuracy 0.581344\n",
      "Epoch 48/300 - 5.028752s - Loss 0.135001 - Accuracy 0.739583 - Test Loss 0.709774 - Test Accuracy 0.587975\n",
      "Epoch 49/300 - 5.047102s - Loss 0.145045 - Accuracy 0.776042 - Test Loss 0.616048 - Test Accuracy 0.588417\n",
      "Epoch 50/300 - 5.021518s - Loss 0.134048 - Accuracy 0.703125 - Test Loss 0.695973 - Test Accuracy 0.584881\n",
      "Epoch 51/300 - 5.033467s - Loss 0.092730 - Accuracy 0.765625 - Test Loss 0.730197 - Test Accuracy 0.585323\n",
      "Epoch 52/300 - 5.022534s - Loss 0.127959 - Accuracy 0.739583 - Test Loss 0.652038 - Test Accuracy 0.587533\n",
      "Epoch 53/300 - 5.031513s - Loss 0.110198 - Accuracy 0.776042 - Test Loss 0.698391 - Test Accuracy 0.592396\n",
      "Epoch 54/300 - 5.027574s - Loss 0.092874 - Accuracy 0.796875 - Test Loss 0.690537 - Test Accuracy 0.591954\n",
      "Epoch 55/300 - 5.044329s - Loss 0.154281 - Accuracy 0.770833 - Test Loss 0.755720 - Test Accuracy 0.592838\n",
      "Epoch 56/300 - 5.031977s - Loss 0.140361 - Accuracy 0.760417 - Test Loss 0.704352 - Test Accuracy 0.590186\n",
      "Epoch 57/300 - 5.039329s - Loss 0.091753 - Accuracy 0.812500 - Test Loss 0.825853 - Test Accuracy 0.596817\n",
      "-----***-----\n",
      "0.596816976127321\n",
      "Epoch 58/300 - 5.031286s - Loss 0.077587 - Accuracy 0.869792 - Test Loss 0.814020 - Test Accuracy 0.593280\n",
      "Epoch 59/300 - 5.032347s - Loss 0.110087 - Accuracy 0.796875 - Test Loss 0.690732 - Test Accuracy 0.590186\n",
      "Epoch 60/300 - 5.034112s - Loss 0.082425 - Accuracy 0.812500 - Test Loss 0.729430 - Test Accuracy 0.596817\n",
      "Epoch 61/300 - 5.025124s - Loss 0.123933 - Accuracy 0.776042 - Test Loss 0.802330 - Test Accuracy 0.593722\n",
      "Epoch 62/300 - 5.032711s - Loss 0.074161 - Accuracy 0.817708 - Test Loss 0.820669 - Test Accuracy 0.598585\n",
      "-----***-----\n",
      "0.5985853227232537\n",
      "Epoch 63/300 - 5.048937s - Loss 0.075823 - Accuracy 0.864583 - Test Loss 0.802846 - Test Accuracy 0.598585\n",
      "Epoch 64/300 - 5.033093s - Loss 0.091102 - Accuracy 0.817708 - Test Loss 0.840869 - Test Accuracy 0.594607\n",
      "Epoch 65/300 - 5.030138s - Loss 0.089377 - Accuracy 0.822917 - Test Loss 0.840252 - Test Accuracy 0.595049\n",
      "Epoch 66/300 - 5.028085s - Loss 0.070512 - Accuracy 0.848958 - Test Loss 0.897554 - Test Accuracy 0.600354\n",
      "-----***-----\n",
      "0.6003536693191865\n",
      "Epoch 67/300 - 5.035999s - Loss 0.103689 - Accuracy 0.770833 - Test Loss 0.896542 - Test Accuracy 0.605659\n",
      "-----***-----\n",
      "0.6056587091069849\n",
      "Epoch 68/300 - 5.046523s - Loss 0.087742 - Accuracy 0.822917 - Test Loss 0.905901 - Test Accuracy 0.588859\n",
      "Epoch 69/300 - 5.032735s - Loss 0.094094 - Accuracy 0.776042 - Test Loss 1.010931 - Test Accuracy 0.595933\n",
      "Epoch 70/300 - 5.027218s - Loss 0.097130 - Accuracy 0.770833 - Test Loss 0.880725 - Test Accuracy 0.595049\n",
      "Epoch 71/300 - 5.028358s - Loss 0.091813 - Accuracy 0.859375 - Test Loss 1.031312 - Test Accuracy 0.590186\n",
      "Epoch 72/300 - 5.055209s - Loss 0.080001 - Accuracy 0.854167 - Test Loss 0.992522 - Test Accuracy 0.598143\n",
      "Epoch 73/300 - 5.040982s - Loss 0.084896 - Accuracy 0.833333 - Test Loss 0.957852 - Test Accuracy 0.587091\n",
      "Epoch 74/300 - 5.027437s - Loss 0.061903 - Accuracy 0.869792 - Test Loss 0.936372 - Test Accuracy 0.598143\n",
      "Epoch 75/300 - 5.040467s - Loss 0.098163 - Accuracy 0.838542 - Test Loss 0.963990 - Test Accuracy 0.591954\n",
      "Epoch 76/300 - 5.031205s - Loss 0.068923 - Accuracy 0.848958 - Test Loss 0.970988 - Test Accuracy 0.595933\n",
      "Epoch 77/300 - 5.040568s - Loss 0.060268 - Accuracy 0.895833 - Test Loss 0.898604 - Test Accuracy 0.597259\n",
      "Epoch 78/300 - 5.031228s - Loss 0.061333 - Accuracy 0.869792 - Test Loss 1.006188 - Test Accuracy 0.602564\n",
      "Epoch 79/300 - 5.023766s - Loss 0.067544 - Accuracy 0.864583 - Test Loss 0.918560 - Test Accuracy 0.599912\n",
      "Epoch 80/300 - 5.019983s - Loss 0.064287 - Accuracy 0.869792 - Test Loss 0.993499 - Test Accuracy 0.591954\n",
      "Epoch 81/300 - 5.048647s - Loss 0.044864 - Accuracy 0.901042 - Test Loss 1.010154 - Test Accuracy 0.602564\n",
      "Epoch 82/300 - 5.030316s - Loss 0.092729 - Accuracy 0.869792 - Test Loss 1.060890 - Test Accuracy 0.598143\n",
      "Epoch 83/300 - 5.038013s - Loss 0.089263 - Accuracy 0.895833 - Test Loss 1.124840 - Test Accuracy 0.594607\n",
      "Epoch 84/300 - 5.027886s - Loss 0.051429 - Accuracy 0.906250 - Test Loss 1.073859 - Test Accuracy 0.591070\n",
      "Epoch 85/300 - 5.026230s - Loss 0.074652 - Accuracy 0.838542 - Test Loss 1.105915 - Test Accuracy 0.597259\n",
      "Epoch 86/300 - 5.016085s - Loss 0.071132 - Accuracy 0.880208 - Test Loss 1.038395 - Test Accuracy 0.598143\n",
      "Epoch 87/300 - 5.038447s - Loss 0.053704 - Accuracy 0.875000 - Test Loss 1.109706 - Test Accuracy 0.596817\n",
      "Epoch 88/300 - 5.026200s - Loss 0.070142 - Accuracy 0.869792 - Test Loss 1.142600 - Test Accuracy 0.596375\n",
      "Epoch 89/300 - 5.028879s - Loss 0.068747 - Accuracy 0.838542 - Test Loss 1.097395 - Test Accuracy 0.595491\n",
      "Epoch 90/300 - 5.031444s - Loss 0.061843 - Accuracy 0.916667 - Test Loss 1.135259 - Test Accuracy 0.598585\n",
      "Epoch 91/300 - 5.033194s - Loss 0.055647 - Accuracy 0.890625 - Test Loss 1.095905 - Test Accuracy 0.595049\n",
      "Epoch 92/300 - 5.017442s - Loss 0.058176 - Accuracy 0.890625 - Test Loss 1.136602 - Test Accuracy 0.596375\n",
      "Epoch 93/300 - 5.045472s - Loss 0.072558 - Accuracy 0.864583 - Test Loss 1.060206 - Test Accuracy 0.597701\n",
      "Epoch 94/300 - 5.032547s - Loss 0.070762 - Accuracy 0.828125 - Test Loss 1.208144 - Test Accuracy 0.604775\n",
      "Epoch 95/300 - 5.038699s - Loss 0.079859 - Accuracy 0.864583 - Test Loss 1.363194 - Test Accuracy 0.601680\n",
      "Epoch 96/300 - 5.034125s - Loss 0.060044 - Accuracy 0.885417 - Test Loss 1.145656 - Test Accuracy 0.598143\n",
      "Epoch 97/300 - 5.033570s - Loss 0.071827 - Accuracy 0.833333 - Test Loss 1.116640 - Test Accuracy 0.600354\n",
      "Epoch 98/300 - 5.022805s - Loss 0.048607 - Accuracy 0.906250 - Test Loss 1.178451 - Test Accuracy 0.593722\n",
      "Epoch 99/300 - 5.043980s - Loss 0.064708 - Accuracy 0.880208 - Test Loss 1.163198 - Test Accuracy 0.598143\n",
      "Epoch 100/300 - 5.033339s - Loss 0.050167 - Accuracy 0.875000 - Test Loss 1.120276 - Test Accuracy 0.596817\n",
      "Epoch 101/300 - 5.036019s - Loss 0.056086 - Accuracy 0.890625 - Test Loss 1.229721 - Test Accuracy 0.594164\n",
      "Epoch 102/300 - 5.027434s - Loss 0.066632 - Accuracy 0.859375 - Test Loss 1.191234 - Test Accuracy 0.591954\n",
      "Epoch 103/300 - 5.051725s - Loss 0.054859 - Accuracy 0.864583 - Test Loss 1.175169 - Test Accuracy 0.600354\n",
      "Epoch 104/300 - 5.027978s - Loss 0.056451 - Accuracy 0.890625 - Test Loss 1.183940 - Test Accuracy 0.596817\n",
      "Epoch 105/300 - 5.039573s - Loss 0.053377 - Accuracy 0.927083 - Test Loss 1.273386 - Test Accuracy 0.600354\n",
      "Epoch 106/300 - 5.037209s - Loss 0.024913 - Accuracy 0.947917 - Test Loss 1.150224 - Test Accuracy 0.605217\n",
      "Epoch 107/300 - 5.032080s - Loss 0.054676 - Accuracy 0.901042 - Test Loss 1.201435 - Test Accuracy 0.599469\n",
      "Epoch 108/300 - 5.040801s - Loss 0.073429 - Accuracy 0.916667 - Test Loss 1.302105 - Test Accuracy 0.591954\n",
      "Epoch 109/300 - 5.038176s - Loss 0.043484 - Accuracy 0.901042 - Test Loss 1.216324 - Test Accuracy 0.603006\n",
      "Epoch 110/300 - 5.033812s - Loss 0.091640 - Accuracy 0.885417 - Test Loss 1.408470 - Test Accuracy 0.603448\n",
      "Epoch 111/300 - 5.033650s - Loss 0.052603 - Accuracy 0.901042 - Test Loss 1.574224 - Test Accuracy 0.593280\n",
      "Epoch 112/300 - 5.025431s - Loss 0.044988 - Accuracy 0.895833 - Test Loss 1.263299 - Test Accuracy 0.597701\n",
      "Epoch 113/300 - 5.038247s - Loss 0.096383 - Accuracy 0.817708 - Test Loss 1.282259 - Test Accuracy 0.584881\n",
      "Epoch 114/300 - 5.029881s - Loss 0.077264 - Accuracy 0.880208 - Test Loss 1.342684 - Test Accuracy 0.586649\n",
      "Epoch 115/300 - 5.040215s - Loss 0.052757 - Accuracy 0.916667 - Test Loss 1.238589 - Test Accuracy 0.598143\n",
      "Epoch 116/300 - 5.033204s - Loss 0.052170 - Accuracy 0.890625 - Test Loss 1.427212 - Test Accuracy 0.595049\n",
      "Epoch 117/300 - 5.051633s - Loss 0.057198 - Accuracy 0.843750 - Test Loss 1.182683 - Test Accuracy 0.594607\n",
      "Epoch 118/300 - 5.040300s - Loss 0.055333 - Accuracy 0.901042 - Test Loss 1.326080 - Test Accuracy 0.606985\n",
      "-----***-----\n",
      "0.6069849690539346\n",
      "Epoch 119/300 - 5.036942s - Loss 0.051740 - Accuracy 0.921875 - Test Loss 1.310074 - Test Accuracy 0.599027\n",
      "Epoch 120/300 - 5.031000s - Loss 0.071085 - Accuracy 0.869792 - Test Loss 1.212672 - Test Accuracy 0.604775\n",
      "Epoch 121/300 - 5.063205s - Loss 0.073874 - Accuracy 0.869792 - Test Loss 1.289027 - Test Accuracy 0.598143\n",
      "Epoch 122/300 - 5.040195s - Loss 0.076197 - Accuracy 0.828125 - Test Loss 1.357465 - Test Accuracy 0.598585\n",
      "Epoch 123/300 - 5.051148s - Loss 0.033440 - Accuracy 0.932292 - Test Loss 1.278895 - Test Accuracy 0.602122\n",
      "Epoch 124/300 - 5.036144s - Loss 0.048824 - Accuracy 0.916667 - Test Loss 1.421330 - Test Accuracy 0.593722\n",
      "Epoch 125/300 - 5.035324s - Loss 0.103337 - Accuracy 0.796875 - Test Loss 1.266345 - Test Accuracy 0.595933\n",
      "Epoch 126/300 - 5.042423s - Loss 0.069072 - Accuracy 0.864583 - Test Loss 1.365268 - Test Accuracy 0.593722\n",
      "Epoch 127/300 - 5.042887s - Loss 0.057101 - Accuracy 0.916667 - Test Loss 1.271730 - Test Accuracy 0.602122\n",
      "Epoch 128/300 - 5.050170s - Loss 0.080264 - Accuracy 0.906250 - Test Loss 1.573992 - Test Accuracy 0.602564\n",
      "Epoch 129/300 - 5.039761s - Loss 0.068937 - Accuracy 0.921875 - Test Loss 1.519442 - Test Accuracy 0.594164\n",
      "Epoch 130/300 - 5.049099s - Loss 0.113599 - Accuracy 0.854167 - Test Loss 1.289468 - Test Accuracy 0.599912\n",
      "Epoch 131/300 - 5.031538s - Loss 0.055192 - Accuracy 0.932292 - Test Loss 1.458029 - Test Accuracy 0.597259\n",
      "Epoch 132/300 - 5.046228s - Loss 0.059035 - Accuracy 0.927083 - Test Loss 1.450228 - Test Accuracy 0.603448\n",
      "Epoch 133/300 - 5.031256s - Loss 0.081625 - Accuracy 0.885417 - Test Loss 1.493224 - Test Accuracy 0.595933\n",
      "Epoch 134/300 - 5.072105s - Loss 0.090181 - Accuracy 0.901042 - Test Loss 1.446681 - Test Accuracy 0.599027\n",
      "Epoch 135/300 - 5.100084s - Loss 0.059921 - Accuracy 0.885417 - Test Loss 1.381835 - Test Accuracy 0.595049\n",
      "Epoch 136/300 - 5.071495s - Loss 0.057628 - Accuracy 0.875000 - Test Loss 1.349604 - Test Accuracy 0.595049\n",
      "Epoch 137/300 - 5.036273s - Loss 0.049695 - Accuracy 0.901042 - Test Loss 1.467374 - Test Accuracy 0.603890\n",
      "Epoch 138/300 - 5.063179s - Loss 0.076581 - Accuracy 0.906250 - Test Loss 1.441902 - Test Accuracy 0.606101\n",
      "Epoch 139/300 - 5.059827s - Loss 0.057255 - Accuracy 0.890625 - Test Loss 1.409570 - Test Accuracy 0.599469\n",
      "Epoch 140/300 - 5.068483s - Loss 0.072385 - Accuracy 0.875000 - Test Loss 1.556118 - Test Accuracy 0.599027\n",
      "Epoch 141/300 - 5.049998s - Loss 0.060083 - Accuracy 0.895833 - Test Loss 1.378802 - Test Accuracy 0.595491\n",
      "Epoch 142/300 - 5.042319s - Loss 0.078316 - Accuracy 0.869792 - Test Loss 1.534017 - Test Accuracy 0.600354\n",
      "Epoch 143/300 - 5.048706s - Loss 0.112001 - Accuracy 0.880208 - Test Loss 1.481131 - Test Accuracy 0.605217\n",
      "Epoch 144/300 - 5.035599s - Loss 0.040181 - Accuracy 0.932292 - Test Loss 1.548853 - Test Accuracy 0.600796\n",
      "Epoch 145/300 - 5.029670s - Loss 0.079940 - Accuracy 0.901042 - Test Loss 1.683187 - Test Accuracy 0.592838\n",
      "Epoch 146/300 - 5.048695s - Loss 0.039753 - Accuracy 0.901042 - Test Loss 1.651180 - Test Accuracy 0.595049\n",
      "Epoch 147/300 - 5.044064s - Loss 0.048444 - Accuracy 0.921875 - Test Loss 1.506359 - Test Accuracy 0.598585\n",
      "Epoch 148/300 - 5.047482s - Loss 0.076360 - Accuracy 0.848958 - Test Loss 1.452731 - Test Accuracy 0.599469\n",
      "Epoch 149/300 - 5.039728s - Loss 0.042688 - Accuracy 0.921875 - Test Loss 1.554243 - Test Accuracy 0.597701\n",
      "Epoch 150/300 - 5.042519s - Loss 0.054409 - Accuracy 0.848958 - Test Loss 1.683206 - Test Accuracy 0.594164\n",
      "Epoch 151/300 - 5.081510s - Loss 0.046417 - Accuracy 0.921875 - Test Loss 1.661646 - Test Accuracy 0.600796\n",
      "Epoch 152/300 - 5.046327s - Loss 0.028640 - Accuracy 0.942708 - Test Loss 1.723971 - Test Accuracy 0.605217\n",
      "Epoch 153/300 - 5.034386s - Loss 0.054843 - Accuracy 0.901042 - Test Loss 1.555830 - Test Accuracy 0.599912\n",
      "Epoch 154/300 - 5.043763s - Loss 0.053639 - Accuracy 0.885417 - Test Loss 1.645843 - Test Accuracy 0.590186\n",
      "Epoch 155/300 - 5.037817s - Loss 0.062754 - Accuracy 0.890625 - Test Loss 1.654583 - Test Accuracy 0.596375\n",
      "Epoch 156/300 - 5.055410s - Loss 0.064390 - Accuracy 0.901042 - Test Loss 1.643238 - Test Accuracy 0.591070\n",
      "Epoch 157/300 - 5.054832s - Loss 0.045046 - Accuracy 0.895833 - Test Loss 1.889898 - Test Accuracy 0.600354\n",
      "Epoch 158/300 - 5.067203s - Loss 0.061469 - Accuracy 0.932292 - Test Loss 1.858436 - Test Accuracy 0.601680\n",
      "Epoch 159/300 - 5.035367s - Loss 0.026569 - Accuracy 0.942708 - Test Loss 1.718941 - Test Accuracy 0.590186\n",
      "Epoch 160/300 - 5.031055s - Loss 0.036366 - Accuracy 0.942708 - Test Loss 1.543199 - Test Accuracy 0.602122\n",
      "Epoch 161/300 - 5.039585s - Loss 0.067528 - Accuracy 0.927083 - Test Loss 1.729519 - Test Accuracy 0.599912\n",
      "Epoch 162/300 - 5.041570s - Loss 0.063678 - Accuracy 0.921875 - Test Loss 1.711042 - Test Accuracy 0.599027\n",
      "Epoch 163/300 - 5.043314s - Loss 0.075514 - Accuracy 0.880208 - Test Loss 1.688136 - Test Accuracy 0.595049\n",
      "Epoch 164/300 - 5.046310s - Loss 0.088438 - Accuracy 0.885417 - Test Loss 1.762263 - Test Accuracy 0.594164\n",
      "Epoch 165/300 - 5.038233s - Loss 0.102959 - Accuracy 0.890625 - Test Loss 1.928734 - Test Accuracy 0.595933\n",
      "Epoch 166/300 - 5.039146s - Loss 0.119307 - Accuracy 0.833333 - Test Loss 1.519961 - Test Accuracy 0.601680\n",
      "Epoch 167/300 - 5.056695s - Loss 0.056848 - Accuracy 0.895833 - Test Loss 1.654425 - Test Accuracy 0.593722\n",
      "Epoch 168/300 - 5.046440s - Loss 0.075419 - Accuracy 0.875000 - Test Loss 1.740089 - Test Accuracy 0.593280\n",
      "Epoch 169/300 - 5.041911s - Loss 0.096885 - Accuracy 0.875000 - Test Loss 1.850312 - Test Accuracy 0.600796\n",
      "Epoch 170/300 - 5.047151s - Loss 0.067630 - Accuracy 0.921875 - Test Loss 1.653586 - Test Accuracy 0.597259\n",
      "Epoch 171/300 - 5.045317s - Loss 0.071155 - Accuracy 0.890625 - Test Loss 1.700270 - Test Accuracy 0.592838\n",
      "Epoch 172/300 - 5.054249s - Loss 0.048985 - Accuracy 0.901042 - Test Loss 1.676980 - Test Accuracy 0.594607\n",
      "Epoch 173/300 - 5.040568s - Loss 0.026185 - Accuracy 0.937500 - Test Loss 1.590789 - Test Accuracy 0.603890\n",
      "Epoch 174/300 - 5.045302s - Loss 0.062800 - Accuracy 0.947917 - Test Loss 1.972654 - Test Accuracy 0.590628\n",
      "Epoch 175/300 - 5.049534s - Loss 0.040843 - Accuracy 0.916667 - Test Loss 1.697326 - Test Accuracy 0.599469\n",
      "Epoch 176/300 - 5.042316s - Loss 0.083425 - Accuracy 0.864583 - Test Loss 1.711094 - Test Accuracy 0.600796\n",
      "Epoch 177/300 - 5.033633s - Loss 0.047080 - Accuracy 0.911458 - Test Loss 1.518326 - Test Accuracy 0.597701\n",
      "Epoch 178/300 - 5.030044s - Loss 0.054968 - Accuracy 0.911458 - Test Loss 1.692809 - Test Accuracy 0.600796\n",
      "Epoch 179/300 - 5.029464s - Loss 0.050319 - Accuracy 0.911458 - Test Loss 1.678005 - Test Accuracy 0.594164\n",
      "Epoch 180/300 - 5.035557s - Loss 0.050414 - Accuracy 0.916667 - Test Loss 1.748349 - Test Accuracy 0.603006\n",
      "Epoch 181/300 - 5.020345s - Loss 0.053687 - Accuracy 0.890625 - Test Loss 1.775347 - Test Accuracy 0.603006\n",
      "Epoch 182/300 - 5.055253s - Loss 0.051959 - Accuracy 0.880208 - Test Loss 1.827642 - Test Accuracy 0.599912\n",
      "Epoch 183/300 - 5.040488s - Loss 0.034785 - Accuracy 0.911458 - Test Loss 1.821940 - Test Accuracy 0.604775\n",
      "Epoch 184/300 - 5.043628s - Loss 0.047439 - Accuracy 0.921875 - Test Loss 1.776720 - Test Accuracy 0.602564\n",
      "Epoch 185/300 - 5.050626s - Loss 0.058298 - Accuracy 0.911458 - Test Loss 1.798698 - Test Accuracy 0.599912\n",
      "Epoch 186/300 - 5.042427s - Loss 0.055461 - Accuracy 0.927083 - Test Loss 1.884873 - Test Accuracy 0.598585\n",
      "Epoch 187/300 - 5.042180s - Loss 0.046835 - Accuracy 0.906250 - Test Loss 1.923072 - Test Accuracy 0.595491\n",
      "Epoch 188/300 - 5.044929s - Loss 0.040273 - Accuracy 0.937500 - Test Loss 2.067902 - Test Accuracy 0.598143\n",
      "Epoch 189/300 - 5.028791s - Loss 0.032522 - Accuracy 0.968750 - Test Loss 2.078443 - Test Accuracy 0.592396\n",
      "Epoch 190/300 - 5.039380s - Loss 0.035953 - Accuracy 0.921875 - Test Loss 2.013224 - Test Accuracy 0.595933\n",
      "Epoch 191/300 - 5.034985s - Loss 0.071979 - Accuracy 0.864583 - Test Loss 1.815753 - Test Accuracy 0.591954\n",
      "Epoch 192/300 - 5.041005s - Loss 0.039557 - Accuracy 0.911458 - Test Loss 1.852191 - Test Accuracy 0.594164\n",
      "Epoch 193/300 - 5.030590s - Loss 0.077222 - Accuracy 0.880208 - Test Loss 1.785145 - Test Accuracy 0.594164\n",
      "Epoch 194/300 - 5.049680s - Loss 0.056454 - Accuracy 0.906250 - Test Loss 1.871249 - Test Accuracy 0.591512\n",
      "Epoch 195/300 - 5.033512s - Loss 0.079944 - Accuracy 0.885417 - Test Loss 1.721486 - Test Accuracy 0.598585\n",
      "Epoch 196/300 - 5.051065s - Loss 0.056154 - Accuracy 0.932292 - Test Loss 1.797654 - Test Accuracy 0.593722\n",
      "Epoch 197/300 - 5.037014s - Loss 0.064401 - Accuracy 0.885417 - Test Loss 1.996930 - Test Accuracy 0.601238\n",
      "Epoch 198/300 - 5.043059s - Loss 0.030870 - Accuracy 0.921875 - Test Loss 2.008050 - Test Accuracy 0.602122\n",
      "Epoch 199/300 - 5.055809s - Loss 0.055664 - Accuracy 0.906250 - Test Loss 2.011808 - Test Accuracy 0.592396\n",
      "Epoch 200/300 - 5.064564s - Loss 0.035784 - Accuracy 0.921875 - Test Loss 1.913061 - Test Accuracy 0.595933\n",
      "Epoch 201/300 - 5.025600s - Loss 0.041378 - Accuracy 0.937500 - Test Loss 1.781131 - Test Accuracy 0.603448\n",
      "Epoch 202/300 - 5.048990s - Loss 0.024521 - Accuracy 0.942708 - Test Loss 1.957607 - Test Accuracy 0.595491\n",
      "Epoch 203/300 - 5.056239s - Loss 0.057342 - Accuracy 0.890625 - Test Loss 1.905238 - Test Accuracy 0.595049\n",
      "Epoch 204/300 - 5.037799s - Loss 0.065605 - Accuracy 0.890625 - Test Loss 1.762491 - Test Accuracy 0.591512\n",
      "Epoch 205/300 - 5.036644s - Loss 0.025361 - Accuracy 0.963542 - Test Loss 1.880442 - Test Accuracy 0.598585\n",
      "Epoch 206/300 - 5.038586s - Loss 0.079097 - Accuracy 0.921875 - Test Loss 2.110554 - Test Accuracy 0.595049\n",
      "Epoch 207/300 - 5.055746s - Loss 0.044372 - Accuracy 0.911458 - Test Loss 1.853398 - Test Accuracy 0.596375\n",
      "Epoch 208/300 - 5.041069s - Loss 0.060949 - Accuracy 0.890625 - Test Loss 1.779924 - Test Accuracy 0.598143\n",
      "Epoch 209/300 - 5.030136s - Loss 0.045605 - Accuracy 0.916667 - Test Loss 1.996623 - Test Accuracy 0.599469\n",
      "Epoch 210/300 - 5.041790s - Loss 0.059830 - Accuracy 0.921875 - Test Loss 2.038062 - Test Accuracy 0.599912\n",
      "Epoch 211/300 - 5.036341s - Loss 0.035764 - Accuracy 0.911458 - Test Loss 1.830705 - Test Accuracy 0.593280\n",
      "Epoch 212/300 - 5.045254s - Loss 0.060794 - Accuracy 0.901042 - Test Loss 1.761764 - Test Accuracy 0.600796\n",
      "Epoch 213/300 - 5.039453s - Loss 0.062732 - Accuracy 0.932292 - Test Loss 1.826060 - Test Accuracy 0.603006\n",
      "Epoch 214/300 - 5.043888s - Loss 0.087670 - Accuracy 0.916667 - Test Loss 1.908341 - Test Accuracy 0.599027\n",
      "Epoch 215/300 - 5.036291s - Loss 0.046424 - Accuracy 0.911458 - Test Loss 1.992892 - Test Accuracy 0.596375\n",
      "Epoch 216/300 - 5.031295s - Loss 0.063720 - Accuracy 0.921875 - Test Loss 1.807486 - Test Accuracy 0.584439\n",
      "Epoch 217/300 - 5.029889s - Loss 0.104333 - Accuracy 0.838542 - Test Loss 1.837119 - Test Accuracy 0.592396\n",
      "Epoch 218/300 - 5.036155s - Loss 0.049214 - Accuracy 0.916667 - Test Loss 1.773326 - Test Accuracy 0.595491\n",
      "Epoch 219/300 - 5.044193s - Loss 0.030984 - Accuracy 0.937500 - Test Loss 2.043382 - Test Accuracy 0.600354\n",
      "Epoch 220/300 - 5.046060s - Loss 0.036665 - Accuracy 0.895833 - Test Loss 1.854993 - Test Accuracy 0.598585\n",
      "Epoch 221/300 - 5.033240s - Loss 0.053297 - Accuracy 0.937500 - Test Loss 1.986133 - Test Accuracy 0.598585\n",
      "Epoch 222/300 - 5.045534s - Loss 0.037575 - Accuracy 0.916667 - Test Loss 2.013000 - Test Accuracy 0.592838\n",
      "Epoch 223/300 - 5.031348s - Loss 0.086628 - Accuracy 0.885417 - Test Loss 1.977251 - Test Accuracy 0.596817\n",
      "Epoch 224/300 - 5.040423s - Loss 0.023777 - Accuracy 0.927083 - Test Loss 2.126266 - Test Accuracy 0.601238\n",
      "Epoch 225/300 - 5.034541s - Loss 0.053370 - Accuracy 0.937500 - Test Loss 1.967856 - Test Accuracy 0.593722\n",
      "Epoch 226/300 - 5.050102s - Loss 0.038098 - Accuracy 0.942708 - Test Loss 1.978603 - Test Accuracy 0.594164\n",
      "Epoch 227/300 - 5.034303s - Loss 0.028310 - Accuracy 0.947917 - Test Loss 1.973025 - Test Accuracy 0.594607\n",
      "Epoch 228/300 - 5.045858s - Loss 0.034487 - Accuracy 0.932292 - Test Loss 2.405709 - Test Accuracy 0.594607\n",
      "Epoch 229/300 - 5.037993s - Loss 0.054330 - Accuracy 0.921875 - Test Loss 1.984913 - Test Accuracy 0.600354\n",
      "Epoch 230/300 - 5.045018s - Loss 0.043995 - Accuracy 0.921875 - Test Loss 1.925492 - Test Accuracy 0.587533\n",
      "Epoch 231/300 - 5.031405s - Loss 0.044160 - Accuracy 0.927083 - Test Loss 2.011749 - Test Accuracy 0.595049\n",
      "Epoch 232/300 - 5.042342s - Loss 0.038589 - Accuracy 0.947917 - Test Loss 2.273226 - Test Accuracy 0.584439\n",
      "Epoch 233/300 - 5.035034s - Loss 0.031565 - Accuracy 0.942708 - Test Loss 1.989086 - Test Accuracy 0.603448\n",
      "Epoch 234/300 - 5.053291s - Loss 0.057132 - Accuracy 0.880208 - Test Loss 2.103001 - Test Accuracy 0.599469\n",
      "Epoch 235/300 - 5.037587s - Loss 0.054549 - Accuracy 0.911458 - Test Loss 2.151772 - Test Accuracy 0.595049\n",
      "Epoch 236/300 - 5.034334s - Loss 0.052355 - Accuracy 0.916667 - Test Loss 1.970680 - Test Accuracy 0.587975\n",
      "Epoch 237/300 - 5.028737s - Loss 0.034897 - Accuracy 0.916667 - Test Loss 2.055632 - Test Accuracy 0.594607\n",
      "Epoch 238/300 - 5.029462s - Loss 0.046386 - Accuracy 0.901042 - Test Loss 1.856631 - Test Accuracy 0.594164\n",
      "Epoch 239/300 - 5.030378s - Loss 0.017555 - Accuracy 0.963542 - Test Loss 2.050716 - Test Accuracy 0.595049\n",
      "Epoch 240/300 - 5.024080s - Loss 0.048467 - Accuracy 0.942708 - Test Loss 1.860116 - Test Accuracy 0.601680\n",
      "Epoch 241/300 - 5.033357s - Loss 0.047843 - Accuracy 0.942708 - Test Loss 1.960639 - Test Accuracy 0.599469\n",
      "Epoch 242/300 - 5.036123s - Loss 0.061187 - Accuracy 0.921875 - Test Loss 1.931600 - Test Accuracy 0.601680\n",
      "Epoch 243/300 - 5.039331s - Loss 0.022037 - Accuracy 0.942708 - Test Loss 2.085608 - Test Accuracy 0.595491\n",
      "Epoch 244/300 - 5.031448s - Loss 0.036211 - Accuracy 0.927083 - Test Loss 2.378119 - Test Accuracy 0.592838\n",
      "Epoch 245/300 - 5.037944s - Loss 0.044645 - Accuracy 0.937500 - Test Loss 1.942076 - Test Accuracy 0.595933\n",
      "Epoch 246/300 - 5.039137s - Loss 0.056653 - Accuracy 0.916667 - Test Loss 1.908923 - Test Accuracy 0.599912\n",
      "Epoch 247/300 - 5.058441s - Loss 0.050675 - Accuracy 0.916667 - Test Loss 1.758466 - Test Accuracy 0.598143\n",
      "Epoch 248/300 - 5.040301s - Loss 0.034828 - Accuracy 0.947917 - Test Loss 2.296931 - Test Accuracy 0.596817\n",
      "Epoch 249/300 - 5.057652s - Loss 0.062404 - Accuracy 0.895833 - Test Loss 1.915563 - Test Accuracy 0.601680\n",
      "Epoch 250/300 - 5.039228s - Loss 0.035162 - Accuracy 0.947917 - Test Loss 2.003250 - Test Accuracy 0.597259\n",
      "Epoch 251/300 - 5.066020s - Loss 0.040371 - Accuracy 0.921875 - Test Loss 1.957380 - Test Accuracy 0.594607\n",
      "Epoch 252/300 - 5.048322s - Loss 0.033415 - Accuracy 0.921875 - Test Loss 1.726305 - Test Accuracy 0.607869\n",
      "-----***-----\n",
      "0.6078691423519009\n",
      "Epoch 253/300 - 5.262875s - Loss 0.034725 - Accuracy 0.901042 - Test Loss 1.966342 - Test Accuracy 0.599912\n",
      "Epoch 254/300 - 5.036351s - Loss 0.050684 - Accuracy 0.942708 - Test Loss 2.023476 - Test Accuracy 0.603006\n",
      "Epoch 255/300 - 5.037446s - Loss 0.046964 - Accuracy 0.906250 - Test Loss 1.984520 - Test Accuracy 0.595049\n",
      "Epoch 256/300 - 5.060037s - Loss 0.047227 - Accuracy 0.932292 - Test Loss 2.289363 - Test Accuracy 0.605217\n",
      "Epoch 257/300 - 5.043203s - Loss 0.047160 - Accuracy 0.906250 - Test Loss 1.743747 - Test Accuracy 0.588417\n",
      "Epoch 258/300 - 5.040487s - Loss 0.043480 - Accuracy 0.911458 - Test Loss 2.114709 - Test Accuracy 0.603890\n",
      "Epoch 259/300 - 5.056311s - Loss 0.047084 - Accuracy 0.906250 - Test Loss 1.968718 - Test Accuracy 0.595491\n",
      "Epoch 260/300 - 5.037878s - Loss 0.035699 - Accuracy 0.953125 - Test Loss 2.087465 - Test Accuracy 0.596817\n",
      "Epoch 261/300 - 5.044765s - Loss 0.078481 - Accuracy 0.906250 - Test Loss 1.936711 - Test Accuracy 0.594164\n",
      "Epoch 262/300 - 5.045146s - Loss 0.038667 - Accuracy 0.953125 - Test Loss 2.132056 - Test Accuracy 0.601238\n",
      "Epoch 263/300 - 5.040070s - Loss 0.045710 - Accuracy 0.916667 - Test Loss 1.917135 - Test Accuracy 0.605659\n",
      "Epoch 264/300 - 5.039716s - Loss 0.039741 - Accuracy 0.916667 - Test Loss 2.100341 - Test Accuracy 0.598585\n",
      "Epoch 265/300 - 5.045784s - Loss 0.039351 - Accuracy 0.911458 - Test Loss 2.105111 - Test Accuracy 0.599469\n",
      "Epoch 266/300 - 5.038699s - Loss 0.024818 - Accuracy 0.958333 - Test Loss 2.347313 - Test Accuracy 0.596375\n",
      "Epoch 267/300 - 5.042315s - Loss 0.035203 - Accuracy 0.942708 - Test Loss 2.204959 - Test Accuracy 0.595049\n",
      "Epoch 268/300 - 5.033202s - Loss 0.028541 - Accuracy 0.968750 - Test Loss 2.261431 - Test Accuracy 0.602122\n",
      "Epoch 269/300 - 5.048303s - Loss 0.044951 - Accuracy 0.916667 - Test Loss 2.284387 - Test Accuracy 0.596375\n",
      "Epoch 270/300 - 5.039555s - Loss 0.031537 - Accuracy 0.968750 - Test Loss 2.219066 - Test Accuracy 0.601238\n",
      "Epoch 271/300 - 5.057932s - Loss 0.032316 - Accuracy 0.932292 - Test Loss 1.913948 - Test Accuracy 0.599912\n",
      "Epoch 272/300 - 5.038718s - Loss 0.041418 - Accuracy 0.911458 - Test Loss 1.968878 - Test Accuracy 0.592396\n",
      "Epoch 273/300 - 5.035865s - Loss 0.020306 - Accuracy 0.953125 - Test Loss 1.981334 - Test Accuracy 0.603890\n",
      "Epoch 274/300 - 5.038879s - Loss 0.032591 - Accuracy 0.973958 - Test Loss 1.915495 - Test Accuracy 0.599027\n",
      "Epoch 275/300 - 5.048695s - Loss 0.033030 - Accuracy 0.932292 - Test Loss 2.018923 - Test Accuracy 0.603890\n",
      "Epoch 276/300 - 5.039290s - Loss 0.014077 - Accuracy 0.958333 - Test Loss 2.115769 - Test Accuracy 0.599912\n",
      "Epoch 277/300 - 5.039507s - Loss 0.020740 - Accuracy 0.947917 - Test Loss 2.174256 - Test Accuracy 0.599912\n",
      "Epoch 278/300 - 5.029798s - Loss 0.046603 - Accuracy 0.927083 - Test Loss 1.990779 - Test Accuracy 0.601680\n",
      "Epoch 279/300 - 5.043841s - Loss 0.051106 - Accuracy 0.947917 - Test Loss 1.922993 - Test Accuracy 0.606543\n",
      "Epoch 280/300 - 5.047287s - Loss 0.017420 - Accuracy 0.958333 - Test Loss 2.347921 - Test Accuracy 0.594607\n",
      "Epoch 281/300 - 5.054424s - Loss 0.057457 - Accuracy 0.927083 - Test Loss 1.944252 - Test Accuracy 0.593280\n",
      "Epoch 282/300 - 5.037786s - Loss 0.052991 - Accuracy 0.895833 - Test Loss 2.019443 - Test Accuracy 0.600796\n",
      "Epoch 283/300 - 5.049824s - Loss 0.051283 - Accuracy 0.916667 - Test Loss 2.059902 - Test Accuracy 0.599469\n",
      "Epoch 284/300 - 5.050609s - Loss 0.084249 - Accuracy 0.864583 - Test Loss 2.228861 - Test Accuracy 0.592838\n",
      "Epoch 285/300 - 5.044960s - Loss 0.055690 - Accuracy 0.916667 - Test Loss 2.293177 - Test Accuracy 0.594607\n",
      "Epoch 286/300 - 5.035452s - Loss 0.045855 - Accuracy 0.916667 - Test Loss 1.953835 - Test Accuracy 0.599912\n",
      "Epoch 287/300 - 5.041007s - Loss 0.047374 - Accuracy 0.927083 - Test Loss 2.197831 - Test Accuracy 0.599912\n",
      "Epoch 288/300 - 5.034723s - Loss 0.043029 - Accuracy 0.921875 - Test Loss 2.060679 - Test Accuracy 0.603890\n",
      "Epoch 289/300 - 5.036489s - Loss 0.028605 - Accuracy 0.932292 - Test Loss 1.895287 - Test Accuracy 0.597259\n",
      "Epoch 290/300 - 5.034383s - Loss 0.059069 - Accuracy 0.906250 - Test Loss 2.306155 - Test Accuracy 0.598585\n",
      "Epoch 291/300 - 5.031854s - Loss 0.030417 - Accuracy 0.953125 - Test Loss 2.321680 - Test Accuracy 0.593722\n",
      "Epoch 292/300 - 5.033232s - Loss 0.038053 - Accuracy 0.937500 - Test Loss 2.156391 - Test Accuracy 0.599027\n",
      "Epoch 293/300 - 5.056440s - Loss 0.047575 - Accuracy 0.901042 - Test Loss 2.135036 - Test Accuracy 0.592838\n",
      "Epoch 294/300 - 5.046323s - Loss 0.019583 - Accuracy 0.958333 - Test Loss 2.096238 - Test Accuracy 0.599027\n",
      "Epoch 295/300 - 5.034989s - Loss 0.029276 - Accuracy 0.958333 - Test Loss 1.907524 - Test Accuracy 0.601238\n",
      "Epoch 296/300 - 5.035039s - Loss 0.040401 - Accuracy 0.921875 - Test Loss 1.775479 - Test Accuracy 0.601680\n",
      "Epoch 297/300 - 5.034814s - Loss 0.058039 - Accuracy 0.927083 - Test Loss 1.874136 - Test Accuracy 0.606543\n",
      "Epoch 298/300 - 5.034943s - Loss 0.025135 - Accuracy 0.932292 - Test Loss 1.883329 - Test Accuracy 0.602564\n",
      "Epoch 299/300 - 5.046466s - Loss 0.044984 - Accuracy 0.942708 - Test Loss 2.098687 - Test Accuracy 0.603448\n",
      "repeat_0 [0.6078691423519009]\n",
      "{'repeat_0': {'0': {'precision': 0.23076923076923078, 'recall': 0.058823529411764705, 'f1-score': 0.09375, 'support': 102}, '1': {'precision': 0.4642857142857143, 'recall': 0.2932330827067669, 'f1-score': 0.3594470046082949, 'support': 133}, '2': {'precision': 0.32727272727272727, 'recall': 0.15517241379310345, 'f1-score': 0.2105263157894737, 'support': 116}, '3': {'precision': 0.3, 'recall': 0.1276595744680851, 'f1-score': 0.17910447761194026, 'support': 94}, '4': {'precision': 0.32727272727272727, 'recall': 0.16822429906542055, 'f1-score': 0.22222222222222218, 'support': 107}, '5': {'precision': 0.32727272727272727, 'recall': 0.16363636363636364, 'f1-score': 0.2181818181818182, 'support': 110}, '6': {'precision': 0.26666666666666666, 'recall': 0.12121212121212122, 'f1-score': 0.16666666666666666, 'support': 99}, '7': {'precision': 0.38596491228070173, 'recall': 0.2, 'f1-score': 0.26347305389221554, 'support': 110}, '8': {'precision': 0.39655172413793105, 'recall': 0.2072072072072072, 'f1-score': 0.27218934911242604, 'support': 111}, 'micro avg': {'precision': 0.35368421052631577, 'recall': 0.1710794297352342, 'f1-score': 0.2306108442004118, 'support': 982}, 'macro avg': {'precision': 0.3362284922176029, 'recall': 0.16612984350009252, 'f1-score': 0.22061787867611748, 'support': 982}, 'weighted avg': {'precision': 0.34149049027542494, 'recall': 0.1710794297352342, 'f1-score': 0.2261697114026531, 'support': 982}, 'samples avg': {'precision': 0.06800766283524903, 'recall': 0.07427055702917772, 'f1-score': 0.07007073386383732, 'support': 982}}}\n",
      "{'repeat_0': {'0': {'precision': 0.23076923076923078, 'recall': 0.058823529411764705, 'f1-score': 0.09375, 'support': 102}, '1': {'precision': 0.4642857142857143, 'recall': 0.2932330827067669, 'f1-score': 0.3594470046082949, 'support': 133}, '2': {'precision': 0.32727272727272727, 'recall': 0.15517241379310345, 'f1-score': 0.2105263157894737, 'support': 116}, '3': {'precision': 0.3, 'recall': 0.1276595744680851, 'f1-score': 0.17910447761194026, 'support': 94}, '4': {'precision': 0.32727272727272727, 'recall': 0.16822429906542055, 'f1-score': 0.22222222222222218, 'support': 107}, '5': {'precision': 0.32727272727272727, 'recall': 0.16363636363636364, 'f1-score': 0.2181818181818182, 'support': 110}, '6': {'precision': 0.26666666666666666, 'recall': 0.12121212121212122, 'f1-score': 0.16666666666666666, 'support': 99}, '7': {'precision': 0.38596491228070173, 'recall': 0.2, 'f1-score': 0.26347305389221554, 'support': 110}, '8': {'precision': 0.39655172413793105, 'recall': 0.2072072072072072, 'f1-score': 0.27218934911242604, 'support': 111}, 'micro avg': {'precision': 0.35368421052631577, 'recall': 0.1710794297352342, 'f1-score': 0.2306108442004118, 'support': 982}, 'macro avg': {'precision': 0.3362284922176029, 'recall': 0.16612984350009252, 'f1-score': 0.22061787867611748, 'support': 982}, 'weighted avg': {'precision': 0.34149049027542494, 'recall': 0.1710794297352342, 'f1-score': 0.2261697114026531, 'support': 982}, 'samples avg': {'precision': 0.06800766283524903, 'recall': 0.07427055702917772, 'f1-score': 0.07007073386383732, 'support': 982}}, 'accuracy': {'avg': 0.6078691423519009, 'std': 0.0}, 'time_train': {'avg': 1513.444905281067, 'std': 0.0}, 'time_test': {'avg': 0.47617125511169434, 'std': 0.0}, 'model': 'THAT', 'task': 'activity', 'data': {'num_users': ['0', '1', '2', '3', '4', '5'], 'wifi_band': ['2.4'], 'environment': ['classroom'], 'length': 3000}, 'nn': {'lr': 0.001, 'epoch': 300, 'batch_size': 64, 'threshold': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()           \n",
    "torch.cuda.empty_cache()  \n",
    "torch.cuda.ipc_collect()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "[file]          run.py\n",
    "[description]   run WiFi-based models and optionally save a multiclass confusion matrix\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# from preset import preset, name_run\n",
    "# from load_data import load_data_x, load_data_y, encode_data_y\n",
    "# from lstm import run_lstm, LSTMM\n",
    "# from bilstm import run_bilstm, BiLSTMM\n",
    "# from that import run_that, THAT\n",
    "# from resnet import run_resnet, ResNet18Model\n",
    "# from strf import run_strf  # if you have the ST-RF implementation\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model\",   default=preset[\"model\"],  type=str)\n",
    "    parser.add_argument(\"--task\",    default=preset[\"task\"],   type=str)\n",
    "    parser.add_argument(\"--repeat\",  default=preset[\"repeat\"], type=int)\n",
    "    parser.add_argument(\"--save_cm\", action=\"store_true\",\n",
    "                        help=\"Save a multiclass confusion matrix of the best model to PDF\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def save_multiclass_confusion_matrix(model, data_loader, device, pdf_path, num_classes):\n",
    "    \"\"\"\n",
    "    Given a model that outputs one-hot logits for a multiclass task,\n",
    "    convert to predicted classes via argmax, then plot and save a\n",
    "    num_classes × num_classes confusion matrix to pdf_path.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            # predicted class is index of max logit\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            trues = torch.argmax(yb, dim=1).cpu().numpy()\n",
    "            y_pred.extend(preds.tolist())\n",
    "            y_true.extend(trues.tolist())\n",
    "\n",
    "    labels = list(range(num_classes))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    disp.plot(ax=ax, xticks_rotation=\"vertical\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "def run():\n",
    "    args       = parse_args()\n",
    "    var_model  = args.model\n",
    "    var_task   = args.task\n",
    "    var_repeat = args.repeat\n",
    "\n",
    "    # --- Load and encode the data ---\n",
    "    data_pd_y = load_data_y(\n",
    "        preset[\"path\"][\"data_y\"],\n",
    "        var_environment=preset[\"data\"][\"environment\"],\n",
    "        var_wifi_band=preset[\"data\"][\"wifi_band\"],\n",
    "        var_num_users=preset[\"data\"][\"num_users\"]\n",
    "    )\n",
    "    labels = data_pd_y[\"label\"].tolist()\n",
    "    data_x = load_data_x(preset[\"path\"][\"data_x\"], labels)\n",
    "    data_y = encode_data_y(data_pd_y, var_task)\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(\n",
    "        data_x, data_y, test_size=0.2, shuffle=True, random_state=39\n",
    "    )\n",
    "\n",
    "    # --- Select which model runner to use ---\n",
    "    if var_model == \"ST-RF\":\n",
    "        from strf import run_strf\n",
    "        run_model = run_strf\n",
    "    elif var_model == \"LSTM\":\n",
    "        run_model = run_lstm\n",
    "    elif var_model == \"bi-LSTM\":\n",
    "        run_model = run_bilstm\n",
    "    elif var_model == \"THAT\":\n",
    "        run_model = run_that\n",
    "    elif var_model == \"ResNet18\":\n",
    "        run_model = run_resnet\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {var_model}\")\n",
    "\n",
    "    # --- Train and evaluate ---\n",
    "    print(f\"Running model: {var_model}\")\n",
    "    result = run_model(train_x, train_y, test_x, test_y, var_repeat)\n",
    "    result[\"model\"] = var_model\n",
    "    result[\"task\"]  = var_task\n",
    "    result[\"data\"]  = preset[\"data\"]\n",
    "    result[\"nn\"]    = preset[\"nn\"]\n",
    "    print(result)\n",
    "\n",
    "    # --- Save results to JSON ---\n",
    "    # with open(preset[\"path\"][\"save\"], \"w\") as f:\n",
    "    #     json.dump(result, f, indent=4)\n",
    "\n",
    "    # # --- Optionally save a multiclass confusion matrix ---\n",
    "    # # if args.save_cm:\n",
    "    # if Confusion_matrix == 1:\n",
    "    #     # 1) completely release GPU memory used for training\n",
    "    #     del run_model                      # if 'model' from training is still in scope\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     torch.cuda.ipc_collect()\n",
    "    \n",
    "    #     # 2) reshape input only if the network is sequence‑based\n",
    "    #     if var_model in (\"LSTM\", \"bi-LSTM\", \"THAT\"):\n",
    "    #         test_x_cm = test_x.reshape(test_x.shape[0], test_x.shape[1], -1)\n",
    "    #     else:                           # ResNet18, ST‑RF\n",
    "    #         test_x_cm = test_x\n",
    "    \n",
    "    #     # 3) build the *same* architecture on CPU and load its weights\n",
    "    #     device_cm = torch.device(\"cpu\")\n",
    "    #     if var_model == \"LSTM\":\n",
    "    #         model_cm = LSTMM(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     elif var_model == \"bi-LSTM\":\n",
    "    #         model_cm = BiLSTMM(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     elif var_model == \"THAT\":\n",
    "    #         model_cm = THAT(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     elif var_model == \"ResNet18\":\n",
    "    #         model_cm = ResNet18Model(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Confusion matrix not supported for {var_model}\")\n",
    "    \n",
    "    #     # best_path = f\"/kaggle/working/{name_run}_best_model.pt\"\n",
    "    #     # model_cm.load_state_dict(torch.load(best_path, map_location=device_cm))\n",
    "    #     # model_cm.eval()\n",
    "    \n",
    "    #     # 4) DataLoader on CPU with a safe batch size\n",
    "    #     test_ds = TensorDataset(torch.from_numpy(test_x_cm).float(),\n",
    "    #                             torch.from_numpy(test_y).float())\n",
    "    #     test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "    \n",
    "    #     # 5) save the confusion matrix PDF\n",
    "    #     num_classes = test_y.shape[1]\n",
    "    #     pdf_name = f\"{name_run}_confusion_matrix.pdf\"\n",
    "    #     save_multiclass_confusion_matrix(model_cm,test_loader,device_cm,pdf_name,num_classes)\n",
    "    #     print(f\"✅ Saved confusion matrix (classes 0–{num_classes-1}) to {pdf_name}\")\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"start\")\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d73c7",
   "metadata": {
    "papermill": {
     "duration": 0.019413,
     "end_time": "2025-12-29T07:59:59.281877",
     "exception": false,
     "start_time": "2025-12-29T07:59:59.262464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 12: Few-shot Learning\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "796d42b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T07:59:59.320360Z",
     "iopub.status.busy": "2025-12-29T07:59:59.320036Z",
     "iopub.status.idle": "2025-12-29T07:59:59.325752Z",
     "shell.execute_reply": "2025-12-29T07:59:59.325225Z"
    },
    "papermill": {
     "duration": 0.026236,
     "end_time": "2025-12-29T07:59:59.326739",
     "exception": false,
     "start_time": "2025-12-29T07:59:59.300503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import torch\n",
    "# import shutil\n",
    "# import json\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# gc.collect()           \n",
    "# torch.cuda.empty_cache()  \n",
    "# torch.cuda.ipc_collect()\n",
    "\n",
    "# # ---------- helper: save multiclass confusion matrix ------------------\n",
    "# def save_multiclass_confusion_matrix(model, data_loader, pdf_path, num_classes):\n",
    "#     \"\"\"\n",
    "#     Forward‑pass on CPU, collect predictions, and write an N×N confusion matrix\n",
    "#     to a single‑page PDF (pdf_path).\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     y_true, y_pred = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in data_loader:\n",
    "#             logits = model(xb.cpu())                       # ensure CPU\n",
    "#             preds  = torch.argmax(logits, dim=1).numpy()\n",
    "#             trues  = torch.argmax(yb, dim=1).numpy()\n",
    "#             y_pred.extend(preds.tolist())\n",
    "#             y_true.extend(trues.tolist())\n",
    "\n",
    "#     labels = list(range(num_classes))\n",
    "#     cm  = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "#     disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "#     fig, ax = plt.subplots(figsize=(8, 8))\n",
    "#     disp.plot(ax=ax, xticks_rotation=\"vertical\")\n",
    "#     ax.set_title(\"Few‑shot Confusion Matrix\")\n",
    "#     with PdfPages(pdf_path) as pdf:\n",
    "#         pdf.savefig(fig)\n",
    "#     plt.close(fig)\n",
    "\n",
    "# # -------------------- pick run_* function ------------------------------\n",
    "# if preset[\"model\"] == \"ST-RF\":\n",
    "#     run_model = run_strf\n",
    "# elif preset[\"model\"] == \"LSTM\":\n",
    "#     run_model = run_lstm\n",
    "# elif preset[\"model\"] == \"bi-LSTM\":\n",
    "#     run_model = run_bilstm\n",
    "# elif preset[\"model\"] == \"THAT\":\n",
    "#     run_model = run_that\n",
    "# elif preset[\"model\"] == \"ResNet18\":\n",
    "#     run_model = run_resnet\n",
    "# else:\n",
    "#     raise ValueError(f\"No few‑shot implementation for {preset['model']}.\")\n",
    "\n",
    "# # ------------------------ load / split data ----------------------------\n",
    "# data_pd_y = load_data_y(preset[\"path\"][\"data_y\"],\n",
    "#                         var_environment=[dest_env],\n",
    "#                         var_wifi_band=preset[\"data\"][\"wifi_band\"],\n",
    "#                         var_num_users=preset[\"data\"][\"num_users\"])\n",
    "\n",
    "# labels_list = data_pd_y[\"label\"].tolist()\n",
    "# data_x = load_data_x(preset[\"path\"][\"data_x\"], labels_list)\n",
    "# data_y = encode_data_y(data_pd_y, preset[\"task\"])\n",
    "\n",
    "# train_x, test_x, train_y, test_y = train_test_split(\n",
    "#     data_x, data_y, test_size=0.2, shuffle=True, random_state=39)\n",
    "\n",
    "# # Few-shot sample size\n",
    "# train_x = train_x[:few_shot_num_samples]\n",
    "# train_y = train_y[:few_shot_num_samples]\n",
    "\n",
    "# # ----------------------- few‑shot training -----------------------------\n",
    "# original_epochs = preset[\"nn\"][\"epoch\"]\n",
    "# preset[\"nn\"][\"epoch\"] = few_shot_epochs\n",
    "\n",
    "# # Load the best model weights\n",
    "# best_model_path = f\"{name_run}_best_model.pt\"\n",
    "\n",
    "# # Initialize the model \n",
    "# if preset[\"model\"] == \"LSTM\":\n",
    "#     model = LSTMM(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "#     # print('train_y_[0].shape:', train_y[0].shape)\n",
    "#     # print('train_x_[0].shape:', train_x[0].reshape(train_x[0].shape[0], -1).shape)\n",
    "# elif preset[\"model\"] == \"bi-LSTM\":\n",
    "#     model = BiLSTMM(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "# elif preset[\"model\"] == \"THAT\":\n",
    "#     model = THAT(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "# elif preset[\"model\"] == \"ResNet18\":\n",
    "#     model = ResNet18Model(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "# else:\n",
    "#     raise ValueError(f\"Model {preset['model']} not supported!\")\n",
    "\n",
    "# # Load the weights into the model\n",
    "# model.load_state_dict(torch.load(best_model_path, map_location=\"cpu\"))\n",
    "# model = model.to('cuda')\n",
    "\n",
    "# # Fine-tune the model on few-shot data (note: `run_model` should now return only the result)\n",
    "# result = run_model(train_x, train_y, test_x, test_y, var_repeat=1, init_model=model)\n",
    "# print(result)\n",
    "\n",
    "# # --------------------- save few‑shot checkpoints -----------------------\n",
    "# # After fine-tuning, save the model\n",
    "# torch.save(model.state_dict(), f\"{name_run}_fewshot_final_model.pt\")\n",
    "# torch.save(model.state_dict(), f\"{name_run}_fewshot_best_model.pt\")\n",
    "\n",
    "# # ------------------- confusion matrix on CPU ---------------------------\n",
    "# if Confusion_matrix == 1 and preset[\"model\"] != \"ST-RF\":\n",
    "\n",
    "#     # reshape for sequence models\n",
    "#     test_x_rs = (test_x.reshape(test_x.shape[0], test_x.shape[1], -1)\n",
    "#                  if preset[\"model\"] in (\"LSTM\", \"bi-LSTM\", \"THAT\") else test_x)\n",
    "\n",
    "#     # instantiate identical architecture on CPU\n",
    "#     if preset[\"model\"] == \"LSTM\":\n",
    "#         model_cpu = LSTMM(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "#     elif preset[\"model\"] == \"bi-LSTM\":\n",
    "#         model_cpu = BiLSTMM(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "#     elif preset[\"model\"] == \"THAT\":\n",
    "#         model_cpu = THAT(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "#     else:  # ResNet18\n",
    "#         model_cpu = ResNet18Model(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "\n",
    "#     # load weights\n",
    "#     model_cpu.load_state_dict(torch.load(f\"{name_run}_fewshot_best_model.pt\", map_location=\"cpu\"))\n",
    "\n",
    "#     # CPU DataLoader with a safe batch size\n",
    "#     test_ds = TensorDataset(torch.from_numpy(test_x_rs).float(),\n",
    "#                             torch.from_numpy(test_y).float())\n",
    "#     test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "#     pdf_name = f\"{name_run}_fewshot_confusion_matrix.pdf\"\n",
    "#     num_classes = test_y.shape[1]\n",
    "#     save_multiclass_confusion_matrix(model_cpu, test_loader, pdf_name, num_classes)\n",
    "#     print(f\"✅ Saved few‑shot confusion matrix (classes 0–{num_classes-1}) to {pdf_name}\")\n",
    "\n",
    "# # ----------------------- restore & persist -----------------------------\n",
    "# preset[\"nn\"][\"epoch\"] = original_epochs\n",
    "\n",
    "# # Save the final result to JSON\n",
    "# with open(\"result_fewshot.json\", \"w\") as f:\n",
    "#     json.dump(result, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "811a874e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T07:59:59.364801Z",
     "iopub.status.busy": "2025-12-29T07:59:59.364616Z",
     "iopub.status.idle": "2025-12-29T07:59:59.373609Z",
     "shell.execute_reply": "2025-12-29T07:59:59.373106Z"
    },
    "papermill": {
     "duration": 0.029406,
     "end_time": "2025-12-29T07:59:59.374689",
     "exception": false,
     "start_time": "2025-12-29T07:59:59.345283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import scipy.io as scio\n",
    "# import time\n",
    "# import torch\n",
    "# import gc\n",
    "# from numpy.linalg import svd\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "# from copy import deepcopy\n",
    "# import json\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# import torch._dynamo\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # --- تنظیمات سیستمی ---\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# # --------------------------\n",
    "# # 1. تنظیمات (Configuration)\n",
    "# # --------------------------\n",
    "# preset = {\n",
    "#     \"model\": \"THAT\",          \n",
    "#     \"task\": \"activity\",       \n",
    "#     \"repeat\": 1,\n",
    "#     \"path\": {\n",
    "#         \"data_x\": \"/kaggle/input/wimans/wifi_csi/amp\",   \n",
    "#         \"data_y\": \"/kaggle/input/wimans/annotation.csv\", \n",
    "#     },\n",
    "#     \"data\": {\n",
    "#         \"num_users\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"],  \n",
    "#         \"wifi_band\": [\"2.4\"],                         \n",
    "#         \"environment\": [\"classroom\"],                 \n",
    "#         \"length\": 3000,\n",
    "        \n",
    "#         # 1.0 = 100% data (Full run) | 0.1 = 10% data (Quick test)\n",
    "#         \"subset_ratio\": 0.5,  \n",
    "#     },\n",
    "#     \"nn\": {\n",
    "#         \"lr\": 1e-3,           \n",
    "#         \"epoch\": 80,          \n",
    "#         \"batch_size\": 32,    \n",
    "#         \"threshold\": 0.5,\n",
    "#         \"patience\": 5,        \n",
    "#         \"factor\": 0.5,        \n",
    "#         \"min_lr\": 1e-6        \n",
    "#     },\n",
    "#     \"encoding\": {\n",
    "#         \"activity\": {\n",
    "#             \"nan\":      [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#             \"nothing\":  [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#             \"walk\":     [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "#             \"rotation\": [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "#             \"jump\":     [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "#             \"wave\":     [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "#             \"lie_down\": [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "#             \"pick_up\":  [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "#             \"sit_down\": [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "#             \"stand_up\": [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "#         },\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# # --------------------------\n",
    "# # 2. توابع RPCA و لود دیتا\n",
    "# # --------------------------\n",
    "# def soft_threshold(x, epsilon):\n",
    "#     return np.maximum(np.abs(x) - epsilon, 0) * np.sign(x)\n",
    "\n",
    "# def robust_pca(M, max_iter=10, tol=1e-4):\n",
    "#     n1, n2 = M.shape\n",
    "#     lambda_param = 1 / np.sqrt(max(n1, n2))\n",
    "#     Y = M / np.maximum(np.linalg.norm(M, 2), np.linalg.norm(M, np.inf) / lambda_param)\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     mu = 1.25 / np.linalg.norm(M, 2)\n",
    "#     rho = 1.5\n",
    "#     for i in range(max_iter):\n",
    "#         temp_L = M - S + (1/mu) * Y\n",
    "#         U, Sigma, Vt = svd(temp_L, full_matrices=False)\n",
    "#         Sigma_thresh = soft_threshold(Sigma, 1/mu)\n",
    "#         L_new = np.dot(U * Sigma_thresh, Vt)\n",
    "#         temp_S = M - L_new + (1/mu) * Y\n",
    "#         S_new = soft_threshold(temp_S, lambda_param/mu)\n",
    "#         error = np.linalg.norm(M - L_new - S_new, 'fro') / np.linalg.norm(M, 'fro')\n",
    "#         L = L_new; S = S_new\n",
    "#         if error < tol: break\n",
    "#         Y = Y + mu * (M - L - S)\n",
    "#         mu = min(mu * rho, 1e7)\n",
    "#     return L, S\n",
    "\n",
    "# def load_data_y(var_path_data_y, var_environment=None, var_wifi_band=None, var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None: data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None: data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None: data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list, use_rpca=True):\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     mode_str = \"WITH RPCA\" if use_rpca else \"RAW DATA (No RPCA)\"\n",
    "#     print(f\"Loading {len(var_path_list)} samples - Mode: {mode_str}...\")\n",
    "#     for i, var_path in enumerate(var_path_list):\n",
    "#         if i % 100 == 0 and i > 0: print(f\"Processing {i}/{len(var_path_list)}...\")\n",
    "#         data_csi = np.load(var_path) \n",
    "#         data_csi_2d = data_csi.reshape(data_csi.shape[0], -1)\n",
    "#         target_len = preset[\"data\"][\"length\"]\n",
    "#         current_len = data_csi_2d.shape[0]\n",
    "#         var_pad_length = target_len - current_len\n",
    "#         if var_pad_length > 0: data_csi_pad = np.pad(data_csi_2d, ((0, var_pad_length), (0, 0)), mode='constant')\n",
    "#         else: data_csi_pad = data_csi_2d[:target_len, :]\n",
    "#         if use_rpca:\n",
    "#             L, S = robust_pca(data_csi_pad)\n",
    "#             final_sample = np.concatenate([L, S], axis=1) \n",
    "#         else:\n",
    "#             final_sample = data_csi_pad\n",
    "#         data_x.append(final_sample)\n",
    "#     data_x = np.array(data_x)\n",
    "#     return data_x\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"activity\": return encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     return encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     cols = [f\"user_{i}_activity\" for i in range(1, 7)]\n",
    "#     data = data_pd_y[cols].to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[y] for y in sample] for sample in data])\n",
    "\n",
    "# # --------------------------\n",
    "# # 3. مدل THAT\n",
    "# # --------------------------\n",
    "# class Gaussian_Position(torch.nn.Module):\n",
    "#     def __init__(self, var_dim_feature, var_dim_time, var_num_gaussian=10):\n",
    "#         super(Gaussian_Position, self).__init__()\n",
    "#         self.var_embedding = torch.nn.Parameter(torch.zeros([var_num_gaussian, var_dim_feature]), requires_grad=True)\n",
    "#         torch.nn.init.xavier_uniform_(self.var_embedding)\n",
    "#         self.var_position = torch.nn.Parameter(torch.arange(0.0, var_dim_time).unsqueeze(1).repeat(1, var_num_gaussian), requires_grad=False)\n",
    "#         self.var_mu = torch.nn.Parameter(torch.arange(0.0, var_dim_time, var_dim_time/var_num_gaussian).unsqueeze(0), requires_grad=True)\n",
    "#         self.var_sigma = torch.nn.Parameter(torch.tensor([50.0] * var_num_gaussian).unsqueeze(0), requires_grad=True)\n",
    "#     def forward(self, var_input):\n",
    "#         var_pdf = - (self.var_position - self.var_mu)**2 / (2 * self.var_sigma**2) - torch.log(self.var_sigma)\n",
    "#         var_pdf = torch.softmax(var_pdf, dim=-1)\n",
    "#         return var_input + torch.matmul(var_pdf, self.var_embedding).unsqueeze(0)\n",
    "\n",
    "# class Encoder(torch.nn.Module):\n",
    "#     def __init__(self, var_dim_feature, var_num_head=10, var_size_cnn=[1, 3, 5]):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.layer_norm_0 = torch.nn.LayerNorm(var_dim_feature, eps=1e-6)\n",
    "#         self.layer_attention = torch.nn.MultiheadAttention(var_dim_feature, var_num_head, batch_first=True)\n",
    "#         self.layer_dropout_0 = torch.nn.Dropout(0.1)\n",
    "#         self.layer_norm_1 = torch.nn.LayerNorm(var_dim_feature, 1e-6)\n",
    "#         self.layer_cnn = torch.nn.ModuleList([torch.nn.Sequential(torch.nn.Conv1d(var_dim_feature, var_dim_feature, s, padding=\"same\"), torch.nn.BatchNorm1d(var_dim_feature), torch.nn.Dropout(0.1), torch.nn.LeakyReLU()) for s in var_size_cnn])\n",
    "#         self.layer_dropout_1 = torch.nn.Dropout(0.1)\n",
    "#     def forward(self, var_input):\n",
    "#         var_t = self.layer_norm_0(var_input)\n",
    "#         var_t, _ = self.layer_attention(var_t, var_t, var_t)\n",
    "#         var_t = self.layer_dropout_0(var_t) + var_input\n",
    "#         var_s = self.layer_norm_1(var_t).permute(0, 2, 1)\n",
    "#         var_c = torch.stack([l(var_s) for l in self.layer_cnn], dim=0)\n",
    "#         var_s = self.layer_dropout_1((torch.sum(var_c, dim=0) / len(self.layer_cnn)).permute(0, 2, 1))\n",
    "#         return var_s + var_t\n",
    "\n",
    "# class THAT(torch.nn.Module):\n",
    "#     def __init__(self, var_x_shape, var_y_shape):\n",
    "#         super(THAT, self).__init__()\n",
    "#         var_dim_feature, var_dim_time = var_x_shape[-1], var_x_shape[-2]\n",
    "#         var_dim_output = var_y_shape[-1]\n",
    "#         self.layer_left_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "#         self.layer_left_gaussian = Gaussian_Position(var_dim_feature, var_dim_time // 20)\n",
    "#         self.layer_left_encoder = torch.nn.ModuleList([Encoder(var_dim_feature, 10, [1, 3, 5]) for _ in range(4)])\n",
    "#         self.layer_left_norm = torch.nn.LayerNorm(var_dim_feature, eps=1e-6)\n",
    "#         self.layer_left_cnn = torch.nn.ModuleList([torch.nn.Conv1d(var_dim_feature, 128, k) for k in [8, 16]])\n",
    "#         self.layer_left_dropout = torch.nn.Dropout(0.5)\n",
    "#         var_dim_right = var_dim_time // 20\n",
    "#         self.layer_right_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "#         self.layer_right_encoder = torch.nn.ModuleList([Encoder(var_dim_right, 10, [1, 2, 3])])\n",
    "#         self.layer_right_norm = torch.nn.LayerNorm(var_dim_right, eps=1e-6)\n",
    "#         self.layer_right_cnn = torch.nn.ModuleList([torch.nn.Conv1d(var_dim_right, 16, k) for k in [2, 4]])\n",
    "#         self.layer_right_dropout = torch.nn.Dropout(0.5)\n",
    "#         self.layer_leakyrelu = torch.nn.LeakyReLU()\n",
    "#         self.layer_output = torch.nn.Linear(256 + 32, var_dim_output)\n",
    "#     def forward(self, var_input):\n",
    "#         v_l = self.layer_left_gaussian(self.layer_left_pooling(var_input.permute(0, 2, 1)).permute(0, 2, 1))\n",
    "#         for l in self.layer_left_encoder: v_l = l(v_l)\n",
    "#         v_l = self.layer_left_norm(v_l).permute(0, 2, 1)\n",
    "#         v_l = torch.cat([torch.sum(self.layer_leakyrelu(cnn(v_l)), dim=-1) for cnn in self.layer_left_cnn], dim=-1)\n",
    "#         v_l = self.layer_left_dropout(v_l)\n",
    "#         v_r = self.layer_right_pooling(var_input.permute(0, 2, 1))\n",
    "#         for l in self.layer_right_encoder: v_r = l(v_r)\n",
    "#         v_r = self.layer_right_norm(v_r).permute(0, 2, 1)\n",
    "#         v_r = torch.cat([torch.sum(self.layer_leakyrelu(cnn(v_r)), dim=-1) for cnn in self.layer_right_cnn], dim=-1)\n",
    "#         v_r = self.layer_right_dropout(v_r)\n",
    "#         return self.layer_output(torch.cat([v_l, v_r], dim=-1))\n",
    "\n",
    "# # --------------------------\n",
    "# # 4. Training Loop\n",
    "# # --------------------------\n",
    "# def train(model, optimizer, loss_fn, train_loader, test_loader, threshold, epochs, device, model_path):\n",
    "#     best_acc = -1.0\n",
    "#     best_w = deepcopy(model.state_dict())\n",
    "    \n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#         optimizer, mode='max', factor=preset[\"nn\"][\"factor\"], patience=preset[\"nn\"][\"patience\"],\n",
    "#         min_lr=preset[\"nn\"][\"min_lr\"], verbose=True\n",
    "#     )\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         t0 = time.time()\n",
    "#         model.train()\n",
    "        \n",
    "#         # --- [MODIFIED] Using requested variable names ---\n",
    "#         for data_batch_x, data_batch_y in train_loader:\n",
    "#             data_batch_x = data_batch_x.to(device)\n",
    "#             data_batch_y = data_batch_y.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             predict_train_y = model(data_batch_x)\n",
    "            \n",
    "#             # --- [REQUESTED LINE] ---\n",
    "#             loss_value = loss_fn(predict_train_y, data_batch_y.reshape(data_batch_y.shape[0], -1).float())\n",
    "            \n",
    "#             loss_value.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             tx, ty = next(iter(test_loader))\n",
    "#             tx, ty = tx.to(device), ty.to(device)\n",
    "#             pred_t = model(tx)\n",
    "            \n",
    "#             p_cls = (torch.sigmoid(pred_t) > threshold).float().cpu().numpy()\n",
    "#             t_cls = ty.cpu().numpy()\n",
    "#             acc = accuracy_score(t_cls.reshape(-1, t_cls.shape[-1]), p_cls.reshape(-1, t_cls.shape[-1]))\n",
    "            \n",
    "#         scheduler.step(acc)\n",
    "#         current_lr = optimizer.param_groups[0]['lr']\n",
    "#         print(f\"Ep {epoch+1}/{epochs} | LR: {current_lr:.6f} | L_tr: {loss_value.item():.4f} | Acc: {acc:.4f}\")\n",
    "        \n",
    "#         if acc > best_acc:\n",
    "#             best_acc = acc\n",
    "#             best_w = deepcopy(model.state_dict())\n",
    "            \n",
    "#     torch.save(best_w, model_path)\n",
    "#     return best_w\n",
    "\n",
    "# def save_multiclass_confusion_matrix(model, data_loader, device, pdf_path, num_classes, title_text):\n",
    "#     model.eval()\n",
    "#     y_true, y_pred = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in data_loader:\n",
    "#             xb = xb.to(device)\n",
    "#             logits = model(xb) \n",
    "#             logits = logits.reshape(-1, num_classes) \n",
    "#             yb = yb.reshape(-1, num_classes)        \n",
    "#             y_pred.extend(torch.argmax(logits, dim=1).cpu().numpy().tolist())\n",
    "#             y_true.extend(torch.argmax(yb, dim=1).cpu().numpy().tolist())\n",
    "    \n",
    "#     labels = list(range(num_classes))\n",
    "#     cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "#     disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "#     fig, ax = plt.subplots(figsize=(12, 12))\n",
    "#     disp.plot(ax=ax, xticks_rotation=\"vertical\", cmap='Blues')\n",
    "#     ax.set_title(title_text)\n",
    "#     with PdfPages(pdf_path) as pdf: pdf.savefig(fig)\n",
    "#     plt.close(fig)\n",
    "\n",
    "# # --------------------------\n",
    "# # 5. اجرا\n",
    "# # --------------------------\n",
    "# def run_experiment(scenario_name, use_rpca):\n",
    "#     print(f\"\\n################################################\")\n",
    "#     print(f\"STARTING SCENARIO: {scenario_name}\")\n",
    "#     print(f\"################################################\")\n",
    "    \n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     current_run_name = f\"{preset['model']}_{preset['task']}_{scenario_name}\"\n",
    "#     model_save_path = f\"{current_run_name}_best_model.pt\"\n",
    "#     json_save_path = f\"result_{current_run_name}.json\"\n",
    "#     pdf_save_path = f\"Confusion_{current_run_name}.pdf\"\n",
    "    \n",
    "#     # 1. Load Labels\n",
    "#     data_pd_y = load_data_y(preset[\"path\"][\"data_y\"], preset[\"data\"][\"environment\"], preset[\"data\"][\"wifi_band\"], preset[\"data\"][\"num_users\"])\n",
    "    \n",
    "#     # Apply Subset Ratio\n",
    "#     subset_ratio = preset[\"data\"][\"subset_ratio\"]\n",
    "#     if subset_ratio < 1.0:\n",
    "#         data_pd_y = data_pd_y.sample(frac=subset_ratio, random_state=42).reset_index(drop=True)\n",
    "#         print(f\"*** DEBUG MODE: Using {subset_ratio*100}% of data ({len(data_pd_y)} samples) ***\")\n",
    "    \n",
    "#     # 2. Load X\n",
    "#     data_x = load_data_x(preset[\"path\"][\"data_x\"], data_pd_y[\"label\"].tolist(), use_rpca=use_rpca)\n",
    "#     data_y = encode_data_y(data_pd_y, preset[\"task\"])\n",
    "    \n",
    "#     # 3. Split\n",
    "#     train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2, shuffle=True, random_state=39)\n",
    "#     train_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "#     test_ds = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "#     train_loader = DataLoader(train_ds, batch_size=preset[\"nn\"][\"batch_size\"], shuffle=True)\n",
    "#     test_loader = DataLoader(test_ds, batch_size=len(test_ds), shuffle=False)\n",
    "    \n",
    "#     result = {\"accuracy\": []}\n",
    "    \n",
    "#     for r in range(preset[\"repeat\"]):\n",
    "#         print(f\"--- Repeat {r+1}/{preset['repeat']} ---\")\n",
    "#         torch.random.manual_seed(r + 39)\n",
    "        \n",
    "#         model = THAT(train_x[0].shape, train_y[0].reshape(-1).shape).to(device)\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=preset[\"nn\"][\"lr\"])\n",
    "#         loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "#         best_w = train(model, optimizer, loss_fn, train_loader, test_loader, \n",
    "#                        preset[\"nn\"][\"threshold\"], preset[\"nn\"][\"epoch\"], device, model_save_path)\n",
    "        \n",
    "#         model.load_state_dict(best_w)\n",
    "#         with torch.no_grad():\n",
    "#             preds = model(torch.from_numpy(test_x).to(device))\n",
    "#             preds_reshaped = (torch.sigmoid(preds) > preset[\"nn\"][\"threshold\"]).float().cpu().numpy().reshape(-1, 9)\n",
    "#             targets_reshaped = test_y.reshape(-1, 9)\n",
    "#             acc = accuracy_score(targets_reshaped, preds_reshaped)\n",
    "#             result[\"accuracy\"].append(acc)\n",
    "            \n",
    "#     print(f\"Final Accuracy ({scenario_name}): {np.mean(result['accuracy']):.4f}\")\n",
    "#     with open(json_save_path, \"w\") as f: json.dump(result, f, indent=4)\n",
    "    \n",
    "#     print(\"Generating Confusion Matrix...\")\n",
    "#     model_cm = THAT(test_x[0].shape, test_y[0].reshape(-1).shape).to(\"cpu\")\n",
    "#     model_cm.load_state_dict(torch.load(model_save_path, map_location=\"cpu\"))\n",
    "#     cm_loader = DataLoader(TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y)), batch_size=32)\n",
    "#     num_classes = test_y.shape[2] \n",
    "#     title = f\"Confusion Matrix: {scenario_name} (Acc: {np.mean(result['accuracy']):.2f} - {subset_ratio*100}% Data)\"\n",
    "#     save_multiclass_confusion_matrix(model_cm, cm_loader, \"cpu\", pdf_save_path, num_classes, title)\n",
    "    \n",
    "#     del model, model_cm, train_x, test_x, data_x\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     print(f\"Done with {scenario_name}.\\n\")\n",
    "\n",
    "# def run():\n",
    "#     scenarios = [\n",
    "#         (\"RPCA\", True),\n",
    "#         (\"RAW\", False)\n",
    "#     ]\n",
    "#     for name, rpca_flag in scenarios:\n",
    "#         run_experiment(name, rpca_flag)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a354783",
   "metadata": {
    "papermill": {
     "duration": 0.018364,
     "end_time": "2025-12-29T07:59:59.412042",
     "exception": false,
     "start_time": "2025-12-29T07:59:59.393678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4451316,
     "sourceId": 7638081,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7503373,
     "sourceId": 11934698,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1581.994443,
   "end_time": "2025-12-29T08:00:02.319593",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-29T07:33:40.325150",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
