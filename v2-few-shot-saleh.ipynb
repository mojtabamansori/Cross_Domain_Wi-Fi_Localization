{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8f485a3",
   "metadata": {
    "papermill": {
     "duration": 0.008447,
     "end_time": "2025-12-28T17:00:44.563405",
     "exception": false,
     "start_time": "2025-12-28T17:00:44.554958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 1: Library Imports\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9b2ac5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T17:00:44.579201Z",
     "iopub.status.busy": "2025-12-28T17:00:44.578935Z",
     "iopub.status.idle": "2025-12-28T17:00:55.109610Z",
     "shell.execute_reply": "2025-12-28T17:00:55.108938Z"
    },
    "papermill": {
     "duration": 10.540319,
     "end_time": "2025-12-28T17:00:55.111013",
     "exception": false,
     "start_time": "2025-12-28T17:00:44.570694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 1: Library Imports\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as scio\n",
    "import time\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch._dynamo\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# from ptflops import get_model_complexity_info\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from copy import deepcopy\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce3b5b",
   "metadata": {
    "papermill": {
     "duration": 0.006802,
     "end_time": "2025-12-28T17:00:55.125431",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.118629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 2: preset.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bce729e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T17:00:55.140612Z",
     "iopub.status.busy": "2025-12-28T17:00:55.140214Z",
     "iopub.status.idle": "2025-12-28T17:00:55.149272Z",
     "shell.execute_reply": "2025-12-28T17:00:55.148439Z"
    },
    "papermill": {
     "duration": 0.018351,
     "end_time": "2025-12-28T17:00:55.150495",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.132144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "few=empty_room,100,5,m=THAT,t=activity,epoch=1000,batch=64,environment=['classroom']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "[file]          preset.py\n",
    "[description]   default settings of WiFi-based models\n",
    "\"\"\"\n",
    "minidata_set = 1\n",
    "preset = {\n",
    "    # define model\n",
    "    \"model\": \"THAT\",  # \"ST-RF\", \"MLP\", \"LSTM\", \"CNN-1D\", \"CNN-2D\", \"CLSTM\", \"ABLSTM\", \"THAT\", \"bi-LSTM\", \"ResNet18\"\n",
    "    # define task\n",
    "    \"task\": \"activity\",  # \"identity\", \"activity\", \"location\", \"count\"\n",
    "    # number of repeated experiments\n",
    "    \"repeat\": 1,\n",
    "    # path of data\n",
    "    \"path\": {\n",
    "        # \"data_x\": \"/kaggle/input/wimans/wifi_csi/mat\",   # directory of CSI amplitude files \n",
    "        \"data_x\": \"/kaggle/input/wimans/wifi_csi/amp\",   # directory of CSI amplitude files \n",
    "        \"data_y\": \"/kaggle/input/wimans/annotation.csv\", # path of annotation file\n",
    "        \"save\": \"result_lstm_epoch=80_batchsize=32_envs=empty_room_wifiband=2.4.json\"               # path to save results\n",
    "    },\n",
    "    # data selection for experiments\n",
    "    \"data\": {\n",
    "        \"num_users\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"],  # select number(s) of users\n",
    "        \"wifi_band\": [\"2.4\"],                         # select WiFi band(s)\n",
    "        \"environment\": [\"classroom\"],                 # select environment(s) [\"classroom\"], [\"meeting_room\"], [\"empty_room\"]\n",
    "        \"length\": 3000,                               # default length of CSI\n",
    "    },\n",
    "    # hyperparameters of models\n",
    "    \"nn\": {\n",
    "        \"lr\": 1e-6,           # learning rate\n",
    "        \"epoch\": 1000,         # number of epochs\n",
    "        \"batch_size\": 64,    # batch size\n",
    "        \"threshold\": 0.5,     # threshold to binarize sigmoid outputs\n",
    "    },\n",
    "    # encoding of activities and locations\n",
    "    \"encoding\": {\n",
    "        \"activity\": {  # encoding of different activities\n",
    "            \"nan\":      [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"nothing\":  [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"walk\":     [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"rotation\": [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            \"jump\":     [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            \"wave\":     [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "            \"lie_down\": [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "            \"pick_up\":  [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "            \"sit_down\": [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "            \"stand_up\": [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        },\n",
    "        \"location\": {  # encoding of different locations\n",
    "            \"nan\":  [0, 0, 0, 0, 0],\n",
    "            \"a\":    [1, 0, 0, 0, 0],\n",
    "            \"b\":    [0, 1, 0, 0, 0],\n",
    "            \"c\":    [0, 0, 1, 0, 0],\n",
    "            \"d\":    [0, 0, 0, 1, 0],\n",
    "            \"e\":    [0, 0, 0, 0, 1],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Few-shot parameters (manually set)\n",
    "dest_env = \"empty_room\"       # Destination environment[\"classroom\"], [\"meeting_room\"], [\"empty_room\"]\n",
    "few_shot_epochs = 100         # Number of epochs for few-shot training\n",
    "few_shot_num_samples = 5     # Number of samples to use from the destination test data\n",
    "\n",
    "Confusion_matrix = 1\n",
    "\n",
    "name_run = \"few={},{},{},m={},t={},epoch={},batch={},environment={}\".format(dest_env, few_shot_epochs, few_shot_num_samples, preset[\"model\"], preset[\"task\"], preset[\"nn\"][\"epoch\"], preset[\"nn\"][\"batch_size\"], preset[\"data\"][\"environment\"])\n",
    "print(name_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e10cb3f",
   "metadata": {
    "papermill": {
     "duration": 0.006688,
     "end_time": "2025-12-28T17:00:55.164419",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.157731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 3: load_data.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1011e391",
   "metadata": {
    "papermill": {
     "duration": 0.006686,
     "end_time": "2025-12-28T17:00:55.177924",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.171238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "raw + sparse\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "982d78d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T17:00:55.192784Z",
     "iopub.status.busy": "2025-12-28T17:00:55.192562Z",
     "iopub.status.idle": "2025-12-28T17:00:55.218917Z",
     "shell.execute_reply": "2025-12-28T17:00:55.218352Z"
    },
    "papermill": {
     "duration": 0.035423,
     "end_time": "2025-12-28T17:00:55.220023",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.184600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 3: load_data.py  (AMP + RPCA sparse, and RAW + alpha*SPARSE)\n",
    "# =========================\n",
    "\"\"\"\n",
    "[file]          load_data.py\n",
    "[description]   load annotation file and CSI amplitude, and encode labels\n",
    "                (Test mode) X_input = X_raw + alpha * X_sparse\n",
    "                where X_sparse is RPCA sparse component computed per (Tx,Rx) link\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd  # Cell1 already imports, but safe here too\n",
    "\n",
    "# =========================================================\n",
    "# NEW TEST SETTINGS\n",
    "# =========================================================\n",
    "# \"raw\"            : use amplitude as-is\n",
    "# \"sparse\"         : use RPCA sparse component only\n",
    "# \"raw_plus_sparse\": raw + alpha*sparse  (your requested test)\n",
    "CSI_INPUT_MODE = \"raw_plus_sparse\"   # <-- \"raw\" / \"sparse\" / \"raw_plus_sparse\"\n",
    "\n",
    "# weight of sparse when adding to raw\n",
    "SPARSE_ALPHA = 3.0\n",
    "\n",
    "# keep amplitudes non-negative after addition (recommended)\n",
    "CLAMP_NONNEG = True\n",
    "\n",
    "# RPCA settings (IALM)\n",
    "RPCA_MAX_ITER = 60\n",
    "RPCA_TOL      = 1e-5\n",
    "RPCA_RHO      = 1.5\n",
    "RPCA_MU_INIT  = None\n",
    "\n",
    "# lambda options you asked before\n",
    "# \"classic\": 1/sqrt(max(m,n))\n",
    "# \"median\" : 1/sqrt(max(m,n)) * median(abs(M))\n",
    "# \"scaled\" : 1.2/sqrt(max(m,n))\n",
    "RPCA_LAMBDA_MODE = \"median\"   # <-- \"classic\" / \"median\" / \"scaled\"\n",
    "\n",
    "# cache for speed\n",
    "CACHE_ENABLED = True\n",
    "CACHE_ROOT = \"/kaggle/working/csi_cache_amp_raw_plus_sparse\"\n",
    "\n",
    "# debug print once to confirm amplitude/phase\n",
    "_DEBUG_PRINT_ONCE = True\n",
    "\n",
    "# expected antenna/subcarrier dims (WiMANS typical)\n",
    "TX = 3\n",
    "RX = 3\n",
    "SC = 30\n",
    "# =========================================================\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RPCA helpers (IALM)\n",
    "# =========================\n",
    "def _compute_lambda(M: np.ndarray) -> float:\n",
    "    m, n = M.shape\n",
    "    base = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "    if RPCA_LAMBDA_MODE == \"classic\":\n",
    "        return base\n",
    "\n",
    "    if RPCA_LAMBDA_MODE == \"scaled\":\n",
    "        return 1.2 * base\n",
    "\n",
    "    if RPCA_LAMBDA_MODE == \"median\":\n",
    "        med = np.median(np.abs(M))\n",
    "        med = float(med) if med > 1e-12 else 1e-12\n",
    "        return base * med\n",
    "\n",
    "    raise ValueError(f\"Unknown RPCA_LAMBDA_MODE: {RPCA_LAMBDA_MODE}\")\n",
    "\n",
    "\n",
    "def _soft_threshold(X: np.ndarray, tau: float) -> np.ndarray:\n",
    "    return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "\n",
    "def _svt(X: np.ndarray, tau: float) -> np.ndarray:\n",
    "    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    s_thr = np.maximum(s - tau, 0.0)\n",
    "    if np.all(s_thr == 0):\n",
    "        return np.zeros_like(X)\n",
    "    return (U * s_thr) @ Vt\n",
    "\n",
    "\n",
    "def _rpca_ialm(M: np.ndarray,\n",
    "              max_iter: int = 60,\n",
    "              tol: float = 1e-5,\n",
    "              rho: float = 1.5,\n",
    "              mu: float | None = None):\n",
    "    \"\"\"\n",
    "    Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "    Decompose: M = L + S\n",
    "    Returns: L, S (float32)\n",
    "    \"\"\"\n",
    "    M = M.astype(np.float64, copy=False)\n",
    "    lam = _compute_lambda(M)\n",
    "\n",
    "    if mu is None:\n",
    "        # spectral norm approx via top singular value\n",
    "        s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "        mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "    L = np.zeros_like(M)\n",
    "    S = np.zeros_like(M)\n",
    "    Y = np.zeros_like(M)\n",
    "\n",
    "    normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "        S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "        R = M - L - S\n",
    "        Y = Y + mu * R\n",
    "\n",
    "        err = np.linalg.norm(R, ord=\"fro\") / normM\n",
    "        if err < tol:\n",
    "            break\n",
    "\n",
    "        mu *= rho\n",
    "\n",
    "    return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Data shape helpers\n",
    "# =========================\n",
    "def _ensure_shape_4d_amp(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Expect amp npy to be either:\n",
    "      - (T, 3, 3, 30)\n",
    "      - (T, 270)  -> reshape to (T,3,3,30)\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim == 4:\n",
    "        return x\n",
    "    if x.ndim == 2 and x.shape[1] == TX * RX * SC:\n",
    "        T = x.shape[0]\n",
    "        return x.reshape(T, TX, RX, SC)\n",
    "    raise ValueError(f\"Unexpected AMP shape: {x.shape} (expected (T,{TX},{RX},{SC}) or (T,{TX*RX*SC}))\")\n",
    "\n",
    "\n",
    "def _debug_print_once(label: str, arr: np.ndarray):\n",
    "    global _DEBUG_PRINT_ONCE\n",
    "    if not _DEBUG_PRINT_ONCE:\n",
    "        return\n",
    "    _DEBUG_PRINT_ONCE = False\n",
    "    print(f\"\\n[DEBUG] First loaded AMP sample: {label}\")\n",
    "    print(f\"[DEBUG] shape={arr.shape}, dtype={arr.dtype}, complex={np.iscomplexobj(arr)}\")\n",
    "    if np.iscomplexobj(arr):\n",
    "        print(\"[DEBUG] ==> WARNING: This looks complex; amp folder usually should be real amplitude.\")\n",
    "    else:\n",
    "        print(\"[DEBUG] ==> Input is REAL -> amplitude-only (no true phase).\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def _cache_path(label: str, mode: str) -> str:\n",
    "    return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "def _make_sparse_component_pairwise(x4: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    x4: (T,3,3,30) real amplitude\n",
    "    Run RPCA on each (Tx,Rx) link separately:\n",
    "      M = (T,30) for each link, compute S, place back.\n",
    "    Return: S_out same shape as x4 (float32)\n",
    "    \"\"\"\n",
    "    x4 = x4.astype(np.float32, copy=False)\n",
    "    T = x4.shape[0]\n",
    "    S_out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "    for tx in range(TX):\n",
    "        for rx in range(RX):\n",
    "            M = x4[:, tx, rx, :]  # (T,30)\n",
    "            _, S = _rpca_ialm(\n",
    "                M,\n",
    "                max_iter=RPCA_MAX_ITER,\n",
    "                tol=RPCA_TOL,\n",
    "                rho=RPCA_RHO,\n",
    "                mu=RPCA_MU_INIT\n",
    "            )\n",
    "            S_out[:, tx, rx, :] = S\n",
    "\n",
    "    return S_out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Public API (used by run.py)\n",
    "# =========================\n",
    "def load_data_y(var_path_data_y,\n",
    "                var_environment=None,\n",
    "                var_wifi_band=None,\n",
    "                var_num_users=None):\n",
    "    \"\"\"\n",
    "    Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "    \"\"\"\n",
    "    data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "    if var_environment is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "    if var_wifi_band is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "    if var_num_users is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "    return data_pd_y\n",
    "\n",
    "\n",
    "def load_data_x(var_path_data_x, var_label_list):\n",
    "    \"\"\"\n",
    "    Load CSI amplitude (*.npy) files based on a label list.\n",
    "\n",
    "    Modes:\n",
    "      - raw:            x_out = x_raw\n",
    "      - sparse:         x_out = S\n",
    "      - raw_plus_sparse:x_out = x_raw + alpha*S\n",
    "    \"\"\"\n",
    "    var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "    data_x = []\n",
    "    target_len = preset[\"data\"][\"length\"]\n",
    "\n",
    "    for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "        x_raw = np.load(var_path)\n",
    "        x_raw = _ensure_shape_4d_amp(x_raw).astype(np.float32, copy=False)\n",
    "\n",
    "        _debug_print_once(var_label, x_raw)\n",
    "\n",
    "        mode = CSI_INPUT_MODE\n",
    "\n",
    "        if mode == \"raw\":\n",
    "            x_out = x_raw\n",
    "\n",
    "        else:\n",
    "            if CACHE_ENABLED:\n",
    "                os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "                p = _cache_path(var_label, mode)\n",
    "                if os.path.exists(p):\n",
    "                    x_out = np.load(p).astype(np.float32, copy=False)\n",
    "                else:\n",
    "                    S = _make_sparse_component_pairwise(x_raw)\n",
    "                    if mode == \"sparse\":\n",
    "                        x_out = S\n",
    "                    elif mode == \"raw_plus_sparse\":\n",
    "                        x_out = x_raw + (SPARSE_ALPHA * S)\n",
    "                        if CLAMP_NONNEG:\n",
    "                            x_out = np.maximum(x_out, 0.0)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown CSI_INPUT_MODE={mode}\")\n",
    "\n",
    "                    np.save(p, x_out.astype(np.float32))\n",
    "            else:\n",
    "                S = _make_sparse_component_pairwise(x_raw)\n",
    "                if mode == \"sparse\":\n",
    "                    x_out = S\n",
    "                elif mode == \"raw_plus_sparse\":\n",
    "                    x_out = x_raw + (SPARSE_ALPHA * S)\n",
    "                    if CLAMP_NONNEG:\n",
    "                        x_out = np.maximum(x_out, 0.0)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown CSI_INPUT_MODE={mode}\")\n",
    "\n",
    "        # trim/pad to target_len (same behavior as your existing code expects)\n",
    "        if x_out.shape[0] > target_len:\n",
    "            x_out = x_out[-target_len:, :, :, :]\n",
    "\n",
    "        pad_len = target_len - x_out.shape[0]\n",
    "        if pad_len > 0:\n",
    "            x_out = np.pad(x_out, ((pad_len, 0), (0, 0), (0, 0), (0, 0)))\n",
    "\n",
    "        data_x.append(x_out)\n",
    "\n",
    "    return np.array(data_x)\n",
    "\n",
    "\n",
    "def encode_data_y(data_pd_y, var_task):\n",
    "    \"\"\"\n",
    "    Encode labels according to specific task.\n",
    "    \"\"\"\n",
    "    if var_task == \"identity\":\n",
    "        data_y = encode_identity(data_pd_y)\n",
    "    elif var_task == \"activity\":\n",
    "        data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "    elif var_task == \"location\":\n",
    "        data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "    elif var_task == \"count\":\n",
    "        data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "    return data_y\n",
    "\n",
    "\n",
    "def encode_identity(data_pd_y):\n",
    "    \"\"\"\n",
    "    Onehot encoding for identity labels.\n",
    "    \"\"\"\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "                                    \"user_3_location\", \"user_4_location\",\n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "    data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "    data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "    return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "def encode_activity(data_pd_y, var_encoding):\n",
    "    \"\"\"\n",
    "    Onehot encoding for activity labels.\n",
    "    \"\"\"\n",
    "    data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "                                    \"user_3_activity\", \"user_4_activity\",\n",
    "                                    \"user_5_activity\", \"user_6_activity\"]]\n",
    "    data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "    return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "def encode_location(data_pd_y, var_encoding):\n",
    "    \"\"\"\n",
    "    Onehot encoding for location labels.\n",
    "    \"\"\"\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "                                    \"user_3_location\", \"user_4_location\",\n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "    return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "def encode_count(data_pd_y, var_encoding):\n",
    "    \"\"\"\n",
    "    Onehot encoding for count labels.\n",
    "    \"\"\"\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "                                    \"user_3_location\", \"user_4_location\",\n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "    data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "    data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "    data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "    count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a9d4ba",
   "metadata": {
    "papermill": {
     "duration": 0.006891,
     "end_time": "2025-12-28T17:00:55.234405",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.227514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "last stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bfdcbd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T17:00:55.251042Z",
     "iopub.status.busy": "2025-12-28T17:00:55.250806Z",
     "iopub.status.idle": "2025-12-28T17:00:55.259246Z",
     "shell.execute_reply": "2025-12-28T17:00:55.258553Z"
    },
    "papermill": {
     "duration": 0.018032,
     "end_time": "2025-12-28T17:00:55.260343",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.242311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI (from .mat), and encode labels\n",
    "# \"\"\"\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import scipy.io as scio\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from numpy.fft import ifft\n",
    "\n",
    "# # =========================================================\n",
    "# #  Settings\n",
    "# # =========================================================\n",
    "# # \"raw\"     : abs(raw complex CSI)  -> float\n",
    "# # \"lowrank\" : abs(L) after RPCA     -> float\n",
    "# # \"sparse\"  : abs(S) after RPCA     -> float\n",
    "# CSI_INPUT_MODE = \"sparse\"   # raw / lowrank / sparse\n",
    "\n",
    "# # expected dimensions\n",
    "# TX = 3\n",
    "# RX = 3\n",
    "# SC = 30\n",
    "\n",
    "# # optional IFFT (frequency -> delay)\n",
    "# USE_IFFT = True\n",
    "\n",
    "# # RPCA iterations (برای شروع کم بگذار؛ اگر لازم شد بیشتر کن)\n",
    "# RPCA_MAX_ITER = 50\n",
    "\n",
    "# # lambda mode (اختیاری)\n",
    "# # \"classic\" : 1/sqrt(max(m,n))\n",
    "# # \"median\"  : 1/sqrt(max(m,n)) * median(abs(M))\n",
    "# # \"scaled\"  : 1.2/sqrt(max(m,n))\n",
    "# RPCA_LAMBDA_MODE = \"classic\"\n",
    "\n",
    "# # cache (very important for speed)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT = \"/kaggle/working/csi_cache_mat_pipeline\"\n",
    "\n",
    "# # print once to verify input is amplitude or complex/phase\n",
    "# _DEBUG_PRINT_ONCE = True\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# # =========================================================\n",
    "# # Minimal R_pca implementation (no external dependency)\n",
    "# # Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "# # =========================================================\n",
    "# class R_pca:\n",
    "#     def __init__(self, D):\n",
    "#         self.D = np.asarray(D, dtype=np.float64)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _shrink(M, tau):\n",
    "#         return np.sign(M) * np.maximum(np.abs(M) - tau, 0.0)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _svt(M, tau):\n",
    "#         U, s, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "#         s = np.maximum(s - tau, 0.0)\n",
    "#         if np.all(s == 0):\n",
    "#             return np.zeros_like(M)\n",
    "#         return (U * s) @ Vt\n",
    "\n",
    "#     def _lambda(self, D):\n",
    "#         m, n = D.shape\n",
    "#         base = 1.0 / np.sqrt(max(m, n))\n",
    "#         if RPCA_LAMBDA_MODE == \"classic\":\n",
    "#             return base\n",
    "#         if RPCA_LAMBDA_MODE == \"scaled\":\n",
    "#             return 1.2 * base\n",
    "#         if RPCA_LAMBDA_MODE == \"median\":\n",
    "#             med = np.median(np.abs(D))\n",
    "#             med = float(med) if med > 1e-12 else 1e-12\n",
    "#             return base * med\n",
    "#         raise ValueError(f\"Unknown RPCA_LAMBDA_MODE={RPCA_LAMBDA_MODE}\")\n",
    "\n",
    "#     def fit(self, max_iter=200, tol=1e-6, rho=1.5, mu=None):\n",
    "#         \"\"\"\n",
    "#         Returns L, S such that D ≈ L + S\n",
    "#         \"\"\"\n",
    "#         D = self.D\n",
    "#         m, n = D.shape\n",
    "#         lam = self._lambda(D)\n",
    "\n",
    "#         # auto mu\n",
    "#         if mu is None:\n",
    "#             s0 = np.linalg.svd(D, compute_uv=False, full_matrices=False)[0] if D.size else 1.0\n",
    "#             mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#         L = np.zeros_like(D)\n",
    "#         S = np.zeros_like(D)\n",
    "#         Y = np.zeros_like(D)\n",
    "\n",
    "#         normD = np.linalg.norm(D, ord=\"fro\") + 1e-12\n",
    "\n",
    "#         for _ in range(max_iter):\n",
    "#             L = self._svt(D - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#             S = self._shrink(D - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#             R = D - L - S\n",
    "#             Y = Y + mu * R\n",
    "\n",
    "#             if (np.linalg.norm(R, ord=\"fro\") / normD) < tol:\n",
    "#                 break\n",
    "\n",
    "#             mu *= rho\n",
    "\n",
    "#         return L.astype(np.float64), S.astype(np.float64)\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 1) Phase calibration (sanitize) - vectorized\n",
    "# # --------------------------------------------------\n",
    "# def phase_sanitize_matrix(X):\n",
    "#     \"\"\"\n",
    "#     X: complex matrix (N_subcarriers, T)\n",
    "#     \"\"\"\n",
    "#     phase = np.unwrap(np.angle(X), axis=0)  # (N,T)\n",
    "#     N, T = phase.shape\n",
    "#     k = np.arange(N, dtype=np.float64)[:, None]  # (N,1)\n",
    "\n",
    "#     A = np.concatenate([k, np.ones((N, 1), dtype=np.float64)], axis=1)  # (N,2)\n",
    "#     pinvA = np.linalg.pinv(A)  # (2,N)\n",
    "#     coeff = pinvA @ phase      # (2,T)\n",
    "#     a = coeff[0:1, :]\n",
    "#     b = coeff[1:2, :]\n",
    "\n",
    "#     phase_corr = phase - (k @ a + b)\n",
    "#     return np.abs(X) * np.exp(1j * phase_corr)\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 2) Preprocess CSI matrix\n",
    "# # --------------------------------------------------\n",
    "# def preprocess_csi(X):\n",
    "#     \"\"\"\n",
    "#     X : complex CSI matrix (N_subcarriers, T)\n",
    "#     \"\"\"\n",
    "#     X_corr = phase_sanitize_matrix(X).astype(np.complex128, copy=False)\n",
    "#     fro = np.linalg.norm(X_corr, \"fro\")\n",
    "#     if fro > 0:\n",
    "#         X_corr /= fro\n",
    "#     return X_corr\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 3) Optional IFFT\n",
    "# # --------------------------------------------------\n",
    "# def csi_to_cir(X):\n",
    "#     return ifft(X, axis=0)\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 4) RPCA on complex matrix (real & imag separately)\n",
    "# # --------------------------------------------------\n",
    "# def rpca_complex(X, max_iter=200):\n",
    "#     Xr = np.real(X)\n",
    "#     Xi = np.imag(X)\n",
    "\n",
    "#     rpca_r = R_pca(Xr)\n",
    "#     Lr, Sr = rpca_r.fit(max_iter=max_iter)\n",
    "\n",
    "#     rpca_i = R_pca(Xi)\n",
    "#     Li, Si = rpca_i.fit(max_iter=max_iter)\n",
    "\n",
    "#     L = Lr + 1j * Li\n",
    "#     S = Sr + 1j * Si\n",
    "#     return L, S\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # 5) Full pipeline\n",
    "# # --------------------------------------------------\n",
    "# def csi_lowrank_sparse_pipeline(X, use_ifft=True, max_iter=200):\n",
    "#     Xp = preprocess_csi(X)\n",
    "#     if use_ifft:\n",
    "#         Xp = csi_to_cir(Xp)\n",
    "#     L, S = rpca_complex(Xp, max_iter=max_iter)\n",
    "#     return L, S\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # MAT loader -> (T,3,3,30) complex\n",
    "# # --------------------------------------------------\n",
    "# def load_csi_from_mat(mat_path):\n",
    "#     m = scio.loadmat(mat_path)\n",
    "#     if \"trace\" not in m:\n",
    "#         raise KeyError(f\"'trace' not found in {mat_path}\")\n",
    "\n",
    "#     trace = m[\"trace\"]  # (T,1) object array\n",
    "#     T = trace.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.complex128)\n",
    "\n",
    "#     for t in range(T):\n",
    "#         out[t] = trace[t, 0][\"csi\"][0, 0]  # (3,3,30) complex\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def _debug_print_once(label, csi_4d):\n",
    "#     global _DEBUG_PRINT_ONCE\n",
    "#     if not _DEBUG_PRINT_ONCE:\n",
    "#         return\n",
    "#     _DEBUG_PRINT_ONCE = False\n",
    "\n",
    "#     is_cplx = np.iscomplexobj(csi_4d)\n",
    "#     print(f\"\\n[DEBUG] First MAT sample: {label}\")\n",
    "#     print(f\"[DEBUG] shape={csi_4d.shape}, dtype={csi_4d.dtype}, complex={is_cplx}\")\n",
    "\n",
    "#     x0 = csi_4d[0, 0, 0, :]\n",
    "#     if is_cplx:\n",
    "#         ang = np.angle(x0)\n",
    "#         print(f\"[DEBUG] abs range:  min={np.min(np.abs(x0)):.6f}, max={np.max(np.abs(x0)):.6f}\")\n",
    "#         print(f\"[DEBUG] phase stats: mean={np.mean(ang):.6f}, std={np.std(ang):.6f}\")\n",
    "#         print(\"[DEBUG] ==> Input is COMPLEX (phase exists).\")\n",
    "#     else:\n",
    "#         print(f\"[DEBUG] value range: min={np.min(x0):.6f}, max={np.max(x0):.6f}\")\n",
    "#         print(\"[DEBUG] ==> Input is REAL (likely amplitude-only).\")\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _apply_pipeline_pairwise_9links(csi_4d_complex, label):\n",
    "#     \"\"\"\n",
    "#     csi_4d_complex: (T,3,3,30) complex\n",
    "#     Run pipeline on each link separately: (30,T) -> RPCA -> abs -> back to (T,3,3,30) float32\n",
    "#     \"\"\"\n",
    "#     _debug_print_once(label, csi_4d_complex)\n",
    "\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return np.abs(csi_4d_complex).astype(np.float32)\n",
    "\n",
    "#     mode = CSI_INPUT_MODE  # lowrank / sparse\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             return np.load(p).astype(np.float32, copy=False)\n",
    "\n",
    "#     T = csi_4d_complex.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "#     for tx in range(TX):\n",
    "#         for rx in range(RX):\n",
    "#             X = csi_4d_complex[:, tx, rx, :].T  # (30,T) complex\n",
    "#             L, S = csi_lowrank_sparse_pipeline(X, use_ifft=USE_IFFT, max_iter=RPCA_MAX_ITER)\n",
    "#             Y = L if mode == \"lowrank\" else S\n",
    "#             out[:, tx, rx, :] = np.abs(Y.T).astype(np.float32)\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out)\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # Existing label loading/encoding\n",
    "# # --------------------------------------------------\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None,\n",
    "#                 var_wifi_band=None,\n",
    "#                 var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     var_path_data_x should point to MAT directory, e.g. /kaggle/input/wimans/wifi_csi/mat\n",
    "#     Each label corresponds to <label>.mat\n",
    "#     \"\"\"\n",
    "#     data_x = []\n",
    "#     target_len = preset[\"data\"][\"length\"]\n",
    "\n",
    "#     for label in var_label_list:\n",
    "#         mat_path = os.path.join(var_path_data_x, label + \".mat\")\n",
    "#         if not os.path.exists(mat_path):\n",
    "#             raise FileNotFoundError(f\"MAT file not found: {mat_path}\")\n",
    "\n",
    "#         csi_complex = load_csi_from_mat(mat_path)  # (T,3,3,30) complex\n",
    "\n",
    "#         # if longer than target_len, keep last target_len frames\n",
    "#         if csi_complex.shape[0] > target_len:\n",
    "#             csi_complex = csi_complex[-target_len:, :, :, :]\n",
    "\n",
    "#         csi_feat = _apply_pipeline_pairwise_9links(csi_complex, label)\n",
    "\n",
    "#         # pad if shorter\n",
    "#         pad_len = target_len - csi_feat.shape[0]\n",
    "#         if pad_len > 0:\n",
    "#             csi_feat = np.pad(csi_feat, ((pad_len, 0), (0, 0), (0, 0), (0, 0)))\n",
    "\n",
    "#         data_x.append(csi_feat)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y)\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "#                                     \"user_3_activity\", \"user_4_activity\",\n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec3ee82",
   "metadata": {
    "papermill": {
     "duration": 0.006899,
     "end_time": "2025-12-28T17:00:55.274856",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.267957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "rpca 30 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d970b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T17:00:55.291155Z",
     "iopub.status.busy": "2025-12-28T17:00:55.290907Z",
     "iopub.status.idle": "2025-12-28T17:00:55.297739Z",
     "shell.execute_reply": "2025-12-28T17:00:55.297088Z"
    },
    "papermill": {
     "duration": 0.016829,
     "end_time": "2025-12-28T17:00:55.298800",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.281971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # =========================================================\n",
    "# # انتخاب نوع دیتای ورودی مدل:\n",
    "# # \"raw\"     : amp خام\n",
    "# # \"lowrank\" : خروجی Low-rank (L) از RPCA\n",
    "# # \"sparse\"  : خروجی Sparse (S) از RPCA\n",
    "# CSI_INPUT_MODE = \"lowrank\"   # <-- raw / lowrank / sparse\n",
    "\n",
    "# # ابعاد مورد انتظار CSI\n",
    "# TX = 3\n",
    "# RX = 3\n",
    "# SC = 30\n",
    "\n",
    "# # RPCA (IALM) تنظیمات سریع‌تر\n",
    "# RPCA_MAX_ITER = 60\n",
    "# RPCA_TOL      = 1e-5\n",
    "# RPCA_RHO      = 1.5\n",
    "# RPCA_MU_INIT  = None\n",
    "\n",
    "# # ✅ انتخاب لامبدا:\n",
    "# # \"median\" : lam = 1/sqrt(max(m,n)) * median(abs(M))\n",
    "# # \"scaled\" : lam = 1.2/sqrt(max(m,n))\n",
    "# RPCA_LAMBDA_MODE = \"median\"  # <-- \"median\" یا \"scaled\"\n",
    "\n",
    "# # Cache (خیلی مهم برای سرعت)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT    = \"/kaggle/working/csi_cache_pairwise\"\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _soft_threshold(X, tau):\n",
    "#     return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "# def _svt(X, tau):\n",
    "#     # Singular Value Thresholding\n",
    "#     U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "#     s = np.maximum(s - tau, 0.0)\n",
    "#     if np.all(s == 0):\n",
    "#         return np.zeros_like(X)\n",
    "#     return (U * s) @ Vt\n",
    "\n",
    "# def _compute_lambda(M, mode):\n",
    "#     \"\"\"\n",
    "#     mode:\n",
    "#       - 'median': 1/sqrt(max(m,n)) * median(abs(M))\n",
    "#       - 'scaled': 1.2/sqrt(max(m,n))\n",
    "#     \"\"\"\n",
    "#     m, n = M.shape\n",
    "#     base = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "#     if mode == \"median\":\n",
    "#         med = np.median(np.abs(M))\n",
    "#         # اگر med خیلی کوچک بود برای پایداری:\n",
    "#         med = float(med) if med > 1e-12 else 1e-12\n",
    "#         return base * med\n",
    "\n",
    "#     if mode == \"scaled\":\n",
    "#         return 1.2 * base\n",
    "\n",
    "#     raise ValueError(f\"Unknown RPCA_LAMBDA_MODE: {mode}. Use 'median' or 'scaled'.\")\n",
    "\n",
    "\n",
    "# def _rpca_ialm(M, mu=None, rho=1.5, max_iter=60, tol=1e-5):\n",
    "#     \"\"\"\n",
    "#     Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "#     Decompose: M = L + S\n",
    "#     M shape: (T, SC) = (3000, 30)\n",
    "#     \"\"\"\n",
    "#     M = M.astype(np.float64, copy=False)\n",
    "#     m, n = M.shape\n",
    "\n",
    "#     # ✅ lambda طبق انتخاب کاربر\n",
    "#     lam = _compute_lambda(M, RPCA_LAMBDA_MODE)\n",
    "\n",
    "#     # mu خودکار یا دستی\n",
    "#     if mu is None:\n",
    "#         s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "#         mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     Y = np.zeros_like(M)\n",
    "\n",
    "#     normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#         S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#         R = M - L - S\n",
    "#         Y = Y + mu * R\n",
    "\n",
    "#         if (np.linalg.norm(R, ord=\"fro\") / normM) < tol:\n",
    "#             break\n",
    "\n",
    "#         mu *= rho\n",
    "\n",
    "#     return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# def _ensure_shape_4d(data_csi):\n",
    "#     \"\"\"\n",
    "#     Ensure CSI shape is (T, TX, RX, SC).\n",
    "#     If input is (T, 270) we reshape to (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     if data_csi.ndim == 4:\n",
    "#         return data_csi\n",
    "\n",
    "#     if data_csi.ndim == 2 and data_csi.shape[1] == TX * RX * SC:\n",
    "#         T = data_csi.shape[0]\n",
    "#         return data_csi.reshape(T, TX, RX, SC)\n",
    "\n",
    "#     raise ValueError(\n",
    "#         f\"Unexpected CSI shape {data_csi.shape}. Expected (T,{TX},{RX},{SC}) or (T,{TX*RX*SC}).\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _rpca_pairwise_9links(data_csi_4d):\n",
    "#     \"\"\"\n",
    "#     data_csi_4d: (T, TX, RX, SC)\n",
    "#     Run RPCA on each (tx,rx) separately on matrix (T, SC) and reassemble.\n",
    "#     Output shape stays (T, TX, RX, SC).\n",
    "#     \"\"\"\n",
    "#     T = data_csi_4d.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "#     for tx in range(TX):\n",
    "#         for rx in range(RX):\n",
    "#             M = data_csi_4d[:, tx, rx, :]  # (T,30)\n",
    "#             L, S = _rpca_ialm(\n",
    "#                 M,\n",
    "#                 mu=RPCA_MU_INIT,\n",
    "#                 rho=RPCA_RHO,\n",
    "#                 max_iter=RPCA_MAX_ITER,\n",
    "#                 tol=RPCA_TOL\n",
    "#             )\n",
    "#             out[:, tx, rx, :] = L if CSI_INPUT_MODE == \"lowrank\" else S\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def _apply_mode_with_cache(data_csi, label):\n",
    "#     \"\"\"\n",
    "#     Apply raw/lowrank/sparse. For lowrank/sparse use pairwise RPCA (9 links of 3000x30).\n",
    "#     Keeps the original 4D shape (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     data_csi = _ensure_shape_4d(np.asarray(data_csi, dtype=np.float32))\n",
    "\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     mode = CSI_INPUT_MODE\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             cached = np.load(p)\n",
    "#             return _ensure_shape_4d(cached).astype(np.float32, copy=False)\n",
    "\n",
    "#     out = _rpca_pairwise_9links(data_csi)\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out.astype(np.float32))\n",
    "\n",
    "#     return out.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None,\n",
    "#                 var_wifi_band=None,\n",
    "#                 var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ RPCA روی 9 لینک جدا (3000x30) و بعد اتصال\n",
    "#         data_csi = _apply_mode_with_cache(data_csi, var_label)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "#                                     \"user_3_activity\", \"user_4_activity\",\n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a114aecc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T17:00:55.314490Z",
     "iopub.status.busy": "2025-12-28T17:00:55.314186Z",
     "iopub.status.idle": "2025-12-28T17:00:55.321557Z",
     "shell.execute_reply": "2025-12-28T17:00:55.320801Z"
    },
    "papermill": {
     "duration": 0.016728,
     "end_time": "2025-12-28T17:00:55.322864",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.306136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # =========================================================\n",
    "# # انتخاب نوع دیتای ورودی مدل:\n",
    "# # \"raw\"     : amp خام\n",
    "# # \"lowrank\" : خروجی Low-rank (L) از RPCA\n",
    "# # \"sparse\"  : خروجی Sparse (S) از RPCA\n",
    "# CSI_INPUT_MODE = \"sparse\"   # <-- raw / lowrank / sparse\n",
    "\n",
    "# # ابعاد مورد انتظار CSI\n",
    "# TX = 3\n",
    "# RX = 3\n",
    "# SC = 30\n",
    "\n",
    "# # RPCA (IALM) تنظیمات سریع‌تر\n",
    "# RPCA_MAX_ITER = 60\n",
    "# RPCA_TOL      = 1e-5\n",
    "# RPCA_RHO      = 1.5\n",
    "# RPCA_MU_INIT  = None\n",
    "# RPCA_LAMBDA   = None     # None -> 1/sqrt(max(m,n))\n",
    "\n",
    "# # Cache (خیلی مهم برای سرعت)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT    = \"/kaggle/working/csi_cache_pairwise\"  # خروجی‌ها اینجا ذخیره میشن\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _soft_threshold(X, tau):\n",
    "#     return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "# def _svt(X, tau):\n",
    "#     # Singular Value Thresholding\n",
    "#     U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "#     s = np.maximum(s - tau, 0.0)\n",
    "#     if np.all(s == 0):\n",
    "#         return np.zeros_like(X)\n",
    "#     return (U * s) @ Vt\n",
    "\n",
    "# def _rpca_ialm(M, lam=None, mu=None, rho=1.5, max_iter=60, tol=1e-5):\n",
    "#     \"\"\"\n",
    "#     Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "#     Decompose: M = L + S\n",
    "#     M shape: (T, SC) = (3000, 30)\n",
    "#     \"\"\"\n",
    "#     M = M.astype(np.float64, copy=False)\n",
    "#     m, n = M.shape\n",
    "\n",
    "#     if lam is None:\n",
    "#         lam = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "#     if mu is None:\n",
    "#         # top singular value as spectral norm approximation\n",
    "#         s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "#         mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     Y = np.zeros_like(M)\n",
    "\n",
    "#     normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#         S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#         R = M - L - S\n",
    "#         Y = Y + mu * R\n",
    "\n",
    "#         if (np.linalg.norm(R, ord=\"fro\") / normM) < tol:\n",
    "#             break\n",
    "\n",
    "#         mu *= rho\n",
    "\n",
    "#     return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# def _ensure_shape_4d(data_csi):\n",
    "#     \"\"\"\n",
    "#     Ensure CSI shape is (T, TX, RX, SC).\n",
    "#     If input is (T, 270) we reshape to (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     if data_csi.ndim == 4:\n",
    "#         return data_csi\n",
    "\n",
    "#     if data_csi.ndim == 2 and data_csi.shape[1] == TX * RX * SC:\n",
    "#         T = data_csi.shape[0]\n",
    "#         return data_csi.reshape(T, TX, RX, SC)\n",
    "\n",
    "#     raise ValueError(\n",
    "#         f\"Unexpected CSI shape {data_csi.shape}. Expected (T,{TX},{RX},{SC}) or (T,{TX*RX*SC}).\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     # mode: \"lowrank\" or \"sparse\"\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _rpca_pairwise_9links(data_csi_4d):\n",
    "#     \"\"\"\n",
    "#     data_csi_4d: (T, TX, RX, SC)\n",
    "#     Run RPCA on each (tx,rx) separately on matrix (T, SC) and reassemble.\n",
    "#     Output shape stays (T, TX, RX, SC).\n",
    "#     \"\"\"\n",
    "#     T = data_csi_4d.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "#     # 9 times RPCA: for each tx-rx link\n",
    "#     for tx in range(TX):\n",
    "#         for rx in range(RX):\n",
    "#             M = data_csi_4d[:, tx, rx, :]  # (T, SC) => (3000,30)\n",
    "#             L, S = _rpca_ialm(\n",
    "#                 M,\n",
    "#                 lam=RPCA_LAMBDA,\n",
    "#                 mu=RPCA_MU_INIT,\n",
    "#                 rho=RPCA_RHO,\n",
    "#                 max_iter=RPCA_MAX_ITER,\n",
    "#                 tol=RPCA_TOL\n",
    "#             )\n",
    "#             if CSI_INPUT_MODE == \"lowrank\":\n",
    "#                 out[:, tx, rx, :] = L\n",
    "#             else:  # \"sparse\"\n",
    "#                 out[:, tx, rx, :] = S\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def _apply_mode_with_cache(data_csi, label):\n",
    "#     \"\"\"\n",
    "#     Apply raw/lowrank/sparse. For lowrank/sparse use pairwise RPCA (9 links of 3000x30).\n",
    "#     Keeps the original 4D shape (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     data_csi = _ensure_shape_4d(np.asarray(data_csi, dtype=np.float32))\n",
    "\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     mode = CSI_INPUT_MODE\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             cached = np.load(p)\n",
    "#             return _ensure_shape_4d(cached).astype(np.float32, copy=False)\n",
    "\n",
    "#     out = _rpca_pairwise_9links(data_csi)\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out.astype(np.float32))\n",
    "\n",
    "#     return out.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     \"\"\"\n",
    "#     Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "#     \"\"\"\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     Load CSI amplitude (*.npy) files based on a label list.\n",
    "#     \"\"\"\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ NEW: RPCA روی 9 لینک جداگانه (3000x30) و بعد چسباندن\n",
    "#         data_csi = _apply_mode_with_cache(data_csi, var_label)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     \"\"\"\n",
    "#     Encode labels according to specific task.\n",
    "#     \"\"\"\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     return data_identity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for activity labels.\n",
    "#     \"\"\"\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "#                                     \"user_3_activity\", \"user_4_activity\", \n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "#     return data_activity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for location labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "#     return data_location_onehot_y\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1)\n",
    "#     count_data = count_data.reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     count_data_onehot = encoder.fit_transform(count_data).astype(\"int8\")\n",
    "#     return count_data_onehot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb09169",
   "metadata": {
    "papermill": {
     "duration": 0.007203,
     "end_time": "2025-12-28T17:00:55.338051",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.330848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "rpca\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80a38c6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T17:00:55.353590Z",
     "iopub.status.busy": "2025-12-28T17:00:55.353253Z",
     "iopub.status.idle": "2025-12-28T17:00:55.359254Z",
     "shell.execute_reply": "2025-12-28T17:00:55.358680Z"
    },
    "papermill": {
     "duration": 0.015166,
     "end_time": "2025-12-28T17:00:55.360261",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.345095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # =========================================================\n",
    "# # انتخاب ورودی مدل:\n",
    "# # \"raw\"     : همون amp خام\n",
    "# # \"lowrank\" : مؤلفه Low-rank از RPCA (L)\n",
    "# # \"sparse\"  : مؤلفه Sparse از RPCA (S)\n",
    "# CSI_INPUT_MODE = \"lowrank\"   # <-- raw / lowrank / sparse\n",
    "\n",
    "# # RPCA (IALM) تنظیمات سریع‌تر\n",
    "# RPCA_MAX_ITER = 80\n",
    "# RPCA_TOL      = 1e-5\n",
    "# RPCA_RHO      = 1.5\n",
    "# RPCA_MU_INIT  = None\n",
    "# RPCA_LAMBDA   = None     # None -> 1/sqrt(max(m,n))\n",
    "\n",
    "# # Cache (خیلی مهم برای سرعت)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT    = \"/kaggle/working/csi_cache\"  # خروجی‌ها اینجا ذخیره میشن\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _soft_threshold(X, tau):\n",
    "#     return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "# def _svt(X, tau):\n",
    "#     U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "#     s = np.maximum(s - tau, 0.0)\n",
    "#     if np.all(s == 0):\n",
    "#         return np.zeros_like(X)\n",
    "#     return (U * s) @ Vt\n",
    "\n",
    "# def _rpca_ialm(M, lam=None, mu=None, rho=1.5, max_iter=80, tol=1e-5):\n",
    "#     M = M.astype(np.float64, copy=False)\n",
    "#     m, n = M.shape\n",
    "\n",
    "#     if lam is None:\n",
    "#         lam = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "#     if mu is None:\n",
    "#         s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "#         mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     Y = np.zeros_like(M)\n",
    "\n",
    "#     normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#         S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#         R = M - L - S\n",
    "#         Y = Y + mu * R\n",
    "\n",
    "#         if (np.linalg.norm(R, ord=\"fro\") / normM) < tol:\n",
    "#             break\n",
    "#         mu *= rho\n",
    "\n",
    "#     return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# def _rpca_keep_shape(X):\n",
    "#     \"\"\"RPCA روی (T,F) و بازگرداندن دقیقاً به shape اولیه\"\"\"\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "#     if X.ndim == 1:\n",
    "#         M = X[:, None]\n",
    "#         L, S = _rpca_ialm(M, RPCA_LAMBDA, RPCA_MU_INIT, RPCA_RHO, RPCA_MAX_ITER, RPCA_TOL)\n",
    "#         return L[:, 0], S[:, 0]\n",
    "\n",
    "#     T = X.shape[0]\n",
    "#     F = int(np.prod(X.shape[1:]))\n",
    "#     M = X.reshape(T, F)\n",
    "\n",
    "#     L, S = _rpca_ialm(M, RPCA_LAMBDA, RPCA_MU_INIT, RPCA_RHO, RPCA_MAX_ITER, RPCA_TOL)\n",
    "#     return L.reshape(X.shape), S.reshape(X.shape)\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     # mode: \"lowrank\" or \"sparse\"\n",
    "#     # فایل خروجی: /kaggle/working/csi_cache/lowrank/<label>.npy\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _apply_mode_with_cache(data_csi, label):\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     # lowrank یا sparse\n",
    "#     mode = CSI_INPUT_MODE\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             return np.load(p).astype(np.float32, copy=False)\n",
    "\n",
    "#     L, S = _rpca_keep_shape(data_csi)\n",
    "#     out = L if mode == \"lowrank\" else S\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out.astype(np.float32))\n",
    "\n",
    "#     return out.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ RPCA lowrank/sparse بدون تغییر shape + با cache\n",
    "#         data_csi = _apply_mode_with_cache(data_csi, var_label)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "#                                     \"user_3_activity\", \"user_4_activity\",\n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a1b33",
   "metadata": {
    "papermill": {
     "duration": 0.00677,
     "end_time": "2025-12-28T17:00:55.374155",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.367385",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "svd\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19539a66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T17:00:55.389583Z",
     "iopub.status.busy": "2025-12-28T17:00:55.389310Z",
     "iopub.status.idle": "2025-12-28T17:00:55.396755Z",
     "shell.execute_reply": "2025-12-28T17:00:55.396187Z"
    },
    "papermill": {
     "duration": 0.016921,
     "end_time": "2025-12-28T17:00:55.397820",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.380899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # Note: All necessary libraries (os, numpy, pandas, etc.) are imported in Cell 1.\n",
    "# # from preset import preset   --> preset is already defined in Cell 2.\n",
    "\n",
    "# # =========================================================\n",
    "# # 🔧 NEW: Choose CSI representation mode here (ONLY EDIT THIS)\n",
    "# # ---------------------------------------------------------\n",
    "# # \"raw\"     : use original CSI amplitude as-is\n",
    "# # \"lowrank\" : use low-rank approximation (SVD)\n",
    "# # \"sparse\"  : keep only large-magnitude entries (dense array with many zeros)\n",
    "# CSI_INPUT_MODE = \"sparse\"     # <-- set to: \"raw\" / \"lowrank\" / \"sparse\"\n",
    "\n",
    "# # Low-rank settings\n",
    "# LOW_RANK_ENERGY = 0.95     # keep enough singular values to preserve this energy\n",
    "# LOW_RANK_RANK   = None     # if set to an int (e.g., 10), it overrides ENERGY\n",
    "\n",
    "# # Sparse settings\n",
    "# SPARSE_KEEP_RATIO = 0.10   # keep top 10% magnitudes (globally per sample)\n",
    "# SPARSE_MIN_ABS    = None   # if set (e.g., 0.5), keeps |x|>=threshold instead of keep_ratio\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _low_rank_approx_keep_shape(X, rank=None, energy=0.95):\n",
    "#     \"\"\"\n",
    "#     Low-rank approximation using SVD while preserving the original shape.\n",
    "#     Works for 1D/2D/ND by flattening all non-time dims into features.\n",
    "#     Assumes first axis is time.\n",
    "#     \"\"\"\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "\n",
    "#     if X.ndim == 1:\n",
    "#         M = X[:, None]  # (T,1)\n",
    "#         U, S, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "#         if rank is None:\n",
    "#             s2 = S**2\n",
    "#             cum = np.cumsum(s2) / (np.sum(s2) + 1e-12)\n",
    "#             rank = int(np.searchsorted(cum, energy) + 1)\n",
    "#         rank = max(1, min(rank, S.shape[0]))\n",
    "#         M_lr = (U[:, :rank] * S[:rank]) @ Vt[:rank, :]\n",
    "#         return M_lr[:, 0].astype(np.float32)\n",
    "\n",
    "#     # ND: reshape to (T, F)\n",
    "#     T = X.shape[0]\n",
    "#     F = int(np.prod(X.shape[1:]))\n",
    "#     M = X.reshape(T, F)\n",
    "\n",
    "#     U, S, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "\n",
    "#     if rank is None:\n",
    "#         s2 = S**2\n",
    "#         cum = np.cumsum(s2) / (np.sum(s2) + 1e-12)\n",
    "#         rank = int(np.searchsorted(cum, energy) + 1)\n",
    "\n",
    "#     rank = max(1, min(rank, S.shape[0]))\n",
    "#     M_lr = (U[:, :rank] * S[:rank]) @ Vt[:rank, :]\n",
    "\n",
    "#     return M_lr.reshape(X.shape).astype(np.float32)\n",
    "\n",
    "\n",
    "# def _to_sparse_dense_keep_shape(X, keep_ratio=0.10, min_abs=None):\n",
    "#     \"\"\"\n",
    "#     Makes X sparse-in-content (many zeros) but keeps it as a dense numpy array\n",
    "#     so the rest of the pipeline (np.save/np.load/pad/model) doesn't change.\n",
    "#     Keeps the same shape.\n",
    "#     \"\"\"\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "#     flat = X.ravel()\n",
    "#     if flat.size == 0:\n",
    "#         return X.astype(np.float32)\n",
    "\n",
    "#     absflat = np.abs(flat)\n",
    "\n",
    "#     if min_abs is not None:\n",
    "#         thr = float(min_abs)\n",
    "#         mask = absflat >= thr\n",
    "#     else:\n",
    "#         k = int(np.ceil(keep_ratio * flat.size))\n",
    "#         k = max(1, min(k, flat.size))\n",
    "#         if k == flat.size:\n",
    "#             mask = np.ones_like(absflat, dtype=bool)\n",
    "#         else:\n",
    "#             thr = np.partition(absflat, -k)[-k]\n",
    "#             mask = absflat >= thr\n",
    "\n",
    "#     out = np.zeros_like(flat, dtype=np.float32)\n",
    "#     out[mask] = flat[mask]\n",
    "#     return out.reshape(X.shape).astype(np.float32)\n",
    "\n",
    "\n",
    "# def _apply_csi_mode(data_csi):\n",
    "#     \"\"\"\n",
    "#     Apply selected CSI_INPUT_MODE to a single sample array.\n",
    "#     \"\"\"\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     elif CSI_INPUT_MODE == \"lowrank\":\n",
    "#         return _low_rank_approx_keep_shape(\n",
    "#             data_csi,\n",
    "#             rank=LOW_RANK_RANK,\n",
    "#             energy=LOW_RANK_ENERGY\n",
    "#         )\n",
    "\n",
    "#     elif CSI_INPUT_MODE == \"sparse\":\n",
    "#         return _to_sparse_dense_keep_shape(\n",
    "#             data_csi,\n",
    "#             keep_ratio=SPARSE_KEEP_RATIO,\n",
    "#             min_abs=SPARSE_MIN_ABS\n",
    "#         )\n",
    "\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown CSI_INPUT_MODE: {CSI_INPUT_MODE}. Use 'raw', 'lowrank', or 'sparse'.\")\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     \"\"\"\n",
    "#     Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "#     \"\"\"\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     Load CSI amplitude (*.npy) files based on a label list.\n",
    "#     \"\"\"\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_path in var_path_list:\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ NEW: convert input CSI according to selected mode (raw/lowrank/sparse)\n",
    "#         data_csi = _apply_csi_mode(data_csi)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "#     data_x = np.array(data_x)\n",
    "#     return data_x\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     \"\"\"\n",
    "#     Encode labels according to specific task.\n",
    "#     \"\"\"\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     return data_identity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for activity labels.\n",
    "#     \"\"\"\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "#                                     \"user_3_activity\", \"user_4_activity\", \n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "#     return data_activity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for location labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "#     return data_location_onehot_y\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     print(\"data_identity_onehot_y\",data_identity_onehot_y.shape)\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1)\n",
    "#     print(\"count_data\",count_data.shape)\n",
    "#     count_data = count_data.reshape(-1, 1)  # shape = (11286, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)  \n",
    "#     count_data_onehot = encoder.fit_transform(count_data)\n",
    "#     print(count_data_onehot.shape)  \n",
    "#     count_data_onehot = count_data_onehot.astype(\"int8\")\n",
    "\n",
    "#     return count_data_onehot\n",
    "\n",
    "\n",
    "# # Test functions (optional)\n",
    "# def test_load_data_y():\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"classroom\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=[\"1\", \"2\", \"3\"]).describe())\n",
    "\n",
    "# def test_load_data_x():\n",
    "#     data_pd_y = load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=None)\n",
    "#     var_label_list = data_pd_y[\"label\"].to_list()\n",
    "#     data_x = load_data_x(preset[\"path\"][\"data_x\"], var_label_list)\n",
    "#     print(data_x.shape)\n",
    "\n",
    "# def test_encode_identity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_identity_onehot_y = encode_identity(data_pd_y)\n",
    "#     print(data_identity_onehot_y.shape)\n",
    "#     print(data_identity_onehot_y[2000])\n",
    "\n",
    "# def test_encode_activity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_activity_onehot_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     print(data_activity_onehot_y.shape)\n",
    "#     print(data_activity_onehot_y[1560])\n",
    "\n",
    "# def test_encode_location():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_location_onehot_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     print(data_location_onehot_y.shape)\n",
    "#     print(data_location_onehot_y[1560])\n",
    "\n",
    "# def test_encode_count():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_count_onehot_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     print(data_count_onehot_y.shape)\n",
    "#     print(data_count_onehot_y[20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0983c6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T17:00:55.413282Z",
     "iopub.status.busy": "2025-12-28T17:00:55.413060Z",
     "iopub.status.idle": "2025-12-28T17:00:55.418801Z",
     "shell.execute_reply": "2025-12-28T17:00:55.418103Z"
    },
    "papermill": {
     "duration": 0.014865,
     "end_time": "2025-12-28T17:00:55.419930",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.405065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # Note: All necessary libraries (os, numpy, pandas, etc.) are imported in Cell 1.\n",
    "# # from preset import preset   --> preset is already defined in Cell 2.\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     \"\"\"\n",
    "#     Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "#     \"\"\"\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     Load CSI amplitude (*.npy) files based on a label list.\n",
    "#     \"\"\"\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_path in var_path_list:\n",
    "#         data_csi = np.load(var_path)\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "#     data_x = np.array(data_x)\n",
    "#     return data_x\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     \"\"\"\n",
    "#     Encode labels according to specific task.\n",
    "#     \"\"\"\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     return data_identity_onehot_y\n",
    "\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for activity labels.\n",
    "#     \"\"\"\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "#                                     \"user_3_activity\", \"user_4_activity\", \n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "#     return data_activity_onehot_y\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for location labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "#     return data_location_onehot_y\n",
    "\n",
    "# # Test functions (optional)\n",
    "# def test_load_data_y():\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"classroom\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=[\"1\", \"2\", \"3\"]).describe())\n",
    "\n",
    "# def test_load_data_x():\n",
    "#     data_pd_y = load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=None)\n",
    "#     var_label_list = data_pd_y[\"label\"].to_list()\n",
    "#     data_x = load_data_x(preset[\"path\"][\"data_x\"], var_label_list)\n",
    "#     print(data_x.shape)\n",
    "\n",
    "# def test_encode_identity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_identity_onehot_y = encode_identity(data_pd_y)\n",
    "#     print(data_identity_onehot_y.shape)\n",
    "#     print(data_identity_onehot_y[2000])\n",
    "\n",
    "# def test_encode_activity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_activity_onehot_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     print(data_activity_onehot_y.shape)\n",
    "#     print(data_activity_onehot_y[1560])\n",
    "\n",
    "# def test_encode_location():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_location_onehot_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     print(data_location_onehot_y.shape)\n",
    "#     print(data_location_onehot_y[1560])\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     print(\"data_identity_onehot_y\",data_identity_onehot_y.shape)\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1)\n",
    "#     print(\"count_data\",count_data.shape)\n",
    "#     count_data = count_data.reshape(-1, 1)  # shape = (11286, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)  \n",
    "#     count_data_onehot = encoder.fit_transform(count_data)\n",
    "#     print(count_data_onehot.shape)  \n",
    "#     count_data_onehot = count_data_onehot.astype(\"int8\")\n",
    "\n",
    "#     return count_data_onehot\n",
    "\n",
    "\n",
    "# def test_encode_count():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_count_onehot_y = encode_count(data_pd_y)\n",
    "#     print(data_count_onehot_y.shape)\n",
    "#     print(data_count_onehot_y[20])\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     test_encode_count()\n",
    "# #     test_load_data_y()\n",
    "# #     test_load_data_x()\n",
    "# #     test_encode_identity()\n",
    "# #     test_encode_activity()\n",
    "# #     test_encode_location()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ce0126",
   "metadata": {
    "papermill": {
     "duration": 0.006695,
     "end_time": "2025-12-28T17:00:55.433901",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.427206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 4: preprocess.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dce15e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T17:00:55.449183Z",
     "iopub.status.busy": "2025-12-28T17:00:55.448927Z",
     "iopub.status.idle": "2025-12-28T17:00:55.463609Z",
     "shell.execute_reply": "2025-12-28T17:00:55.463063Z"
    },
    "papermill": {
     "duration": 0.024087,
     "end_time": "2025-12-28T17:00:55.464841",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.440754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          preprocess.py\n",
    "[description]   preprocess WiFi CSI data\n",
    "\"\"\"\n",
    "\n",
    "# All necessary libraries are already imported in Cell 1.\n",
    "\n",
    "# def mat_to_amp(data_mat):\n",
    "#     \"\"\"\n",
    "#     Calculate amplitude of raw WiFi CSI data.\n",
    "#     \"\"\"\n",
    "#     var_length = data_mat[\"trace\"].shape[0]\n",
    "#     data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "#     data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "#     return data_csi_amp\n",
    "\n",
    "def extract_csi_amp(var_dir_mat, var_dir_amp):\n",
    "    \"\"\"\n",
    "    Read raw WiFi CSI (*.mat) files, calculate CSI amplitude, and save as (*.npy).\n",
    "    \"\"\"\n",
    "    var_path_mat = os.listdir(var_dir_mat)\n",
    "    for var_c, var_path in enumerate(var_path_mat):\n",
    "        data_mat = scio.loadmat(os.path.join(var_dir_mat, var_path))\n",
    "        data_csi_amp = mat_to_amp(data_mat)\n",
    "        # print(var_c, data_csi_amp.shape)\n",
    "        var_path_save = os.path.join(var_dir_amp, var_path.replace(\".mat\", \".npy\"))\n",
    "        with open(var_path_save, \"wb\") as var_file:\n",
    "            np.save(var_file, data_csi_amp)\n",
    "\n",
    "\n",
    "\n",
    "# # تنظیمات low-rank (بدون تغییر ورودی mat_to_amp)\n",
    "# LOW_RANK_ENERGY = 0.95   # مثلاً 95% انرژی\n",
    "# LOW_RANK_RANK = None     # اگر عدد بذاری (مثلاً 5)، به جای ENERGY از rank ثابت استفاده میشه\n",
    "\n",
    "# def _low_rank_approx(X, rank=None, energy=0.95):\n",
    "#     X = np.asarray(X)\n",
    "\n",
    "#     was_1d = (X.ndim == 1)\n",
    "#     if was_1d:\n",
    "#         X = X[:, None]\n",
    "\n",
    "#     U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "#     if rank is None:\n",
    "#         s2 = S**2\n",
    "#         cum = np.cumsum(s2) / (np.sum(s2) + 1e-12)\n",
    "#         rank = int(np.searchsorted(cum, energy) + 1)\n",
    "\n",
    "#     rank = max(1, min(rank, S.shape[0]))\n",
    "#     X_lr = (U[:, :rank] * S[:rank]) @ Vt[:rank, :]\n",
    "\n",
    "#     if was_1d:\n",
    "#         X_lr = X_lr[:, 0]\n",
    "\n",
    "#     return X_lr.astype(np.float32)\n",
    "\n",
    "# def mat_to_amp(data_mat):\n",
    "#     \"\"\"\n",
    "#     Calculate amplitude of raw WiFi CSI data, then return its low-rank approximation.\n",
    "#     (ورودی تابع تغییر نکرده)\n",
    "#     \"\"\"\n",
    "#     var_length = data_mat[\"trace\"].shape[0]\n",
    "#     data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "#     data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "#     # خروجی low-rank با همان ابعاد\n",
    "#     data_csi_amp_lr = _low_rank_approx(\n",
    "#         data_csi_amp,\n",
    "#         rank=LOW_RANK_RANK,\n",
    "#         energy=LOW_RANK_ENERGY\n",
    "#     )\n",
    "#     return data_csi_amp_lr\n",
    "\n",
    "\n",
    "\n",
    "# # تنظیمات sparsity (بدون تغییر ورودی mat_to_amp)\n",
    "# SPARSE_KEEP_RATIO = 0.10   # مثلا فقط 10% بزرگترین مقادیر نگه داشته بشن\n",
    "# SPARSE_MIN_ABS = None      # اگر عدد بذاری (مثلا 0.5)، به جای keep_ratio آستانه ثابت میشه\n",
    "\n",
    "# def _to_sparse(X, keep_ratio=0.10, min_abs=None):\n",
    "#     \"\"\"\n",
    "#     Convert X to a sparse representation by keeping only large-magnitude entries.\n",
    "#     Returns:\n",
    "#       - scipy.sparse.csr_matrix if SciPy is available\n",
    "#       - otherwise returns a dense array with many zeros (still \"sparse\" in content)\n",
    "#     \"\"\"\n",
    "#     X = np.asarray(X)\n",
    "#     flat = X.ravel()\n",
    "#     absflat = np.abs(flat)\n",
    "\n",
    "#     if flat.size == 0:\n",
    "#         return X.astype(np.float32)\n",
    "\n",
    "#     # انتخاب آستانه\n",
    "#     if min_abs is not None:\n",
    "#         thr = float(min_abs)\n",
    "#         mask = absflat >= thr\n",
    "#     else:\n",
    "#         k = int(np.ceil(keep_ratio * flat.size))\n",
    "#         k = max(1, min(k, flat.size))\n",
    "#         if k == flat.size:\n",
    "#             mask = np.ones_like(absflat, dtype=bool)\n",
    "#         else:\n",
    "#             thr = np.partition(absflat, -k)[-k]  # kth largest magnitude\n",
    "#             mask = absflat >= thr\n",
    "\n",
    "#     idx = np.nonzero(mask)[0]\n",
    "#     data = flat[idx].astype(np.float32)\n",
    "\n",
    "#     # اگر SciPy هست: sparse واقعی بساز\n",
    "#     try:\n",
    "#         # معمولاً تو Cell1 یا از قبل import شده؛ اگر هم نشده باشه اینجا تلاش می‌کنه.\n",
    "#         import scipy.sparse as sp\n",
    "\n",
    "#         if X.ndim == 1:\n",
    "#             rows = idx\n",
    "#             cols = np.zeros_like(rows)\n",
    "#             shape = (X.shape[0], 1)\n",
    "#         else:\n",
    "#             rows, cols = np.unravel_index(idx, X.shape)\n",
    "#             shape = X.shape\n",
    "\n",
    "#         return sp.coo_matrix((data, (rows, cols)), shape=shape).tocsr()\n",
    "\n",
    "#     except Exception:\n",
    "#         # fallback: آرایه‌ی dense با صفرهای زیاد\n",
    "#         out = np.zeros_like(flat, dtype=np.float32)\n",
    "#         out[idx] = data\n",
    "#         return out.reshape(X.shape)\n",
    "\n",
    "def mat_to_amp(data_mat):\n",
    "    \"\"\"\n",
    "    Calculate amplitude of raw WiFi CSI data, then return its sparse version.\n",
    "    (ورودی تابع تغییر نکرده)\n",
    "    \"\"\"\n",
    "    var_length = data_mat[\"trace\"].shape[0]\n",
    "    data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "    data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "    # خروجی sparse (CSR اگر SciPy باشد)\n",
    "    return _to_sparse(data_csi_amp, keep_ratio=SPARSE_KEEP_RATIO, min_abs=SPARSE_MIN_ABS)\n",
    "\n",
    "# تنظیمات RPCA (می‌تونی عوضشون کنی)\n",
    "RPCA_MAX_ITER = 500\n",
    "RPCA_TOL = 1e-7\n",
    "RPCA_RHO = 1.5\n",
    "RPCA_MU_INIT = None     # None یعنی خودکار\n",
    "RPCA_LAMBDA = None      # None یعنی 1/sqrt(max(m,n))\n",
    "\n",
    "def _soft_threshold(X, tau):\n",
    "    return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "def _svt(X, tau):\n",
    "    # Singular Value Thresholding\n",
    "    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    s_thr = np.maximum(s - tau, 0.0)\n",
    "    # اگر همه صفر شد، سریع برگرد\n",
    "    if np.all(s_thr == 0):\n",
    "        return np.zeros_like(X)\n",
    "    return (U * s_thr) @ Vt\n",
    "\n",
    "def _rpca_ialm(M, lam=None, mu=None, rho=1.5, max_iter=500, tol=1e-7):\n",
    "    \"\"\"\n",
    "    Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "    Decompose: M = L + S\n",
    "    Returns: L, S (same shape as M)\n",
    "    \"\"\"\n",
    "    M = M.astype(np.float64, copy=False)\n",
    "    m, n = M.shape\n",
    "\n",
    "    if lam is None:\n",
    "        lam = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "    # mu پیشنهادی (خودکار)\n",
    "    if mu is None:\n",
    "        # ||M||_2 تقریباً بزرگ‌ترین singular value است\n",
    "        norm2 = np.linalg.svd(M, compute_uv=False)[0] if M.size else 1.0\n",
    "        mu = 1.25 / (norm2 + 1e-12)\n",
    "\n",
    "    L = np.zeros_like(M)\n",
    "    S = np.zeros_like(M)\n",
    "    Y = np.zeros_like(M)\n",
    "\n",
    "    normM = np.linalg.norm(M, ord='fro') + 1e-12\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # L update\n",
    "        L = _svt(M - S + (1.0/mu)*Y, 1.0/mu)\n",
    "\n",
    "        # S update (sparse)\n",
    "        S = _soft_threshold(M - L + (1.0/mu)*Y, lam/mu)\n",
    "\n",
    "        # dual update\n",
    "        R = M - L - S\n",
    "        Y = Y + mu * R\n",
    "\n",
    "        # stop\n",
    "        err = np.linalg.norm(R, ord='fro') / normM\n",
    "        if err < tol:\n",
    "            break\n",
    "\n",
    "        mu *= rho\n",
    "\n",
    "    return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "# def mat_to_amp(data_mat):\n",
    "#     \"\"\"\n",
    "#     Calculate amplitude of raw WiFi CSI data, then return RPCA sparse component S.\n",
    "#     (ورودی تابع تغییر نکرده)\n",
    "#     \"\"\"\n",
    "#     var_length = data_mat[\"trace\"].shape[0]\n",
    "#     data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "#     data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "#     was_1d = (data_csi_amp.ndim == 1)\n",
    "#     M = data_csi_amp[:, None] if was_1d else data_csi_amp\n",
    "\n",
    "#     _, S = _rpca_ialm(\n",
    "#         M,\n",
    "#         lam=RPCA_LAMBDA,\n",
    "#         mu=RPCA_MU_INIT,\n",
    "#         rho=RPCA_RHO,\n",
    "#         max_iter=RPCA_MAX_ITER,\n",
    "#         tol=RPCA_TOL\n",
    "#     )\n",
    "\n",
    "#     if was_1d:\n",
    "#         S = S[:, 0]\n",
    "\n",
    "#     return S\n",
    "\n",
    "def mat_to_amp(data_mat):\n",
    "    \"\"\"\n",
    "    Calculate amplitude of raw WiFi CSI data, then return RPCA low-rank component L.\n",
    "    (ورودی تابع تغییر نکرده)\n",
    "    \"\"\"\n",
    "    var_length = data_mat[\"trace\"].shape[0]\n",
    "    data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "    data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "    was_1d = (data_csi_amp.ndim == 1)\n",
    "    M = data_csi_amp[:, None] if was_1d else data_csi_amp\n",
    "\n",
    "    L, _ = _rpca_ialm(\n",
    "        M,\n",
    "        lam=RPCA_LAMBDA,\n",
    "        mu=RPCA_MU_INIT,\n",
    "        rho=RPCA_RHO,\n",
    "        max_iter=RPCA_MAX_ITER,\n",
    "        tol=RPCA_TOL\n",
    "    )\n",
    "\n",
    "    if was_1d:\n",
    "        L = L[:, 0]\n",
    "\n",
    "    return L\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parse arguments from input.\n",
    "    \"\"\"\n",
    "    var_args = argparse.ArgumentParser()\n",
    "    var_args.add_argument(\"--dir_mat\", default=\"/kaggle/input/wimans/wifi_csi/mat\", type=str)\n",
    "    var_args.add_argument(\"--dir_amp\", default=\"/kaggle/input/wimans/wifi_csi/amp\", type=str)\n",
    "    return var_args.parse_args()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     var_args = parse_args()\n",
    "#     extract_csi_amp(var_dir_mat=var_args.dir_mat, var_dir_amp=var_args.dir_amp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49e0d8",
   "metadata": {
    "papermill": {
     "duration": 0.007004,
     "end_time": "2025-12-28T17:00:55.479353",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.472349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 5: that.py (WiFi-based Model THAT)\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46213d1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T17:00:55.495279Z",
     "iopub.status.busy": "2025-12-28T17:00:55.494946Z",
     "iopub.status.idle": "2025-12-28T17:00:55.520387Z",
     "shell.execute_reply": "2025-12-28T17:00:55.519806Z"
    },
    "papermill": {
     "duration": 0.035081,
     "end_time": "2025-12-28T17:00:55.521679",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.486598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          that.py\n",
    "[description]   implement and evaluate WiFi-based model THAT\n",
    "                https://github.com/windofshadow/THAT\n",
    "\"\"\"\n",
    "\n",
    "# All necessary libraries are imported in Cell 1.\n",
    "# from train import train   --> Defined in Cell 6.\n",
    "# from preset import preset --> Defined in Cell 2.\n",
    "\n",
    "class Gaussian_Position(torch.nn.Module):\n",
    "    def __init__(self, var_dim_feature, var_dim_time, var_num_gaussian=10):\n",
    "        super(Gaussian_Position, self).__init__()\n",
    "        var_embedding = torch.zeros([var_num_gaussian, var_dim_feature], dtype=torch.float)\n",
    "        self.var_embedding = torch.nn.Parameter(var_embedding, requires_grad=True)\n",
    "        torch.nn.init.xavier_uniform_(self.var_embedding)\n",
    "        var_position = torch.arange(0.0, var_dim_time).unsqueeze(1).repeat(1, var_num_gaussian)\n",
    "        self.var_position = torch.nn.Parameter(var_position, requires_grad=False)\n",
    "        var_mu = torch.arange(0.0, var_dim_time, var_dim_time/var_num_gaussian).unsqueeze(0)\n",
    "        self.var_mu = torch.nn.Parameter(var_mu, requires_grad=True)\n",
    "        var_sigma = torch.tensor([50.0] * var_num_gaussian).unsqueeze(0)\n",
    "        self.var_sigma = torch.nn.Parameter(var_sigma, requires_grad=True)\n",
    "\n",
    "    def calculate_pdf(self, var_position, var_mu, var_sigma):\n",
    "        var_pdf = var_position - var_mu\n",
    "        var_pdf = - var_pdf * var_pdf\n",
    "        var_pdf = var_pdf / var_sigma / var_sigma / 2\n",
    "        var_pdf = var_pdf - torch.log(var_sigma)\n",
    "        return var_pdf\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        var_pdf = self.calculate_pdf(self.var_position, self.var_mu, self.var_sigma)\n",
    "        var_pdf = torch.softmax(var_pdf, dim=-1)\n",
    "        var_position_encoding = torch.matmul(var_pdf, self.var_embedding)\n",
    "        var_output = var_input + var_position_encoding.unsqueeze(0)\n",
    "        return var_output\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, var_dim_feature, var_num_head=10, var_size_cnn=[1, 3, 5]):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layer_norm_0 = torch.nn.LayerNorm(var_dim_feature, eps=1e-6)\n",
    "        self.layer_attention = torch.nn.MultiheadAttention(var_dim_feature, var_num_head, batch_first=True)\n",
    "        self.layer_dropout_0 = torch.nn.Dropout(0.1)\n",
    "        self.layer_norm_1 = torch.nn.LayerNorm(var_dim_feature, 1e-6)\n",
    "        layer_cnn = []\n",
    "        for var_size in var_size_cnn:\n",
    "            layer = torch.nn.Sequential(\n",
    "                torch.nn.Conv1d(var_dim_feature, var_dim_feature, var_size, padding=\"same\"),\n",
    "                torch.nn.BatchNorm1d(var_dim_feature),\n",
    "                torch.nn.Dropout(0.1),\n",
    "                torch.nn.LeakyReLU()\n",
    "            )\n",
    "            layer_cnn.append(layer)\n",
    "        self.layer_cnn = torch.nn.ModuleList(layer_cnn)\n",
    "        self.layer_dropout_1 = torch.nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        var_t = var_input\n",
    "        var_t = self.layer_norm_0(var_t)\n",
    "        var_t, _ = self.layer_attention(var_t, var_t, var_t)\n",
    "        var_t = self.layer_dropout_0(var_t)\n",
    "        var_t = var_t + var_input\n",
    "        var_s = self.layer_norm_1(var_t)\n",
    "        var_s = torch.permute(var_s, (0, 2, 1))\n",
    "        var_c = torch.stack([layer(var_s) for layer in self.layer_cnn], dim=0)\n",
    "        var_s = torch.sum(var_c, dim=0) / len(self.layer_cnn)\n",
    "        var_s = self.layer_dropout_1(var_s)\n",
    "        var_s = torch.permute(var_s, (0, 2, 1))\n",
    "        var_output = var_s + var_t\n",
    "        return var_output\n",
    "\n",
    "class THAT(torch.nn.Module):\n",
    "    def __init__(self, var_x_shape, var_y_shape):\n",
    "        super(THAT, self).__init__()\n",
    "        var_dim_feature = var_x_shape[-1]\n",
    "        var_dim_time = var_x_shape[-2]\n",
    "        var_dim_output = var_y_shape[-1]\n",
    "        # Left branch\n",
    "        self.layer_left_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "        self.layer_left_gaussian = Gaussian_Position(var_dim_feature, var_dim_time // 20)\n",
    "        var_num_left = 4\n",
    "        var_dim_left = var_dim_feature\n",
    "        self.layer_left_encoder = torch.nn.ModuleList([\n",
    "            Encoder(var_dim_feature=var_dim_left, var_num_head=10, var_size_cnn=[1, 3, 5])\n",
    "            for _ in range(var_num_left)\n",
    "        ])\n",
    "        self.layer_left_norm = torch.nn.LayerNorm(var_dim_left, eps=1e-6)\n",
    "        self.layer_left_cnn_0 = torch.nn.Conv1d(in_channels=var_dim_left, out_channels=128, kernel_size=8)\n",
    "        self.layer_left_cnn_1 = torch.nn.Conv1d(in_channels=var_dim_left, out_channels=128, kernel_size=16)\n",
    "        self.layer_left_dropout = torch.nn.Dropout(0.5)\n",
    "        # Right branch\n",
    "        self.layer_right_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "        var_num_right = 1\n",
    "        var_dim_right = var_dim_time // 20\n",
    "        self.layer_right_encoder = torch.nn.ModuleList([\n",
    "            Encoder(var_dim_feature=var_dim_right, var_num_head=10, var_size_cnn=[1, 2, 3])\n",
    "            for _ in range(var_num_right)\n",
    "        ])\n",
    "        self.layer_right_norm = torch.nn.LayerNorm(var_dim_right, eps=1e-6)\n",
    "        self.layer_right_cnn_0 = torch.nn.Conv1d(in_channels=var_dim_right, out_channels=16, kernel_size=2)\n",
    "        self.layer_right_cnn_1 = torch.nn.Conv1d(in_channels=var_dim_right, out_channels=16, kernel_size=4)\n",
    "        self.layer_right_dropout = torch.nn.Dropout(0.5)\n",
    "        self.layer_leakyrelu = torch.nn.LeakyReLU()\n",
    "        self.layer_output = torch.nn.Linear(256 + 32, var_dim_output)\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        var_t = var_input  # shape: (batch_size, time_steps, features)\n",
    "        # Left branch\n",
    "        var_left = torch.permute(var_t, (0, 2, 1))\n",
    "        var_left = self.layer_left_pooling(var_left)\n",
    "        var_left = torch.permute(var_left, (0, 2, 1))\n",
    "        var_left = self.layer_left_gaussian(var_left)\n",
    "        for layer in self.layer_left_encoder:\n",
    "            var_left = layer(var_left)\n",
    "        var_left = self.layer_left_norm(var_left)\n",
    "        var_left = torch.permute(var_left, (0, 2, 1))\n",
    "        var_left_0 = self.layer_leakyrelu(self.layer_left_cnn_0(var_left))\n",
    "        var_left_1 = self.layer_leakyrelu(self.layer_left_cnn_1(var_left))\n",
    "        var_left_0 = torch.sum(var_left_0, dim=-1)\n",
    "        var_left_1 = torch.sum(var_left_1, dim=-1)\n",
    "        var_left = torch.concat([var_left_0, var_left_1], dim=-1)\n",
    "        var_left = self.layer_left_dropout(var_left)\n",
    "        # Right branch\n",
    "        var_right = torch.permute(var_t, (0, 2, 1))\n",
    "        var_right = self.layer_right_pooling(var_right)\n",
    "        for layer in self.layer_right_encoder:\n",
    "            var_right = layer(var_right)\n",
    "        var_right = self.layer_right_norm(var_right)\n",
    "        var_right = torch.permute(var_right, (0, 2, 1))\n",
    "        var_right_0 = self.layer_leakyrelu(self.layer_right_cnn_0(var_right))\n",
    "        var_right_1 = self.layer_leakyrelu(self.layer_right_cnn_1(var_right))\n",
    "        var_right_0 = torch.sum(var_right_0, dim=-1)\n",
    "        var_right_1 = torch.sum(var_right_1, dim=-1)\n",
    "        var_right = torch.concat([var_right_0, var_right_1], dim=-1)\n",
    "        var_right = self.layer_right_dropout(var_right)\n",
    "        # Concatenate branches\n",
    "        var_t = torch.concat([var_left, var_right], dim=-1)\n",
    "        var_output = self.layer_output(var_t)\n",
    "        return var_output\n",
    "\n",
    "def run_that(data_train_x, data_train_y, data_test_x, data_test_y, var_repeat=10, init_model=None):\n",
    "    \"\"\"\n",
    "    Run WiFi-based model THAT.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data_train_x = data_train_x.reshape(data_train_x.shape[0], data_train_x.shape[1], -1)\n",
    "    data_test_x = data_test_x.reshape(data_test_x.shape[0], data_test_x.shape[1], -1)\n",
    "    var_x_shape, var_y_shape = data_train_x[0].shape, data_train_y[0].reshape(-1).shape\n",
    "    data_train_set = TensorDataset(torch.from_numpy(data_train_x), torch.from_numpy(data_train_y))\n",
    "    data_test_set = TensorDataset(torch.from_numpy(data_test_x), torch.from_numpy(data_test_y))\n",
    "    \n",
    "    result = {}\n",
    "    result_accuracy = []\n",
    "    result_time_train = []\n",
    "    result_time_test = []\n",
    "    \n",
    "    # var_macs, var_params = get_model_complexity_info(THAT(var_x_shape, var_y_shape), var_x_shape, as_strings=False)\n",
    "    # print(\"Parameters:\", var_params, \"- FLOPs:\", var_macs * 2)\n",
    "    \n",
    "    for var_r in range(var_repeat):\n",
    "        print(\"Repeat\", var_r)\n",
    "        torch.random.manual_seed(var_r + 39)\n",
    "        if init_model is not None:\n",
    "            model_that = init_model\n",
    "            lr2 = preset[\"nn\"][\"lr\"] /10\n",
    "        else:\n",
    "            model_that = THAT(var_x_shape, var_y_shape).to(device)\n",
    "            lr2 = preset[\"nn\"][\"lr\"]\n",
    "\n",
    "        optimizer = torch.optim.Adam(model_that.parameters(), lr=lr2, weight_decay=0)\n",
    "        loss = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([4] * var_y_shape[-1]).to(device))\n",
    "        var_time_0 = time.time()\n",
    "        \n",
    "        # Train\n",
    "        var_best_weight = train(model=model_that, optimizer=optimizer, loss=loss, \n",
    "                                  data_train_set=data_train_set, data_test_set=data_test_set,\n",
    "                                  var_threshold=preset[\"nn\"][\"threshold\"],\n",
    "                                  var_batch_size=preset[\"nn\"][\"batch_size\"],\n",
    "                                  var_epochs=preset[\"nn\"][\"epoch\"],\n",
    "                                  device=device)\n",
    "        var_time_1 = time.time()\n",
    "        \n",
    "        # Test\n",
    "        model_that.load_state_dict(var_best_weight)\n",
    "        with torch.no_grad():\n",
    "            predict_test_y = model_that(torch.from_numpy(data_test_x).to(device))\n",
    "        predict_test_y = (torch.sigmoid(predict_test_y) > preset[\"nn\"][\"threshold\"]).float()\n",
    "        predict_test_y = predict_test_y.detach().cpu().numpy()\n",
    "        var_time_2 = time.time()\n",
    "        \n",
    "        # Evaluate\n",
    "        data_test_y_c = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "        predict_test_y_c = predict_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "        result_acc = accuracy_score(data_test_y_c.astype(int), predict_test_y_c.astype(int))\n",
    "        result_dict = classification_report(data_test_y_c, predict_test_y_c, digits=6, zero_division=0, output_dict=True)\n",
    "        result[\"repeat_\" + str(var_r)] = result_dict\n",
    "        result_accuracy.append(result_acc)\n",
    "        result_time_train.append(var_time_1 - var_time_0)\n",
    "        result_time_test.append(var_time_2 - var_time_1)\n",
    "        print(\"repeat_\" + str(var_r), result_accuracy)\n",
    "        print(result)\n",
    "    \n",
    "    result[\"accuracy\"] = {\"avg\": np.mean(result_accuracy), \"std\": np.std(result_accuracy)}\n",
    "    result[\"time_train\"] = {\"avg\": np.mean(result_time_train), \"std\": np.std(result_time_train)}\n",
    "    result[\"time_test\"] = {\"avg\": np.mean(result_time_test), \"std\": np.std(result_time_test)}\n",
    "    # result[\"complexity\"] = {\"parameter\": var_params, \"flops\": var_macs * 2}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94daf641",
   "metadata": {
    "papermill": {
     "duration": 0.006906,
     "end_time": "2025-12-28T17:00:55.536269",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.529363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50d684e9",
   "metadata": {
    "papermill": {
     "duration": 0.007134,
     "end_time": "2025-12-28T17:00:55.550374",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.543240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell7: for RESNET18 Model\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23d9680f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T17:00:55.565755Z",
     "iopub.status.busy": "2025-12-28T17:00:55.565480Z",
     "iopub.status.idle": "2025-12-28T17:00:55.572521Z",
     "shell.execute_reply": "2025-12-28T17:00:55.571928Z"
    },
    "papermill": {
     "duration": 0.016507,
     "end_time": "2025-12-28T17:00:55.573838",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.557331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# import torch._dynamo\n",
    "# torch._dynamo.config.suppress_errors = True\n",
    "# import time\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "# import numpy as np\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# import torchvision.models as models\n",
    "# from copy import deepcopy\n",
    "\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "# torch._dynamo.config.cache_size_limit = 65536\n",
    "\n",
    "# # فرض می‌کنیم preset قبلاً تعریف شده باشه\n",
    "# # preset = { \"nn\": {\"lr\": 1e-3, \"epoch\": 10, \"batch_size\": 4, \"threshold\": 0.5}, ... }\n",
    "\n",
    "# class ResNet18Model(torch.nn.Module):\n",
    "#     def __init__(self, var_x_shape, var_y_shape):\n",
    "#         super(ResNet18Model, self).__init__()\n",
    "#         model_resnet = models.resnet18(weights=None)\n",
    "#         model_resnet.conv1 = torch.nn.Conv2d(1, 64, 7, 3, 2, bias=False)\n",
    "#         in_features_fc = model_resnet.fc.in_features  # معمولاً 512\n",
    "#         out_features_fc = var_y_shape[-1]\n",
    "#         model_resnet.fc = torch.nn.Linear(in_features_fc, out_features_fc)\n",
    "#         self.resnet = model_resnet\n",
    "\n",
    "#     def forward(self, var_input):\n",
    "#         var_input = var_input.reshape(var_input.size(0), 1, 3000, 270)\n",
    "#         return self.resnet(var_input)\n",
    "\n",
    "# def run_resnet(data_train_x, data_train_y, data_test_x, data_test_y, var_repeat=10, init_model=None):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     var_x_shape = data_train_x[0].shape\n",
    "#     var_y_shape = data_train_y[0].reshape(-1).shape\n",
    "\n",
    "#     # تغییر شکل داده‌ها روی CPU\n",
    "#     data_train_x = data_train_x.reshape(data_train_x.shape[0], 1, data_train_x.shape[1],\n",
    "#                                         data_train_x.shape[2]*data_train_x.shape[3]*data_train_x.shape[4])\n",
    "#     data_test_x  = data_test_x.reshape(data_test_x.shape[0], 1, data_test_x.shape[1],\n",
    "#                                        data_test_x.shape[2]*data_test_x.shape[3]*data_test_x.shape[4])\n",
    "    \n",
    "#     # دیتاست‌ها روی CPU\n",
    "#     data_train_set = TensorDataset(torch.from_numpy(data_train_x).float(),\n",
    "#                                    torch.from_numpy(data_train_y).float())\n",
    "#     data_test_set  = TensorDataset(torch.from_numpy(data_test_x).float(),\n",
    "#                                    torch.from_numpy(data_test_y).float())\n",
    "    \n",
    "#     result = {}\n",
    "#     result_accuracy = []\n",
    "#     result_time_train = []\n",
    "#     result_time_test = []\n",
    "    \n",
    "#     for var_r in range(var_repeat):\n",
    "#         print(\"Repeat\", var_r)\n",
    "#         torch.random.manual_seed(var_r + 39)\n",
    "        \n",
    "#         # ساخت مدل و انتقال به GPU\n",
    "#         if init_model is not None:\n",
    "#             model_resnet = init_model\n",
    "#             lr2 = preset[\"nn\"][\"lr\"] /10\n",
    "            \n",
    "#         else:\n",
    "#             model_resnet = ResNet18Model(var_x_shape, var_y_shape).to(device)\n",
    "#             lr2 = preset[\"nn\"][\"lr\"]\n",
    "\n",
    "#         optimizer = torch.optim.Adam(model_resnet.parameters(), lr=lr2, weight_decay=0)\n",
    "#         loss_func = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([6] * var_y_shape[-1]).to(device))\n",
    "        \n",
    "#         # تابع آموزش داخلی؛ دیتا روی CPU باقی می‌مونه و فقط هنگام محاسبه batch به GPU میره\n",
    "#         def train_inner():\n",
    "#             train_loader = DataLoader(data_train_set, preset[\"nn\"][\"batch_size\"], shuffle=True, pin_memory=False)\n",
    "#             test_loader = DataLoader(data_test_set, preset[\"nn\"][\"batch_size\"], shuffle=False, pin_memory=False)\n",
    "#             best_accuracy = 0\n",
    "#             best_weight = None\n",
    "            \n",
    "#             for epoch in range(preset[\"nn\"][\"epoch\"]):\n",
    "#                 t0 = time.time()\n",
    "#                 model_resnet.train()\n",
    "#                 # متغیرهای مربوط به آخرین batch آموزش\n",
    "#                 last_train_loss = None\n",
    "#                 last_train_acc = None\n",
    "#                 for batch in train_loader:\n",
    "#                     batch_x, batch_y = batch\n",
    "#                     batch_x = batch_x.to(device)\n",
    "#                     batch_y = batch_y.to(device)\n",
    "#                     outputs = model_resnet(batch_x)\n",
    "#                     loss_val = loss_func(outputs, batch_y.reshape(batch_y.shape[0], -1).float())\n",
    "#                     optimizer.zero_grad()\n",
    "#                     loss_val.backward()\n",
    "#                     optimizer.step()\n",
    "#                     last_train_loss = loss_val.item()\n",
    "#                     # محاسبه دقت آخرین batch آموزش\n",
    "#                     train_preds = (torch.sigmoid(outputs) > preset[\"nn\"][\"threshold\"]).float()\n",
    "#                     last_train_acc = accuracy_score(batch_y.reshape(batch_y.shape[0], -1).detach().cpu().numpy().astype(int),\n",
    "#                                                     train_preds.detach().cpu().numpy().astype(int))\n",
    "                \n",
    "#                 # ارزیابی روی دیتاست تست به صورت batch به batch\n",
    "#                 model_resnet.eval()\n",
    "#                 all_preds = []\n",
    "#                 all_labels = []\n",
    "#                 test_loss_val = None\n",
    "#                 with torch.no_grad():\n",
    "#                     for t_batch in test_loader:\n",
    "#                         t_x, t_y = t_batch\n",
    "#                         t_x = t_x.to(device)\n",
    "#                         outputs_test = model_resnet(t_x)\n",
    "#                         outputs_test = (torch.sigmoid(outputs_test) > preset[\"nn\"][\"threshold\"]).float()\n",
    "#                         all_preds.append(outputs_test.detach().cpu().numpy())\n",
    "#                         all_labels.append(t_y.cpu().numpy())  # اینجا تغییر دادیم\n",
    "#                 preds_cat = np.vstack(all_preds)\n",
    "#                 labels_cat = np.vstack(all_labels)\n",
    "#                 print(\"preds_cat\",preds_cat.shape)\n",
    "#                 # تبدیل به شکل (n, 6, 5)\n",
    "                \n",
    "#                 # preds_cat = preds_cat.reshape(-1, 6, 5)\n",
    "#                 # labels_cat = labels_cat.reshape(-1, 6, 5)\n",
    "\n",
    "#                 preds_cat = preds_cat.reshape(-1, 6)\n",
    "#                 labels_cat = labels_cat.reshape(-1, 6)\n",
    "                \n",
    "#                 # برای محاسبه دقت، مسطح می‌کنیم\n",
    "#                 test_acc = accuracy_score(labels_cat.reshape(labels_cat.shape[0], -1).astype(int),\n",
    "#                                           preds_cat.reshape(preds_cat.shape[0], -1).astype(int))\n",
    "#                 epoch_time = time.time() - t0\n",
    "#                 print(f\"Epoch {epoch}/{preset['nn']['epoch']} - \"\n",
    "#                       f\"Train Loss: {(last_train_loss if last_train_loss is not None else 0.0):.6f}, \"\n",
    "#                       f\"Train Acc: {(last_train_acc if last_train_acc is not None else 0.0):.6f}, \"\n",
    "#                       f\"Test Loss: {(test_loss_val if test_loss_val is not None else 0.0):.6f}, \"\n",
    "#                       f\"Test Acc: {(test_acc if test_acc is not None else 0.0):.6f} - \"\n",
    "#                       f\"Time: {epoch_time:.4f}s\")\n",
    "\n",
    "#                 if test_acc > best_accuracy:\n",
    "#                     best_accuracy = test_acc\n",
    "#                     print('-----***-----')\n",
    "#                     print(best_accuracy)\n",
    "#                     best_weight = deepcopy(model_resnet.state_dict())\n",
    "#             return best_weight\n",
    "        \n",
    "#         t0_run = time.time()\n",
    "#         best_weight = train_inner()\n",
    "#         t1_run = time.time()\n",
    "        \n",
    "#         torch.save(model_resnet.state_dict(), f\"{name_run}_model_final.pt\")\n",
    "#         model_resnet.load_state_dict(best_weight)\n",
    "#         torch.save(model_resnet.state_dict(), f\"{name_run}_best_model.pt\")\n",
    "\n",
    "#         # bad age niaz bod load koni\n",
    "#         # model_resnet = ResNet18Model(var_x_shape, var_y_shape).to(device)\n",
    "#         # model_resnet.load_state_dict(torch.load(\"resnet_model_repeat0.pt\"))\n",
    "#         # model_resnet.eval()\n",
    "\n",
    "        \n",
    "#         # ارزیابی نهایی مدل روی دیتاست تست (استفاده از batchهای کوچک)\n",
    "#         model_resnet.eval()\n",
    "#         all_preds = []\n",
    "#         test_loader_final = DataLoader(data_test_set, preset[\"nn\"][\"batch_size\"], shuffle=False, pin_memory=False)\n",
    "#         with torch.no_grad():\n",
    "#             for batch in test_loader_final:\n",
    "#                 batch_x, _ = batch\n",
    "#                 batch_x = batch_x.to(device)\n",
    "#                 all_preds.append(model_resnet(batch_x))\n",
    "#         preds_all = torch.cat(all_preds, dim=0)\n",
    "#         preds_final = (torch.sigmoid(preds_all) > preset[\"nn\"][\"threshold\"]).float().detach().cpu().numpy()\n",
    "#         t2_run = time.time()\n",
    "        \n",
    "#         data_test_y_np = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "#         preds_final = preds_final.reshape(-1, data_test_y.shape[-1])\n",
    "#         acc_final = accuracy_score(data_test_y_np.astype(int), preds_final.astype(int))\n",
    "#         result[f\"repeat_{var_r}\"] = {\"accuracy\": acc_final}\n",
    "#         result_accuracy.append(acc_final)\n",
    "#         result_time_train.append(t1_run - t0_run)\n",
    "#         result_time_test.append(t2_run - t1_run)\n",
    "#         print(\"Repeat\", var_r, \"Final Test Accuracy:\", acc_final)\n",
    "    \n",
    "#     result[\"accuracy\"] = {\"avg\": np.mean(result_accuracy), \"std\": np.std(result_accuracy)}\n",
    "#     result[\"time_train\"] = {\"avg\": np.mean(result_time_train), \"std\": np.std(result_time_train)}\n",
    "#     result[\"time_test\"] = {\"avg\": np.mean(result_time_test), \"std\": np.std(result_time_test)}\n",
    "#     return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c559c0",
   "metadata": {
    "papermill": {
     "duration": 0.007341,
     "end_time": "2025-12-28T17:00:55.588519",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.581178",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 9: train.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90d17c19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T17:00:55.603894Z",
     "iopub.status.busy": "2025-12-28T17:00:55.603628Z",
     "iopub.status.idle": "2025-12-28T17:00:56.151779Z",
     "shell.execute_reply": "2025-12-28T17:00:56.151139Z"
    },
    "papermill": {
     "duration": 0.557609,
     "end_time": "2025-12-28T17:00:56.153149",
     "exception": false,
     "start_time": "2025-12-28T17:00:55.595540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          train.py\n",
    "[description]   function to train WiFi-based models\n",
    "\"\"\"\n",
    "\n",
    "# All necessary libraries are imported in Cell 1.\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch._dynamo.config.cache_size_limit = 65536\n",
    "\n",
    "def train(model, optimizer, loss, data_train_set, data_test_set, var_threshold, var_batch_size, var_epochs, device):\n",
    "    \"\"\"\n",
    "    Generic training function for WiFi-based models.\n",
    "    \"\"\"\n",
    "    # دیتا رو روی CPU نگه می‌داریم (pin_memory=False)\n",
    "    data_train_loader = DataLoader(data_train_set, var_batch_size, shuffle=True, pin_memory=False)\n",
    "    data_test_loader = DataLoader(data_test_set, batch_size=len(data_test_set), shuffle=False, pin_memory=False)\n",
    "    \n",
    "    var_best_accuracy = -1.0\n",
    "    var_best_weight   = deepcopy(model.state_dict())\n",
    "    \n",
    "    \n",
    "    for var_epoch in range(var_epochs):\n",
    "        var_time_e0 = time.time()\n",
    "        model.train()\n",
    "        for data_batch in data_train_loader:\n",
    "            data_batch_x, data_batch_y = data_batch\n",
    "            # انتقال موقتی داده به GPU فقط برای forward pass\n",
    "            data_batch_x = data_batch_x.to(device)\n",
    "            data_batch_y = data_batch_y.to(device)\n",
    "            predict_train_y = model(data_batch_x)\n",
    "            var_loss_train = loss(predict_train_y, data_batch_y.reshape(data_batch_y.shape[0], -1).float())\n",
    "            optimizer.zero_grad()\n",
    "            var_loss_train.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # محاسبه دقت روی آخرین batch و انتقال نتایج به CPU\n",
    "        predict_train_y = (torch.sigmoid(predict_train_y) > var_threshold).float()\n",
    "        data_batch_y = data_batch_y.detach().cpu().numpy()\n",
    "        predict_train_y = predict_train_y.detach().cpu().numpy()\n",
    "        \n",
    "        predict_train_y = predict_train_y.reshape(-1, data_batch_y.shape[-1])\n",
    "        data_batch_y = data_batch_y.reshape(-1, data_batch_y.shape[-1])\n",
    "        var_accuracy_train = accuracy_score(data_batch_y.astype(int), predict_train_y.astype(int))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            data_test_x, data_test_y = next(iter(data_test_loader))\n",
    "            # انتقال موقتی دیتا تست به GPU برای محاسبات\n",
    "            data_test_x = data_test_x.to(device)\n",
    "            data_test_y = data_test_y.to(device)\n",
    "            \n",
    "            predict_test_y = model(data_test_x)\n",
    "            var_loss_test = loss(predict_test_y, data_test_y.reshape(data_test_y.shape[0], -1).float())\n",
    "            \n",
    "            predict_test_y = (torch.sigmoid(predict_test_y) > var_threshold).float()\n",
    "            \n",
    "            # انتقال نتایج به CPU برای ارزیابی\n",
    "            data_test_y = data_test_y.detach().cpu().numpy()\n",
    "            predict_test_y = predict_test_y.detach().cpu().numpy()\n",
    "            \n",
    "            predict_test_y = predict_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            data_test_y = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            var_accuracy_test = accuracy_score(data_test_y.astype(int), predict_test_y.astype(int))\n",
    "        \n",
    "        print(f\"Epoch {var_epoch}/{var_epochs}\",\n",
    "              \"- %.6fs\"%(time.time() - var_time_e0),\n",
    "              \"- Loss %.6f\"%var_loss_train.cpu(),\n",
    "              \"- Accuracy %.6f\"%var_accuracy_train,\n",
    "              \"- Test Loss %.6f\"%var_loss_test.cpu(),\n",
    "              \"- Test Accuracy %.6f\"%var_accuracy_test)\n",
    "            \n",
    "        if var_accuracy_test > var_best_accuracy:\n",
    "            var_best_accuracy = var_accuracy_test\n",
    "            print('-----***-----')\n",
    "            print(var_best_accuracy)\n",
    "            var_best_weight = deepcopy(model.state_dict())\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{name_run}_model_final.pt\")\n",
    "    torch.save(var_best_weight, f\"{name_run}_best_model.pt\")\n",
    "\n",
    "    \n",
    "    return var_best_weight\n",
    "\n",
    "\n",
    "\n",
    "# === importsِ لازم را یک‌بار بالای فایل اضافه کن ===\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- تابع کمکی ----------\n",
    "def save_confusion_matrix(model, data_loader, threshold, device, pdf_path):\n",
    "    \"\"\"\n",
    "    Runs the model on `data_loader`, builds a confusion matrix and writes it to `pdf_path`.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "\n",
    "            preds = (torch.sigmoid(logits) > threshold).float().cpu().numpy().ravel()\n",
    "            yb    = yb.cpu().numpy().ravel()\n",
    "\n",
    "            y_true.extend(yb)\n",
    "            y_pred.extend(preds)\n",
    "\n",
    "    cm  = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    ConfusionMatrixDisplay(cm).plot(ax=ax)\n",
    "    ax.set_title(\"Confusion Matrix – Test\")\n",
    "\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "# ---------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf7cdff",
   "metadata": {
    "papermill": {
     "duration": 0.007071,
     "end_time": "2025-12-28T17:00:56.167913",
     "exception": false,
     "start_time": "2025-12-28T17:00:56.160842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 11: run.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b5a2e9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T17:00:56.183443Z",
     "iopub.status.busy": "2025-12-28T17:00:56.182994Z",
     "iopub.status.idle": "2025-12-28T19:03:59.281781Z",
     "shell.execute_reply": "2025-12-28T19:03:59.280841Z"
    },
    "papermill": {
     "duration": 7383.108133,
     "end_time": "2025-12-28T19:03:59.283104",
     "exception": false,
     "start_time": "2025-12-28T17:00:56.174971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "\n",
      "[DEBUG] First loaded AMP sample: act_1_1\n",
      "[DEBUG] shape=(2835, 3, 3, 30), dtype=float32, complex=False\n",
      "[DEBUG] ==> Input is REAL -> amplitude-only (no true phase).\n",
      "\n",
      "Running model: THAT\n",
      "Repeat 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py:370: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1036.)\n",
      "  return F.conv1d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000 - 6.054622s - Loss 22.330406 - Accuracy 0.000000 - Test Loss 15.471610 - Test Accuracy 0.000000\n",
      "-----***-----\n",
      "0.0\n",
      "Epoch 1/1000 - 5.018663s - Loss 20.260187 - Accuracy 0.000000 - Test Loss 14.077542 - Test Accuracy 0.000000\n",
      "Epoch 2/1000 - 4.964706s - Loss 19.130066 - Accuracy 0.000000 - Test Loss 12.832545 - Test Accuracy 0.000000\n",
      "Epoch 3/1000 - 4.969963s - Loss 17.902674 - Accuracy 0.005208 - Test Loss 11.723947 - Test Accuracy 0.001768\n",
      "-----***-----\n",
      "0.0017683465959328027\n",
      "Epoch 4/1000 - 4.959840s - Loss 15.958658 - Accuracy 0.005208 - Test Loss 10.751902 - Test Accuracy 0.002653\n",
      "-----***-----\n",
      "0.002652519893899204\n",
      "Epoch 5/1000 - 5.008501s - Loss 14.776037 - Accuracy 0.005208 - Test Loss 9.883332 - Test Accuracy 0.004421\n",
      "-----***-----\n",
      "0.004420866489832007\n",
      "Epoch 6/1000 - 5.016026s - Loss 14.913952 - Accuracy 0.005208 - Test Loss 9.138152 - Test Accuracy 0.006631\n",
      "-----***-----\n",
      "0.006631299734748011\n",
      "Epoch 7/1000 - 5.019254s - Loss 15.172420 - Accuracy 0.005208 - Test Loss 8.488782 - Test Accuracy 0.010610\n",
      "-----***-----\n",
      "0.010610079575596816\n",
      "Epoch 8/1000 - 5.010062s - Loss 13.799868 - Accuracy 0.010417 - Test Loss 7.931100 - Test Accuracy 0.014147\n",
      "-----***-----\n",
      "0.014146772767462422\n",
      "Epoch 9/1000 - 4.972569s - Loss 12.881275 - Accuracy 0.005208 - Test Loss 7.460820 - Test Accuracy 0.018568\n",
      "-----***-----\n",
      "0.01856763925729443\n",
      "Epoch 10/1000 - 4.969424s - Loss 12.681668 - Accuracy 0.005208 - Test Loss 7.050875 - Test Accuracy 0.027851\n",
      "-----***-----\n",
      "0.027851458885941646\n",
      "Epoch 11/1000 - 4.964890s - Loss 12.640973 - Accuracy 0.015625 - Test Loss 6.689679 - Test Accuracy 0.038462\n",
      "-----***-----\n",
      "0.038461538461538464\n",
      "Epoch 12/1000 - 4.959573s - Loss 12.839850 - Accuracy 0.005208 - Test Loss 6.366479 - Test Accuracy 0.049072\n",
      "-----***-----\n",
      "0.04907161803713528\n",
      "Epoch 13/1000 - 4.978096s - Loss 10.795078 - Accuracy 0.026042 - Test Loss 6.074764 - Test Accuracy 0.061892\n",
      "-----***-----\n",
      "0.0618921308576481\n",
      "Epoch 14/1000 - 4.976942s - Loss 10.550399 - Accuracy 0.026042 - Test Loss 5.825115 - Test Accuracy 0.078691\n",
      "-----***-----\n",
      "0.07869142351900972\n",
      "Epoch 15/1000 - 4.967948s - Loss 11.719815 - Accuracy 0.026042 - Test Loss 5.607784 - Test Accuracy 0.092838\n",
      "-----***-----\n",
      "0.09283819628647215\n",
      "Epoch 16/1000 - 4.968176s - Loss 12.054565 - Accuracy 0.000000 - Test Loss 5.400972 - Test Accuracy 0.106985\n",
      "-----***-----\n",
      "0.10698496905393456\n",
      "Epoch 17/1000 - 5.013587s - Loss 11.902559 - Accuracy 0.015625 - Test Loss 5.215096 - Test Accuracy 0.117153\n",
      "-----***-----\n",
      "0.11715296198054818\n",
      "Epoch 18/1000 - 4.966173s - Loss 11.721647 - Accuracy 0.015625 - Test Loss 5.060555 - Test Accuracy 0.125995\n",
      "-----***-----\n",
      "0.1259946949602122\n",
      "Epoch 19/1000 - 4.974834s - Loss 9.888476 - Accuracy 0.005208 - Test Loss 4.901701 - Test Accuracy 0.138815\n",
      "-----***-----\n",
      "0.138815207780725\n",
      "Epoch 20/1000 - 5.027302s - Loss 11.179841 - Accuracy 0.036458 - Test Loss 4.761002 - Test Accuracy 0.145889\n",
      "-----***-----\n",
      "0.14588859416445624\n",
      "Epoch 21/1000 - 4.972104s - Loss 8.660622 - Accuracy 0.020833 - Test Loss 4.636768 - Test Accuracy 0.149867\n",
      "-----***-----\n",
      "0.14986737400530503\n",
      "Epoch 22/1000 - 4.977628s - Loss 8.504408 - Accuracy 0.052083 - Test Loss 4.521483 - Test Accuracy 0.160920\n",
      "-----***-----\n",
      "0.16091954022988506\n",
      "Epoch 23/1000 - 4.976091s - Loss 10.815303 - Accuracy 0.036458 - Test Loss 4.416076 - Test Accuracy 0.169761\n",
      "-----***-----\n",
      "0.16976127320954906\n",
      "Epoch 24/1000 - 5.013524s - Loss 8.607794 - Accuracy 0.041667 - Test Loss 4.317667 - Test Accuracy 0.179929\n",
      "-----***-----\n",
      "0.1799292661361627\n",
      "Epoch 25/1000 - 4.972168s - Loss 9.365118 - Accuracy 0.005208 - Test Loss 4.229432 - Test Accuracy 0.189213\n",
      "-----***-----\n",
      "0.1892130857648099\n",
      "Epoch 26/1000 - 5.002442s - Loss 9.194969 - Accuracy 0.026042 - Test Loss 4.143845 - Test Accuracy 0.196286\n",
      "-----***-----\n",
      "0.1962864721485411\n",
      "Epoch 27/1000 - 4.979083s - Loss 8.975125 - Accuracy 0.052083 - Test Loss 4.063861 - Test Accuracy 0.201149\n",
      "-----***-----\n",
      "0.20114942528735633\n",
      "Epoch 28/1000 - 4.969381s - Loss 9.355738 - Accuracy 0.026042 - Test Loss 3.989957 - Test Accuracy 0.206454\n",
      "-----***-----\n",
      "0.20645446507515472\n",
      "Epoch 29/1000 - 4.964446s - Loss 9.054231 - Accuracy 0.052083 - Test Loss 3.916797 - Test Accuracy 0.213528\n",
      "-----***-----\n",
      "0.21352785145888595\n",
      "Epoch 30/1000 - 5.017056s - Loss 8.597603 - Accuracy 0.036458 - Test Loss 3.850010 - Test Accuracy 0.221485\n",
      "-----***-----\n",
      "0.22148541114058357\n",
      "Epoch 31/1000 - 4.964819s - Loss 7.570887 - Accuracy 0.041667 - Test Loss 3.787806 - Test Accuracy 0.229443\n",
      "-----***-----\n",
      "0.22944297082228116\n",
      "Epoch 32/1000 - 5.041158s - Loss 7.766851 - Accuracy 0.057292 - Test Loss 3.732361 - Test Accuracy 0.235190\n",
      "-----***-----\n",
      "0.23519009725906279\n",
      "Epoch 33/1000 - 5.030512s - Loss 8.423088 - Accuracy 0.026042 - Test Loss 3.669646 - Test Accuracy 0.240053\n",
      "-----***-----\n",
      "0.24005305039787797\n",
      "Epoch 34/1000 - 5.003647s - Loss 8.734118 - Accuracy 0.010417 - Test Loss 3.613502 - Test Accuracy 0.246684\n",
      "-----***-----\n",
      "0.246684350132626\n",
      "Epoch 35/1000 - 4.968784s - Loss 7.675390 - Accuracy 0.057292 - Test Loss 3.562499 - Test Accuracy 0.249337\n",
      "-----***-----\n",
      "0.2493368700265252\n",
      "Epoch 36/1000 - 5.010033s - Loss 7.430573 - Accuracy 0.088542 - Test Loss 3.512731 - Test Accuracy 0.256410\n",
      "-----***-----\n",
      "0.2564102564102564\n",
      "Epoch 37/1000 - 4.970779s - Loss 7.304179 - Accuracy 0.036458 - Test Loss 3.472786 - Test Accuracy 0.260389\n",
      "-----***-----\n",
      "0.2603890362511052\n",
      "Epoch 38/1000 - 5.025429s - Loss 7.843439 - Accuracy 0.067708 - Test Loss 3.432266 - Test Accuracy 0.265694\n",
      "-----***-----\n",
      "0.2656940760389036\n",
      "Epoch 39/1000 - 4.976238s - Loss 7.582803 - Accuracy 0.062500 - Test Loss 3.388938 - Test Accuracy 0.271883\n",
      "-----***-----\n",
      "0.2718832891246684\n",
      "Epoch 40/1000 - 4.980305s - Loss 7.119596 - Accuracy 0.078125 - Test Loss 3.349176 - Test Accuracy 0.273652\n",
      "-----***-----\n",
      "0.27365163572060125\n",
      "Epoch 41/1000 - 4.963968s - Loss 7.729742 - Accuracy 0.052083 - Test Loss 3.312118 - Test Accuracy 0.274094\n",
      "-----***-----\n",
      "0.27409372236958446\n",
      "Epoch 42/1000 - 4.967793s - Loss 8.152167 - Accuracy 0.052083 - Test Loss 3.277616 - Test Accuracy 0.280283\n",
      "-----***-----\n",
      "0.28028293545534927\n",
      "Epoch 43/1000 - 4.964805s - Loss 6.406796 - Accuracy 0.052083 - Test Loss 3.244681 - Test Accuracy 0.286030\n",
      "-----***-----\n",
      "0.28603006189213087\n",
      "Epoch 44/1000 - 4.971434s - Loss 6.889752 - Accuracy 0.041667 - Test Loss 3.209770 - Test Accuracy 0.290009\n",
      "-----***-----\n",
      "0.29000884173297964\n",
      "Epoch 45/1000 - 4.962604s - Loss 6.551861 - Accuracy 0.026042 - Test Loss 3.171533 - Test Accuracy 0.295314\n",
      "-----***-----\n",
      "0.2953138815207781\n",
      "Epoch 46/1000 - 5.004740s - Loss 6.171898 - Accuracy 0.067708 - Test Loss 3.137183 - Test Accuracy 0.301945\n",
      "-----***-----\n",
      "0.3019451812555261\n",
      "Epoch 47/1000 - 4.970884s - Loss 8.414060 - Accuracy 0.031250 - Test Loss 3.109617 - Test Accuracy 0.309461\n",
      "-----***-----\n",
      "0.3094606542882405\n",
      "Epoch 48/1000 - 5.006858s - Loss 7.283919 - Accuracy 0.052083 - Test Loss 3.084977 - Test Accuracy 0.313439\n",
      "-----***-----\n",
      "0.3134394341290893\n",
      "Epoch 49/1000 - 5.024650s - Loss 6.927126 - Accuracy 0.046875 - Test Loss 3.061449 - Test Accuracy 0.316976\n",
      "-----***-----\n",
      "0.3169761273209549\n",
      "Epoch 50/1000 - 5.010354s - Loss 7.850417 - Accuracy 0.026042 - Test Loss 3.031675 - Test Accuracy 0.317418\n",
      "-----***-----\n",
      "0.31741821396993813\n",
      "Epoch 51/1000 - 4.964772s - Loss 5.864173 - Accuracy 0.062500 - Test Loss 3.004265 - Test Accuracy 0.323165\n",
      "-----***-----\n",
      "0.32316534040671974\n",
      "Epoch 52/1000 - 4.974290s - Loss 7.226559 - Accuracy 0.026042 - Test Loss 2.977230 - Test Accuracy 0.326702\n",
      "-----***-----\n",
      "0.3267020335985853\n",
      "Epoch 53/1000 - 4.973261s - Loss 7.048228 - Accuracy 0.052083 - Test Loss 2.958594 - Test Accuracy 0.328028\n",
      "-----***-----\n",
      "0.3280282935455349\n",
      "Epoch 54/1000 - 4.970428s - Loss 6.773331 - Accuracy 0.036458 - Test Loss 2.938566 - Test Accuracy 0.331123\n",
      "-----***-----\n",
      "0.3311229000884173\n",
      "Epoch 55/1000 - 4.960525s - Loss 7.149560 - Accuracy 0.067708 - Test Loss 2.914854 - Test Accuracy 0.332891\n",
      "-----***-----\n",
      "0.3328912466843501\n",
      "Epoch 56/1000 - 4.965601s - Loss 6.105787 - Accuracy 0.046875 - Test Loss 2.894399 - Test Accuracy 0.333333\n",
      "-----***-----\n",
      "0.3333333333333333\n",
      "Epoch 57/1000 - 4.978412s - Loss 6.546999 - Accuracy 0.067708 - Test Loss 2.876126 - Test Accuracy 0.335102\n",
      "-----***-----\n",
      "0.33510167992926615\n",
      "Epoch 58/1000 - 4.974334s - Loss 6.942247 - Accuracy 0.083333 - Test Loss 2.853297 - Test Accuracy 0.337754\n",
      "-----***-----\n",
      "0.33775419982316535\n",
      "Epoch 59/1000 - 4.971206s - Loss 5.735717 - Accuracy 0.078125 - Test Loss 2.836464 - Test Accuracy 0.340849\n",
      "-----***-----\n",
      "0.34084880636604775\n",
      "Epoch 60/1000 - 5.005893s - Loss 5.299461 - Accuracy 0.098958 - Test Loss 2.812836 - Test Accuracy 0.344385\n",
      "-----***-----\n",
      "0.34438549955791337\n",
      "Epoch 61/1000 - 4.965656s - Loss 6.039251 - Accuracy 0.104167 - Test Loss 2.792378 - Test Accuracy 0.345270\n",
      "-----***-----\n",
      "0.34526967285587973\n",
      "Epoch 62/1000 - 4.965600s - Loss 5.760345 - Accuracy 0.057292 - Test Loss 2.771012 - Test Accuracy 0.349691\n",
      "-----***-----\n",
      "0.34969053934571176\n",
      "Epoch 63/1000 - 4.962882s - Loss 6.728350 - Accuracy 0.062500 - Test Loss 2.750929 - Test Accuracy 0.345712\n",
      "Epoch 64/1000 - 4.967144s - Loss 6.287288 - Accuracy 0.057292 - Test Loss 2.733543 - Test Accuracy 0.347038\n",
      "Epoch 65/1000 - 5.029422s - Loss 5.747926 - Accuracy 0.083333 - Test Loss 2.715439 - Test Accuracy 0.347480\n",
      "Epoch 66/1000 - 5.017963s - Loss 6.160438 - Accuracy 0.057292 - Test Loss 2.697791 - Test Accuracy 0.349248\n",
      "Epoch 67/1000 - 4.971642s - Loss 6.248240 - Accuracy 0.046875 - Test Loss 2.682169 - Test Accuracy 0.350133\n",
      "-----***-----\n",
      "0.35013262599469497\n",
      "Epoch 68/1000 - 4.979692s - Loss 5.738837 - Accuracy 0.067708 - Test Loss 2.667770 - Test Accuracy 0.351459\n",
      "-----***-----\n",
      "0.35145888594164454\n",
      "Epoch 69/1000 - 4.969188s - Loss 5.835067 - Accuracy 0.062500 - Test Loss 2.651310 - Test Accuracy 0.354553\n",
      "-----***-----\n",
      "0.35455349248452694\n",
      "Epoch 70/1000 - 5.020703s - Loss 5.553153 - Accuracy 0.078125 - Test Loss 2.634972 - Test Accuracy 0.360743\n",
      "-----***-----\n",
      "0.36074270557029176\n",
      "Epoch 71/1000 - 4.983551s - Loss 6.160640 - Accuracy 0.088542 - Test Loss 2.619146 - Test Accuracy 0.358532\n",
      "Epoch 72/1000 - 5.000854s - Loss 5.004892 - Accuracy 0.057292 - Test Loss 2.601215 - Test Accuracy 0.363837\n",
      "-----***-----\n",
      "0.36383731211317416\n",
      "Epoch 73/1000 - 4.979975s - Loss 5.776112 - Accuracy 0.057292 - Test Loss 2.585433 - Test Accuracy 0.361627\n",
      "Epoch 74/1000 - 4.966548s - Loss 3.988119 - Accuracy 0.088542 - Test Loss 2.567711 - Test Accuracy 0.361185\n",
      "Epoch 75/1000 - 4.973349s - Loss 4.902631 - Accuracy 0.062500 - Test Loss 2.551177 - Test Accuracy 0.365164\n",
      "-----***-----\n",
      "0.3651635720601238\n",
      "Epoch 76/1000 - 4.976119s - Loss 5.014936 - Accuracy 0.088542 - Test Loss 2.535732 - Test Accuracy 0.367374\n",
      "-----***-----\n",
      "0.3673740053050398\n",
      "Epoch 77/1000 - 5.021344s - Loss 5.158583 - Accuracy 0.098958 - Test Loss 2.522760 - Test Accuracy 0.368700\n",
      "-----***-----\n",
      "0.3687002652519894\n",
      "Epoch 78/1000 - 4.964419s - Loss 5.149112 - Accuracy 0.083333 - Test Loss 2.507487 - Test Accuracy 0.371353\n",
      "-----***-----\n",
      "0.3713527851458886\n",
      "Epoch 79/1000 - 5.012147s - Loss 5.611002 - Accuracy 0.062500 - Test Loss 2.497894 - Test Accuracy 0.372237\n",
      "-----***-----\n",
      "0.372236958443855\n",
      "Epoch 80/1000 - 4.975605s - Loss 6.016502 - Accuracy 0.062500 - Test Loss 2.485348 - Test Accuracy 0.373563\n",
      "-----***-----\n",
      "0.3735632183908046\n",
      "Epoch 81/1000 - 4.981592s - Loss 4.673568 - Accuracy 0.098958 - Test Loss 2.469597 - Test Accuracy 0.376216\n",
      "-----***-----\n",
      "0.3762157382847038\n",
      "Epoch 82/1000 - 5.021516s - Loss 5.273673 - Accuracy 0.067708 - Test Loss 2.453531 - Test Accuracy 0.377100\n",
      "-----***-----\n",
      "0.3770999115826702\n",
      "Epoch 83/1000 - 5.020927s - Loss 5.400515 - Accuracy 0.083333 - Test Loss 2.435366 - Test Accuracy 0.377100\n",
      "Epoch 84/1000 - 4.966483s - Loss 5.242977 - Accuracy 0.104167 - Test Loss 2.424149 - Test Accuracy 0.380195\n",
      "-----***-----\n",
      "0.3801945181255526\n",
      "Epoch 85/1000 - 4.979048s - Loss 4.904835 - Accuracy 0.046875 - Test Loss 2.408500 - Test Accuracy 0.380637\n",
      "-----***-----\n",
      "0.3806366047745358\n",
      "Epoch 86/1000 - 5.006034s - Loss 5.095458 - Accuracy 0.083333 - Test Loss 2.393362 - Test Accuracy 0.382405\n",
      "-----***-----\n",
      "0.3824049513704686\n",
      "Epoch 87/1000 - 5.009694s - Loss 5.156413 - Accuracy 0.052083 - Test Loss 2.384331 - Test Accuracy 0.385500\n",
      "-----***-----\n",
      "0.385499557913351\n",
      "Epoch 88/1000 - 4.962012s - Loss 4.977183 - Accuracy 0.104167 - Test Loss 2.372684 - Test Accuracy 0.384173\n",
      "Epoch 89/1000 - 4.972355s - Loss 5.210003 - Accuracy 0.062500 - Test Loss 2.360072 - Test Accuracy 0.385057\n",
      "Epoch 90/1000 - 4.969510s - Loss 4.606487 - Accuracy 0.072917 - Test Loss 2.350774 - Test Accuracy 0.386384\n",
      "-----***-----\n",
      "0.3863837312113174\n",
      "Epoch 91/1000 - 4.961883s - Loss 4.942237 - Accuracy 0.062500 - Test Loss 2.335607 - Test Accuracy 0.386384\n",
      "Epoch 92/1000 - 4.972776s - Loss 5.499432 - Accuracy 0.057292 - Test Loss 2.320973 - Test Accuracy 0.387268\n",
      "-----***-----\n",
      "0.38726790450928383\n",
      "Epoch 93/1000 - 4.975349s - Loss 5.801290 - Accuracy 0.093750 - Test Loss 2.306576 - Test Accuracy 0.389920\n",
      "-----***-----\n",
      "0.38992042440318303\n",
      "Epoch 94/1000 - 4.969774s - Loss 5.245994 - Accuracy 0.072917 - Test Loss 2.293763 - Test Accuracy 0.387710\n",
      "Epoch 95/1000 - 5.025670s - Loss 5.118502 - Accuracy 0.098958 - Test Loss 2.282595 - Test Accuracy 0.387710\n",
      "Epoch 96/1000 - 4.979990s - Loss 5.157012 - Accuracy 0.072917 - Test Loss 2.271027 - Test Accuracy 0.389920\n",
      "Epoch 97/1000 - 5.032965s - Loss 4.925402 - Accuracy 0.072917 - Test Loss 2.259337 - Test Accuracy 0.389036\n",
      "Epoch 98/1000 - 4.976898s - Loss 4.702314 - Accuracy 0.062500 - Test Loss 2.245192 - Test Accuracy 0.390805\n",
      "-----***-----\n",
      "0.39080459770114945\n",
      "Epoch 99/1000 - 4.994551s - Loss 4.408587 - Accuracy 0.109375 - Test Loss 2.237833 - Test Accuracy 0.392131\n",
      "-----***-----\n",
      "0.392130857648099\n",
      "Epoch 100/1000 - 5.021695s - Loss 4.840411 - Accuracy 0.067708 - Test Loss 2.224733 - Test Accuracy 0.397436\n",
      "-----***-----\n",
      "0.3974358974358974\n",
      "Epoch 101/1000 - 4.976612s - Loss 5.425490 - Accuracy 0.088542 - Test Loss 2.216828 - Test Accuracy 0.398762\n",
      "-----***-----\n",
      "0.39876215738284704\n",
      "Epoch 102/1000 - 4.992008s - Loss 5.043488 - Accuracy 0.072917 - Test Loss 2.203816 - Test Accuracy 0.400088\n",
      "-----***-----\n",
      "0.40008841732979666\n",
      "Epoch 103/1000 - 4.980537s - Loss 4.499648 - Accuracy 0.057292 - Test Loss 2.197711 - Test Accuracy 0.398320\n",
      "Epoch 104/1000 - 4.966390s - Loss 4.763211 - Accuracy 0.093750 - Test Loss 2.189070 - Test Accuracy 0.400973\n",
      "-----***-----\n",
      "0.400972590627763\n",
      "Epoch 105/1000 - 5.027249s - Loss 3.879476 - Accuracy 0.119792 - Test Loss 2.177000 - Test Accuracy 0.400531\n",
      "Epoch 106/1000 - 5.021735s - Loss 4.156179 - Accuracy 0.088542 - Test Loss 2.167161 - Test Accuracy 0.402741\n",
      "-----***-----\n",
      "0.40274093722369586\n",
      "Epoch 107/1000 - 4.986720s - Loss 4.267957 - Accuracy 0.072917 - Test Loss 2.153522 - Test Accuracy 0.404067\n",
      "-----***-----\n",
      "0.40406719717064543\n",
      "Epoch 108/1000 - 4.976139s - Loss 4.743694 - Accuracy 0.088542 - Test Loss 2.139282 - Test Accuracy 0.401857\n",
      "Epoch 109/1000 - 5.009893s - Loss 4.681482 - Accuracy 0.114583 - Test Loss 2.129732 - Test Accuracy 0.402299\n",
      "Epoch 110/1000 - 4.966350s - Loss 4.588842 - Accuracy 0.119792 - Test Loss 2.116989 - Test Accuracy 0.406278\n",
      "-----***-----\n",
      "0.4062776304155615\n",
      "Epoch 111/1000 - 4.987630s - Loss 3.976455 - Accuracy 0.130208 - Test Loss 2.104888 - Test Accuracy 0.408488\n",
      "-----***-----\n",
      "0.40848806366047746\n",
      "Epoch 112/1000 - 4.980117s - Loss 4.385282 - Accuracy 0.104167 - Test Loss 2.092124 - Test Accuracy 0.406278\n",
      "Epoch 113/1000 - 4.972817s - Loss 4.392392 - Accuracy 0.114583 - Test Loss 2.083984 - Test Accuracy 0.408930\n",
      "-----***-----\n",
      "0.40893015030946067\n",
      "Epoch 114/1000 - 4.975745s - Loss 4.241647 - Accuracy 0.062500 - Test Loss 2.074252 - Test Accuracy 0.408488\n",
      "Epoch 115/1000 - 4.978096s - Loss 3.579132 - Accuracy 0.130208 - Test Loss 2.064569 - Test Accuracy 0.411141\n",
      "-----***-----\n",
      "0.41114058355437666\n",
      "Epoch 116/1000 - 4.976492s - Loss 3.782603 - Accuracy 0.093750 - Test Loss 2.053733 - Test Accuracy 0.417330\n",
      "-----***-----\n",
      "0.41732979664014147\n",
      "Epoch 117/1000 - 4.978778s - Loss 4.432873 - Accuracy 0.098958 - Test Loss 2.044838 - Test Accuracy 0.418214\n",
      "-----***-----\n",
      "0.4182139699381079\n",
      "Epoch 118/1000 - 4.992533s - Loss 3.925617 - Accuracy 0.083333 - Test Loss 2.037757 - Test Accuracy 0.418214\n",
      "Epoch 119/1000 - 5.009527s - Loss 4.480183 - Accuracy 0.093750 - Test Loss 2.029243 - Test Accuracy 0.418214\n",
      "Epoch 120/1000 - 4.972600s - Loss 4.114496 - Accuracy 0.151042 - Test Loss 2.021225 - Test Accuracy 0.418214\n",
      "Epoch 121/1000 - 4.957217s - Loss 3.917344 - Accuracy 0.104167 - Test Loss 2.013324 - Test Accuracy 0.419098\n",
      "-----***-----\n",
      "0.41909814323607425\n",
      "Epoch 122/1000 - 4.972334s - Loss 3.794993 - Accuracy 0.109375 - Test Loss 2.004069 - Test Accuracy 0.420866\n",
      "-----***-----\n",
      "0.4208664898320071\n",
      "Epoch 123/1000 - 4.971093s - Loss 3.504614 - Accuracy 0.140625 - Test Loss 1.994917 - Test Accuracy 0.425287\n",
      "-----***-----\n",
      "0.42528735632183906\n",
      "Epoch 124/1000 - 4.973169s - Loss 3.666217 - Accuracy 0.135417 - Test Loss 1.982384 - Test Accuracy 0.426172\n",
      "-----***-----\n",
      "0.4261715296198055\n",
      "Epoch 125/1000 - 4.991181s - Loss 3.823194 - Accuracy 0.104167 - Test Loss 1.981674 - Test Accuracy 0.428824\n",
      "-----***-----\n",
      "0.4288240495137047\n",
      "Epoch 126/1000 - 5.019669s - Loss 4.043879 - Accuracy 0.088542 - Test Loss 1.973627 - Test Accuracy 0.428382\n",
      "Epoch 127/1000 - 4.972005s - Loss 4.502140 - Accuracy 0.072917 - Test Loss 1.964078 - Test Accuracy 0.429266\n",
      "-----***-----\n",
      "0.4292661361626879\n",
      "Epoch 128/1000 - 4.973430s - Loss 4.301540 - Accuracy 0.083333 - Test Loss 1.953825 - Test Accuracy 0.431034\n",
      "-----***-----\n",
      "0.43103448275862066\n",
      "Epoch 129/1000 - 4.970983s - Loss 4.040712 - Accuracy 0.109375 - Test Loss 1.939923 - Test Accuracy 0.428382\n",
      "Epoch 130/1000 - 4.981169s - Loss 4.020517 - Accuracy 0.119792 - Test Loss 1.931999 - Test Accuracy 0.430592\n",
      "Epoch 131/1000 - 4.990356s - Loss 3.923502 - Accuracy 0.119792 - Test Loss 1.924412 - Test Accuracy 0.431034\n",
      "Epoch 132/1000 - 4.978260s - Loss 4.307770 - Accuracy 0.135417 - Test Loss 1.915344 - Test Accuracy 0.433687\n",
      "-----***-----\n",
      "0.4336870026525199\n",
      "Epoch 133/1000 - 4.997490s - Loss 3.394336 - Accuracy 0.119792 - Test Loss 1.905045 - Test Accuracy 0.432803\n",
      "Epoch 134/1000 - 4.986670s - Loss 3.336493 - Accuracy 0.119792 - Test Loss 1.895185 - Test Accuracy 0.433245\n",
      "Epoch 135/1000 - 4.976754s - Loss 3.574155 - Accuracy 0.088542 - Test Loss 1.884977 - Test Accuracy 0.435013\n",
      "-----***-----\n",
      "0.4350132625994695\n",
      "Epoch 136/1000 - 4.981804s - Loss 3.292407 - Accuracy 0.140625 - Test Loss 1.877334 - Test Accuracy 0.433245\n",
      "Epoch 137/1000 - 4.985440s - Loss 2.841922 - Accuracy 0.151042 - Test Loss 1.871837 - Test Accuracy 0.434571\n",
      "Epoch 138/1000 - 4.963075s - Loss 3.728720 - Accuracy 0.114583 - Test Loss 1.866035 - Test Accuracy 0.435013\n",
      "Epoch 139/1000 - 5.018485s - Loss 3.889958 - Accuracy 0.130208 - Test Loss 1.856154 - Test Accuracy 0.432803\n",
      "Epoch 140/1000 - 4.973305s - Loss 3.312928 - Accuracy 0.109375 - Test Loss 1.848098 - Test Accuracy 0.435013\n",
      "Epoch 141/1000 - 5.027637s - Loss 3.437956 - Accuracy 0.083333 - Test Loss 1.837468 - Test Accuracy 0.432361\n",
      "Epoch 142/1000 - 4.982616s - Loss 3.366918 - Accuracy 0.078125 - Test Loss 1.829677 - Test Accuracy 0.436340\n",
      "-----***-----\n",
      "0.4363395225464191\n",
      "Epoch 143/1000 - 4.972952s - Loss 4.055098 - Accuracy 0.114583 - Test Loss 1.818112 - Test Accuracy 0.434129\n",
      "Epoch 144/1000 - 4.968536s - Loss 2.889113 - Accuracy 0.171875 - Test Loss 1.807680 - Test Accuracy 0.432803\n",
      "Epoch 145/1000 - 4.973277s - Loss 3.285170 - Accuracy 0.104167 - Test Loss 1.799332 - Test Accuracy 0.433245\n",
      "Epoch 146/1000 - 4.972287s - Loss 2.918470 - Accuracy 0.098958 - Test Loss 1.793154 - Test Accuracy 0.435455\n",
      "Epoch 147/1000 - 4.979985s - Loss 3.774677 - Accuracy 0.078125 - Test Loss 1.784582 - Test Accuracy 0.439434\n",
      "-----***-----\n",
      "0.4394341290893015\n",
      "Epoch 148/1000 - 4.965733s - Loss 3.702514 - Accuracy 0.104167 - Test Loss 1.777455 - Test Accuracy 0.438108\n",
      "Epoch 149/1000 - 4.979852s - Loss 3.305918 - Accuracy 0.125000 - Test Loss 1.772106 - Test Accuracy 0.438550\n",
      "Epoch 150/1000 - 4.970509s - Loss 2.810634 - Accuracy 0.125000 - Test Loss 1.763654 - Test Accuracy 0.441645\n",
      "-----***-----\n",
      "0.4416445623342175\n",
      "Epoch 151/1000 - 4.988710s - Loss 3.301730 - Accuracy 0.114583 - Test Loss 1.756277 - Test Accuracy 0.443855\n",
      "-----***-----\n",
      "0.4438549955791335\n",
      "Epoch 152/1000 - 4.974635s - Loss 3.906332 - Accuracy 0.072917 - Test Loss 1.746711 - Test Accuracy 0.439434\n",
      "Epoch 153/1000 - 4.981833s - Loss 3.199579 - Accuracy 0.109375 - Test Loss 1.737152 - Test Accuracy 0.439434\n",
      "Epoch 154/1000 - 4.992778s - Loss 2.963617 - Accuracy 0.114583 - Test Loss 1.728520 - Test Accuracy 0.441645\n",
      "Epoch 155/1000 - 5.031603s - Loss 3.726347 - Accuracy 0.135417 - Test Loss 1.722943 - Test Accuracy 0.443855\n",
      "Epoch 156/1000 - 5.015460s - Loss 3.448323 - Accuracy 0.078125 - Test Loss 1.714314 - Test Accuracy 0.443413\n",
      "Epoch 157/1000 - 4.979029s - Loss 3.895321 - Accuracy 0.130208 - Test Loss 1.707808 - Test Accuracy 0.443413\n",
      "Epoch 158/1000 - 4.963157s - Loss 3.335927 - Accuracy 0.093750 - Test Loss 1.700975 - Test Accuracy 0.440760\n",
      "Epoch 159/1000 - 5.025619s - Loss 2.954709 - Accuracy 0.109375 - Test Loss 1.693962 - Test Accuracy 0.442971\n",
      "Epoch 160/1000 - 5.009207s - Loss 3.179407 - Accuracy 0.109375 - Test Loss 1.688239 - Test Accuracy 0.444739\n",
      "-----***-----\n",
      "0.4447391688770999\n",
      "Epoch 161/1000 - 4.975283s - Loss 2.984224 - Accuracy 0.145833 - Test Loss 1.679329 - Test Accuracy 0.442529\n",
      "Epoch 162/1000 - 5.043454s - Loss 3.540097 - Accuracy 0.140625 - Test Loss 1.672921 - Test Accuracy 0.446508\n",
      "-----***-----\n",
      "0.4465075154730327\n",
      "Epoch 163/1000 - 5.018361s - Loss 3.095235 - Accuracy 0.182292 - Test Loss 1.666120 - Test Accuracy 0.446950\n",
      "-----***-----\n",
      "0.4469496021220159\n",
      "Epoch 164/1000 - 5.015003s - Loss 2.988296 - Accuracy 0.182292 - Test Loss 1.657966 - Test Accuracy 0.449160\n",
      "-----***-----\n",
      "0.44916003536693194\n",
      "Epoch 165/1000 - 5.025158s - Loss 3.766516 - Accuracy 0.119792 - Test Loss 1.648641 - Test Accuracy 0.448718\n",
      "Epoch 166/1000 - 4.979635s - Loss 2.764081 - Accuracy 0.140625 - Test Loss 1.641949 - Test Accuracy 0.450044\n",
      "-----***-----\n",
      "0.4500442086648983\n",
      "Epoch 167/1000 - 4.999932s - Loss 2.878468 - Accuracy 0.140625 - Test Loss 1.634627 - Test Accuracy 0.450044\n",
      "Epoch 168/1000 - 4.983179s - Loss 3.468765 - Accuracy 0.062500 - Test Loss 1.627326 - Test Accuracy 0.447834\n",
      "Epoch 169/1000 - 5.013766s - Loss 2.677985 - Accuracy 0.182292 - Test Loss 1.618800 - Test Accuracy 0.443855\n",
      "Epoch 170/1000 - 4.972386s - Loss 3.555772 - Accuracy 0.140625 - Test Loss 1.615213 - Test Accuracy 0.443855\n",
      "Epoch 171/1000 - 4.982459s - Loss 3.445294 - Accuracy 0.104167 - Test Loss 1.610581 - Test Accuracy 0.446065\n",
      "Epoch 172/1000 - 4.989158s - Loss 3.381428 - Accuracy 0.109375 - Test Loss 1.603174 - Test Accuracy 0.448276\n",
      "Epoch 173/1000 - 4.997618s - Loss 2.979975 - Accuracy 0.135417 - Test Loss 1.596086 - Test Accuracy 0.449602\n",
      "Epoch 174/1000 - 4.976638s - Loss 3.221439 - Accuracy 0.067708 - Test Loss 1.589697 - Test Accuracy 0.450486\n",
      "-----***-----\n",
      "0.4504862953138815\n",
      "Epoch 175/1000 - 4.969060s - Loss 2.611872 - Accuracy 0.114583 - Test Loss 1.580115 - Test Accuracy 0.451370\n",
      "-----***-----\n",
      "0.45137046861184793\n",
      "Epoch 176/1000 - 4.988685s - Loss 2.870108 - Accuracy 0.140625 - Test Loss 1.569700 - Test Accuracy 0.451370\n",
      "Epoch 177/1000 - 4.987277s - Loss 2.460307 - Accuracy 0.182292 - Test Loss 1.563322 - Test Accuracy 0.451370\n",
      "Epoch 178/1000 - 4.980113s - Loss 2.593574 - Accuracy 0.145833 - Test Loss 1.556886 - Test Accuracy 0.452255\n",
      "-----***-----\n",
      "0.45225464190981435\n",
      "Epoch 179/1000 - 4.981588s - Loss 2.767617 - Accuracy 0.161458 - Test Loss 1.551002 - Test Accuracy 0.453139\n",
      "-----***-----\n",
      "0.4531388152077807\n",
      "Epoch 180/1000 - 4.989931s - Loss 2.812435 - Accuracy 0.114583 - Test Loss 1.545330 - Test Accuracy 0.451370\n",
      "Epoch 181/1000 - 5.020401s - Loss 2.809921 - Accuracy 0.145833 - Test Loss 1.539726 - Test Accuracy 0.450486\n",
      "Epoch 182/1000 - 4.971870s - Loss 2.590219 - Accuracy 0.140625 - Test Loss 1.531890 - Test Accuracy 0.450044\n",
      "Epoch 183/1000 - 5.029458s - Loss 3.026819 - Accuracy 0.104167 - Test Loss 1.525326 - Test Accuracy 0.449602\n",
      "Epoch 184/1000 - 4.982126s - Loss 2.748616 - Accuracy 0.135417 - Test Loss 1.517593 - Test Accuracy 0.452697\n",
      "Epoch 185/1000 - 5.039929s - Loss 2.794351 - Accuracy 0.104167 - Test Loss 1.510613 - Test Accuracy 0.455349\n",
      "-----***-----\n",
      "0.45534924845269675\n",
      "Epoch 186/1000 - 4.972492s - Loss 2.245590 - Accuracy 0.161458 - Test Loss 1.506489 - Test Accuracy 0.454023\n",
      "Epoch 187/1000 - 5.019167s - Loss 2.685906 - Accuracy 0.109375 - Test Loss 1.499741 - Test Accuracy 0.453581\n",
      "Epoch 188/1000 - 4.975111s - Loss 2.931695 - Accuracy 0.109375 - Test Loss 1.490915 - Test Accuracy 0.452697\n",
      "Epoch 189/1000 - 4.974824s - Loss 2.367896 - Accuracy 0.213542 - Test Loss 1.483639 - Test Accuracy 0.454465\n",
      "Epoch 190/1000 - 5.013935s - Loss 2.464843 - Accuracy 0.166667 - Test Loss 1.478109 - Test Accuracy 0.454465\n",
      "Epoch 191/1000 - 5.000277s - Loss 2.896493 - Accuracy 0.171875 - Test Loss 1.471578 - Test Accuracy 0.453581\n",
      "Epoch 192/1000 - 4.977641s - Loss 2.617237 - Accuracy 0.151042 - Test Loss 1.465363 - Test Accuracy 0.452255\n",
      "Epoch 193/1000 - 4.980699s - Loss 2.594131 - Accuracy 0.098958 - Test Loss 1.457387 - Test Accuracy 0.451370\n",
      "Epoch 194/1000 - 4.984478s - Loss 2.617347 - Accuracy 0.140625 - Test Loss 1.454462 - Test Accuracy 0.453581\n",
      "Epoch 195/1000 - 5.040236s - Loss 2.671234 - Accuracy 0.130208 - Test Loss 1.446711 - Test Accuracy 0.451813\n",
      "Epoch 196/1000 - 4.987140s - Loss 1.988522 - Accuracy 0.187500 - Test Loss 1.437175 - Test Accuracy 0.450486\n",
      "Epoch 197/1000 - 4.988797s - Loss 2.582067 - Accuracy 0.088542 - Test Loss 1.434796 - Test Accuracy 0.451813\n",
      "Epoch 198/1000 - 4.970798s - Loss 2.750580 - Accuracy 0.156250 - Test Loss 1.427712 - Test Accuracy 0.453139\n",
      "Epoch 199/1000 - 5.000896s - Loss 2.573077 - Accuracy 0.098958 - Test Loss 1.422277 - Test Accuracy 0.454023\n",
      "Epoch 200/1000 - 5.016631s - Loss 2.433778 - Accuracy 0.171875 - Test Loss 1.411390 - Test Accuracy 0.455349\n",
      "Epoch 201/1000 - 4.997409s - Loss 2.408021 - Accuracy 0.109375 - Test Loss 1.405104 - Test Accuracy 0.450486\n",
      "Epoch 202/1000 - 4.973143s - Loss 2.550339 - Accuracy 0.130208 - Test Loss 1.399002 - Test Accuracy 0.447392\n",
      "Epoch 203/1000 - 4.986763s - Loss 2.016279 - Accuracy 0.197917 - Test Loss 1.392222 - Test Accuracy 0.447392\n",
      "Epoch 204/1000 - 4.977905s - Loss 2.335447 - Accuracy 0.177083 - Test Loss 1.386885 - Test Accuracy 0.447392\n",
      "Epoch 205/1000 - 4.983551s - Loss 3.020691 - Accuracy 0.125000 - Test Loss 1.381956 - Test Accuracy 0.449602\n",
      "Epoch 206/1000 - 4.968678s - Loss 2.609686 - Accuracy 0.145833 - Test Loss 1.376369 - Test Accuracy 0.449160\n",
      "Epoch 207/1000 - 4.982357s - Loss 2.592036 - Accuracy 0.109375 - Test Loss 1.370422 - Test Accuracy 0.449602\n",
      "Epoch 208/1000 - 4.999097s - Loss 2.021785 - Accuracy 0.119792 - Test Loss 1.364973 - Test Accuracy 0.448276\n",
      "Epoch 209/1000 - 4.994470s - Loss 2.238070 - Accuracy 0.140625 - Test Loss 1.357267 - Test Accuracy 0.451813\n",
      "Epoch 210/1000 - 4.984847s - Loss 2.250589 - Accuracy 0.218750 - Test Loss 1.351886 - Test Accuracy 0.449160\n",
      "Epoch 211/1000 - 4.988447s - Loss 2.559547 - Accuracy 0.156250 - Test Loss 1.346226 - Test Accuracy 0.449160\n",
      "Epoch 212/1000 - 4.971390s - Loss 2.713923 - Accuracy 0.192708 - Test Loss 1.343391 - Test Accuracy 0.449160\n",
      "Epoch 213/1000 - 4.995198s - Loss 2.529940 - Accuracy 0.109375 - Test Loss 1.336930 - Test Accuracy 0.450928\n",
      "Epoch 214/1000 - 4.989205s - Loss 1.726135 - Accuracy 0.187500 - Test Loss 1.328883 - Test Accuracy 0.450486\n",
      "Epoch 215/1000 - 4.981966s - Loss 2.071785 - Accuracy 0.187500 - Test Loss 1.322080 - Test Accuracy 0.446065\n",
      "Epoch 216/1000 - 5.020911s - Loss 2.031563 - Accuracy 0.145833 - Test Loss 1.315732 - Test Accuracy 0.448718\n",
      "Epoch 217/1000 - 4.993147s - Loss 2.420308 - Accuracy 0.135417 - Test Loss 1.308120 - Test Accuracy 0.446950\n",
      "Epoch 218/1000 - 4.988658s - Loss 2.582046 - Accuracy 0.156250 - Test Loss 1.305645 - Test Accuracy 0.451813\n",
      "Epoch 219/1000 - 4.989043s - Loss 2.158033 - Accuracy 0.130208 - Test Loss 1.301873 - Test Accuracy 0.458002\n",
      "-----***-----\n",
      "0.45800176834659595\n",
      "Epoch 220/1000 - 5.030836s - Loss 2.104795 - Accuracy 0.166667 - Test Loss 1.295884 - Test Accuracy 0.453581\n",
      "Epoch 221/1000 - 5.000948s - Loss 2.632105 - Accuracy 0.130208 - Test Loss 1.289990 - Test Accuracy 0.454023\n",
      "Epoch 222/1000 - 4.963281s - Loss 2.413473 - Accuracy 0.098958 - Test Loss 1.287559 - Test Accuracy 0.455349\n",
      "Epoch 223/1000 - 4.974567s - Loss 2.443946 - Accuracy 0.140625 - Test Loss 1.281402 - Test Accuracy 0.456676\n",
      "Epoch 224/1000 - 5.018812s - Loss 2.229596 - Accuracy 0.114583 - Test Loss 1.273155 - Test Accuracy 0.452697\n",
      "Epoch 225/1000 - 5.009374s - Loss 1.896692 - Accuracy 0.182292 - Test Loss 1.268264 - Test Accuracy 0.455349\n",
      "Epoch 226/1000 - 4.982457s - Loss 2.050971 - Accuracy 0.208333 - Test Loss 1.259545 - Test Accuracy 0.455349\n",
      "Epoch 227/1000 - 4.982431s - Loss 2.054016 - Accuracy 0.182292 - Test Loss 1.253572 - Test Accuracy 0.456233\n",
      "Epoch 228/1000 - 5.019448s - Loss 2.122237 - Accuracy 0.119792 - Test Loss 1.250453 - Test Accuracy 0.456676\n",
      "Epoch 229/1000 - 4.990193s - Loss 2.400160 - Accuracy 0.119792 - Test Loss 1.245968 - Test Accuracy 0.457118\n",
      "Epoch 230/1000 - 4.980017s - Loss 2.285219 - Accuracy 0.151042 - Test Loss 1.240685 - Test Accuracy 0.457560\n",
      "Epoch 231/1000 - 4.984137s - Loss 2.167718 - Accuracy 0.229167 - Test Loss 1.234270 - Test Accuracy 0.458886\n",
      "-----***-----\n",
      "0.4588859416445623\n",
      "Epoch 232/1000 - 4.974610s - Loss 2.897333 - Accuracy 0.114583 - Test Loss 1.228500 - Test Accuracy 0.458002\n",
      "Epoch 233/1000 - 4.980154s - Loss 2.429060 - Accuracy 0.135417 - Test Loss 1.224457 - Test Accuracy 0.458886\n",
      "Epoch 234/1000 - 5.021775s - Loss 2.156214 - Accuracy 0.125000 - Test Loss 1.218571 - Test Accuracy 0.458444\n",
      "Epoch 235/1000 - 4.976998s - Loss 1.745425 - Accuracy 0.218750 - Test Loss 1.211452 - Test Accuracy 0.459770\n",
      "-----***-----\n",
      "0.45977011494252873\n",
      "Epoch 236/1000 - 5.044068s - Loss 1.875925 - Accuracy 0.161458 - Test Loss 1.209146 - Test Accuracy 0.464191\n",
      "-----***-----\n",
      "0.46419098143236076\n",
      "Epoch 237/1000 - 5.026717s - Loss 2.494800 - Accuracy 0.125000 - Test Loss 1.205244 - Test Accuracy 0.462423\n",
      "Epoch 238/1000 - 5.021556s - Loss 1.808392 - Accuracy 0.156250 - Test Loss 1.200582 - Test Accuracy 0.462423\n",
      "Epoch 239/1000 - 5.011748s - Loss 1.854692 - Accuracy 0.244792 - Test Loss 1.194649 - Test Accuracy 0.461096\n",
      "Epoch 240/1000 - 4.977182s - Loss 1.985965 - Accuracy 0.151042 - Test Loss 1.188154 - Test Accuracy 0.458002\n",
      "Epoch 241/1000 - 5.018008s - Loss 2.019483 - Accuracy 0.223958 - Test Loss 1.183702 - Test Accuracy 0.458886\n",
      "Epoch 242/1000 - 5.198635s - Loss 1.700362 - Accuracy 0.166667 - Test Loss 1.176973 - Test Accuracy 0.460212\n",
      "Epoch 243/1000 - 4.998381s - Loss 1.853572 - Accuracy 0.151042 - Test Loss 1.173565 - Test Accuracy 0.461096\n",
      "Epoch 244/1000 - 4.995421s - Loss 2.345144 - Accuracy 0.130208 - Test Loss 1.168541 - Test Accuracy 0.457560\n",
      "Epoch 245/1000 - 4.996824s - Loss 2.218499 - Accuracy 0.161458 - Test Loss 1.162392 - Test Accuracy 0.455791\n",
      "Epoch 246/1000 - 4.980638s - Loss 1.978097 - Accuracy 0.161458 - Test Loss 1.157724 - Test Accuracy 0.454907\n",
      "Epoch 247/1000 - 5.004085s - Loss 2.333223 - Accuracy 0.125000 - Test Loss 1.150734 - Test Accuracy 0.454023\n",
      "Epoch 248/1000 - 5.029222s - Loss 1.645720 - Accuracy 0.161458 - Test Loss 1.146841 - Test Accuracy 0.457560\n",
      "Epoch 249/1000 - 5.030046s - Loss 1.810326 - Accuracy 0.151042 - Test Loss 1.141386 - Test Accuracy 0.457560\n",
      "Epoch 250/1000 - 4.985538s - Loss 1.974980 - Accuracy 0.151042 - Test Loss 1.138495 - Test Accuracy 0.455791\n",
      "Epoch 251/1000 - 5.031569s - Loss 2.274206 - Accuracy 0.125000 - Test Loss 1.135217 - Test Accuracy 0.456233\n",
      "Epoch 252/1000 - 4.985622s - Loss 2.031832 - Accuracy 0.166667 - Test Loss 1.128868 - Test Accuracy 0.456676\n",
      "Epoch 253/1000 - 4.982102s - Loss 2.225462 - Accuracy 0.130208 - Test Loss 1.123531 - Test Accuracy 0.457118\n",
      "Epoch 254/1000 - 5.028352s - Loss 1.849049 - Accuracy 0.177083 - Test Loss 1.120106 - Test Accuracy 0.460654\n",
      "Epoch 255/1000 - 4.992010s - Loss 1.906391 - Accuracy 0.104167 - Test Loss 1.117667 - Test Accuracy 0.462423\n",
      "Epoch 256/1000 - 4.993126s - Loss 1.815938 - Accuracy 0.203125 - Test Loss 1.111694 - Test Accuracy 0.462865\n",
      "Epoch 257/1000 - 5.044227s - Loss 1.817747 - Accuracy 0.125000 - Test Loss 1.108930 - Test Accuracy 0.461981\n",
      "Epoch 258/1000 - 4.985980s - Loss 1.530876 - Accuracy 0.250000 - Test Loss 1.103172 - Test Accuracy 0.461538\n",
      "Epoch 259/1000 - 4.989130s - Loss 1.957333 - Accuracy 0.187500 - Test Loss 1.099944 - Test Accuracy 0.461538\n",
      "Epoch 260/1000 - 4.978034s - Loss 1.762427 - Accuracy 0.135417 - Test Loss 1.095497 - Test Accuracy 0.461096\n",
      "Epoch 261/1000 - 4.988959s - Loss 1.761392 - Accuracy 0.145833 - Test Loss 1.089890 - Test Accuracy 0.460654\n",
      "Epoch 262/1000 - 4.976673s - Loss 2.208740 - Accuracy 0.161458 - Test Loss 1.085992 - Test Accuracy 0.462865\n",
      "Epoch 263/1000 - 4.993258s - Loss 2.140947 - Accuracy 0.203125 - Test Loss 1.080701 - Test Accuracy 0.462865\n",
      "Epoch 264/1000 - 4.983740s - Loss 2.213678 - Accuracy 0.140625 - Test Loss 1.076568 - Test Accuracy 0.462423\n",
      "Epoch 265/1000 - 4.982154s - Loss 1.609229 - Accuracy 0.151042 - Test Loss 1.071677 - Test Accuracy 0.464191\n",
      "Epoch 266/1000 - 4.991424s - Loss 1.336072 - Accuracy 0.244792 - Test Loss 1.068915 - Test Accuracy 0.465517\n",
      "-----***-----\n",
      "0.46551724137931033\n",
      "Epoch 267/1000 - 4.978792s - Loss 1.768166 - Accuracy 0.187500 - Test Loss 1.065543 - Test Accuracy 0.466401\n",
      "-----***-----\n",
      "0.46640141467727675\n",
      "Epoch 268/1000 - 4.972336s - Loss 1.990135 - Accuracy 0.145833 - Test Loss 1.060981 - Test Accuracy 0.462423\n",
      "Epoch 269/1000 - 5.006757s - Loss 1.743911 - Accuracy 0.156250 - Test Loss 1.057014 - Test Accuracy 0.463307\n",
      "Epoch 270/1000 - 4.976895s - Loss 1.280663 - Accuracy 0.213542 - Test Loss 1.050424 - Test Accuracy 0.461981\n",
      "Epoch 271/1000 - 4.979562s - Loss 1.793539 - Accuracy 0.187500 - Test Loss 1.047452 - Test Accuracy 0.463749\n",
      "Epoch 272/1000 - 4.981325s - Loss 1.818605 - Accuracy 0.166667 - Test Loss 1.043730 - Test Accuracy 0.461981\n",
      "Epoch 273/1000 - 4.977167s - Loss 1.408576 - Accuracy 0.213542 - Test Loss 1.039210 - Test Accuracy 0.464633\n",
      "Epoch 274/1000 - 4.978834s - Loss 1.760023 - Accuracy 0.151042 - Test Loss 1.035560 - Test Accuracy 0.464191\n",
      "Epoch 275/1000 - 4.980828s - Loss 2.104742 - Accuracy 0.125000 - Test Loss 1.030099 - Test Accuracy 0.465075\n",
      "Epoch 276/1000 - 4.981817s - Loss 1.216084 - Accuracy 0.270833 - Test Loss 1.024686 - Test Accuracy 0.466844\n",
      "-----***-----\n",
      "0.46684350132625996\n",
      "Epoch 277/1000 - 4.982348s - Loss 1.705489 - Accuracy 0.156250 - Test Loss 1.020610 - Test Accuracy 0.465959\n",
      "Epoch 278/1000 - 4.988186s - Loss 1.904556 - Accuracy 0.177083 - Test Loss 1.016118 - Test Accuracy 0.465075\n",
      "Epoch 279/1000 - 4.979771s - Loss 1.439720 - Accuracy 0.223958 - Test Loss 1.012087 - Test Accuracy 0.465517\n",
      "Epoch 280/1000 - 4.989456s - Loss 1.536011 - Accuracy 0.218750 - Test Loss 1.007937 - Test Accuracy 0.465075\n",
      "Epoch 281/1000 - 4.978852s - Loss 1.679028 - Accuracy 0.151042 - Test Loss 1.005239 - Test Accuracy 0.467286\n",
      "-----***-----\n",
      "0.46728558797524317\n",
      "Epoch 282/1000 - 4.982297s - Loss 1.592186 - Accuracy 0.239583 - Test Loss 1.002471 - Test Accuracy 0.465517\n",
      "Epoch 283/1000 - 5.024844s - Loss 1.515176 - Accuracy 0.218750 - Test Loss 0.997369 - Test Accuracy 0.464633\n",
      "Epoch 284/1000 - 5.043846s - Loss 1.829635 - Accuracy 0.125000 - Test Loss 0.994962 - Test Accuracy 0.466401\n",
      "Epoch 285/1000 - 4.980716s - Loss 1.734277 - Accuracy 0.171875 - Test Loss 0.990371 - Test Accuracy 0.464633\n",
      "Epoch 286/1000 - 4.987326s - Loss 1.746670 - Accuracy 0.135417 - Test Loss 0.985235 - Test Accuracy 0.465959\n",
      "Epoch 287/1000 - 4.975849s - Loss 1.524408 - Accuracy 0.177083 - Test Loss 0.982251 - Test Accuracy 0.467286\n",
      "Epoch 288/1000 - 4.970583s - Loss 1.884136 - Accuracy 0.140625 - Test Loss 0.978880 - Test Accuracy 0.464191\n",
      "Epoch 289/1000 - 4.975867s - Loss 1.502471 - Accuracy 0.203125 - Test Loss 0.975476 - Test Accuracy 0.464633\n",
      "Epoch 290/1000 - 4.982601s - Loss 1.745030 - Accuracy 0.182292 - Test Loss 0.971067 - Test Accuracy 0.465075\n",
      "Epoch 291/1000 - 4.976382s - Loss 1.903036 - Accuracy 0.140625 - Test Loss 0.966597 - Test Accuracy 0.467286\n",
      "Epoch 292/1000 - 4.981478s - Loss 1.536385 - Accuracy 0.177083 - Test Loss 0.962963 - Test Accuracy 0.468170\n",
      "-----***-----\n",
      "0.46816976127320953\n",
      "Epoch 293/1000 - 5.010877s - Loss 1.514854 - Accuracy 0.203125 - Test Loss 0.959096 - Test Accuracy 0.466844\n",
      "Epoch 294/1000 - 5.015404s - Loss 1.366676 - Accuracy 0.130208 - Test Loss 0.955602 - Test Accuracy 0.465075\n",
      "Epoch 295/1000 - 5.011324s - Loss 1.207774 - Accuracy 0.239583 - Test Loss 0.952395 - Test Accuracy 0.465517\n",
      "Epoch 296/1000 - 4.992413s - Loss 1.214690 - Accuracy 0.229167 - Test Loss 0.948662 - Test Accuracy 0.466401\n",
      "Epoch 297/1000 - 5.029317s - Loss 1.784842 - Accuracy 0.140625 - Test Loss 0.945107 - Test Accuracy 0.465517\n",
      "Epoch 298/1000 - 4.985089s - Loss 1.414568 - Accuracy 0.166667 - Test Loss 0.942292 - Test Accuracy 0.467286\n",
      "Epoch 299/1000 - 4.996734s - Loss 1.680410 - Accuracy 0.161458 - Test Loss 0.940795 - Test Accuracy 0.469054\n",
      "-----***-----\n",
      "0.46905393457117595\n",
      "Epoch 300/1000 - 4.982968s - Loss 1.440695 - Accuracy 0.229167 - Test Loss 0.935561 - Test Accuracy 0.466844\n",
      "Epoch 301/1000 - 4.986130s - Loss 1.260706 - Accuracy 0.177083 - Test Loss 0.932092 - Test Accuracy 0.466401\n",
      "Epoch 302/1000 - 4.973354s - Loss 1.722199 - Accuracy 0.145833 - Test Loss 0.927971 - Test Accuracy 0.462865\n",
      "Epoch 303/1000 - 5.041811s - Loss 1.395365 - Accuracy 0.229167 - Test Loss 0.925931 - Test Accuracy 0.466401\n",
      "Epoch 304/1000 - 5.012547s - Loss 1.548708 - Accuracy 0.177083 - Test Loss 0.922232 - Test Accuracy 0.466844\n",
      "Epoch 305/1000 - 4.996919s - Loss 1.273966 - Accuracy 0.182292 - Test Loss 0.920086 - Test Accuracy 0.465075\n",
      "Epoch 306/1000 - 4.977047s - Loss 1.404004 - Accuracy 0.213542 - Test Loss 0.916935 - Test Accuracy 0.466844\n",
      "Epoch 307/1000 - 4.992640s - Loss 1.454691 - Accuracy 0.161458 - Test Loss 0.912131 - Test Accuracy 0.468170\n",
      "Epoch 308/1000 - 5.020636s - Loss 1.642346 - Accuracy 0.182292 - Test Loss 0.908582 - Test Accuracy 0.467286\n",
      "Epoch 309/1000 - 4.969945s - Loss 1.459584 - Accuracy 0.197917 - Test Loss 0.905184 - Test Accuracy 0.463749\n",
      "Epoch 310/1000 - 4.991906s - Loss 1.496451 - Accuracy 0.177083 - Test Loss 0.900961 - Test Accuracy 0.464633\n",
      "Epoch 311/1000 - 4.964638s - Loss 1.315843 - Accuracy 0.250000 - Test Loss 0.896966 - Test Accuracy 0.464633\n",
      "Epoch 312/1000 - 4.985868s - Loss 1.096807 - Accuracy 0.250000 - Test Loss 0.893819 - Test Accuracy 0.465517\n",
      "Epoch 313/1000 - 4.986365s - Loss 1.905830 - Accuracy 0.192708 - Test Loss 0.891572 - Test Accuracy 0.466844\n",
      "Epoch 314/1000 - 4.978201s - Loss 1.327689 - Accuracy 0.234375 - Test Loss 0.887172 - Test Accuracy 0.468612\n",
      "Epoch 315/1000 - 4.980820s - Loss 1.385195 - Accuracy 0.187500 - Test Loss 0.884384 - Test Accuracy 0.469054\n",
      "Epoch 316/1000 - 4.979303s - Loss 1.368127 - Accuracy 0.255208 - Test Loss 0.880298 - Test Accuracy 0.468170\n",
      "Epoch 317/1000 - 5.003834s - Loss 1.088113 - Accuracy 0.276042 - Test Loss 0.877625 - Test Accuracy 0.466844\n",
      "Epoch 318/1000 - 4.977219s - Loss 1.706532 - Accuracy 0.093750 - Test Loss 0.875013 - Test Accuracy 0.464633\n",
      "Epoch 319/1000 - 5.047403s - Loss 1.496927 - Accuracy 0.255208 - Test Loss 0.870540 - Test Accuracy 0.468170\n",
      "Epoch 320/1000 - 5.024326s - Loss 1.112534 - Accuracy 0.286458 - Test Loss 0.867411 - Test Accuracy 0.470380\n",
      "-----***-----\n",
      "0.47038019451812557\n",
      "Epoch 321/1000 - 5.035529s - Loss 1.255142 - Accuracy 0.192708 - Test Loss 0.865244 - Test Accuracy 0.468612\n",
      "Epoch 322/1000 - 5.023420s - Loss 1.178001 - Accuracy 0.187500 - Test Loss 0.862638 - Test Accuracy 0.466844\n",
      "Epoch 323/1000 - 5.025258s - Loss 1.454962 - Accuracy 0.218750 - Test Loss 0.860949 - Test Accuracy 0.465517\n",
      "Epoch 324/1000 - 5.049368s - Loss 1.139263 - Accuracy 0.213542 - Test Loss 0.858872 - Test Accuracy 0.465517\n",
      "Epoch 325/1000 - 5.037990s - Loss 1.395065 - Accuracy 0.255208 - Test Loss 0.855648 - Test Accuracy 0.465517\n",
      "Epoch 326/1000 - 5.026919s - Loss 1.426573 - Accuracy 0.281250 - Test Loss 0.852356 - Test Accuracy 0.466401\n",
      "Epoch 327/1000 - 5.026710s - Loss 1.172761 - Accuracy 0.192708 - Test Loss 0.847454 - Test Accuracy 0.466844\n",
      "Epoch 328/1000 - 5.021360s - Loss 1.282667 - Accuracy 0.234375 - Test Loss 0.845013 - Test Accuracy 0.469054\n",
      "Epoch 329/1000 - 5.057666s - Loss 1.234338 - Accuracy 0.218750 - Test Loss 0.841955 - Test Accuracy 0.469496\n",
      "Epoch 330/1000 - 5.037750s - Loss 1.482232 - Accuracy 0.187500 - Test Loss 0.841332 - Test Accuracy 0.469054\n",
      "Epoch 331/1000 - 5.021629s - Loss 1.214902 - Accuracy 0.265625 - Test Loss 0.838485 - Test Accuracy 0.469496\n",
      "Epoch 332/1000 - 5.020259s - Loss 1.196747 - Accuracy 0.218750 - Test Loss 0.833158 - Test Accuracy 0.467286\n",
      "Epoch 333/1000 - 5.037687s - Loss 1.079291 - Accuracy 0.182292 - Test Loss 0.832451 - Test Accuracy 0.466844\n",
      "Epoch 334/1000 - 5.024407s - Loss 1.229535 - Accuracy 0.250000 - Test Loss 0.829699 - Test Accuracy 0.466844\n",
      "Epoch 335/1000 - 5.036769s - Loss 1.328316 - Accuracy 0.229167 - Test Loss 0.828267 - Test Accuracy 0.467286\n",
      "Epoch 336/1000 - 5.023424s - Loss 1.159446 - Accuracy 0.218750 - Test Loss 0.825046 - Test Accuracy 0.467286\n",
      "Epoch 337/1000 - 5.023331s - Loss 1.350082 - Accuracy 0.171875 - Test Loss 0.823649 - Test Accuracy 0.467286\n",
      "Epoch 338/1000 - 5.018204s - Loss 1.135706 - Accuracy 0.234375 - Test Loss 0.819067 - Test Accuracy 0.469054\n",
      "Epoch 339/1000 - 5.041198s - Loss 1.429685 - Accuracy 0.234375 - Test Loss 0.816415 - Test Accuracy 0.468612\n",
      "Epoch 340/1000 - 5.057681s - Loss 1.227981 - Accuracy 0.213542 - Test Loss 0.813671 - Test Accuracy 0.469938\n",
      "Epoch 341/1000 - 5.030433s - Loss 1.220631 - Accuracy 0.187500 - Test Loss 0.810872 - Test Accuracy 0.469054\n",
      "Epoch 342/1000 - 5.015662s - Loss 1.192768 - Accuracy 0.270833 - Test Loss 0.808727 - Test Accuracy 0.468170\n",
      "Epoch 343/1000 - 5.027671s - Loss 1.247181 - Accuracy 0.239583 - Test Loss 0.806128 - Test Accuracy 0.469938\n",
      "Epoch 344/1000 - 5.021038s - Loss 1.047261 - Accuracy 0.312500 - Test Loss 0.802531 - Test Accuracy 0.470822\n",
      "-----***-----\n",
      "0.4708222811671088\n",
      "Epoch 345/1000 - 5.027438s - Loss 1.295467 - Accuracy 0.177083 - Test Loss 0.800241 - Test Accuracy 0.469054\n",
      "Epoch 346/1000 - 5.046681s - Loss 1.401032 - Accuracy 0.229167 - Test Loss 0.799285 - Test Accuracy 0.469938\n",
      "Epoch 347/1000 - 5.025790s - Loss 1.107899 - Accuracy 0.197917 - Test Loss 0.794851 - Test Accuracy 0.470822\n",
      "Epoch 348/1000 - 5.023620s - Loss 1.206358 - Accuracy 0.187500 - Test Loss 0.792086 - Test Accuracy 0.470380\n",
      "Epoch 349/1000 - 4.983677s - Loss 1.037034 - Accuracy 0.244792 - Test Loss 0.789649 - Test Accuracy 0.469938\n",
      "Epoch 350/1000 - 4.980193s - Loss 0.789598 - Accuracy 0.380208 - Test Loss 0.786158 - Test Accuracy 0.471706\n",
      "-----***-----\n",
      "0.47170645446507514\n",
      "Epoch 351/1000 - 5.007792s - Loss 0.952415 - Accuracy 0.276042 - Test Loss 0.782958 - Test Accuracy 0.473033\n",
      "-----***-----\n",
      "0.47303271441202477\n",
      "Epoch 352/1000 - 4.988980s - Loss 0.877914 - Accuracy 0.312500 - Test Loss 0.781961 - Test Accuracy 0.472149\n",
      "Epoch 353/1000 - 4.986167s - Loss 0.961349 - Accuracy 0.281250 - Test Loss 0.779957 - Test Accuracy 0.470822\n",
      "Epoch 354/1000 - 4.979418s - Loss 1.244991 - Accuracy 0.187500 - Test Loss 0.777162 - Test Accuracy 0.470380\n",
      "Epoch 355/1000 - 4.972826s - Loss 1.196635 - Accuracy 0.234375 - Test Loss 0.774813 - Test Accuracy 0.470822\n",
      "Epoch 356/1000 - 4.998526s - Loss 0.995771 - Accuracy 0.255208 - Test Loss 0.771425 - Test Accuracy 0.473033\n",
      "Epoch 357/1000 - 4.998141s - Loss 1.268465 - Accuracy 0.244792 - Test Loss 0.769227 - Test Accuracy 0.473475\n",
      "-----***-----\n",
      "0.473474801061008\n",
      "Epoch 358/1000 - 4.983941s - Loss 1.188493 - Accuracy 0.260417 - Test Loss 0.766776 - Test Accuracy 0.471706\n",
      "Epoch 359/1000 - 4.991527s - Loss 0.928849 - Accuracy 0.218750 - Test Loss 0.764032 - Test Accuracy 0.472149\n",
      "Epoch 360/1000 - 4.979836s - Loss 1.218967 - Accuracy 0.208333 - Test Loss 0.762455 - Test Accuracy 0.473475\n",
      "Epoch 361/1000 - 5.054199s - Loss 0.802431 - Accuracy 0.276042 - Test Loss 0.759906 - Test Accuracy 0.473475\n",
      "Epoch 362/1000 - 5.025317s - Loss 1.149323 - Accuracy 0.182292 - Test Loss 0.757687 - Test Accuracy 0.471706\n",
      "Epoch 363/1000 - 4.979342s - Loss 1.134249 - Accuracy 0.218750 - Test Loss 0.756828 - Test Accuracy 0.474359\n",
      "-----***-----\n",
      "0.47435897435897434\n",
      "Epoch 364/1000 - 5.010607s - Loss 0.987261 - Accuracy 0.197917 - Test Loss 0.754095 - Test Accuracy 0.472591\n",
      "Epoch 365/1000 - 4.980206s - Loss 1.101923 - Accuracy 0.296875 - Test Loss 0.753358 - Test Accuracy 0.473475\n",
      "Epoch 366/1000 - 4.978145s - Loss 1.020174 - Accuracy 0.203125 - Test Loss 0.750231 - Test Accuracy 0.473475\n",
      "Epoch 367/1000 - 4.982870s - Loss 1.095438 - Accuracy 0.218750 - Test Loss 0.749572 - Test Accuracy 0.476569\n",
      "-----***-----\n",
      "0.4765694076038904\n",
      "Epoch 368/1000 - 4.979955s - Loss 1.220978 - Accuracy 0.192708 - Test Loss 0.748094 - Test Accuracy 0.476127\n",
      "Epoch 369/1000 - 4.986180s - Loss 1.365945 - Accuracy 0.171875 - Test Loss 0.745942 - Test Accuracy 0.474359\n",
      "Epoch 370/1000 - 5.008993s - Loss 1.121581 - Accuracy 0.270833 - Test Loss 0.743728 - Test Accuracy 0.478338\n",
      "-----***-----\n",
      "0.47833775419982316\n",
      "Epoch 371/1000 - 4.993728s - Loss 0.988325 - Accuracy 0.265625 - Test Loss 0.740161 - Test Accuracy 0.475243\n",
      "Epoch 372/1000 - 4.973927s - Loss 0.997441 - Accuracy 0.229167 - Test Loss 0.738281 - Test Accuracy 0.475685\n",
      "Epoch 373/1000 - 4.980419s - Loss 1.088335 - Accuracy 0.223958 - Test Loss 0.735286 - Test Accuracy 0.474801\n",
      "Epoch 374/1000 - 5.016140s - Loss 0.950225 - Accuracy 0.223958 - Test Loss 0.733370 - Test Accuracy 0.476569\n",
      "Epoch 375/1000 - 4.994006s - Loss 0.817427 - Accuracy 0.255208 - Test Loss 0.730589 - Test Accuracy 0.475243\n",
      "Epoch 376/1000 - 4.985662s - Loss 1.207429 - Accuracy 0.223958 - Test Loss 0.728507 - Test Accuracy 0.474801\n",
      "Epoch 377/1000 - 4.981725s - Loss 0.929772 - Accuracy 0.291667 - Test Loss 0.726137 - Test Accuracy 0.475243\n",
      "Epoch 378/1000 - 4.984678s - Loss 0.830262 - Accuracy 0.255208 - Test Loss 0.724480 - Test Accuracy 0.477454\n",
      "Epoch 379/1000 - 4.988046s - Loss 0.980678 - Accuracy 0.213542 - Test Loss 0.722406 - Test Accuracy 0.477454\n",
      "Epoch 380/1000 - 4.970274s - Loss 0.963487 - Accuracy 0.234375 - Test Loss 0.721443 - Test Accuracy 0.477896\n",
      "Epoch 381/1000 - 5.005513s - Loss 1.033298 - Accuracy 0.239583 - Test Loss 0.719095 - Test Accuracy 0.477011\n",
      "Epoch 382/1000 - 4.983108s - Loss 0.957498 - Accuracy 0.312500 - Test Loss 0.716951 - Test Accuracy 0.477011\n",
      "Epoch 383/1000 - 5.025610s - Loss 1.077979 - Accuracy 0.244792 - Test Loss 0.714847 - Test Accuracy 0.479664\n",
      "-----***-----\n",
      "0.4796640141467728\n",
      "Epoch 384/1000 - 4.979176s - Loss 1.098491 - Accuracy 0.161458 - Test Loss 0.712904 - Test Accuracy 0.479664\n",
      "Epoch 385/1000 - 4.992230s - Loss 1.003878 - Accuracy 0.250000 - Test Loss 0.711664 - Test Accuracy 0.479222\n",
      "Epoch 386/1000 - 4.974270s - Loss 1.028305 - Accuracy 0.302083 - Test Loss 0.709334 - Test Accuracy 0.480106\n",
      "-----***-----\n",
      "0.48010610079575594\n",
      "Epoch 387/1000 - 4.979745s - Loss 0.901796 - Accuracy 0.208333 - Test Loss 0.707386 - Test Accuracy 0.481432\n",
      "-----***-----\n",
      "0.48143236074270557\n",
      "Epoch 388/1000 - 4.967507s - Loss 0.975873 - Accuracy 0.270833 - Test Loss 0.706582 - Test Accuracy 0.481432\n",
      "Epoch 389/1000 - 4.980141s - Loss 0.878914 - Accuracy 0.312500 - Test Loss 0.704730 - Test Accuracy 0.479222\n",
      "Epoch 390/1000 - 5.034718s - Loss 0.882717 - Accuracy 0.260417 - Test Loss 0.703453 - Test Accuracy 0.482317\n",
      "-----***-----\n",
      "0.482316534040672\n",
      "Epoch 391/1000 - 4.974236s - Loss 0.940720 - Accuracy 0.234375 - Test Loss 0.701282 - Test Accuracy 0.486295\n",
      "-----***-----\n",
      "0.48629531388152075\n",
      "Epoch 392/1000 - 5.017177s - Loss 0.822913 - Accuracy 0.296875 - Test Loss 0.699037 - Test Accuracy 0.484527\n",
      "Epoch 393/1000 - 4.978254s - Loss 0.907173 - Accuracy 0.281250 - Test Loss 0.696665 - Test Accuracy 0.484527\n",
      "Epoch 394/1000 - 5.027637s - Loss 0.867365 - Accuracy 0.250000 - Test Loss 0.694467 - Test Accuracy 0.481432\n",
      "Epoch 395/1000 - 4.988338s - Loss 1.012938 - Accuracy 0.234375 - Test Loss 0.693590 - Test Accuracy 0.483643\n",
      "Epoch 396/1000 - 4.978135s - Loss 0.829756 - Accuracy 0.286458 - Test Loss 0.690907 - Test Accuracy 0.484969\n",
      "Epoch 397/1000 - 4.979268s - Loss 0.912096 - Accuracy 0.322917 - Test Loss 0.689686 - Test Accuracy 0.485411\n",
      "Epoch 398/1000 - 4.972185s - Loss 0.811154 - Accuracy 0.276042 - Test Loss 0.687756 - Test Accuracy 0.483643\n",
      "Epoch 399/1000 - 4.977734s - Loss 0.989331 - Accuracy 0.239583 - Test Loss 0.685513 - Test Accuracy 0.485853\n",
      "Epoch 400/1000 - 4.985762s - Loss 0.792449 - Accuracy 0.338542 - Test Loss 0.683632 - Test Accuracy 0.487179\n",
      "-----***-----\n",
      "0.48717948717948717\n",
      "Epoch 401/1000 - 4.981098s - Loss 0.947294 - Accuracy 0.229167 - Test Loss 0.682681 - Test Accuracy 0.485853\n",
      "Epoch 402/1000 - 4.974461s - Loss 1.072082 - Accuracy 0.208333 - Test Loss 0.681728 - Test Accuracy 0.487622\n",
      "-----***-----\n",
      "0.4876215738284704\n",
      "Epoch 403/1000 - 4.977931s - Loss 0.982970 - Accuracy 0.239583 - Test Loss 0.680632 - Test Accuracy 0.490274\n",
      "-----***-----\n",
      "0.4902740937223696\n",
      "Epoch 404/1000 - 4.981662s - Loss 0.873438 - Accuracy 0.192708 - Test Loss 0.677784 - Test Accuracy 0.488948\n",
      "Epoch 405/1000 - 4.995185s - Loss 0.987017 - Accuracy 0.281250 - Test Loss 0.676712 - Test Accuracy 0.486295\n",
      "Epoch 406/1000 - 5.164807s - Loss 0.814005 - Accuracy 0.229167 - Test Loss 0.673173 - Test Accuracy 0.486737\n",
      "Epoch 407/1000 - 5.033067s - Loss 0.941929 - Accuracy 0.270833 - Test Loss 0.672142 - Test Accuracy 0.489390\n",
      "Epoch 408/1000 - 4.982221s - Loss 0.956818 - Accuracy 0.234375 - Test Loss 0.671208 - Test Accuracy 0.487179\n",
      "Epoch 409/1000 - 5.007798s - Loss 0.867361 - Accuracy 0.239583 - Test Loss 0.670051 - Test Accuracy 0.487622\n",
      "Epoch 410/1000 - 4.980830s - Loss 0.820692 - Accuracy 0.307292 - Test Loss 0.669562 - Test Accuracy 0.492042\n",
      "-----***-----\n",
      "0.4920424403183024\n",
      "Epoch 411/1000 - 4.985202s - Loss 1.059370 - Accuracy 0.229167 - Test Loss 0.667457 - Test Accuracy 0.490716\n",
      "Epoch 412/1000 - 4.976861s - Loss 0.701873 - Accuracy 0.291667 - Test Loss 0.664983 - Test Accuracy 0.490274\n",
      "Epoch 413/1000 - 4.970826s - Loss 0.790065 - Accuracy 0.307292 - Test Loss 0.663675 - Test Accuracy 0.489390\n",
      "Epoch 414/1000 - 4.969717s - Loss 0.774468 - Accuracy 0.312500 - Test Loss 0.662042 - Test Accuracy 0.491600\n",
      "Epoch 415/1000 - 4.987105s - Loss 1.020422 - Accuracy 0.286458 - Test Loss 0.661406 - Test Accuracy 0.491600\n",
      "Epoch 416/1000 - 4.973040s - Loss 0.850169 - Accuracy 0.260417 - Test Loss 0.659309 - Test Accuracy 0.493369\n",
      "-----***-----\n",
      "0.493368700265252\n",
      "Epoch 417/1000 - 4.978724s - Loss 0.994591 - Accuracy 0.218750 - Test Loss 0.658446 - Test Accuracy 0.492927\n",
      "Epoch 418/1000 - 4.983778s - Loss 0.876709 - Accuracy 0.244792 - Test Loss 0.656185 - Test Accuracy 0.494695\n",
      "-----***-----\n",
      "0.4946949602122016\n",
      "Epoch 419/1000 - 5.031942s - Loss 0.771205 - Accuracy 0.380208 - Test Loss 0.654831 - Test Accuracy 0.495137\n",
      "-----***-----\n",
      "0.4951370468611848\n",
      "Epoch 420/1000 - 4.969991s - Loss 0.758633 - Accuracy 0.244792 - Test Loss 0.653812 - Test Accuracy 0.497790\n",
      "-----***-----\n",
      "0.497789566755084\n",
      "Epoch 421/1000 - 4.972696s - Loss 0.816647 - Accuracy 0.390625 - Test Loss 0.651965 - Test Accuracy 0.497790\n",
      "Epoch 422/1000 - 4.992578s - Loss 0.842223 - Accuracy 0.333333 - Test Loss 0.651854 - Test Accuracy 0.497790\n",
      "Epoch 423/1000 - 4.979725s - Loss 1.174730 - Accuracy 0.234375 - Test Loss 0.650285 - Test Accuracy 0.496021\n",
      "Epoch 424/1000 - 4.978997s - Loss 0.680709 - Accuracy 0.328125 - Test Loss 0.647744 - Test Accuracy 0.495579\n",
      "Epoch 425/1000 - 5.043419s - Loss 0.914534 - Accuracy 0.244792 - Test Loss 0.646949 - Test Accuracy 0.498674\n",
      "-----***-----\n",
      "0.4986737400530504\n",
      "Epoch 426/1000 - 4.982427s - Loss 0.674767 - Accuracy 0.223958 - Test Loss 0.644568 - Test Accuracy 0.497347\n",
      "Epoch 427/1000 - 4.998830s - Loss 0.915806 - Accuracy 0.322917 - Test Loss 0.644243 - Test Accuracy 0.496905\n",
      "Epoch 428/1000 - 4.983570s - Loss 0.691901 - Accuracy 0.338542 - Test Loss 0.641886 - Test Accuracy 0.497347\n",
      "Epoch 429/1000 - 4.977622s - Loss 0.812681 - Accuracy 0.250000 - Test Loss 0.641045 - Test Accuracy 0.498674\n",
      "Epoch 430/1000 - 5.012528s - Loss 0.860588 - Accuracy 0.218750 - Test Loss 0.640239 - Test Accuracy 0.497790\n",
      "Epoch 431/1000 - 4.994493s - Loss 0.937295 - Accuracy 0.197917 - Test Loss 0.639029 - Test Accuracy 0.499116\n",
      "-----***-----\n",
      "0.4991158267020336\n",
      "Epoch 432/1000 - 5.034259s - Loss 0.803175 - Accuracy 0.234375 - Test Loss 0.636641 - Test Accuracy 0.499116\n",
      "Epoch 433/1000 - 4.978947s - Loss 0.872540 - Accuracy 0.234375 - Test Loss 0.635699 - Test Accuracy 0.497347\n",
      "Epoch 434/1000 - 4.971047s - Loss 0.857772 - Accuracy 0.312500 - Test Loss 0.633798 - Test Accuracy 0.496905\n",
      "Epoch 435/1000 - 4.990350s - Loss 0.702048 - Accuracy 0.348958 - Test Loss 0.631612 - Test Accuracy 0.496463\n",
      "Epoch 436/1000 - 4.975909s - Loss 0.780972 - Accuracy 0.307292 - Test Loss 0.629993 - Test Accuracy 0.497790\n",
      "Epoch 437/1000 - 4.978042s - Loss 0.837590 - Accuracy 0.333333 - Test Loss 0.629095 - Test Accuracy 0.499558\n",
      "-----***-----\n",
      "0.4995579133510168\n",
      "Epoch 438/1000 - 4.973694s - Loss 0.805622 - Accuracy 0.223958 - Test Loss 0.627158 - Test Accuracy 0.498674\n",
      "Epoch 439/1000 - 4.988787s - Loss 0.964313 - Accuracy 0.197917 - Test Loss 0.626659 - Test Accuracy 0.498674\n",
      "Epoch 440/1000 - 5.015981s - Loss 0.673042 - Accuracy 0.375000 - Test Loss 0.624850 - Test Accuracy 0.499116\n",
      "Epoch 441/1000 - 4.989079s - Loss 0.748610 - Accuracy 0.328125 - Test Loss 0.622860 - Test Accuracy 0.499558\n",
      "Epoch 442/1000 - 4.978674s - Loss 0.773068 - Accuracy 0.270833 - Test Loss 0.621651 - Test Accuracy 0.502210\n",
      "-----***-----\n",
      "0.502210433244916\n",
      "Epoch 443/1000 - 4.977701s - Loss 0.960579 - Accuracy 0.208333 - Test Loss 0.619833 - Test Accuracy 0.501768\n",
      "Epoch 444/1000 - 4.988827s - Loss 0.785266 - Accuracy 0.312500 - Test Loss 0.619064 - Test Accuracy 0.498674\n",
      "Epoch 445/1000 - 4.996931s - Loss 0.673646 - Accuracy 0.359375 - Test Loss 0.617920 - Test Accuracy 0.500884\n",
      "Epoch 446/1000 - 4.986177s - Loss 1.078439 - Accuracy 0.203125 - Test Loss 0.616989 - Test Accuracy 0.497790\n",
      "Epoch 447/1000 - 4.982743s - Loss 0.975557 - Accuracy 0.286458 - Test Loss 0.615976 - Test Accuracy 0.500442\n",
      "Epoch 448/1000 - 4.991212s - Loss 0.888942 - Accuracy 0.317708 - Test Loss 0.615489 - Test Accuracy 0.502653\n",
      "-----***-----\n",
      "0.5026525198938993\n",
      "Epoch 449/1000 - 5.018110s - Loss 0.851727 - Accuracy 0.317708 - Test Loss 0.614144 - Test Accuracy 0.503537\n",
      "-----***-----\n",
      "0.5035366931918656\n",
      "Epoch 450/1000 - 4.968310s - Loss 0.938225 - Accuracy 0.255208 - Test Loss 0.613033 - Test Accuracy 0.505305\n",
      "-----***-----\n",
      "0.5053050397877984\n",
      "Epoch 451/1000 - 5.027915s - Loss 0.652057 - Accuracy 0.276042 - Test Loss 0.611064 - Test Accuracy 0.505305\n",
      "Epoch 452/1000 - 4.973982s - Loss 0.752840 - Accuracy 0.291667 - Test Loss 0.610433 - Test Accuracy 0.505305\n",
      "Epoch 453/1000 - 4.983209s - Loss 0.794860 - Accuracy 0.265625 - Test Loss 0.608940 - Test Accuracy 0.503537\n",
      "Epoch 454/1000 - 5.028534s - Loss 0.696738 - Accuracy 0.317708 - Test Loss 0.608320 - Test Accuracy 0.505747\n",
      "-----***-----\n",
      "0.5057471264367817\n",
      "Epoch 455/1000 - 4.987053s - Loss 0.763319 - Accuracy 0.276042 - Test Loss 0.606896 - Test Accuracy 0.502210\n",
      "Epoch 456/1000 - 4.988993s - Loss 0.649544 - Accuracy 0.307292 - Test Loss 0.606710 - Test Accuracy 0.506631\n",
      "-----***-----\n",
      "0.506631299734748\n",
      "Epoch 457/1000 - 5.001018s - Loss 0.581022 - Accuracy 0.380208 - Test Loss 0.604874 - Test Accuracy 0.505305\n",
      "Epoch 458/1000 - 4.996964s - Loss 0.681886 - Accuracy 0.281250 - Test Loss 0.604060 - Test Accuracy 0.505747\n",
      "Epoch 459/1000 - 4.992087s - Loss 0.681085 - Accuracy 0.338542 - Test Loss 0.603107 - Test Accuracy 0.505747\n",
      "Epoch 460/1000 - 4.990329s - Loss 0.985990 - Accuracy 0.244792 - Test Loss 0.601974 - Test Accuracy 0.505305\n",
      "Epoch 461/1000 - 4.999885s - Loss 0.624489 - Accuracy 0.317708 - Test Loss 0.600939 - Test Accuracy 0.508842\n",
      "-----***-----\n",
      "0.5088417329796641\n",
      "Epoch 462/1000 - 4.980714s - Loss 0.734362 - Accuracy 0.333333 - Test Loss 0.598543 - Test Accuracy 0.510168\n",
      "-----***-----\n",
      "0.5101679929266136\n",
      "Epoch 463/1000 - 4.974989s - Loss 0.781506 - Accuracy 0.223958 - Test Loss 0.598369 - Test Accuracy 0.511052\n",
      "-----***-----\n",
      "0.51105216622458\n",
      "Epoch 464/1000 - 4.988916s - Loss 0.766409 - Accuracy 0.223958 - Test Loss 0.596483 - Test Accuracy 0.507515\n",
      "Epoch 465/1000 - 4.980409s - Loss 0.726205 - Accuracy 0.354167 - Test Loss 0.596299 - Test Accuracy 0.508400\n",
      "Epoch 466/1000 - 4.975988s - Loss 0.729890 - Accuracy 0.343750 - Test Loss 0.595153 - Test Accuracy 0.508400\n",
      "Epoch 467/1000 - 4.990146s - Loss 0.661804 - Accuracy 0.359375 - Test Loss 0.594052 - Test Accuracy 0.508400\n",
      "Epoch 468/1000 - 4.989397s - Loss 0.684408 - Accuracy 0.296875 - Test Loss 0.593170 - Test Accuracy 0.511494\n",
      "-----***-----\n",
      "0.5114942528735632\n",
      "Epoch 469/1000 - 5.034442s - Loss 0.856318 - Accuracy 0.250000 - Test Loss 0.592082 - Test Accuracy 0.510610\n",
      "Epoch 470/1000 - 4.987882s - Loss 0.761779 - Accuracy 0.270833 - Test Loss 0.590311 - Test Accuracy 0.509284\n",
      "Epoch 471/1000 - 4.980105s - Loss 0.774697 - Accuracy 0.281250 - Test Loss 0.589957 - Test Accuracy 0.510168\n",
      "Epoch 472/1000 - 5.024568s - Loss 0.678690 - Accuracy 0.296875 - Test Loss 0.589307 - Test Accuracy 0.512378\n",
      "-----***-----\n",
      "0.5123784261715296\n",
      "Epoch 473/1000 - 4.992422s - Loss 0.693673 - Accuracy 0.265625 - Test Loss 0.588815 - Test Accuracy 0.511494\n",
      "Epoch 474/1000 - 4.981893s - Loss 0.759444 - Accuracy 0.244792 - Test Loss 0.587662 - Test Accuracy 0.509726\n",
      "Epoch 475/1000 - 4.974700s - Loss 0.681130 - Accuracy 0.328125 - Test Loss 0.585723 - Test Accuracy 0.510168\n",
      "Epoch 476/1000 - 5.005882s - Loss 0.698670 - Accuracy 0.333333 - Test Loss 0.585609 - Test Accuracy 0.512378\n",
      "Epoch 477/1000 - 4.988305s - Loss 0.590936 - Accuracy 0.343750 - Test Loss 0.584546 - Test Accuracy 0.511052\n",
      "Epoch 478/1000 - 4.985561s - Loss 0.750817 - Accuracy 0.286458 - Test Loss 0.583432 - Test Accuracy 0.512821\n",
      "-----***-----\n",
      "0.5128205128205128\n",
      "Epoch 479/1000 - 4.983542s - Loss 0.726387 - Accuracy 0.286458 - Test Loss 0.581930 - Test Accuracy 0.511936\n",
      "Epoch 480/1000 - 5.031611s - Loss 0.645206 - Accuracy 0.390625 - Test Loss 0.581357 - Test Accuracy 0.514147\n",
      "-----***-----\n",
      "0.5141467727674625\n",
      "Epoch 481/1000 - 4.982895s - Loss 0.682415 - Accuracy 0.296875 - Test Loss 0.580098 - Test Accuracy 0.514589\n",
      "-----***-----\n",
      "0.5145888594164456\n",
      "Epoch 482/1000 - 4.990597s - Loss 0.714571 - Accuracy 0.338542 - Test Loss 0.579161 - Test Accuracy 0.514147\n",
      "Epoch 483/1000 - 4.974125s - Loss 0.694187 - Accuracy 0.312500 - Test Loss 0.578863 - Test Accuracy 0.514147\n",
      "Epoch 484/1000 - 4.979935s - Loss 0.698870 - Accuracy 0.281250 - Test Loss 0.577480 - Test Accuracy 0.512821\n",
      "Epoch 485/1000 - 5.031532s - Loss 0.730624 - Accuracy 0.265625 - Test Loss 0.576197 - Test Accuracy 0.512821\n",
      "Epoch 486/1000 - 5.066800s - Loss 0.668159 - Accuracy 0.302083 - Test Loss 0.575483 - Test Accuracy 0.513705\n",
      "Epoch 487/1000 - 4.976473s - Loss 0.672703 - Accuracy 0.302083 - Test Loss 0.573699 - Test Accuracy 0.514147\n",
      "Epoch 488/1000 - 4.978788s - Loss 0.595123 - Accuracy 0.354167 - Test Loss 0.572991 - Test Accuracy 0.514589\n",
      "Epoch 489/1000 - 4.971499s - Loss 0.656722 - Accuracy 0.312500 - Test Loss 0.572725 - Test Accuracy 0.513263\n",
      "Epoch 490/1000 - 4.988688s - Loss 0.873534 - Accuracy 0.375000 - Test Loss 0.571634 - Test Accuracy 0.513263\n",
      "Epoch 491/1000 - 4.971845s - Loss 0.746571 - Accuracy 0.270833 - Test Loss 0.571663 - Test Accuracy 0.515031\n",
      "-----***-----\n",
      "0.5150309460654289\n",
      "Epoch 492/1000 - 4.987245s - Loss 0.636899 - Accuracy 0.317708 - Test Loss 0.570425 - Test Accuracy 0.514147\n",
      "Epoch 493/1000 - 5.016155s - Loss 0.776217 - Accuracy 0.270833 - Test Loss 0.569773 - Test Accuracy 0.514589\n",
      "Epoch 494/1000 - 4.995837s - Loss 0.697762 - Accuracy 0.234375 - Test Loss 0.568952 - Test Accuracy 0.514147\n",
      "Epoch 495/1000 - 4.978925s - Loss 0.629768 - Accuracy 0.270833 - Test Loss 0.568885 - Test Accuracy 0.517241\n",
      "-----***-----\n",
      "0.5172413793103449\n",
      "Epoch 496/1000 - 4.976895s - Loss 0.883421 - Accuracy 0.328125 - Test Loss 0.568068 - Test Accuracy 0.517683\n",
      "-----***-----\n",
      "0.517683465959328\n",
      "Epoch 497/1000 - 4.985720s - Loss 0.706850 - Accuracy 0.333333 - Test Loss 0.565714 - Test Accuracy 0.515473\n",
      "Epoch 498/1000 - 4.989868s - Loss 0.916744 - Accuracy 0.270833 - Test Loss 0.566083 - Test Accuracy 0.517683\n",
      "Epoch 499/1000 - 4.974728s - Loss 0.588977 - Accuracy 0.359375 - Test Loss 0.565391 - Test Accuracy 0.518126\n",
      "-----***-----\n",
      "0.5181255526083113\n",
      "Epoch 500/1000 - 4.982711s - Loss 0.648601 - Accuracy 0.296875 - Test Loss 0.564129 - Test Accuracy 0.517683\n",
      "Epoch 501/1000 - 4.985673s - Loss 0.885954 - Accuracy 0.250000 - Test Loss 0.562784 - Test Accuracy 0.516799\n",
      "Epoch 502/1000 - 5.029097s - Loss 0.756638 - Accuracy 0.312500 - Test Loss 0.562790 - Test Accuracy 0.517241\n",
      "Epoch 503/1000 - 4.970092s - Loss 0.588359 - Accuracy 0.359375 - Test Loss 0.560359 - Test Accuracy 0.519010\n",
      "-----***-----\n",
      "0.5190097259062776\n",
      "Epoch 504/1000 - 4.997205s - Loss 0.753870 - Accuracy 0.338542 - Test Loss 0.559856 - Test Accuracy 0.518126\n",
      "Epoch 505/1000 - 4.986893s - Loss 0.781632 - Accuracy 0.291667 - Test Loss 0.559418 - Test Accuracy 0.516357\n",
      "Epoch 506/1000 - 5.009244s - Loss 0.661818 - Accuracy 0.322917 - Test Loss 0.559168 - Test Accuracy 0.520336\n",
      "-----***-----\n",
      "0.5203359858532273\n",
      "Epoch 507/1000 - 4.979262s - Loss 0.680515 - Accuracy 0.338542 - Test Loss 0.558750 - Test Accuracy 0.521220\n",
      "-----***-----\n",
      "0.5212201591511937\n",
      "Epoch 508/1000 - 4.972306s - Loss 0.610685 - Accuracy 0.307292 - Test Loss 0.557212 - Test Accuracy 0.518568\n",
      "Epoch 509/1000 - 4.981449s - Loss 0.590105 - Accuracy 0.380208 - Test Loss 0.555753 - Test Accuracy 0.517683\n",
      "Epoch 510/1000 - 4.974476s - Loss 0.599854 - Accuracy 0.390625 - Test Loss 0.555222 - Test Accuracy 0.517241\n",
      "Epoch 511/1000 - 4.988839s - Loss 0.645131 - Accuracy 0.411458 - Test Loss 0.555007 - Test Accuracy 0.519452\n",
      "Epoch 512/1000 - 4.969326s - Loss 0.648874 - Accuracy 0.395833 - Test Loss 0.554720 - Test Accuracy 0.519010\n",
      "Epoch 513/1000 - 4.976139s - Loss 0.821039 - Accuracy 0.239583 - Test Loss 0.554993 - Test Accuracy 0.518126\n",
      "Epoch 514/1000 - 5.030369s - Loss 0.611800 - Accuracy 0.369792 - Test Loss 0.553607 - Test Accuracy 0.520336\n",
      "Epoch 515/1000 - 4.984136s - Loss 0.665522 - Accuracy 0.302083 - Test Loss 0.552478 - Test Accuracy 0.519452\n",
      "Epoch 516/1000 - 5.025293s - Loss 0.723439 - Accuracy 0.328125 - Test Loss 0.551094 - Test Accuracy 0.520778\n",
      "Epoch 517/1000 - 4.983578s - Loss 0.712377 - Accuracy 0.270833 - Test Loss 0.550127 - Test Accuracy 0.521220\n",
      "Epoch 518/1000 - 5.035400s - Loss 0.643287 - Accuracy 0.302083 - Test Loss 0.549504 - Test Accuracy 0.522989\n",
      "-----***-----\n",
      "0.5229885057471264\n",
      "Epoch 519/1000 - 4.984528s - Loss 0.657190 - Accuracy 0.302083 - Test Loss 0.548697 - Test Accuracy 0.522104\n",
      "Epoch 520/1000 - 4.973871s - Loss 0.573401 - Accuracy 0.401042 - Test Loss 0.547582 - Test Accuracy 0.519894\n",
      "Epoch 521/1000 - 4.979410s - Loss 0.577354 - Accuracy 0.390625 - Test Loss 0.546504 - Test Accuracy 0.520778\n",
      "Epoch 522/1000 - 4.982413s - Loss 0.694759 - Accuracy 0.322917 - Test Loss 0.546701 - Test Accuracy 0.520778\n",
      "Epoch 523/1000 - 5.035107s - Loss 0.697612 - Accuracy 0.291667 - Test Loss 0.546387 - Test Accuracy 0.523873\n",
      "-----***-----\n",
      "0.5238726790450928\n",
      "Epoch 524/1000 - 4.971708s - Loss 0.719279 - Accuracy 0.276042 - Test Loss 0.545640 - Test Accuracy 0.523873\n",
      "Epoch 525/1000 - 4.986443s - Loss 0.627258 - Accuracy 0.390625 - Test Loss 0.545573 - Test Accuracy 0.523431\n",
      "Epoch 526/1000 - 4.971066s - Loss 0.714183 - Accuracy 0.338542 - Test Loss 0.544925 - Test Accuracy 0.524315\n",
      "-----***-----\n",
      "0.5243147656940761\n",
      "Epoch 527/1000 - 4.973014s - Loss 0.708302 - Accuracy 0.364583 - Test Loss 0.543893 - Test Accuracy 0.523873\n",
      "Epoch 528/1000 - 5.024391s - Loss 0.564500 - Accuracy 0.276042 - Test Loss 0.543128 - Test Accuracy 0.525199\n",
      "-----***-----\n",
      "0.5251989389920424\n",
      "Epoch 529/1000 - 4.988972s - Loss 0.631930 - Accuracy 0.369792 - Test Loss 0.542722 - Test Accuracy 0.524757\n",
      "Epoch 530/1000 - 4.991718s - Loss 0.634756 - Accuracy 0.291667 - Test Loss 0.542436 - Test Accuracy 0.526967\n",
      "-----***-----\n",
      "0.5269672855879752\n",
      "Epoch 531/1000 - 4.980307s - Loss 0.665418 - Accuracy 0.338542 - Test Loss 0.541690 - Test Accuracy 0.524315\n",
      "Epoch 532/1000 - 4.989735s - Loss 0.539993 - Accuracy 0.406250 - Test Loss 0.540259 - Test Accuracy 0.525641\n",
      "Epoch 533/1000 - 4.962802s - Loss 0.518739 - Accuracy 0.416667 - Test Loss 0.539876 - Test Accuracy 0.525641\n",
      "Epoch 534/1000 - 4.968448s - Loss 0.590748 - Accuracy 0.416667 - Test Loss 0.539367 - Test Accuracy 0.526525\n",
      "Epoch 535/1000 - 4.975260s - Loss 0.620213 - Accuracy 0.343750 - Test Loss 0.537936 - Test Accuracy 0.526083\n",
      "Epoch 536/1000 - 4.975358s - Loss 0.606914 - Accuracy 0.333333 - Test Loss 0.536850 - Test Accuracy 0.525641\n",
      "Epoch 537/1000 - 4.966441s - Loss 0.640392 - Accuracy 0.291667 - Test Loss 0.536272 - Test Accuracy 0.528736\n",
      "-----***-----\n",
      "0.5287356321839081\n",
      "Epoch 538/1000 - 4.955691s - Loss 0.583397 - Accuracy 0.380208 - Test Loss 0.535654 - Test Accuracy 0.526967\n",
      "Epoch 539/1000 - 4.981458s - Loss 0.707243 - Accuracy 0.317708 - Test Loss 0.535646 - Test Accuracy 0.527409\n",
      "Epoch 540/1000 - 4.974204s - Loss 0.598346 - Accuracy 0.354167 - Test Loss 0.535665 - Test Accuracy 0.529178\n",
      "-----***-----\n",
      "0.5291777188328912\n",
      "Epoch 541/1000 - 5.028050s - Loss 0.607330 - Accuracy 0.364583 - Test Loss 0.534244 - Test Accuracy 0.528736\n",
      "Epoch 542/1000 - 4.971432s - Loss 0.609585 - Accuracy 0.333333 - Test Loss 0.532752 - Test Accuracy 0.528736\n",
      "Epoch 543/1000 - 4.968334s - Loss 0.493504 - Accuracy 0.447917 - Test Loss 0.531861 - Test Accuracy 0.528294\n",
      "Epoch 544/1000 - 4.962379s - Loss 0.554806 - Accuracy 0.421875 - Test Loss 0.532038 - Test Accuracy 0.528294\n",
      "Epoch 545/1000 - 5.035969s - Loss 0.584821 - Accuracy 0.369792 - Test Loss 0.531597 - Test Accuracy 0.529620\n",
      "-----***-----\n",
      "0.5296198054818745\n",
      "Epoch 546/1000 - 5.022522s - Loss 0.679087 - Accuracy 0.317708 - Test Loss 0.530742 - Test Accuracy 0.528736\n",
      "Epoch 547/1000 - 4.973604s - Loss 0.636958 - Accuracy 0.333333 - Test Loss 0.530364 - Test Accuracy 0.528736\n",
      "Epoch 548/1000 - 4.974527s - Loss 0.579361 - Accuracy 0.338542 - Test Loss 0.529873 - Test Accuracy 0.531388\n",
      "-----***-----\n",
      "0.5313881520778072\n",
      "Epoch 549/1000 - 4.985544s - Loss 0.624649 - Accuracy 0.364583 - Test Loss 0.528971 - Test Accuracy 0.530062\n",
      "Epoch 550/1000 - 4.966337s - Loss 0.559729 - Accuracy 0.421875 - Test Loss 0.528048 - Test Accuracy 0.533156\n",
      "-----***-----\n",
      "0.53315649867374\n",
      "Epoch 551/1000 - 4.983249s - Loss 0.602246 - Accuracy 0.322917 - Test Loss 0.527421 - Test Accuracy 0.531830\n",
      "Epoch 552/1000 - 5.017434s - Loss 0.692721 - Accuracy 0.343750 - Test Loss 0.528045 - Test Accuracy 0.534483\n",
      "-----***-----\n",
      "0.5344827586206896\n",
      "Epoch 553/1000 - 5.017409s - Loss 0.593838 - Accuracy 0.338542 - Test Loss 0.526778 - Test Accuracy 0.531830\n",
      "Epoch 554/1000 - 4.978045s - Loss 0.480234 - Accuracy 0.442708 - Test Loss 0.526562 - Test Accuracy 0.529620\n",
      "Epoch 555/1000 - 5.021600s - Loss 0.698922 - Accuracy 0.286458 - Test Loss 0.525867 - Test Accuracy 0.530946\n",
      "Epoch 556/1000 - 4.971371s - Loss 0.732433 - Accuracy 0.244792 - Test Loss 0.525035 - Test Accuracy 0.530946\n",
      "Epoch 557/1000 - 4.968869s - Loss 0.629773 - Accuracy 0.333333 - Test Loss 0.524020 - Test Accuracy 0.532714\n",
      "Epoch 558/1000 - 4.972889s - Loss 0.489742 - Accuracy 0.390625 - Test Loss 0.523753 - Test Accuracy 0.533599\n",
      "Epoch 559/1000 - 4.977877s - Loss 0.440494 - Accuracy 0.401042 - Test Loss 0.524337 - Test Accuracy 0.534041\n",
      "Epoch 560/1000 - 4.981016s - Loss 0.530610 - Accuracy 0.401042 - Test Loss 0.523479 - Test Accuracy 0.533156\n",
      "Epoch 561/1000 - 4.980124s - Loss 0.544695 - Accuracy 0.343750 - Test Loss 0.522962 - Test Accuracy 0.533599\n",
      "Epoch 562/1000 - 4.976734s - Loss 0.581364 - Accuracy 0.421875 - Test Loss 0.522155 - Test Accuracy 0.531830\n",
      "Epoch 563/1000 - 4.975904s - Loss 0.486406 - Accuracy 0.411458 - Test Loss 0.521386 - Test Accuracy 0.533599\n",
      "Epoch 564/1000 - 4.974845s - Loss 0.543487 - Accuracy 0.328125 - Test Loss 0.521114 - Test Accuracy 0.533599\n",
      "Epoch 565/1000 - 4.980682s - Loss 0.515670 - Accuracy 0.364583 - Test Loss 0.520342 - Test Accuracy 0.533156\n",
      "Epoch 566/1000 - 5.024500s - Loss 0.547148 - Accuracy 0.348958 - Test Loss 0.520092 - Test Accuracy 0.532272\n",
      "Epoch 567/1000 - 5.016936s - Loss 0.555746 - Accuracy 0.395833 - Test Loss 0.521180 - Test Accuracy 0.534925\n",
      "-----***-----\n",
      "0.5349248452696729\n",
      "Epoch 568/1000 - 4.985662s - Loss 0.594878 - Accuracy 0.317708 - Test Loss 0.519833 - Test Accuracy 0.535367\n",
      "-----***-----\n",
      "0.535366931918656\n",
      "Epoch 569/1000 - 4.990449s - Loss 0.620190 - Accuracy 0.333333 - Test Loss 0.519244 - Test Accuracy 0.535809\n",
      "-----***-----\n",
      "0.5358090185676393\n",
      "Epoch 570/1000 - 4.976061s - Loss 0.573075 - Accuracy 0.322917 - Test Loss 0.517750 - Test Accuracy 0.533599\n",
      "Epoch 571/1000 - 4.978475s - Loss 0.575911 - Accuracy 0.406250 - Test Loss 0.516999 - Test Accuracy 0.535809\n",
      "Epoch 572/1000 - 5.012381s - Loss 0.536316 - Accuracy 0.375000 - Test Loss 0.515681 - Test Accuracy 0.534041\n",
      "Epoch 573/1000 - 4.986198s - Loss 0.459718 - Accuracy 0.458333 - Test Loss 0.515824 - Test Accuracy 0.535367\n",
      "Epoch 574/1000 - 4.985877s - Loss 0.631538 - Accuracy 0.333333 - Test Loss 0.516249 - Test Accuracy 0.534483\n",
      "Epoch 575/1000 - 4.976483s - Loss 0.611558 - Accuracy 0.369792 - Test Loss 0.516231 - Test Accuracy 0.534925\n",
      "Epoch 576/1000 - 4.973026s - Loss 0.589624 - Accuracy 0.364583 - Test Loss 0.514837 - Test Accuracy 0.536693\n",
      "-----***-----\n",
      "0.5366931918656057\n",
      "Epoch 577/1000 - 4.986116s - Loss 0.592912 - Accuracy 0.296875 - Test Loss 0.514051 - Test Accuracy 0.535367\n",
      "Epoch 578/1000 - 4.983988s - Loss 0.587099 - Accuracy 0.338542 - Test Loss 0.513692 - Test Accuracy 0.535809\n",
      "Epoch 579/1000 - 4.982762s - Loss 0.473293 - Accuracy 0.406250 - Test Loss 0.513166 - Test Accuracy 0.536693\n",
      "Epoch 580/1000 - 5.027316s - Loss 0.504431 - Accuracy 0.416667 - Test Loss 0.513264 - Test Accuracy 0.536251\n",
      "Epoch 581/1000 - 4.984790s - Loss 0.531618 - Accuracy 0.411458 - Test Loss 0.513275 - Test Accuracy 0.539346\n",
      "-----***-----\n",
      "0.5393457117595049\n",
      "Epoch 582/1000 - 4.975445s - Loss 0.649619 - Accuracy 0.322917 - Test Loss 0.513897 - Test Accuracy 0.538462\n",
      "Epoch 583/1000 - 4.982597s - Loss 0.501474 - Accuracy 0.390625 - Test Loss 0.512330 - Test Accuracy 0.535809\n",
      "Epoch 584/1000 - 5.025729s - Loss 0.578934 - Accuracy 0.406250 - Test Loss 0.512463 - Test Accuracy 0.537577\n",
      "Epoch 585/1000 - 4.997641s - Loss 0.648312 - Accuracy 0.338542 - Test Loss 0.511861 - Test Accuracy 0.536693\n",
      "Epoch 586/1000 - 5.026549s - Loss 0.517599 - Accuracy 0.380208 - Test Loss 0.511557 - Test Accuracy 0.536251\n",
      "Epoch 587/1000 - 4.986483s - Loss 0.629518 - Accuracy 0.364583 - Test Loss 0.511313 - Test Accuracy 0.538019\n",
      "Epoch 588/1000 - 4.976614s - Loss 0.642165 - Accuracy 0.401042 - Test Loss 0.510835 - Test Accuracy 0.539788\n",
      "-----***-----\n",
      "0.5397877984084881\n",
      "Epoch 589/1000 - 4.985533s - Loss 0.507976 - Accuracy 0.385417 - Test Loss 0.510411 - Test Accuracy 0.538904\n",
      "Epoch 590/1000 - 4.979146s - Loss 0.618666 - Accuracy 0.354167 - Test Loss 0.509760 - Test Accuracy 0.540230\n",
      "-----***-----\n",
      "0.5402298850574713\n",
      "Epoch 591/1000 - 4.976615s - Loss 0.512088 - Accuracy 0.416667 - Test Loss 0.510040 - Test Accuracy 0.540230\n",
      "Epoch 592/1000 - 4.984010s - Loss 0.524777 - Accuracy 0.364583 - Test Loss 0.508764 - Test Accuracy 0.540230\n",
      "Epoch 593/1000 - 4.985516s - Loss 0.544834 - Accuracy 0.348958 - Test Loss 0.509218 - Test Accuracy 0.540230\n",
      "Epoch 594/1000 - 4.971452s - Loss 0.553130 - Accuracy 0.369792 - Test Loss 0.508098 - Test Accuracy 0.540672\n",
      "-----***-----\n",
      "0.5406719717064544\n",
      "Epoch 595/1000 - 5.025316s - Loss 0.484489 - Accuracy 0.385417 - Test Loss 0.507326 - Test Accuracy 0.541998\n",
      "-----***-----\n",
      "0.5419982316534041\n",
      "Epoch 596/1000 - 4.971239s - Loss 0.514002 - Accuracy 0.348958 - Test Loss 0.506511 - Test Accuracy 0.541998\n",
      "Epoch 597/1000 - 4.980804s - Loss 0.515123 - Accuracy 0.437500 - Test Loss 0.506330 - Test Accuracy 0.541998\n",
      "Epoch 598/1000 - 4.975338s - Loss 0.537112 - Accuracy 0.364583 - Test Loss 0.505733 - Test Accuracy 0.543324\n",
      "-----***-----\n",
      "0.5433244916003537\n",
      "Epoch 599/1000 - 4.990621s - Loss 0.508339 - Accuracy 0.395833 - Test Loss 0.505462 - Test Accuracy 0.541998\n",
      "Epoch 600/1000 - 4.985861s - Loss 0.490852 - Accuracy 0.406250 - Test Loss 0.504745 - Test Accuracy 0.541998\n",
      "Epoch 601/1000 - 5.024118s - Loss 0.555964 - Accuracy 0.354167 - Test Loss 0.505188 - Test Accuracy 0.542882\n",
      "Epoch 602/1000 - 4.970060s - Loss 0.605180 - Accuracy 0.395833 - Test Loss 0.504220 - Test Accuracy 0.541998\n",
      "Epoch 603/1000 - 5.035331s - Loss 0.662707 - Accuracy 0.312500 - Test Loss 0.504530 - Test Accuracy 0.543324\n",
      "Epoch 604/1000 - 4.976434s - Loss 0.528609 - Accuracy 0.463542 - Test Loss 0.504317 - Test Accuracy 0.543324\n",
      "Epoch 605/1000 - 4.981450s - Loss 0.590791 - Accuracy 0.390625 - Test Loss 0.503671 - Test Accuracy 0.543324\n",
      "Epoch 606/1000 - 4.988002s - Loss 0.592847 - Accuracy 0.416667 - Test Loss 0.502825 - Test Accuracy 0.543324\n",
      "Epoch 607/1000 - 4.997728s - Loss 0.624138 - Accuracy 0.307292 - Test Loss 0.502355 - Test Accuracy 0.543767\n",
      "-----***-----\n",
      "0.5437665782493368\n",
      "Epoch 608/1000 - 4.979759s - Loss 0.546812 - Accuracy 0.348958 - Test Loss 0.502101 - Test Accuracy 0.543324\n",
      "Epoch 609/1000 - 4.981519s - Loss 0.639206 - Accuracy 0.375000 - Test Loss 0.502246 - Test Accuracy 0.544651\n",
      "-----***-----\n",
      "0.5446507515473032\n",
      "Epoch 610/1000 - 4.966784s - Loss 0.485881 - Accuracy 0.473958 - Test Loss 0.502260 - Test Accuracy 0.546861\n",
      "-----***-----\n",
      "0.5468611847922192\n",
      "Epoch 611/1000 - 4.977425s - Loss 0.588402 - Accuracy 0.364583 - Test Loss 0.500236 - Test Accuracy 0.544209\n",
      "Epoch 612/1000 - 4.969263s - Loss 0.524821 - Accuracy 0.359375 - Test Loss 0.500412 - Test Accuracy 0.542882\n",
      "Epoch 613/1000 - 4.979558s - Loss 0.497102 - Accuracy 0.406250 - Test Loss 0.500517 - Test Accuracy 0.544651\n",
      "Epoch 614/1000 - 5.030578s - Loss 0.682692 - Accuracy 0.348958 - Test Loss 0.500143 - Test Accuracy 0.541998\n",
      "Epoch 615/1000 - 4.987655s - Loss 0.480239 - Accuracy 0.432292 - Test Loss 0.499543 - Test Accuracy 0.545093\n",
      "Epoch 616/1000 - 4.970816s - Loss 0.484259 - Accuracy 0.458333 - Test Loss 0.499648 - Test Accuracy 0.548630\n",
      "-----***-----\n",
      "0.5486295313881521\n",
      "Epoch 617/1000 - 4.987531s - Loss 0.527219 - Accuracy 0.364583 - Test Loss 0.499105 - Test Accuracy 0.548630\n",
      "Epoch 618/1000 - 5.016526s - Loss 0.588537 - Accuracy 0.380208 - Test Loss 0.498026 - Test Accuracy 0.546419\n",
      "Epoch 619/1000 - 4.995133s - Loss 0.520801 - Accuracy 0.406250 - Test Loss 0.498126 - Test Accuracy 0.548630\n",
      "Epoch 620/1000 - 4.977321s - Loss 0.500394 - Accuracy 0.421875 - Test Loss 0.497141 - Test Accuracy 0.547745\n",
      "Epoch 621/1000 - 4.984472s - Loss 0.561265 - Accuracy 0.411458 - Test Loss 0.496520 - Test Accuracy 0.545093\n",
      "Epoch 622/1000 - 5.050998s - Loss 0.559763 - Accuracy 0.369792 - Test Loss 0.496666 - Test Accuracy 0.547303\n",
      "Epoch 623/1000 - 4.973461s - Loss 0.544477 - Accuracy 0.406250 - Test Loss 0.496570 - Test Accuracy 0.548187\n",
      "Epoch 624/1000 - 4.965737s - Loss 0.474282 - Accuracy 0.484375 - Test Loss 0.495349 - Test Accuracy 0.546861\n",
      "Epoch 625/1000 - 4.980480s - Loss 0.450688 - Accuracy 0.432292 - Test Loss 0.495675 - Test Accuracy 0.547303\n",
      "Epoch 626/1000 - 4.979236s - Loss 0.455495 - Accuracy 0.473958 - Test Loss 0.495742 - Test Accuracy 0.546861\n",
      "Epoch 627/1000 - 4.989057s - Loss 0.530076 - Accuracy 0.411458 - Test Loss 0.495836 - Test Accuracy 0.547303\n",
      "Epoch 628/1000 - 4.966066s - Loss 0.512030 - Accuracy 0.406250 - Test Loss 0.494823 - Test Accuracy 0.547745\n",
      "Epoch 629/1000 - 4.974302s - Loss 0.629282 - Accuracy 0.343750 - Test Loss 0.494233 - Test Accuracy 0.546419\n",
      "Epoch 630/1000 - 4.980150s - Loss 0.450254 - Accuracy 0.484375 - Test Loss 0.494126 - Test Accuracy 0.547745\n",
      "Epoch 631/1000 - 4.973751s - Loss 0.592968 - Accuracy 0.364583 - Test Loss 0.493993 - Test Accuracy 0.545977\n",
      "Epoch 632/1000 - 5.007005s - Loss 0.615013 - Accuracy 0.317708 - Test Loss 0.493287 - Test Accuracy 0.546861\n",
      "Epoch 633/1000 - 4.983135s - Loss 0.490380 - Accuracy 0.463542 - Test Loss 0.493241 - Test Accuracy 0.546861\n",
      "Epoch 634/1000 - 4.976297s - Loss 0.539233 - Accuracy 0.390625 - Test Loss 0.492649 - Test Accuracy 0.546861\n",
      "Epoch 635/1000 - 4.984363s - Loss 0.525710 - Accuracy 0.421875 - Test Loss 0.491907 - Test Accuracy 0.547745\n",
      "Epoch 636/1000 - 4.974420s - Loss 0.503030 - Accuracy 0.442708 - Test Loss 0.491392 - Test Accuracy 0.548187\n",
      "Epoch 637/1000 - 4.982465s - Loss 0.552621 - Accuracy 0.359375 - Test Loss 0.491005 - Test Accuracy 0.549514\n",
      "-----***-----\n",
      "0.5495137046861185\n",
      "Epoch 638/1000 - 4.979447s - Loss 0.464607 - Accuracy 0.458333 - Test Loss 0.490882 - Test Accuracy 0.548630\n",
      "Epoch 639/1000 - 4.981071s - Loss 0.503023 - Accuracy 0.401042 - Test Loss 0.490555 - Test Accuracy 0.550398\n",
      "-----***-----\n",
      "0.5503978779840849\n",
      "Epoch 640/1000 - 4.978992s - Loss 0.491983 - Accuracy 0.395833 - Test Loss 0.490022 - Test Accuracy 0.551282\n",
      "-----***-----\n",
      "0.5512820512820513\n",
      "Epoch 641/1000 - 4.977044s - Loss 0.475839 - Accuracy 0.375000 - Test Loss 0.490436 - Test Accuracy 0.550840\n",
      "Epoch 642/1000 - 4.976054s - Loss 0.501489 - Accuracy 0.385417 - Test Loss 0.489809 - Test Accuracy 0.552166\n",
      "-----***-----\n",
      "0.5521662245800176\n",
      "Epoch 643/1000 - 4.980294s - Loss 0.533915 - Accuracy 0.395833 - Test Loss 0.489613 - Test Accuracy 0.550840\n",
      "Epoch 644/1000 - 4.985069s - Loss 0.390539 - Accuracy 0.500000 - Test Loss 0.488450 - Test Accuracy 0.550840\n",
      "Epoch 645/1000 - 4.988637s - Loss 0.413000 - Accuracy 0.526042 - Test Loss 0.489009 - Test Accuracy 0.549956\n",
      "Epoch 646/1000 - 4.992333s - Loss 0.548381 - Accuracy 0.411458 - Test Loss 0.488528 - Test Accuracy 0.552166\n",
      "Epoch 647/1000 - 4.970913s - Loss 0.488134 - Accuracy 0.395833 - Test Loss 0.487916 - Test Accuracy 0.549956\n",
      "Epoch 648/1000 - 4.983399s - Loss 0.557683 - Accuracy 0.385417 - Test Loss 0.487816 - Test Accuracy 0.551724\n",
      "Epoch 649/1000 - 4.999811s - Loss 0.513712 - Accuracy 0.380208 - Test Loss 0.486926 - Test Accuracy 0.549072\n",
      "Epoch 650/1000 - 4.978685s - Loss 0.510874 - Accuracy 0.401042 - Test Loss 0.486602 - Test Accuracy 0.549956\n",
      "Epoch 651/1000 - 4.984924s - Loss 0.536505 - Accuracy 0.385417 - Test Loss 0.487131 - Test Accuracy 0.553050\n",
      "-----***-----\n",
      "0.553050397877984\n",
      "Epoch 652/1000 - 4.976081s - Loss 0.465415 - Accuracy 0.447917 - Test Loss 0.486796 - Test Accuracy 0.552166\n",
      "Epoch 653/1000 - 5.015546s - Loss 0.452782 - Accuracy 0.458333 - Test Loss 0.486498 - Test Accuracy 0.550398\n",
      "Epoch 654/1000 - 5.023767s - Loss 0.486945 - Accuracy 0.494792 - Test Loss 0.486860 - Test Accuracy 0.550840\n",
      "Epoch 655/1000 - 5.021529s - Loss 0.526723 - Accuracy 0.447917 - Test Loss 0.485925 - Test Accuracy 0.549956\n",
      "Epoch 656/1000 - 4.974323s - Loss 0.377459 - Accuracy 0.536458 - Test Loss 0.485941 - Test Accuracy 0.551724\n",
      "Epoch 657/1000 - 4.984776s - Loss 0.500064 - Accuracy 0.380208 - Test Loss 0.486353 - Test Accuracy 0.550840\n",
      "Epoch 658/1000 - 4.986270s - Loss 0.557388 - Accuracy 0.369792 - Test Loss 0.485423 - Test Accuracy 0.552166\n",
      "Epoch 659/1000 - 4.987247s - Loss 0.512453 - Accuracy 0.432292 - Test Loss 0.485391 - Test Accuracy 0.551282\n",
      "Epoch 660/1000 - 4.973578s - Loss 0.572984 - Accuracy 0.343750 - Test Loss 0.485078 - Test Accuracy 0.550398\n",
      "Epoch 661/1000 - 4.991602s - Loss 0.508011 - Accuracy 0.479167 - Test Loss 0.484583 - Test Accuracy 0.551724\n",
      "Epoch 662/1000 - 4.980611s - Loss 0.490366 - Accuracy 0.390625 - Test Loss 0.484650 - Test Accuracy 0.551282\n",
      "Epoch 663/1000 - 4.977695s - Loss 0.481942 - Accuracy 0.442708 - Test Loss 0.483564 - Test Accuracy 0.551282\n",
      "Epoch 664/1000 - 5.023772s - Loss 0.494432 - Accuracy 0.406250 - Test Loss 0.483430 - Test Accuracy 0.551282\n",
      "Epoch 665/1000 - 4.979687s - Loss 0.555394 - Accuracy 0.385417 - Test Loss 0.483492 - Test Accuracy 0.553050\n",
      "Epoch 666/1000 - 5.022539s - Loss 0.453861 - Accuracy 0.447917 - Test Loss 0.482636 - Test Accuracy 0.551282\n",
      "Epoch 667/1000 - 5.024396s - Loss 0.501249 - Accuracy 0.421875 - Test Loss 0.483307 - Test Accuracy 0.553050\n",
      "Epoch 668/1000 - 4.978742s - Loss 0.470508 - Accuracy 0.437500 - Test Loss 0.482902 - Test Accuracy 0.552166\n",
      "Epoch 669/1000 - 4.979302s - Loss 0.494469 - Accuracy 0.458333 - Test Loss 0.482493 - Test Accuracy 0.551282\n",
      "Epoch 670/1000 - 4.981297s - Loss 0.475883 - Accuracy 0.427083 - Test Loss 0.481816 - Test Accuracy 0.552166\n",
      "Epoch 671/1000 - 4.984120s - Loss 0.541795 - Accuracy 0.296875 - Test Loss 0.481723 - Test Accuracy 0.553492\n",
      "-----***-----\n",
      "0.5534924845269673\n",
      "Epoch 672/1000 - 4.971990s - Loss 0.422041 - Accuracy 0.437500 - Test Loss 0.480551 - Test Accuracy 0.554819\n",
      "-----***-----\n",
      "0.5548187444739169\n",
      "Epoch 673/1000 - 4.990206s - Loss 0.469336 - Accuracy 0.416667 - Test Loss 0.480627 - Test Accuracy 0.553050\n",
      "Epoch 674/1000 - 4.976024s - Loss 0.461382 - Accuracy 0.416667 - Test Loss 0.480791 - Test Accuracy 0.553050\n",
      "Epoch 675/1000 - 5.019933s - Loss 0.563384 - Accuracy 0.369792 - Test Loss 0.480277 - Test Accuracy 0.554377\n",
      "Epoch 676/1000 - 4.975292s - Loss 0.473939 - Accuracy 0.401042 - Test Loss 0.479791 - Test Accuracy 0.554819\n",
      "Epoch 677/1000 - 4.994474s - Loss 0.447599 - Accuracy 0.458333 - Test Loss 0.479909 - Test Accuracy 0.555261\n",
      "-----***-----\n",
      "0.5552608311229\n",
      "Epoch 678/1000 - 5.023242s - Loss 0.492700 - Accuracy 0.447917 - Test Loss 0.479523 - Test Accuracy 0.555261\n",
      "Epoch 679/1000 - 4.989301s - Loss 0.493429 - Accuracy 0.411458 - Test Loss 0.479642 - Test Accuracy 0.555703\n",
      "-----***-----\n",
      "0.5557029177718833\n",
      "Epoch 680/1000 - 4.981516s - Loss 0.520202 - Accuracy 0.390625 - Test Loss 0.479292 - Test Accuracy 0.555261\n",
      "Epoch 681/1000 - 4.978576s - Loss 0.543705 - Accuracy 0.380208 - Test Loss 0.479018 - Test Accuracy 0.556145\n",
      "-----***-----\n",
      "0.5561450044208665\n",
      "Epoch 682/1000 - 4.987395s - Loss 0.537563 - Accuracy 0.380208 - Test Loss 0.479639 - Test Accuracy 0.556587\n",
      "-----***-----\n",
      "0.5565870910698497\n",
      "Epoch 683/1000 - 4.976929s - Loss 0.502428 - Accuracy 0.411458 - Test Loss 0.478108 - Test Accuracy 0.556145\n",
      "Epoch 684/1000 - 4.973073s - Loss 0.470045 - Accuracy 0.437500 - Test Loss 0.478251 - Test Accuracy 0.557029\n",
      "-----***-----\n",
      "0.5570291777188329\n",
      "Epoch 685/1000 - 4.977813s - Loss 0.451793 - Accuracy 0.520833 - Test Loss 0.478476 - Test Accuracy 0.557471\n",
      "-----***-----\n",
      "0.5574712643678161\n",
      "Epoch 686/1000 - 4.988068s - Loss 0.501781 - Accuracy 0.416667 - Test Loss 0.477295 - Test Accuracy 0.557029\n",
      "Epoch 687/1000 - 4.989336s - Loss 0.479935 - Accuracy 0.395833 - Test Loss 0.477374 - Test Accuracy 0.557471\n",
      "Epoch 688/1000 - 4.977454s - Loss 0.479667 - Accuracy 0.442708 - Test Loss 0.477172 - Test Accuracy 0.555261\n",
      "Epoch 689/1000 - 4.974216s - Loss 0.453245 - Accuracy 0.479167 - Test Loss 0.477676 - Test Accuracy 0.557029\n",
      "Epoch 690/1000 - 5.035258s - Loss 0.496659 - Accuracy 0.385417 - Test Loss 0.477122 - Test Accuracy 0.557913\n",
      "-----***-----\n",
      "0.5579133510167993\n",
      "Epoch 691/1000 - 4.972294s - Loss 0.497177 - Accuracy 0.437500 - Test Loss 0.476940 - Test Accuracy 0.556145\n",
      "Epoch 692/1000 - 4.989282s - Loss 0.586145 - Accuracy 0.322917 - Test Loss 0.477300 - Test Accuracy 0.556145\n",
      "Epoch 693/1000 - 5.030986s - Loss 0.501598 - Accuracy 0.458333 - Test Loss 0.477151 - Test Accuracy 0.557029\n",
      "Epoch 694/1000 - 5.003374s - Loss 0.452969 - Accuracy 0.484375 - Test Loss 0.477282 - Test Accuracy 0.557913\n",
      "Epoch 695/1000 - 5.004668s - Loss 0.498806 - Accuracy 0.390625 - Test Loss 0.477221 - Test Accuracy 0.557471\n",
      "Epoch 696/1000 - 4.991716s - Loss 0.539097 - Accuracy 0.406250 - Test Loss 0.476288 - Test Accuracy 0.557913\n",
      "Epoch 697/1000 - 5.012789s - Loss 0.430325 - Accuracy 0.453125 - Test Loss 0.475308 - Test Accuracy 0.557029\n",
      "Epoch 698/1000 - 5.019489s - Loss 0.502139 - Accuracy 0.463542 - Test Loss 0.475242 - Test Accuracy 0.556145\n",
      "Epoch 699/1000 - 5.025938s - Loss 0.421179 - Accuracy 0.520833 - Test Loss 0.475132 - Test Accuracy 0.559240\n",
      "-----***-----\n",
      "0.5592396109637489\n",
      "Epoch 700/1000 - 4.974434s - Loss 0.451336 - Accuracy 0.489583 - Test Loss 0.475222 - Test Accuracy 0.557471\n",
      "Epoch 701/1000 - 4.986279s - Loss 0.572104 - Accuracy 0.343750 - Test Loss 0.474978 - Test Accuracy 0.557029\n",
      "Epoch 702/1000 - 5.032940s - Loss 0.524315 - Accuracy 0.354167 - Test Loss 0.474849 - Test Accuracy 0.557471\n",
      "Epoch 703/1000 - 4.985833s - Loss 0.484665 - Accuracy 0.390625 - Test Loss 0.474155 - Test Accuracy 0.557029\n",
      "Epoch 704/1000 - 4.986125s - Loss 0.497268 - Accuracy 0.432292 - Test Loss 0.474656 - Test Accuracy 0.560124\n",
      "-----***-----\n",
      "0.5601237842617153\n",
      "Epoch 705/1000 - 4.977270s - Loss 0.374015 - Accuracy 0.500000 - Test Loss 0.474223 - Test Accuracy 0.560124\n",
      "Epoch 706/1000 - 5.021566s - Loss 0.423069 - Accuracy 0.500000 - Test Loss 0.474206 - Test Accuracy 0.559682\n",
      "Epoch 707/1000 - 4.986726s - Loss 0.536904 - Accuracy 0.453125 - Test Loss 0.474140 - Test Accuracy 0.560124\n",
      "Epoch 708/1000 - 5.035088s - Loss 0.490691 - Accuracy 0.359375 - Test Loss 0.473749 - Test Accuracy 0.560124\n",
      "Epoch 709/1000 - 5.030374s - Loss 0.434145 - Accuracy 0.526042 - Test Loss 0.473041 - Test Accuracy 0.560566\n",
      "-----***-----\n",
      "0.5605658709106985\n",
      "Epoch 710/1000 - 4.983021s - Loss 0.481943 - Accuracy 0.447917 - Test Loss 0.472763 - Test Accuracy 0.561450\n",
      "-----***-----\n",
      "0.5614500442086648\n",
      "Epoch 711/1000 - 4.980375s - Loss 0.497223 - Accuracy 0.416667 - Test Loss 0.472151 - Test Accuracy 0.560566\n",
      "Epoch 712/1000 - 5.031458s - Loss 0.444669 - Accuracy 0.463542 - Test Loss 0.471596 - Test Accuracy 0.559682\n",
      "Epoch 713/1000 - 5.002338s - Loss 0.454910 - Accuracy 0.484375 - Test Loss 0.472340 - Test Accuracy 0.559682\n",
      "Epoch 714/1000 - 5.035070s - Loss 0.382538 - Accuracy 0.593750 - Test Loss 0.471846 - Test Accuracy 0.560566\n",
      "Epoch 715/1000 - 4.986261s - Loss 0.444642 - Accuracy 0.473958 - Test Loss 0.470630 - Test Accuracy 0.560566\n",
      "Epoch 716/1000 - 4.975244s - Loss 0.419057 - Accuracy 0.458333 - Test Loss 0.470314 - Test Accuracy 0.560124\n",
      "Epoch 717/1000 - 5.019404s - Loss 0.481234 - Accuracy 0.510417 - Test Loss 0.470910 - Test Accuracy 0.561450\n",
      "Epoch 718/1000 - 4.984364s - Loss 0.481460 - Accuracy 0.531250 - Test Loss 0.470455 - Test Accuracy 0.560124\n",
      "Epoch 719/1000 - 4.980409s - Loss 0.438342 - Accuracy 0.484375 - Test Loss 0.470091 - Test Accuracy 0.559240\n",
      "Epoch 720/1000 - 4.978461s - Loss 0.468433 - Accuracy 0.489583 - Test Loss 0.470507 - Test Accuracy 0.559240\n",
      "Epoch 721/1000 - 4.981208s - Loss 0.482501 - Accuracy 0.421875 - Test Loss 0.469846 - Test Accuracy 0.559240\n",
      "Epoch 722/1000 - 4.979931s - Loss 0.448691 - Accuracy 0.473958 - Test Loss 0.469714 - Test Accuracy 0.559682\n",
      "Epoch 723/1000 - 4.978666s - Loss 0.462957 - Accuracy 0.442708 - Test Loss 0.469645 - Test Accuracy 0.560124\n",
      "Epoch 724/1000 - 5.030555s - Loss 0.500832 - Accuracy 0.401042 - Test Loss 0.469516 - Test Accuracy 0.559240\n",
      "Epoch 725/1000 - 4.980393s - Loss 0.469770 - Accuracy 0.427083 - Test Loss 0.468906 - Test Accuracy 0.560124\n",
      "Epoch 726/1000 - 4.995076s - Loss 0.459867 - Accuracy 0.489583 - Test Loss 0.468799 - Test Accuracy 0.560124\n",
      "Epoch 727/1000 - 4.967357s - Loss 0.405336 - Accuracy 0.500000 - Test Loss 0.468961 - Test Accuracy 0.560566\n",
      "Epoch 728/1000 - 5.020576s - Loss 0.499552 - Accuracy 0.447917 - Test Loss 0.469236 - Test Accuracy 0.560566\n",
      "Epoch 729/1000 - 4.990837s - Loss 0.447622 - Accuracy 0.463542 - Test Loss 0.468979 - Test Accuracy 0.561450\n",
      "Epoch 730/1000 - 4.988471s - Loss 0.399117 - Accuracy 0.484375 - Test Loss 0.468490 - Test Accuracy 0.559682\n",
      "Epoch 731/1000 - 4.977845s - Loss 0.425872 - Accuracy 0.473958 - Test Loss 0.467438 - Test Accuracy 0.561008\n",
      "Epoch 732/1000 - 4.983284s - Loss 0.383554 - Accuracy 0.500000 - Test Loss 0.467536 - Test Accuracy 0.560566\n",
      "Epoch 733/1000 - 4.993606s - Loss 0.489939 - Accuracy 0.468750 - Test Loss 0.466955 - Test Accuracy 0.560124\n",
      "Epoch 734/1000 - 4.986039s - Loss 0.366446 - Accuracy 0.515625 - Test Loss 0.466825 - Test Accuracy 0.559240\n",
      "Epoch 735/1000 - 4.983645s - Loss 0.503265 - Accuracy 0.375000 - Test Loss 0.467506 - Test Accuracy 0.557913\n",
      "Epoch 736/1000 - 4.978417s - Loss 0.469426 - Accuracy 0.479167 - Test Loss 0.467542 - Test Accuracy 0.559682\n",
      "Epoch 737/1000 - 5.033039s - Loss 0.419106 - Accuracy 0.546875 - Test Loss 0.466666 - Test Accuracy 0.559240\n",
      "Epoch 738/1000 - 4.981048s - Loss 0.465699 - Accuracy 0.484375 - Test Loss 0.467108 - Test Accuracy 0.558798\n",
      "Epoch 739/1000 - 4.987263s - Loss 0.457836 - Accuracy 0.479167 - Test Loss 0.466714 - Test Accuracy 0.559240\n",
      "Epoch 740/1000 - 5.026059s - Loss 0.499691 - Accuracy 0.416667 - Test Loss 0.466704 - Test Accuracy 0.558798\n",
      "Epoch 741/1000 - 4.988520s - Loss 0.559632 - Accuracy 0.380208 - Test Loss 0.466407 - Test Accuracy 0.561008\n",
      "Epoch 742/1000 - 4.977483s - Loss 0.438387 - Accuracy 0.411458 - Test Loss 0.465772 - Test Accuracy 0.558798\n",
      "Epoch 743/1000 - 4.973576s - Loss 0.459635 - Accuracy 0.463542 - Test Loss 0.465793 - Test Accuracy 0.560566\n",
      "Epoch 744/1000 - 4.965131s - Loss 0.430864 - Accuracy 0.526042 - Test Loss 0.465256 - Test Accuracy 0.559240\n",
      "Epoch 745/1000 - 4.983443s - Loss 0.516649 - Accuracy 0.380208 - Test Loss 0.465282 - Test Accuracy 0.559240\n",
      "Epoch 746/1000 - 5.036973s - Loss 0.531002 - Accuracy 0.406250 - Test Loss 0.465421 - Test Accuracy 0.561008\n",
      "Epoch 747/1000 - 4.989847s - Loss 0.532139 - Accuracy 0.401042 - Test Loss 0.466048 - Test Accuracy 0.561008\n",
      "Epoch 748/1000 - 5.029477s - Loss 0.464515 - Accuracy 0.473958 - Test Loss 0.465461 - Test Accuracy 0.562776\n",
      "-----***-----\n",
      "0.5627763041556145\n",
      "Epoch 749/1000 - 5.019336s - Loss 0.418184 - Accuracy 0.442708 - Test Loss 0.464709 - Test Accuracy 0.560124\n",
      "Epoch 750/1000 - 5.021070s - Loss 0.530981 - Accuracy 0.395833 - Test Loss 0.465440 - Test Accuracy 0.560124\n",
      "Epoch 751/1000 - 5.021091s - Loss 0.493086 - Accuracy 0.406250 - Test Loss 0.465352 - Test Accuracy 0.560124\n",
      "Epoch 752/1000 - 5.021064s - Loss 0.453044 - Accuracy 0.473958 - Test Loss 0.465198 - Test Accuracy 0.561008\n",
      "Epoch 753/1000 - 5.041764s - Loss 0.479848 - Accuracy 0.468750 - Test Loss 0.463825 - Test Accuracy 0.561450\n",
      "Epoch 754/1000 - 5.015389s - Loss 0.419519 - Accuracy 0.484375 - Test Loss 0.464096 - Test Accuracy 0.562334\n",
      "Epoch 755/1000 - 5.016996s - Loss 0.411406 - Accuracy 0.572917 - Test Loss 0.463892 - Test Accuracy 0.563218\n",
      "-----***-----\n",
      "0.5632183908045977\n",
      "Epoch 756/1000 - 5.039292s - Loss 0.421796 - Accuracy 0.515625 - Test Loss 0.463890 - Test Accuracy 0.561008\n",
      "Epoch 757/1000 - 4.995702s - Loss 0.460672 - Accuracy 0.401042 - Test Loss 0.463213 - Test Accuracy 0.561892\n",
      "Epoch 758/1000 - 4.986551s - Loss 0.392770 - Accuracy 0.515625 - Test Loss 0.463281 - Test Accuracy 0.562334\n",
      "Epoch 759/1000 - 4.990486s - Loss 0.407893 - Accuracy 0.557292 - Test Loss 0.463772 - Test Accuracy 0.562776\n",
      "Epoch 760/1000 - 4.972672s - Loss 0.435829 - Accuracy 0.489583 - Test Loss 0.463424 - Test Accuracy 0.562776\n",
      "Epoch 761/1000 - 4.979877s - Loss 0.455262 - Accuracy 0.442708 - Test Loss 0.463096 - Test Accuracy 0.562334\n",
      "Epoch 762/1000 - 4.977040s - Loss 0.419857 - Accuracy 0.453125 - Test Loss 0.463402 - Test Accuracy 0.563660\n",
      "-----***-----\n",
      "0.5636604774535809\n",
      "Epoch 763/1000 - 4.980776s - Loss 0.366873 - Accuracy 0.562500 - Test Loss 0.463098 - Test Accuracy 0.564987\n",
      "-----***-----\n",
      "0.5649867374005305\n",
      "Epoch 764/1000 - 4.975003s - Loss 0.464238 - Accuracy 0.416667 - Test Loss 0.462779 - Test Accuracy 0.564103\n",
      "Epoch 765/1000 - 5.022663s - Loss 0.474121 - Accuracy 0.468750 - Test Loss 0.462709 - Test Accuracy 0.563660\n",
      "Epoch 766/1000 - 4.992505s - Loss 0.470951 - Accuracy 0.489583 - Test Loss 0.462975 - Test Accuracy 0.564987\n",
      "Epoch 767/1000 - 5.030290s - Loss 0.497873 - Accuracy 0.447917 - Test Loss 0.462891 - Test Accuracy 0.563660\n",
      "Epoch 768/1000 - 4.970382s - Loss 0.444003 - Accuracy 0.473958 - Test Loss 0.462475 - Test Accuracy 0.564103\n",
      "Epoch 769/1000 - 5.034190s - Loss 0.389137 - Accuracy 0.520833 - Test Loss 0.462217 - Test Accuracy 0.563218\n",
      "Epoch 770/1000 - 4.974490s - Loss 0.401215 - Accuracy 0.489583 - Test Loss 0.461250 - Test Accuracy 0.564987\n",
      "Epoch 771/1000 - 4.986830s - Loss 0.367372 - Accuracy 0.552083 - Test Loss 0.461629 - Test Accuracy 0.564987\n",
      "Epoch 772/1000 - 4.977253s - Loss 0.428204 - Accuracy 0.500000 - Test Loss 0.461008 - Test Accuracy 0.565429\n",
      "-----***-----\n",
      "0.5654288240495137\n",
      "Epoch 773/1000 - 4.982477s - Loss 0.440157 - Accuracy 0.505208 - Test Loss 0.461011 - Test Accuracy 0.564103\n",
      "Epoch 774/1000 - 4.983021s - Loss 0.434410 - Accuracy 0.458333 - Test Loss 0.460823 - Test Accuracy 0.564987\n",
      "Epoch 775/1000 - 4.976480s - Loss 0.349058 - Accuracy 0.557292 - Test Loss 0.461447 - Test Accuracy 0.567197\n",
      "-----***-----\n",
      "0.5671971706454465\n",
      "Epoch 776/1000 - 5.015765s - Loss 0.494915 - Accuracy 0.468750 - Test Loss 0.461971 - Test Accuracy 0.564103\n",
      "Epoch 777/1000 - 4.978756s - Loss 0.491560 - Accuracy 0.432292 - Test Loss 0.461655 - Test Accuracy 0.564103\n",
      "Epoch 778/1000 - 5.019829s - Loss 0.457307 - Accuracy 0.473958 - Test Loss 0.461200 - Test Accuracy 0.564987\n",
      "Epoch 779/1000 - 5.033521s - Loss 0.480466 - Accuracy 0.453125 - Test Loss 0.460495 - Test Accuracy 0.565871\n",
      "Epoch 780/1000 - 4.989799s - Loss 0.541624 - Accuracy 0.411458 - Test Loss 0.461157 - Test Accuracy 0.565429\n",
      "Epoch 781/1000 - 4.980098s - Loss 0.438228 - Accuracy 0.427083 - Test Loss 0.460442 - Test Accuracy 0.565871\n",
      "Epoch 782/1000 - 5.040010s - Loss 0.362533 - Accuracy 0.572917 - Test Loss 0.460144 - Test Accuracy 0.567197\n",
      "Epoch 783/1000 - 4.971868s - Loss 0.442874 - Accuracy 0.473958 - Test Loss 0.460382 - Test Accuracy 0.567639\n",
      "-----***-----\n",
      "0.5676392572944297\n",
      "Epoch 784/1000 - 4.974131s - Loss 0.413260 - Accuracy 0.437500 - Test Loss 0.460542 - Test Accuracy 0.566755\n",
      "Epoch 785/1000 - 4.999127s - Loss 0.452387 - Accuracy 0.515625 - Test Loss 0.460105 - Test Accuracy 0.567197\n",
      "Epoch 786/1000 - 4.981086s - Loss 0.380524 - Accuracy 0.432292 - Test Loss 0.459796 - Test Accuracy 0.567197\n",
      "Epoch 787/1000 - 4.974893s - Loss 0.514542 - Accuracy 0.411458 - Test Loss 0.459428 - Test Accuracy 0.568966\n",
      "-----***-----\n",
      "0.5689655172413793\n",
      "Epoch 788/1000 - 4.973223s - Loss 0.379157 - Accuracy 0.562500 - Test Loss 0.458387 - Test Accuracy 0.567197\n",
      "Epoch 789/1000 - 4.975487s - Loss 0.415252 - Accuracy 0.500000 - Test Loss 0.459569 - Test Accuracy 0.568081\n",
      "Epoch 790/1000 - 4.996389s - Loss 0.512944 - Accuracy 0.468750 - Test Loss 0.459782 - Test Accuracy 0.567639\n",
      "Epoch 791/1000 - 4.997044s - Loss 0.391606 - Accuracy 0.484375 - Test Loss 0.458637 - Test Accuracy 0.568523\n",
      "Epoch 792/1000 - 4.973283s - Loss 0.445972 - Accuracy 0.463542 - Test Loss 0.459080 - Test Accuracy 0.567639\n",
      "Epoch 793/1000 - 4.997340s - Loss 0.434935 - Accuracy 0.447917 - Test Loss 0.458569 - Test Accuracy 0.565871\n",
      "Epoch 794/1000 - 4.971492s - Loss 0.418414 - Accuracy 0.463542 - Test Loss 0.458288 - Test Accuracy 0.567197\n",
      "Epoch 795/1000 - 4.986504s - Loss 0.389296 - Accuracy 0.510417 - Test Loss 0.457789 - Test Accuracy 0.567639\n",
      "Epoch 796/1000 - 4.992034s - Loss 0.449487 - Accuracy 0.526042 - Test Loss 0.458527 - Test Accuracy 0.569408\n",
      "-----***-----\n",
      "0.5694076038903625\n",
      "Epoch 797/1000 - 4.983573s - Loss 0.409658 - Accuracy 0.489583 - Test Loss 0.457246 - Test Accuracy 0.567639\n",
      "Epoch 798/1000 - 4.980304s - Loss 0.404356 - Accuracy 0.505208 - Test Loss 0.457835 - Test Accuracy 0.568523\n",
      "Epoch 799/1000 - 4.989058s - Loss 0.431763 - Accuracy 0.500000 - Test Loss 0.458657 - Test Accuracy 0.568081\n",
      "Epoch 800/1000 - 4.971509s - Loss 0.392471 - Accuracy 0.546875 - Test Loss 0.457919 - Test Accuracy 0.568966\n",
      "Epoch 801/1000 - 4.980856s - Loss 0.470893 - Accuracy 0.406250 - Test Loss 0.457448 - Test Accuracy 0.568081\n",
      "Epoch 802/1000 - 4.990531s - Loss 0.356025 - Accuracy 0.505208 - Test Loss 0.457574 - Test Accuracy 0.567197\n",
      "Epoch 803/1000 - 4.993515s - Loss 0.409330 - Accuracy 0.500000 - Test Loss 0.457046 - Test Accuracy 0.566313\n",
      "Epoch 804/1000 - 4.977428s - Loss 0.455198 - Accuracy 0.473958 - Test Loss 0.457478 - Test Accuracy 0.566755\n",
      "Epoch 805/1000 - 4.986405s - Loss 0.445712 - Accuracy 0.458333 - Test Loss 0.457379 - Test Accuracy 0.567197\n",
      "Epoch 806/1000 - 4.976733s - Loss 0.408276 - Accuracy 0.510417 - Test Loss 0.456872 - Test Accuracy 0.567197\n",
      "Epoch 807/1000 - 4.985250s - Loss 0.385296 - Accuracy 0.541667 - Test Loss 0.456489 - Test Accuracy 0.566313\n",
      "Epoch 808/1000 - 5.041139s - Loss 0.473827 - Accuracy 0.463542 - Test Loss 0.456877 - Test Accuracy 0.564987\n",
      "Epoch 809/1000 - 4.983623s - Loss 0.345104 - Accuracy 0.536458 - Test Loss 0.456792 - Test Accuracy 0.565429\n",
      "Epoch 810/1000 - 4.961167s - Loss 0.414983 - Accuracy 0.520833 - Test Loss 0.457041 - Test Accuracy 0.565429\n",
      "Epoch 811/1000 - 4.987842s - Loss 0.436012 - Accuracy 0.458333 - Test Loss 0.457183 - Test Accuracy 0.564987\n",
      "Epoch 812/1000 - 5.021126s - Loss 0.375029 - Accuracy 0.531250 - Test Loss 0.456437 - Test Accuracy 0.566755\n",
      "Epoch 813/1000 - 4.980654s - Loss 0.480988 - Accuracy 0.411458 - Test Loss 0.456405 - Test Accuracy 0.566755\n",
      "Epoch 814/1000 - 4.984589s - Loss 0.420742 - Accuracy 0.468750 - Test Loss 0.455533 - Test Accuracy 0.566755\n",
      "Epoch 815/1000 - 4.996921s - Loss 0.536625 - Accuracy 0.348958 - Test Loss 0.456160 - Test Accuracy 0.567197\n",
      "Epoch 816/1000 - 4.978340s - Loss 0.476404 - Accuracy 0.437500 - Test Loss 0.455956 - Test Accuracy 0.566755\n",
      "Epoch 817/1000 - 5.000311s - Loss 0.488598 - Accuracy 0.447917 - Test Loss 0.455345 - Test Accuracy 0.568523\n",
      "Epoch 818/1000 - 5.016567s - Loss 0.386048 - Accuracy 0.598958 - Test Loss 0.455369 - Test Accuracy 0.568966\n",
      "Epoch 819/1000 - 5.018602s - Loss 0.475267 - Accuracy 0.473958 - Test Loss 0.455742 - Test Accuracy 0.567639\n",
      "Epoch 820/1000 - 4.989502s - Loss 0.378308 - Accuracy 0.541667 - Test Loss 0.456809 - Test Accuracy 0.568081\n",
      "Epoch 821/1000 - 4.973118s - Loss 0.420309 - Accuracy 0.437500 - Test Loss 0.456102 - Test Accuracy 0.566755\n",
      "Epoch 822/1000 - 4.974589s - Loss 0.427560 - Accuracy 0.411458 - Test Loss 0.456133 - Test Accuracy 0.566755\n",
      "Epoch 823/1000 - 5.027116s - Loss 0.439438 - Accuracy 0.453125 - Test Loss 0.455753 - Test Accuracy 0.566313\n",
      "Epoch 824/1000 - 5.016782s - Loss 0.343866 - Accuracy 0.598958 - Test Loss 0.455449 - Test Accuracy 0.567197\n",
      "Epoch 825/1000 - 4.976341s - Loss 0.413152 - Accuracy 0.473958 - Test Loss 0.454539 - Test Accuracy 0.568081\n",
      "Epoch 826/1000 - 5.022450s - Loss 0.455546 - Accuracy 0.526042 - Test Loss 0.454836 - Test Accuracy 0.569408\n",
      "Epoch 827/1000 - 4.983655s - Loss 0.411750 - Accuracy 0.500000 - Test Loss 0.453766 - Test Accuracy 0.569408\n",
      "Epoch 828/1000 - 4.990050s - Loss 0.447182 - Accuracy 0.479167 - Test Loss 0.454128 - Test Accuracy 0.568966\n",
      "Epoch 829/1000 - 5.002094s - Loss 0.354383 - Accuracy 0.593750 - Test Loss 0.454371 - Test Accuracy 0.567197\n",
      "Epoch 830/1000 - 4.996864s - Loss 0.419344 - Accuracy 0.541667 - Test Loss 0.454491 - Test Accuracy 0.567639\n",
      "Epoch 831/1000 - 5.029292s - Loss 0.517158 - Accuracy 0.427083 - Test Loss 0.454460 - Test Accuracy 0.567639\n",
      "Epoch 832/1000 - 4.990235s - Loss 0.411105 - Accuracy 0.458333 - Test Loss 0.454403 - Test Accuracy 0.568966\n",
      "Epoch 833/1000 - 5.052246s - Loss 0.410830 - Accuracy 0.562500 - Test Loss 0.454444 - Test Accuracy 0.569408\n",
      "Epoch 834/1000 - 5.208614s - Loss 0.488232 - Accuracy 0.406250 - Test Loss 0.454405 - Test Accuracy 0.569408\n",
      "Epoch 835/1000 - 4.989942s - Loss 0.457528 - Accuracy 0.473958 - Test Loss 0.454240 - Test Accuracy 0.568081\n",
      "Epoch 836/1000 - 4.986275s - Loss 0.469038 - Accuracy 0.447917 - Test Loss 0.453830 - Test Accuracy 0.569850\n",
      "-----***-----\n",
      "0.5698496905393458\n",
      "Epoch 837/1000 - 4.991293s - Loss 0.497831 - Accuracy 0.489583 - Test Loss 0.453190 - Test Accuracy 0.567639\n",
      "Epoch 838/1000 - 5.043339s - Loss 0.434909 - Accuracy 0.505208 - Test Loss 0.453831 - Test Accuracy 0.568966\n",
      "Epoch 839/1000 - 5.045630s - Loss 0.375322 - Accuracy 0.557292 - Test Loss 0.452732 - Test Accuracy 0.568966\n",
      "Epoch 840/1000 - 5.045424s - Loss 0.395966 - Accuracy 0.494792 - Test Loss 0.452506 - Test Accuracy 0.569408\n",
      "Epoch 841/1000 - 4.997120s - Loss 0.436354 - Accuracy 0.515625 - Test Loss 0.452703 - Test Accuracy 0.568081\n",
      "Epoch 842/1000 - 5.026403s - Loss 0.461248 - Accuracy 0.447917 - Test Loss 0.452653 - Test Accuracy 0.568523\n",
      "Epoch 843/1000 - 4.983294s - Loss 0.320547 - Accuracy 0.604167 - Test Loss 0.452173 - Test Accuracy 0.568966\n",
      "Epoch 844/1000 - 5.046229s - Loss 0.446482 - Accuracy 0.458333 - Test Loss 0.452393 - Test Accuracy 0.568523\n",
      "Epoch 845/1000 - 4.985224s - Loss 0.434489 - Accuracy 0.500000 - Test Loss 0.452529 - Test Accuracy 0.570292\n",
      "-----***-----\n",
      "0.5702917771883289\n",
      "Epoch 846/1000 - 4.986628s - Loss 0.458159 - Accuracy 0.427083 - Test Loss 0.452878 - Test Accuracy 0.569850\n",
      "Epoch 847/1000 - 4.990793s - Loss 0.466278 - Accuracy 0.447917 - Test Loss 0.451843 - Test Accuracy 0.569850\n",
      "Epoch 848/1000 - 4.978340s - Loss 0.436435 - Accuracy 0.526042 - Test Loss 0.452248 - Test Accuracy 0.569850\n",
      "Epoch 849/1000 - 5.219960s - Loss 0.383986 - Accuracy 0.567708 - Test Loss 0.452262 - Test Accuracy 0.569850\n",
      "Epoch 850/1000 - 4.982779s - Loss 0.398878 - Accuracy 0.526042 - Test Loss 0.452417 - Test Accuracy 0.568966\n",
      "Epoch 851/1000 - 4.992722s - Loss 0.387491 - Accuracy 0.531250 - Test Loss 0.452382 - Test Accuracy 0.568523\n",
      "Epoch 852/1000 - 4.983696s - Loss 0.473534 - Accuracy 0.479167 - Test Loss 0.452802 - Test Accuracy 0.567639\n",
      "Epoch 853/1000 - 4.996216s - Loss 0.432882 - Accuracy 0.453125 - Test Loss 0.452842 - Test Accuracy 0.568523\n",
      "Epoch 854/1000 - 5.007660s - Loss 0.333046 - Accuracy 0.604167 - Test Loss 0.453270 - Test Accuracy 0.568081\n",
      "Epoch 855/1000 - 4.994807s - Loss 0.372462 - Accuracy 0.515625 - Test Loss 0.453555 - Test Accuracy 0.568523\n",
      "Epoch 856/1000 - 4.985488s - Loss 0.376503 - Accuracy 0.510417 - Test Loss 0.452240 - Test Accuracy 0.568966\n",
      "Epoch 857/1000 - 4.982639s - Loss 0.416449 - Accuracy 0.447917 - Test Loss 0.452412 - Test Accuracy 0.568966\n",
      "Epoch 858/1000 - 4.983039s - Loss 0.420429 - Accuracy 0.442708 - Test Loss 0.451911 - Test Accuracy 0.570292\n",
      "Epoch 859/1000 - 5.006106s - Loss 0.405981 - Accuracy 0.515625 - Test Loss 0.451871 - Test Accuracy 0.568966\n",
      "Epoch 860/1000 - 4.981439s - Loss 0.432424 - Accuracy 0.515625 - Test Loss 0.453039 - Test Accuracy 0.568081\n",
      "Epoch 861/1000 - 4.985869s - Loss 0.468678 - Accuracy 0.416667 - Test Loss 0.452151 - Test Accuracy 0.569408\n",
      "Epoch 862/1000 - 5.002305s - Loss 0.394789 - Accuracy 0.515625 - Test Loss 0.450265 - Test Accuracy 0.570734\n",
      "-----***-----\n",
      "0.5707338638373121\n",
      "Epoch 863/1000 - 5.010894s - Loss 0.439143 - Accuracy 0.515625 - Test Loss 0.451197 - Test Accuracy 0.568966\n",
      "Epoch 864/1000 - 4.978469s - Loss 0.422655 - Accuracy 0.489583 - Test Loss 0.451012 - Test Accuracy 0.570292\n",
      "Epoch 865/1000 - 5.057069s - Loss 0.457116 - Accuracy 0.468750 - Test Loss 0.451515 - Test Accuracy 0.569408\n",
      "Epoch 866/1000 - 4.980579s - Loss 0.381493 - Accuracy 0.562500 - Test Loss 0.451278 - Test Accuracy 0.570292\n",
      "Epoch 867/1000 - 4.992182s - Loss 0.408584 - Accuracy 0.447917 - Test Loss 0.451252 - Test Accuracy 0.569408\n",
      "Epoch 868/1000 - 5.024426s - Loss 0.456972 - Accuracy 0.473958 - Test Loss 0.450961 - Test Accuracy 0.568966\n",
      "Epoch 869/1000 - 4.985432s - Loss 0.398932 - Accuracy 0.557292 - Test Loss 0.451058 - Test Accuracy 0.568966\n",
      "Epoch 870/1000 - 4.981935s - Loss 0.420135 - Accuracy 0.515625 - Test Loss 0.450556 - Test Accuracy 0.568966\n",
      "Epoch 871/1000 - 4.983861s - Loss 0.496798 - Accuracy 0.411458 - Test Loss 0.450641 - Test Accuracy 0.568966\n",
      "Epoch 872/1000 - 5.029206s - Loss 0.388579 - Accuracy 0.557292 - Test Loss 0.450298 - Test Accuracy 0.569408\n",
      "Epoch 873/1000 - 4.987948s - Loss 0.339510 - Accuracy 0.562500 - Test Loss 0.449906 - Test Accuracy 0.568081\n",
      "Epoch 874/1000 - 4.966347s - Loss 0.380979 - Accuracy 0.562500 - Test Loss 0.450416 - Test Accuracy 0.567197\n",
      "Epoch 875/1000 - 5.047338s - Loss 0.404644 - Accuracy 0.510417 - Test Loss 0.450421 - Test Accuracy 0.567639\n",
      "Epoch 876/1000 - 4.985434s - Loss 0.408418 - Accuracy 0.432292 - Test Loss 0.450451 - Test Accuracy 0.569408\n",
      "Epoch 877/1000 - 5.008897s - Loss 0.407197 - Accuracy 0.489583 - Test Loss 0.451378 - Test Accuracy 0.568966\n",
      "Epoch 878/1000 - 4.981965s - Loss 0.414783 - Accuracy 0.520833 - Test Loss 0.450459 - Test Accuracy 0.570734\n",
      "Epoch 879/1000 - 5.045326s - Loss 0.534399 - Accuracy 0.359375 - Test Loss 0.449850 - Test Accuracy 0.569408\n",
      "Epoch 880/1000 - 4.987815s - Loss 0.364049 - Accuracy 0.536458 - Test Loss 0.449690 - Test Accuracy 0.570734\n",
      "Epoch 881/1000 - 4.999894s - Loss 0.403855 - Accuracy 0.557292 - Test Loss 0.449817 - Test Accuracy 0.569850\n",
      "Epoch 882/1000 - 5.000783s - Loss 0.437015 - Accuracy 0.546875 - Test Loss 0.449970 - Test Accuracy 0.569850\n",
      "Epoch 883/1000 - 5.040365s - Loss 0.427343 - Accuracy 0.463542 - Test Loss 0.449712 - Test Accuracy 0.570734\n",
      "Epoch 884/1000 - 4.997737s - Loss 0.389968 - Accuracy 0.500000 - Test Loss 0.449198 - Test Accuracy 0.570292\n",
      "Epoch 885/1000 - 4.985130s - Loss 0.437458 - Accuracy 0.473958 - Test Loss 0.450231 - Test Accuracy 0.570292\n",
      "Epoch 886/1000 - 4.972136s - Loss 0.396571 - Accuracy 0.541667 - Test Loss 0.449870 - Test Accuracy 0.572502\n",
      "-----***-----\n",
      "0.5725022104332449\n",
      "Epoch 887/1000 - 4.988048s - Loss 0.420859 - Accuracy 0.520833 - Test Loss 0.450501 - Test Accuracy 0.570734\n",
      "Epoch 888/1000 - 4.981782s - Loss 0.385669 - Accuracy 0.546875 - Test Loss 0.449837 - Test Accuracy 0.569850\n",
      "Epoch 889/1000 - 4.982282s - Loss 0.453213 - Accuracy 0.473958 - Test Loss 0.449749 - Test Accuracy 0.572502\n",
      "Epoch 890/1000 - 4.996079s - Loss 0.448890 - Accuracy 0.468750 - Test Loss 0.448930 - Test Accuracy 0.571618\n",
      "Epoch 891/1000 - 4.998182s - Loss 0.374121 - Accuracy 0.572917 - Test Loss 0.449716 - Test Accuracy 0.571618\n",
      "Epoch 892/1000 - 4.990968s - Loss 0.351014 - Accuracy 0.598958 - Test Loss 0.449312 - Test Accuracy 0.571618\n",
      "Epoch 893/1000 - 4.984347s - Loss 0.410839 - Accuracy 0.505208 - Test Loss 0.448550 - Test Accuracy 0.572502\n",
      "Epoch 894/1000 - 4.982256s - Loss 0.443398 - Accuracy 0.510417 - Test Loss 0.449498 - Test Accuracy 0.572944\n",
      "-----***-----\n",
      "0.5729442970822282\n",
      "Epoch 895/1000 - 4.984199s - Loss 0.390822 - Accuracy 0.526042 - Test Loss 0.448688 - Test Accuracy 0.570734\n",
      "Epoch 896/1000 - 5.030564s - Loss 0.384927 - Accuracy 0.453125 - Test Loss 0.449092 - Test Accuracy 0.571618\n",
      "Epoch 897/1000 - 5.039615s - Loss 0.409498 - Accuracy 0.468750 - Test Loss 0.448901 - Test Accuracy 0.570734\n",
      "Epoch 898/1000 - 5.029242s - Loss 0.405747 - Accuracy 0.494792 - Test Loss 0.449185 - Test Accuracy 0.571176\n",
      "Epoch 899/1000 - 4.996250s - Loss 0.437262 - Accuracy 0.463542 - Test Loss 0.448693 - Test Accuracy 0.571618\n",
      "Epoch 900/1000 - 4.973512s - Loss 0.384902 - Accuracy 0.520833 - Test Loss 0.448974 - Test Accuracy 0.572060\n",
      "Epoch 901/1000 - 4.995362s - Loss 0.409564 - Accuracy 0.437500 - Test Loss 0.449989 - Test Accuracy 0.572060\n",
      "Epoch 902/1000 - 4.985618s - Loss 0.421772 - Accuracy 0.510417 - Test Loss 0.449069 - Test Accuracy 0.571618\n",
      "Epoch 903/1000 - 4.987026s - Loss 0.423346 - Accuracy 0.468750 - Test Loss 0.448706 - Test Accuracy 0.571618\n",
      "Epoch 904/1000 - 4.991472s - Loss 0.405064 - Accuracy 0.484375 - Test Loss 0.448778 - Test Accuracy 0.572060\n",
      "Epoch 905/1000 - 5.042422s - Loss 0.411489 - Accuracy 0.531250 - Test Loss 0.449411 - Test Accuracy 0.572502\n",
      "Epoch 906/1000 - 5.046228s - Loss 0.412234 - Accuracy 0.494792 - Test Loss 0.449777 - Test Accuracy 0.572060\n",
      "Epoch 907/1000 - 4.993720s - Loss 0.416969 - Accuracy 0.489583 - Test Loss 0.448741 - Test Accuracy 0.569408\n",
      "Epoch 908/1000 - 5.030677s - Loss 0.457551 - Accuracy 0.489583 - Test Loss 0.447393 - Test Accuracy 0.570734\n",
      "Epoch 909/1000 - 4.981739s - Loss 0.396351 - Accuracy 0.510417 - Test Loss 0.447077 - Test Accuracy 0.570734\n",
      "Epoch 910/1000 - 4.977262s - Loss 0.410459 - Accuracy 0.536458 - Test Loss 0.447719 - Test Accuracy 0.572944\n",
      "Epoch 911/1000 - 4.984246s - Loss 0.363952 - Accuracy 0.536458 - Test Loss 0.447599 - Test Accuracy 0.572060\n",
      "Epoch 912/1000 - 4.982093s - Loss 0.370090 - Accuracy 0.505208 - Test Loss 0.447331 - Test Accuracy 0.572944\n",
      "Epoch 913/1000 - 5.019027s - Loss 0.413586 - Accuracy 0.479167 - Test Loss 0.447484 - Test Accuracy 0.570734\n",
      "Epoch 914/1000 - 4.975120s - Loss 0.462057 - Accuracy 0.437500 - Test Loss 0.447350 - Test Accuracy 0.569850\n",
      "Epoch 915/1000 - 4.986029s - Loss 0.406853 - Accuracy 0.494792 - Test Loss 0.447464 - Test Accuracy 0.572060\n",
      "Epoch 916/1000 - 4.983156s - Loss 0.457652 - Accuracy 0.484375 - Test Loss 0.446397 - Test Accuracy 0.571618\n",
      "Epoch 917/1000 - 4.999110s - Loss 0.436686 - Accuracy 0.515625 - Test Loss 0.447532 - Test Accuracy 0.570292\n",
      "Epoch 918/1000 - 4.984429s - Loss 0.383906 - Accuracy 0.572917 - Test Loss 0.447271 - Test Accuracy 0.571618\n",
      "Epoch 919/1000 - 5.043368s - Loss 0.357218 - Accuracy 0.536458 - Test Loss 0.446687 - Test Accuracy 0.573386\n",
      "-----***-----\n",
      "0.5733863837312113\n",
      "Epoch 920/1000 - 4.978107s - Loss 0.425157 - Accuracy 0.473958 - Test Loss 0.447210 - Test Accuracy 0.572502\n",
      "Epoch 921/1000 - 4.992114s - Loss 0.444029 - Accuracy 0.473958 - Test Loss 0.446737 - Test Accuracy 0.572502\n",
      "Epoch 922/1000 - 5.050497s - Loss 0.415696 - Accuracy 0.505208 - Test Loss 0.446925 - Test Accuracy 0.571176\n",
      "Epoch 923/1000 - 4.996130s - Loss 0.385817 - Accuracy 0.562500 - Test Loss 0.446807 - Test Accuracy 0.572944\n",
      "Epoch 924/1000 - 4.991044s - Loss 0.366000 - Accuracy 0.484375 - Test Loss 0.445998 - Test Accuracy 0.572060\n",
      "Epoch 925/1000 - 4.995189s - Loss 0.415324 - Accuracy 0.526042 - Test Loss 0.446110 - Test Accuracy 0.572060\n",
      "Epoch 926/1000 - 4.985142s - Loss 0.396857 - Accuracy 0.541667 - Test Loss 0.447008 - Test Accuracy 0.571176\n",
      "Epoch 927/1000 - 5.034280s - Loss 0.365111 - Accuracy 0.500000 - Test Loss 0.446200 - Test Accuracy 0.573828\n",
      "-----***-----\n",
      "0.5738284703801945\n",
      "Epoch 928/1000 - 4.987856s - Loss 0.434336 - Accuracy 0.494792 - Test Loss 0.446580 - Test Accuracy 0.572502\n",
      "Epoch 929/1000 - 4.984531s - Loss 0.351434 - Accuracy 0.593750 - Test Loss 0.446878 - Test Accuracy 0.572060\n",
      "Epoch 930/1000 - 4.987058s - Loss 0.397932 - Accuracy 0.520833 - Test Loss 0.447552 - Test Accuracy 0.572944\n",
      "Epoch 931/1000 - 5.028203s - Loss 0.366918 - Accuracy 0.541667 - Test Loss 0.446659 - Test Accuracy 0.572944\n",
      "Epoch 932/1000 - 4.995264s - Loss 0.336993 - Accuracy 0.567708 - Test Loss 0.446705 - Test Accuracy 0.572944\n",
      "Epoch 933/1000 - 5.031848s - Loss 0.380010 - Accuracy 0.531250 - Test Loss 0.446114 - Test Accuracy 0.572060\n",
      "Epoch 934/1000 - 4.974094s - Loss 0.390304 - Accuracy 0.536458 - Test Loss 0.445913 - Test Accuracy 0.573386\n",
      "Epoch 935/1000 - 5.039547s - Loss 0.378001 - Accuracy 0.578125 - Test Loss 0.446230 - Test Accuracy 0.573386\n",
      "Epoch 936/1000 - 4.978988s - Loss 0.438951 - Accuracy 0.458333 - Test Loss 0.446638 - Test Accuracy 0.574271\n",
      "-----***-----\n",
      "0.5742705570291777\n",
      "Epoch 937/1000 - 5.022220s - Loss 0.389453 - Accuracy 0.505208 - Test Loss 0.446204 - Test Accuracy 0.573386\n",
      "Epoch 938/1000 - 5.002471s - Loss 0.326916 - Accuracy 0.604167 - Test Loss 0.446321 - Test Accuracy 0.573828\n",
      "Epoch 939/1000 - 4.985965s - Loss 0.435476 - Accuracy 0.500000 - Test Loss 0.446417 - Test Accuracy 0.572944\n",
      "Epoch 940/1000 - 5.001842s - Loss 0.380186 - Accuracy 0.572917 - Test Loss 0.445708 - Test Accuracy 0.571618\n",
      "Epoch 941/1000 - 4.988765s - Loss 0.430655 - Accuracy 0.479167 - Test Loss 0.446690 - Test Accuracy 0.572060\n",
      "Epoch 942/1000 - 4.987094s - Loss 0.323633 - Accuracy 0.635417 - Test Loss 0.446752 - Test Accuracy 0.572944\n",
      "Epoch 943/1000 - 5.036580s - Loss 0.364407 - Accuracy 0.541667 - Test Loss 0.446496 - Test Accuracy 0.571618\n",
      "Epoch 944/1000 - 4.989783s - Loss 0.409450 - Accuracy 0.447917 - Test Loss 0.446481 - Test Accuracy 0.572502\n",
      "Epoch 945/1000 - 4.994657s - Loss 0.400686 - Accuracy 0.531250 - Test Loss 0.446395 - Test Accuracy 0.573386\n",
      "Epoch 946/1000 - 4.983925s - Loss 0.417471 - Accuracy 0.416667 - Test Loss 0.446404 - Test Accuracy 0.574271\n",
      "Epoch 947/1000 - 4.983889s - Loss 0.350849 - Accuracy 0.583333 - Test Loss 0.446714 - Test Accuracy 0.572944\n",
      "Epoch 948/1000 - 4.988052s - Loss 0.384115 - Accuracy 0.541667 - Test Loss 0.446154 - Test Accuracy 0.574713\n",
      "-----***-----\n",
      "0.5747126436781609\n",
      "Epoch 949/1000 - 4.993067s - Loss 0.427248 - Accuracy 0.473958 - Test Loss 0.445893 - Test Accuracy 0.573828\n",
      "Epoch 950/1000 - 4.988114s - Loss 0.389886 - Accuracy 0.572917 - Test Loss 0.446536 - Test Accuracy 0.575155\n",
      "-----***-----\n",
      "0.5751547303271441\n",
      "Epoch 951/1000 - 4.981363s - Loss 0.397327 - Accuracy 0.536458 - Test Loss 0.446028 - Test Accuracy 0.574713\n",
      "Epoch 952/1000 - 4.987246s - Loss 0.341938 - Accuracy 0.604167 - Test Loss 0.445712 - Test Accuracy 0.574713\n",
      "Epoch 953/1000 - 4.986731s - Loss 0.402890 - Accuracy 0.468750 - Test Loss 0.445441 - Test Accuracy 0.573828\n",
      "Epoch 954/1000 - 5.002322s - Loss 0.404317 - Accuracy 0.541667 - Test Loss 0.445037 - Test Accuracy 0.572502\n",
      "Epoch 955/1000 - 4.986075s - Loss 0.423323 - Accuracy 0.526042 - Test Loss 0.445060 - Test Accuracy 0.572502\n",
      "Epoch 956/1000 - 5.037869s - Loss 0.437367 - Accuracy 0.510417 - Test Loss 0.444735 - Test Accuracy 0.572060\n",
      "Epoch 957/1000 - 4.985175s - Loss 0.376127 - Accuracy 0.567708 - Test Loss 0.445598 - Test Accuracy 0.575155\n",
      "Epoch 958/1000 - 4.975713s - Loss 0.452403 - Accuracy 0.489583 - Test Loss 0.445286 - Test Accuracy 0.572502\n",
      "Epoch 959/1000 - 4.982512s - Loss 0.406966 - Accuracy 0.562500 - Test Loss 0.445253 - Test Accuracy 0.573386\n",
      "Epoch 960/1000 - 4.976651s - Loss 0.466891 - Accuracy 0.479167 - Test Loss 0.445674 - Test Accuracy 0.572944\n",
      "Epoch 961/1000 - 4.992365s - Loss 0.421244 - Accuracy 0.489583 - Test Loss 0.444862 - Test Accuracy 0.572502\n",
      "Epoch 962/1000 - 4.983218s - Loss 0.454141 - Accuracy 0.453125 - Test Loss 0.444411 - Test Accuracy 0.573386\n",
      "Epoch 963/1000 - 4.989150s - Loss 0.406255 - Accuracy 0.541667 - Test Loss 0.444121 - Test Accuracy 0.572060\n",
      "Epoch 964/1000 - 4.984134s - Loss 0.401318 - Accuracy 0.453125 - Test Loss 0.443936 - Test Accuracy 0.571618\n",
      "Epoch 965/1000 - 4.986978s - Loss 0.374579 - Accuracy 0.515625 - Test Loss 0.443735 - Test Accuracy 0.571176\n",
      "Epoch 966/1000 - 4.988368s - Loss 0.378280 - Accuracy 0.500000 - Test Loss 0.444643 - Test Accuracy 0.572060\n",
      "Epoch 967/1000 - 4.990801s - Loss 0.379962 - Accuracy 0.536458 - Test Loss 0.444871 - Test Accuracy 0.571176\n",
      "Epoch 968/1000 - 4.982383s - Loss 0.353229 - Accuracy 0.557292 - Test Loss 0.443870 - Test Accuracy 0.572502\n",
      "Epoch 969/1000 - 5.004816s - Loss 0.377736 - Accuracy 0.567708 - Test Loss 0.444390 - Test Accuracy 0.570734\n",
      "Epoch 970/1000 - 5.043232s - Loss 0.359232 - Accuracy 0.593750 - Test Loss 0.443631 - Test Accuracy 0.572944\n",
      "Epoch 971/1000 - 4.976849s - Loss 0.424231 - Accuracy 0.458333 - Test Loss 0.444185 - Test Accuracy 0.571176\n",
      "Epoch 972/1000 - 5.020760s - Loss 0.394718 - Accuracy 0.510417 - Test Loss 0.443095 - Test Accuracy 0.572060\n",
      "Epoch 973/1000 - 4.996040s - Loss 0.393562 - Accuracy 0.515625 - Test Loss 0.443991 - Test Accuracy 0.572944\n",
      "Epoch 974/1000 - 4.985896s - Loss 0.422963 - Accuracy 0.473958 - Test Loss 0.444381 - Test Accuracy 0.572060\n",
      "Epoch 975/1000 - 4.993380s - Loss 0.368556 - Accuracy 0.484375 - Test Loss 0.444239 - Test Accuracy 0.571618\n",
      "Epoch 976/1000 - 4.989949s - Loss 0.352591 - Accuracy 0.604167 - Test Loss 0.444226 - Test Accuracy 0.571618\n",
      "Epoch 977/1000 - 4.994571s - Loss 0.386565 - Accuracy 0.489583 - Test Loss 0.443974 - Test Accuracy 0.572060\n",
      "Epoch 978/1000 - 5.001489s - Loss 0.422412 - Accuracy 0.484375 - Test Loss 0.444045 - Test Accuracy 0.572502\n",
      "Epoch 979/1000 - 5.033113s - Loss 0.368508 - Accuracy 0.593750 - Test Loss 0.444488 - Test Accuracy 0.572944\n",
      "Epoch 980/1000 - 4.987211s - Loss 0.398908 - Accuracy 0.505208 - Test Loss 0.444519 - Test Accuracy 0.572502\n",
      "Epoch 981/1000 - 4.980512s - Loss 0.391212 - Accuracy 0.520833 - Test Loss 0.444449 - Test Accuracy 0.573828\n",
      "Epoch 982/1000 - 4.983193s - Loss 0.368368 - Accuracy 0.510417 - Test Loss 0.444470 - Test Accuracy 0.572944\n",
      "Epoch 983/1000 - 4.994289s - Loss 0.429190 - Accuracy 0.442708 - Test Loss 0.444182 - Test Accuracy 0.572060\n",
      "Epoch 984/1000 - 4.985032s - Loss 0.365006 - Accuracy 0.526042 - Test Loss 0.443434 - Test Accuracy 0.573386\n",
      "Epoch 985/1000 - 4.986810s - Loss 0.434577 - Accuracy 0.473958 - Test Loss 0.442228 - Test Accuracy 0.571618\n",
      "Epoch 986/1000 - 4.986347s - Loss 0.368788 - Accuracy 0.567708 - Test Loss 0.443368 - Test Accuracy 0.571618\n",
      "Epoch 987/1000 - 4.978092s - Loss 0.373919 - Accuracy 0.541667 - Test Loss 0.443636 - Test Accuracy 0.572944\n",
      "Epoch 988/1000 - 4.984797s - Loss 0.362043 - Accuracy 0.520833 - Test Loss 0.443745 - Test Accuracy 0.571618\n",
      "Epoch 989/1000 - 4.980752s - Loss 0.384820 - Accuracy 0.541667 - Test Loss 0.442950 - Test Accuracy 0.571176\n",
      "Epoch 990/1000 - 5.005971s - Loss 0.392852 - Accuracy 0.541667 - Test Loss 0.443307 - Test Accuracy 0.572060\n",
      "Epoch 991/1000 - 4.995101s - Loss 0.340214 - Accuracy 0.567708 - Test Loss 0.442869 - Test Accuracy 0.572502\n",
      "Epoch 992/1000 - 4.981826s - Loss 0.455281 - Accuracy 0.479167 - Test Loss 0.444049 - Test Accuracy 0.573386\n",
      "Epoch 993/1000 - 5.001217s - Loss 0.381243 - Accuracy 0.583333 - Test Loss 0.443308 - Test Accuracy 0.572502\n",
      "Epoch 994/1000 - 4.979430s - Loss 0.387524 - Accuracy 0.598958 - Test Loss 0.443286 - Test Accuracy 0.572502\n",
      "Epoch 995/1000 - 5.016362s - Loss 0.366454 - Accuracy 0.572917 - Test Loss 0.443116 - Test Accuracy 0.573386\n",
      "Epoch 996/1000 - 4.977575s - Loss 0.370504 - Accuracy 0.531250 - Test Loss 0.443178 - Test Accuracy 0.572944\n",
      "Epoch 997/1000 - 4.989069s - Loss 0.365943 - Accuracy 0.593750 - Test Loss 0.443803 - Test Accuracy 0.572502\n",
      "Epoch 998/1000 - 4.985650s - Loss 0.392796 - Accuracy 0.520833 - Test Loss 0.442037 - Test Accuracy 0.572944\n",
      "Epoch 999/1000 - 5.015303s - Loss 0.312172 - Accuracy 0.598958 - Test Loss 0.442434 - Test Accuracy 0.572060\n",
      "repeat_0 [0.5751547303271441]\n",
      "{'repeat_0': {'0': {'precision': 0.25806451612903225, 'recall': 0.0784313725490196, 'f1-score': 0.12030075187969926, 'support': 102}, '1': {'precision': 0.38271604938271603, 'recall': 0.23308270676691728, 'f1-score': 0.2897196261682243, 'support': 133}, '2': {'precision': 0.23214285714285715, 'recall': 0.11206896551724138, 'f1-score': 0.15116279069767444, 'support': 116}, '3': {'precision': 0.171875, 'recall': 0.11702127659574468, 'f1-score': 0.13924050632911392, 'support': 94}, '4': {'precision': 0.2564102564102564, 'recall': 0.09345794392523364, 'f1-score': 0.136986301369863, 'support': 107}, '5': {'precision': 0.2, 'recall': 0.06363636363636363, 'f1-score': 0.09655172413793102, 'support': 110}, '6': {'precision': 0.13513513513513514, 'recall': 0.050505050505050504, 'f1-score': 0.07352941176470588, 'support': 99}, '7': {'precision': 0.2647058823529412, 'recall': 0.16363636363636364, 'f1-score': 0.20224719101123595, 'support': 110}, '8': {'precision': 0.2222222222222222, 'recall': 0.12612612612612611, 'f1-score': 0.16091954022988506, 'support': 111}, 'micro avg': {'precision': 0.2468354430379747, 'recall': 0.11914460285132383, 'f1-score': 0.1607142857142857, 'support': 982}, 'macro avg': {'precision': 0.23591910208612893, 'recall': 0.1153295743620067, 'f1-score': 0.1522953159542592, 'support': 982}, 'weighted avg': {'precision': 0.24124972116232174, 'recall': 0.11914460285132383, 'f1-score': 0.15691830339223717, 'support': 982}, 'samples avg': {'precision': 0.041519304450338926, 'recall': 0.05172413793103448, 'f1-score': 0.04468022399056882, 'support': 982}}}\n",
      "{'repeat_0': {'0': {'precision': 0.25806451612903225, 'recall': 0.0784313725490196, 'f1-score': 0.12030075187969926, 'support': 102}, '1': {'precision': 0.38271604938271603, 'recall': 0.23308270676691728, 'f1-score': 0.2897196261682243, 'support': 133}, '2': {'precision': 0.23214285714285715, 'recall': 0.11206896551724138, 'f1-score': 0.15116279069767444, 'support': 116}, '3': {'precision': 0.171875, 'recall': 0.11702127659574468, 'f1-score': 0.13924050632911392, 'support': 94}, '4': {'precision': 0.2564102564102564, 'recall': 0.09345794392523364, 'f1-score': 0.136986301369863, 'support': 107}, '5': {'precision': 0.2, 'recall': 0.06363636363636363, 'f1-score': 0.09655172413793102, 'support': 110}, '6': {'precision': 0.13513513513513514, 'recall': 0.050505050505050504, 'f1-score': 0.07352941176470588, 'support': 99}, '7': {'precision': 0.2647058823529412, 'recall': 0.16363636363636364, 'f1-score': 0.20224719101123595, 'support': 110}, '8': {'precision': 0.2222222222222222, 'recall': 0.12612612612612611, 'f1-score': 0.16091954022988506, 'support': 111}, 'micro avg': {'precision': 0.2468354430379747, 'recall': 0.11914460285132383, 'f1-score': 0.1607142857142857, 'support': 982}, 'macro avg': {'precision': 0.23591910208612893, 'recall': 0.1153295743620067, 'f1-score': 0.1522953159542592, 'support': 982}, 'weighted avg': {'precision': 0.24124972116232174, 'recall': 0.11914460285132383, 'f1-score': 0.15691830339223717, 'support': 982}, 'samples avg': {'precision': 0.041519304450338926, 'recall': 0.05172413793103448, 'f1-score': 0.04468022399056882, 'support': 982}}, 'accuracy': {'avg': 0.5751547303271441, 'std': 0.0}, 'time_train': {'avg': 4997.4926404953, 'std': 0.0}, 'time_test': {'avg': 0.4791829586029053, 'std': 0.0}, 'model': 'THAT', 'task': 'activity', 'data': {'num_users': ['0', '1', '2', '3', '4', '5'], 'wifi_band': ['2.4'], 'environment': ['classroom'], 'length': 3000}, 'nn': {'lr': 1e-06, 'epoch': 1000, 'batch_size': 64, 'threshold': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()           \n",
    "torch.cuda.empty_cache()  \n",
    "torch.cuda.ipc_collect()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "[file]          run.py\n",
    "[description]   run WiFi-based models and optionally save a multiclass confusion matrix\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# from preset import preset, name_run\n",
    "# from load_data import load_data_x, load_data_y, encode_data_y\n",
    "# from lstm import run_lstm, LSTMM\n",
    "# from bilstm import run_bilstm, BiLSTMM\n",
    "# from that import run_that, THAT\n",
    "# from resnet import run_resnet, ResNet18Model\n",
    "# from strf import run_strf  # if you have the ST-RF implementation\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model\",   default=preset[\"model\"],  type=str)\n",
    "    parser.add_argument(\"--task\",    default=preset[\"task\"],   type=str)\n",
    "    parser.add_argument(\"--repeat\",  default=preset[\"repeat\"], type=int)\n",
    "    parser.add_argument(\"--save_cm\", action=\"store_true\",\n",
    "                        help=\"Save a multiclass confusion matrix of the best model to PDF\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def save_multiclass_confusion_matrix(model, data_loader, device, pdf_path, num_classes):\n",
    "    \"\"\"\n",
    "    Given a model that outputs one-hot logits for a multiclass task,\n",
    "    convert to predicted classes via argmax, then plot and save a\n",
    "    num_classes × num_classes confusion matrix to pdf_path.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            # predicted class is index of max logit\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            trues = torch.argmax(yb, dim=1).cpu().numpy()\n",
    "            y_pred.extend(preds.tolist())\n",
    "            y_true.extend(trues.tolist())\n",
    "\n",
    "    labels = list(range(num_classes))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    disp.plot(ax=ax, xticks_rotation=\"vertical\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "def run():\n",
    "    args       = parse_args()\n",
    "    var_model  = args.model\n",
    "    var_task   = args.task\n",
    "    var_repeat = args.repeat\n",
    "\n",
    "    # --- Load and encode the data ---\n",
    "    data_pd_y = load_data_y(\n",
    "        preset[\"path\"][\"data_y\"],\n",
    "        var_environment=preset[\"data\"][\"environment\"],\n",
    "        var_wifi_band=preset[\"data\"][\"wifi_band\"],\n",
    "        var_num_users=preset[\"data\"][\"num_users\"]\n",
    "    )\n",
    "    labels = data_pd_y[\"label\"].tolist()\n",
    "    data_x = load_data_x(preset[\"path\"][\"data_x\"], labels)\n",
    "    data_y = encode_data_y(data_pd_y, var_task)\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(\n",
    "        data_x, data_y, test_size=0.2, shuffle=True, random_state=39\n",
    "    )\n",
    "\n",
    "    # --- Select which model runner to use ---\n",
    "    if var_model == \"ST-RF\":\n",
    "        from strf import run_strf\n",
    "        run_model = run_strf\n",
    "    elif var_model == \"LSTM\":\n",
    "        run_model = run_lstm\n",
    "    elif var_model == \"bi-LSTM\":\n",
    "        run_model = run_bilstm\n",
    "    elif var_model == \"THAT\":\n",
    "        run_model = run_that\n",
    "    elif var_model == \"ResNet18\":\n",
    "        run_model = run_resnet\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {var_model}\")\n",
    "\n",
    "    # --- Train and evaluate ---\n",
    "    print(f\"Running model: {var_model}\")\n",
    "    result = run_model(train_x, train_y, test_x, test_y, var_repeat)\n",
    "    result[\"model\"] = var_model\n",
    "    result[\"task\"]  = var_task\n",
    "    result[\"data\"]  = preset[\"data\"]\n",
    "    result[\"nn\"]    = preset[\"nn\"]\n",
    "    print(result)\n",
    "\n",
    "    # --- Save results to JSON ---\n",
    "    # with open(preset[\"path\"][\"save\"], \"w\") as f:\n",
    "    #     json.dump(result, f, indent=4)\n",
    "\n",
    "    # # --- Optionally save a multiclass confusion matrix ---\n",
    "    # # if args.save_cm:\n",
    "    # if Confusion_matrix == 1:\n",
    "    #     # 1) completely release GPU memory used for training\n",
    "    #     del run_model                      # if 'model' from training is still in scope\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     torch.cuda.ipc_collect()\n",
    "    \n",
    "    #     # 2) reshape input only if the network is sequence‑based\n",
    "    #     if var_model in (\"LSTM\", \"bi-LSTM\", \"THAT\"):\n",
    "    #         test_x_cm = test_x.reshape(test_x.shape[0], test_x.shape[1], -1)\n",
    "    #     else:                           # ResNet18, ST‑RF\n",
    "    #         test_x_cm = test_x\n",
    "    \n",
    "    #     # 3) build the *same* architecture on CPU and load its weights\n",
    "    #     device_cm = torch.device(\"cpu\")\n",
    "    #     if var_model == \"LSTM\":\n",
    "    #         model_cm = LSTMM(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     elif var_model == \"bi-LSTM\":\n",
    "    #         model_cm = BiLSTMM(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     elif var_model == \"THAT\":\n",
    "    #         model_cm = THAT(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     elif var_model == \"ResNet18\":\n",
    "    #         model_cm = ResNet18Model(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Confusion matrix not supported for {var_model}\")\n",
    "    \n",
    "    #     # best_path = f\"/kaggle/working/{name_run}_best_model.pt\"\n",
    "    #     # model_cm.load_state_dict(torch.load(best_path, map_location=device_cm))\n",
    "    #     # model_cm.eval()\n",
    "    \n",
    "    #     # 4) DataLoader on CPU with a safe batch size\n",
    "    #     test_ds = TensorDataset(torch.from_numpy(test_x_cm).float(),\n",
    "    #                             torch.from_numpy(test_y).float())\n",
    "    #     test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "    \n",
    "    #     # 5) save the confusion matrix PDF\n",
    "    #     num_classes = test_y.shape[1]\n",
    "    #     pdf_name = f\"{name_run}_confusion_matrix.pdf\"\n",
    "    #     save_multiclass_confusion_matrix(model_cm,test_loader,device_cm,pdf_name,num_classes)\n",
    "    #     print(f\"✅ Saved confusion matrix (classes 0–{num_classes-1}) to {pdf_name}\")\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"start\")\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c0423",
   "metadata": {
    "papermill": {
     "duration": 0.045386,
     "end_time": "2025-12-28T19:03:59.374699",
     "exception": false,
     "start_time": "2025-12-28T19:03:59.329313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 12: Few-shot Learning\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d01b4a1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T19:03:59.471335Z",
     "iopub.status.busy": "2025-12-28T19:03:59.471056Z",
     "iopub.status.idle": "2025-12-28T19:03:59.476488Z",
     "shell.execute_reply": "2025-12-28T19:03:59.475934Z"
    },
    "papermill": {
     "duration": 0.054152,
     "end_time": "2025-12-28T19:03:59.477588",
     "exception": false,
     "start_time": "2025-12-28T19:03:59.423436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import torch\n",
    "# import shutil\n",
    "# import json\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# gc.collect()           \n",
    "# torch.cuda.empty_cache()  \n",
    "# torch.cuda.ipc_collect()\n",
    "\n",
    "# # ---------- helper: save multiclass confusion matrix ------------------\n",
    "# def save_multiclass_confusion_matrix(model, data_loader, pdf_path, num_classes):\n",
    "#     \"\"\"\n",
    "#     Forward‑pass on CPU, collect predictions, and write an N×N confusion matrix\n",
    "#     to a single‑page PDF (pdf_path).\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     y_true, y_pred = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in data_loader:\n",
    "#             logits = model(xb.cpu())                       # ensure CPU\n",
    "#             preds  = torch.argmax(logits, dim=1).numpy()\n",
    "#             trues  = torch.argmax(yb, dim=1).numpy()\n",
    "#             y_pred.extend(preds.tolist())\n",
    "#             y_true.extend(trues.tolist())\n",
    "\n",
    "#     labels = list(range(num_classes))\n",
    "#     cm  = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "#     disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "#     fig, ax = plt.subplots(figsize=(8, 8))\n",
    "#     disp.plot(ax=ax, xticks_rotation=\"vertical\")\n",
    "#     ax.set_title(\"Few‑shot Confusion Matrix\")\n",
    "#     with PdfPages(pdf_path) as pdf:\n",
    "#         pdf.savefig(fig)\n",
    "#     plt.close(fig)\n",
    "\n",
    "# # -------------------- pick run_* function ------------------------------\n",
    "# if preset[\"model\"] == \"ST-RF\":\n",
    "#     run_model = run_strf\n",
    "# elif preset[\"model\"] == \"LSTM\":\n",
    "#     run_model = run_lstm\n",
    "# elif preset[\"model\"] == \"bi-LSTM\":\n",
    "#     run_model = run_bilstm\n",
    "# elif preset[\"model\"] == \"THAT\":\n",
    "#     run_model = run_that\n",
    "# elif preset[\"model\"] == \"ResNet18\":\n",
    "#     run_model = run_resnet\n",
    "# else:\n",
    "#     raise ValueError(f\"No few‑shot implementation for {preset['model']}.\")\n",
    "\n",
    "# # ------------------------ load / split data ----------------------------\n",
    "# data_pd_y = load_data_y(preset[\"path\"][\"data_y\"],\n",
    "#                         var_environment=[dest_env],\n",
    "#                         var_wifi_band=preset[\"data\"][\"wifi_band\"],\n",
    "#                         var_num_users=preset[\"data\"][\"num_users\"])\n",
    "\n",
    "# labels_list = data_pd_y[\"label\"].tolist()\n",
    "# data_x = load_data_x(preset[\"path\"][\"data_x\"], labels_list)\n",
    "# data_y = encode_data_y(data_pd_y, preset[\"task\"])\n",
    "\n",
    "# train_x, test_x, train_y, test_y = train_test_split(\n",
    "#     data_x, data_y, test_size=0.2, shuffle=True, random_state=39)\n",
    "\n",
    "# # Few-shot sample size\n",
    "# train_x = train_x[:few_shot_num_samples]\n",
    "# train_y = train_y[:few_shot_num_samples]\n",
    "\n",
    "# # ----------------------- few‑shot training -----------------------------\n",
    "# original_epochs = preset[\"nn\"][\"epoch\"]\n",
    "# preset[\"nn\"][\"epoch\"] = few_shot_epochs\n",
    "\n",
    "# # Load the best model weights\n",
    "# best_model_path = f\"{name_run}_best_model.pt\"\n",
    "\n",
    "# # Initialize the model \n",
    "# if preset[\"model\"] == \"LSTM\":\n",
    "#     model = LSTMM(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "#     # print('train_y_[0].shape:', train_y[0].shape)\n",
    "#     # print('train_x_[0].shape:', train_x[0].reshape(train_x[0].shape[0], -1).shape)\n",
    "# elif preset[\"model\"] == \"bi-LSTM\":\n",
    "#     model = BiLSTMM(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "# elif preset[\"model\"] == \"THAT\":\n",
    "#     model = THAT(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "# elif preset[\"model\"] == \"ResNet18\":\n",
    "#     model = ResNet18Model(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "# else:\n",
    "#     raise ValueError(f\"Model {preset['model']} not supported!\")\n",
    "\n",
    "# # Load the weights into the model\n",
    "# model.load_state_dict(torch.load(best_model_path, map_location=\"cpu\"))\n",
    "# model = model.to('cuda')\n",
    "\n",
    "# # Fine-tune the model on few-shot data (note: `run_model` should now return only the result)\n",
    "# result = run_model(train_x, train_y, test_x, test_y, var_repeat=1, init_model=model)\n",
    "# print(result)\n",
    "\n",
    "# # --------------------- save few‑shot checkpoints -----------------------\n",
    "# # After fine-tuning, save the model\n",
    "# torch.save(model.state_dict(), f\"{name_run}_fewshot_final_model.pt\")\n",
    "# torch.save(model.state_dict(), f\"{name_run}_fewshot_best_model.pt\")\n",
    "\n",
    "# # ------------------- confusion matrix on CPU ---------------------------\n",
    "# if Confusion_matrix == 1 and preset[\"model\"] != \"ST-RF\":\n",
    "\n",
    "#     # reshape for sequence models\n",
    "#     test_x_rs = (test_x.reshape(test_x.shape[0], test_x.shape[1], -1)\n",
    "#                  if preset[\"model\"] in (\"LSTM\", \"bi-LSTM\", \"THAT\") else test_x)\n",
    "\n",
    "#     # instantiate identical architecture on CPU\n",
    "#     if preset[\"model\"] == \"LSTM\":\n",
    "#         model_cpu = LSTMM(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "#     elif preset[\"model\"] == \"bi-LSTM\":\n",
    "#         model_cpu = BiLSTMM(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "#     elif preset[\"model\"] == \"THAT\":\n",
    "#         model_cpu = THAT(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "#     else:  # ResNet18\n",
    "#         model_cpu = ResNet18Model(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "\n",
    "#     # load weights\n",
    "#     model_cpu.load_state_dict(torch.load(f\"{name_run}_fewshot_best_model.pt\", map_location=\"cpu\"))\n",
    "\n",
    "#     # CPU DataLoader with a safe batch size\n",
    "#     test_ds = TensorDataset(torch.from_numpy(test_x_rs).float(),\n",
    "#                             torch.from_numpy(test_y).float())\n",
    "#     test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "#     pdf_name = f\"{name_run}_fewshot_confusion_matrix.pdf\"\n",
    "#     num_classes = test_y.shape[1]\n",
    "#     save_multiclass_confusion_matrix(model_cpu, test_loader, pdf_name, num_classes)\n",
    "#     print(f\"✅ Saved few‑shot confusion matrix (classes 0–{num_classes-1}) to {pdf_name}\")\n",
    "\n",
    "# # ----------------------- restore & persist -----------------------------\n",
    "# preset[\"nn\"][\"epoch\"] = original_epochs\n",
    "\n",
    "# # Save the final result to JSON\n",
    "# with open(\"result_fewshot.json\", \"w\") as f:\n",
    "#     json.dump(result, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a42014c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T19:03:59.572640Z",
     "iopub.status.busy": "2025-12-28T19:03:59.572130Z",
     "iopub.status.idle": "2025-12-28T19:03:59.582724Z",
     "shell.execute_reply": "2025-12-28T19:03:59.582126Z"
    },
    "papermill": {
     "duration": 0.059639,
     "end_time": "2025-12-28T19:03:59.583814",
     "exception": false,
     "start_time": "2025-12-28T19:03:59.524175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import scipy.io as scio\n",
    "# import time\n",
    "# import torch\n",
    "# import gc\n",
    "# from numpy.linalg import svd\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "# from copy import deepcopy\n",
    "# import json\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# import torch._dynamo\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # --- تنظیمات سیستمی ---\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# # --------------------------\n",
    "# # 1. تنظیمات (Configuration)\n",
    "# # --------------------------\n",
    "# preset = {\n",
    "#     \"model\": \"THAT\",          \n",
    "#     \"task\": \"activity\",       \n",
    "#     \"repeat\": 1,\n",
    "#     \"path\": {\n",
    "#         \"data_x\": \"/kaggle/input/wimans/wifi_csi/amp\",   \n",
    "#         \"data_y\": \"/kaggle/input/wimans/annotation.csv\", \n",
    "#     },\n",
    "#     \"data\": {\n",
    "#         \"num_users\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"],  \n",
    "#         \"wifi_band\": [\"2.4\"],                         \n",
    "#         \"environment\": [\"classroom\"],                 \n",
    "#         \"length\": 3000,\n",
    "        \n",
    "#         # 1.0 = 100% data (Full run) | 0.1 = 10% data (Quick test)\n",
    "#         \"subset_ratio\": 0.5,  \n",
    "#     },\n",
    "#     \"nn\": {\n",
    "#         \"lr\": 1e-3,           \n",
    "#         \"epoch\": 80,          \n",
    "#         \"batch_size\": 32,    \n",
    "#         \"threshold\": 0.5,\n",
    "#         \"patience\": 5,        \n",
    "#         \"factor\": 0.5,        \n",
    "#         \"min_lr\": 1e-6        \n",
    "#     },\n",
    "#     \"encoding\": {\n",
    "#         \"activity\": {\n",
    "#             \"nan\":      [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#             \"nothing\":  [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#             \"walk\":     [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "#             \"rotation\": [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "#             \"jump\":     [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "#             \"wave\":     [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "#             \"lie_down\": [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "#             \"pick_up\":  [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "#             \"sit_down\": [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "#             \"stand_up\": [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "#         },\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# # --------------------------\n",
    "# # 2. توابع RPCA و لود دیتا\n",
    "# # --------------------------\n",
    "# def soft_threshold(x, epsilon):\n",
    "#     return np.maximum(np.abs(x) - epsilon, 0) * np.sign(x)\n",
    "\n",
    "# def robust_pca(M, max_iter=10, tol=1e-4):\n",
    "#     n1, n2 = M.shape\n",
    "#     lambda_param = 1 / np.sqrt(max(n1, n2))\n",
    "#     Y = M / np.maximum(np.linalg.norm(M, 2), np.linalg.norm(M, np.inf) / lambda_param)\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     mu = 1.25 / np.linalg.norm(M, 2)\n",
    "#     rho = 1.5\n",
    "#     for i in range(max_iter):\n",
    "#         temp_L = M - S + (1/mu) * Y\n",
    "#         U, Sigma, Vt = svd(temp_L, full_matrices=False)\n",
    "#         Sigma_thresh = soft_threshold(Sigma, 1/mu)\n",
    "#         L_new = np.dot(U * Sigma_thresh, Vt)\n",
    "#         temp_S = M - L_new + (1/mu) * Y\n",
    "#         S_new = soft_threshold(temp_S, lambda_param/mu)\n",
    "#         error = np.linalg.norm(M - L_new - S_new, 'fro') / np.linalg.norm(M, 'fro')\n",
    "#         L = L_new; S = S_new\n",
    "#         if error < tol: break\n",
    "#         Y = Y + mu * (M - L - S)\n",
    "#         mu = min(mu * rho, 1e7)\n",
    "#     return L, S\n",
    "\n",
    "# def load_data_y(var_path_data_y, var_environment=None, var_wifi_band=None, var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None: data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None: data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None: data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list, use_rpca=True):\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     mode_str = \"WITH RPCA\" if use_rpca else \"RAW DATA (No RPCA)\"\n",
    "#     print(f\"Loading {len(var_path_list)} samples - Mode: {mode_str}...\")\n",
    "#     for i, var_path in enumerate(var_path_list):\n",
    "#         if i % 100 == 0 and i > 0: print(f\"Processing {i}/{len(var_path_list)}...\")\n",
    "#         data_csi = np.load(var_path) \n",
    "#         data_csi_2d = data_csi.reshape(data_csi.shape[0], -1)\n",
    "#         target_len = preset[\"data\"][\"length\"]\n",
    "#         current_len = data_csi_2d.shape[0]\n",
    "#         var_pad_length = target_len - current_len\n",
    "#         if var_pad_length > 0: data_csi_pad = np.pad(data_csi_2d, ((0, var_pad_length), (0, 0)), mode='constant')\n",
    "#         else: data_csi_pad = data_csi_2d[:target_len, :]\n",
    "#         if use_rpca:\n",
    "#             L, S = robust_pca(data_csi_pad)\n",
    "#             final_sample = np.concatenate([L, S], axis=1) \n",
    "#         else:\n",
    "#             final_sample = data_csi_pad\n",
    "#         data_x.append(final_sample)\n",
    "#     data_x = np.array(data_x)\n",
    "#     return data_x\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"activity\": return encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     return encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     cols = [f\"user_{i}_activity\" for i in range(1, 7)]\n",
    "#     data = data_pd_y[cols].to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[y] for y in sample] for sample in data])\n",
    "\n",
    "# # --------------------------\n",
    "# # 3. مدل THAT\n",
    "# # --------------------------\n",
    "# class Gaussian_Position(torch.nn.Module):\n",
    "#     def __init__(self, var_dim_feature, var_dim_time, var_num_gaussian=10):\n",
    "#         super(Gaussian_Position, self).__init__()\n",
    "#         self.var_embedding = torch.nn.Parameter(torch.zeros([var_num_gaussian, var_dim_feature]), requires_grad=True)\n",
    "#         torch.nn.init.xavier_uniform_(self.var_embedding)\n",
    "#         self.var_position = torch.nn.Parameter(torch.arange(0.0, var_dim_time).unsqueeze(1).repeat(1, var_num_gaussian), requires_grad=False)\n",
    "#         self.var_mu = torch.nn.Parameter(torch.arange(0.0, var_dim_time, var_dim_time/var_num_gaussian).unsqueeze(0), requires_grad=True)\n",
    "#         self.var_sigma = torch.nn.Parameter(torch.tensor([50.0] * var_num_gaussian).unsqueeze(0), requires_grad=True)\n",
    "#     def forward(self, var_input):\n",
    "#         var_pdf = - (self.var_position - self.var_mu)**2 / (2 * self.var_sigma**2) - torch.log(self.var_sigma)\n",
    "#         var_pdf = torch.softmax(var_pdf, dim=-1)\n",
    "#         return var_input + torch.matmul(var_pdf, self.var_embedding).unsqueeze(0)\n",
    "\n",
    "# class Encoder(torch.nn.Module):\n",
    "#     def __init__(self, var_dim_feature, var_num_head=10, var_size_cnn=[1, 3, 5]):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.layer_norm_0 = torch.nn.LayerNorm(var_dim_feature, eps=1e-6)\n",
    "#         self.layer_attention = torch.nn.MultiheadAttention(var_dim_feature, var_num_head, batch_first=True)\n",
    "#         self.layer_dropout_0 = torch.nn.Dropout(0.1)\n",
    "#         self.layer_norm_1 = torch.nn.LayerNorm(var_dim_feature, 1e-6)\n",
    "#         self.layer_cnn = torch.nn.ModuleList([torch.nn.Sequential(torch.nn.Conv1d(var_dim_feature, var_dim_feature, s, padding=\"same\"), torch.nn.BatchNorm1d(var_dim_feature), torch.nn.Dropout(0.1), torch.nn.LeakyReLU()) for s in var_size_cnn])\n",
    "#         self.layer_dropout_1 = torch.nn.Dropout(0.1)\n",
    "#     def forward(self, var_input):\n",
    "#         var_t = self.layer_norm_0(var_input)\n",
    "#         var_t, _ = self.layer_attention(var_t, var_t, var_t)\n",
    "#         var_t = self.layer_dropout_0(var_t) + var_input\n",
    "#         var_s = self.layer_norm_1(var_t).permute(0, 2, 1)\n",
    "#         var_c = torch.stack([l(var_s) for l in self.layer_cnn], dim=0)\n",
    "#         var_s = self.layer_dropout_1((torch.sum(var_c, dim=0) / len(self.layer_cnn)).permute(0, 2, 1))\n",
    "#         return var_s + var_t\n",
    "\n",
    "# class THAT(torch.nn.Module):\n",
    "#     def __init__(self, var_x_shape, var_y_shape):\n",
    "#         super(THAT, self).__init__()\n",
    "#         var_dim_feature, var_dim_time = var_x_shape[-1], var_x_shape[-2]\n",
    "#         var_dim_output = var_y_shape[-1]\n",
    "#         self.layer_left_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "#         self.layer_left_gaussian = Gaussian_Position(var_dim_feature, var_dim_time // 20)\n",
    "#         self.layer_left_encoder = torch.nn.ModuleList([Encoder(var_dim_feature, 10, [1, 3, 5]) for _ in range(4)])\n",
    "#         self.layer_left_norm = torch.nn.LayerNorm(var_dim_feature, eps=1e-6)\n",
    "#         self.layer_left_cnn = torch.nn.ModuleList([torch.nn.Conv1d(var_dim_feature, 128, k) for k in [8, 16]])\n",
    "#         self.layer_left_dropout = torch.nn.Dropout(0.5)\n",
    "#         var_dim_right = var_dim_time // 20\n",
    "#         self.layer_right_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "#         self.layer_right_encoder = torch.nn.ModuleList([Encoder(var_dim_right, 10, [1, 2, 3])])\n",
    "#         self.layer_right_norm = torch.nn.LayerNorm(var_dim_right, eps=1e-6)\n",
    "#         self.layer_right_cnn = torch.nn.ModuleList([torch.nn.Conv1d(var_dim_right, 16, k) for k in [2, 4]])\n",
    "#         self.layer_right_dropout = torch.nn.Dropout(0.5)\n",
    "#         self.layer_leakyrelu = torch.nn.LeakyReLU()\n",
    "#         self.layer_output = torch.nn.Linear(256 + 32, var_dim_output)\n",
    "#     def forward(self, var_input):\n",
    "#         v_l = self.layer_left_gaussian(self.layer_left_pooling(var_input.permute(0, 2, 1)).permute(0, 2, 1))\n",
    "#         for l in self.layer_left_encoder: v_l = l(v_l)\n",
    "#         v_l = self.layer_left_norm(v_l).permute(0, 2, 1)\n",
    "#         v_l = torch.cat([torch.sum(self.layer_leakyrelu(cnn(v_l)), dim=-1) for cnn in self.layer_left_cnn], dim=-1)\n",
    "#         v_l = self.layer_left_dropout(v_l)\n",
    "#         v_r = self.layer_right_pooling(var_input.permute(0, 2, 1))\n",
    "#         for l in self.layer_right_encoder: v_r = l(v_r)\n",
    "#         v_r = self.layer_right_norm(v_r).permute(0, 2, 1)\n",
    "#         v_r = torch.cat([torch.sum(self.layer_leakyrelu(cnn(v_r)), dim=-1) for cnn in self.layer_right_cnn], dim=-1)\n",
    "#         v_r = self.layer_right_dropout(v_r)\n",
    "#         return self.layer_output(torch.cat([v_l, v_r], dim=-1))\n",
    "\n",
    "# # --------------------------\n",
    "# # 4. Training Loop\n",
    "# # --------------------------\n",
    "# def train(model, optimizer, loss_fn, train_loader, test_loader, threshold, epochs, device, model_path):\n",
    "#     best_acc = -1.0\n",
    "#     best_w = deepcopy(model.state_dict())\n",
    "    \n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#         optimizer, mode='max', factor=preset[\"nn\"][\"factor\"], patience=preset[\"nn\"][\"patience\"],\n",
    "#         min_lr=preset[\"nn\"][\"min_lr\"], verbose=True\n",
    "#     )\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         t0 = time.time()\n",
    "#         model.train()\n",
    "        \n",
    "#         # --- [MODIFIED] Using requested variable names ---\n",
    "#         for data_batch_x, data_batch_y in train_loader:\n",
    "#             data_batch_x = data_batch_x.to(device)\n",
    "#             data_batch_y = data_batch_y.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             predict_train_y = model(data_batch_x)\n",
    "            \n",
    "#             # --- [REQUESTED LINE] ---\n",
    "#             loss_value = loss_fn(predict_train_y, data_batch_y.reshape(data_batch_y.shape[0], -1).float())\n",
    "            \n",
    "#             loss_value.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             tx, ty = next(iter(test_loader))\n",
    "#             tx, ty = tx.to(device), ty.to(device)\n",
    "#             pred_t = model(tx)\n",
    "            \n",
    "#             p_cls = (torch.sigmoid(pred_t) > threshold).float().cpu().numpy()\n",
    "#             t_cls = ty.cpu().numpy()\n",
    "#             acc = accuracy_score(t_cls.reshape(-1, t_cls.shape[-1]), p_cls.reshape(-1, t_cls.shape[-1]))\n",
    "            \n",
    "#         scheduler.step(acc)\n",
    "#         current_lr = optimizer.param_groups[0]['lr']\n",
    "#         print(f\"Ep {epoch+1}/{epochs} | LR: {current_lr:.6f} | L_tr: {loss_value.item():.4f} | Acc: {acc:.4f}\")\n",
    "        \n",
    "#         if acc > best_acc:\n",
    "#             best_acc = acc\n",
    "#             best_w = deepcopy(model.state_dict())\n",
    "            \n",
    "#     torch.save(best_w, model_path)\n",
    "#     return best_w\n",
    "\n",
    "# def save_multiclass_confusion_matrix(model, data_loader, device, pdf_path, num_classes, title_text):\n",
    "#     model.eval()\n",
    "#     y_true, y_pred = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in data_loader:\n",
    "#             xb = xb.to(device)\n",
    "#             logits = model(xb) \n",
    "#             logits = logits.reshape(-1, num_classes) \n",
    "#             yb = yb.reshape(-1, num_classes)        \n",
    "#             y_pred.extend(torch.argmax(logits, dim=1).cpu().numpy().tolist())\n",
    "#             y_true.extend(torch.argmax(yb, dim=1).cpu().numpy().tolist())\n",
    "    \n",
    "#     labels = list(range(num_classes))\n",
    "#     cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "#     disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "#     fig, ax = plt.subplots(figsize=(12, 12))\n",
    "#     disp.plot(ax=ax, xticks_rotation=\"vertical\", cmap='Blues')\n",
    "#     ax.set_title(title_text)\n",
    "#     with PdfPages(pdf_path) as pdf: pdf.savefig(fig)\n",
    "#     plt.close(fig)\n",
    "\n",
    "# # --------------------------\n",
    "# # 5. اجرا\n",
    "# # --------------------------\n",
    "# def run_experiment(scenario_name, use_rpca):\n",
    "#     print(f\"\\n################################################\")\n",
    "#     print(f\"STARTING SCENARIO: {scenario_name}\")\n",
    "#     print(f\"################################################\")\n",
    "    \n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     current_run_name = f\"{preset['model']}_{preset['task']}_{scenario_name}\"\n",
    "#     model_save_path = f\"{current_run_name}_best_model.pt\"\n",
    "#     json_save_path = f\"result_{current_run_name}.json\"\n",
    "#     pdf_save_path = f\"Confusion_{current_run_name}.pdf\"\n",
    "    \n",
    "#     # 1. Load Labels\n",
    "#     data_pd_y = load_data_y(preset[\"path\"][\"data_y\"], preset[\"data\"][\"environment\"], preset[\"data\"][\"wifi_band\"], preset[\"data\"][\"num_users\"])\n",
    "    \n",
    "#     # Apply Subset Ratio\n",
    "#     subset_ratio = preset[\"data\"][\"subset_ratio\"]\n",
    "#     if subset_ratio < 1.0:\n",
    "#         data_pd_y = data_pd_y.sample(frac=subset_ratio, random_state=42).reset_index(drop=True)\n",
    "#         print(f\"*** DEBUG MODE: Using {subset_ratio*100}% of data ({len(data_pd_y)} samples) ***\")\n",
    "    \n",
    "#     # 2. Load X\n",
    "#     data_x = load_data_x(preset[\"path\"][\"data_x\"], data_pd_y[\"label\"].tolist(), use_rpca=use_rpca)\n",
    "#     data_y = encode_data_y(data_pd_y, preset[\"task\"])\n",
    "    \n",
    "#     # 3. Split\n",
    "#     train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2, shuffle=True, random_state=39)\n",
    "#     train_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "#     test_ds = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "#     train_loader = DataLoader(train_ds, batch_size=preset[\"nn\"][\"batch_size\"], shuffle=True)\n",
    "#     test_loader = DataLoader(test_ds, batch_size=len(test_ds), shuffle=False)\n",
    "    \n",
    "#     result = {\"accuracy\": []}\n",
    "    \n",
    "#     for r in range(preset[\"repeat\"]):\n",
    "#         print(f\"--- Repeat {r+1}/{preset['repeat']} ---\")\n",
    "#         torch.random.manual_seed(r + 39)\n",
    "        \n",
    "#         model = THAT(train_x[0].shape, train_y[0].reshape(-1).shape).to(device)\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=preset[\"nn\"][\"lr\"])\n",
    "#         loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "#         best_w = train(model, optimizer, loss_fn, train_loader, test_loader, \n",
    "#                        preset[\"nn\"][\"threshold\"], preset[\"nn\"][\"epoch\"], device, model_save_path)\n",
    "        \n",
    "#         model.load_state_dict(best_w)\n",
    "#         with torch.no_grad():\n",
    "#             preds = model(torch.from_numpy(test_x).to(device))\n",
    "#             preds_reshaped = (torch.sigmoid(preds) > preset[\"nn\"][\"threshold\"]).float().cpu().numpy().reshape(-1, 9)\n",
    "#             targets_reshaped = test_y.reshape(-1, 9)\n",
    "#             acc = accuracy_score(targets_reshaped, preds_reshaped)\n",
    "#             result[\"accuracy\"].append(acc)\n",
    "            \n",
    "#     print(f\"Final Accuracy ({scenario_name}): {np.mean(result['accuracy']):.4f}\")\n",
    "#     with open(json_save_path, \"w\") as f: json.dump(result, f, indent=4)\n",
    "    \n",
    "#     print(\"Generating Confusion Matrix...\")\n",
    "#     model_cm = THAT(test_x[0].shape, test_y[0].reshape(-1).shape).to(\"cpu\")\n",
    "#     model_cm.load_state_dict(torch.load(model_save_path, map_location=\"cpu\"))\n",
    "#     cm_loader = DataLoader(TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y)), batch_size=32)\n",
    "#     num_classes = test_y.shape[2] \n",
    "#     title = f\"Confusion Matrix: {scenario_name} (Acc: {np.mean(result['accuracy']):.2f} - {subset_ratio*100}% Data)\"\n",
    "#     save_multiclass_confusion_matrix(model_cm, cm_loader, \"cpu\", pdf_save_path, num_classes, title)\n",
    "    \n",
    "#     del model, model_cm, train_x, test_x, data_x\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     print(f\"Done with {scenario_name}.\\n\")\n",
    "\n",
    "# def run():\n",
    "#     scenarios = [\n",
    "#         (\"RPCA\", True),\n",
    "#         (\"RAW\", False)\n",
    "#     ]\n",
    "#     for name, rpca_flag in scenarios:\n",
    "#         run_experiment(name, rpca_flag)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4416d1ed",
   "metadata": {
    "papermill": {
     "duration": 0.04535,
     "end_time": "2025-12-28T19:03:59.676131",
     "exception": false,
     "start_time": "2025-12-28T19:03:59.630781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4451316,
     "sourceId": 7638081,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7503373,
     "sourceId": 11934698,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7401.980528,
   "end_time": "2025-12-28T19:04:02.452748",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-28T17:00:40.472220",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
