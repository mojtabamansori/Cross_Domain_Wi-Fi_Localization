{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70ee36fd",
   "metadata": {
    "papermill": {
     "duration": 0.007874,
     "end_time": "2025-12-25T22:23:37.518875",
     "exception": false,
     "start_time": "2025-12-25T22:23:37.511001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 1: Library Imports\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a1e8009",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T22:23:37.533087Z",
     "iopub.status.busy": "2025-12-25T22:23:37.532789Z",
     "iopub.status.idle": "2025-12-25T22:23:47.328676Z",
     "shell.execute_reply": "2025-12-25T22:23:47.327918Z"
    },
    "papermill": {
     "duration": 9.804597,
     "end_time": "2025-12-25T22:23:47.330144",
     "exception": false,
     "start_time": "2025-12-25T22:23:37.525547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 1: Library Imports\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as scio\n",
    "import time\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch._dynamo\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# from ptflops import get_model_complexity_info\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from copy import deepcopy\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a5229",
   "metadata": {
    "papermill": {
     "duration": 0.006722,
     "end_time": "2025-12-25T22:23:47.343802",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.337080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 2: preset.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59927645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T22:23:47.357957Z",
     "iopub.status.busy": "2025-12-25T22:23:47.357626Z",
     "iopub.status.idle": "2025-12-25T22:23:47.367319Z",
     "shell.execute_reply": "2025-12-25T22:23:47.366638Z"
    },
    "papermill": {
     "duration": 0.018193,
     "end_time": "2025-12-25T22:23:47.368468",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.350275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "few=empty_room,100,5,m=THAT,t=activity,epoch=1000,batch=64,environment=['classroom']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "[file]          preset.py\n",
    "[description]   default settings of WiFi-based models\n",
    "\"\"\"\n",
    "minidata_set = 1\n",
    "preset = {\n",
    "    # define model\n",
    "    \"model\": \"THAT\",  # \"ST-RF\", \"MLP\", \"LSTM\", \"CNN-1D\", \"CNN-2D\", \"CLSTM\", \"ABLSTM\", \"THAT\", \"bi-LSTM\", \"ResNet18\"\n",
    "    # define task\n",
    "    \"task\": \"activity\",  # \"identity\", \"activity\", \"location\", \"count\"\n",
    "    # number of repeated experiments\n",
    "    \"repeat\": 1,\n",
    "    # path of data\n",
    "    \"path\": {\n",
    "        \"data_x\": \"/kaggle/input/wimans/wifi_csi/mat\",   # directory of CSI amplitude files \n",
    "        # \"data_x\": \"/kaggle/input/wimans/wifi_csi/amp\",   # directory of CSI amplitude files \n",
    "        \"data_y\": \"/kaggle/input/wimans/annotation.csv\", # path of annotation file\n",
    "        \"save\": \"result_lstm_epoch=80_batchsize=32_envs=empty_room_wifiband=2.4.json\"               # path to save results\n",
    "    },\n",
    "    # data selection for experiments\n",
    "    \"data\": {\n",
    "        \"num_users\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"],  # select number(s) of users\n",
    "        \"wifi_band\": [\"2.4\"],                         # select WiFi band(s)\n",
    "        \"environment\": [\"classroom\"],                 # select environment(s) [\"classroom\"], [\"meeting_room\"], [\"empty_room\"]\n",
    "        \"length\": 3000,                               # default length of CSI\n",
    "    },\n",
    "    # hyperparameters of models\n",
    "    \"nn\": {\n",
    "        \"lr\": 5e-4,           # learning rate\n",
    "        \"epoch\": 1000,         # number of epochs\n",
    "        \"batch_size\": 64,    # batch size\n",
    "        \"threshold\": 0.5,     # threshold to binarize sigmoid outputs\n",
    "    },\n",
    "    # encoding of activities and locations\n",
    "    \"encoding\": {\n",
    "        \"activity\": {  # encoding of different activities\n",
    "            \"nan\":      [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"nothing\":  [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"walk\":     [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"rotation\": [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            \"jump\":     [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            \"wave\":     [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "            \"lie_down\": [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "            \"pick_up\":  [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "            \"sit_down\": [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "            \"stand_up\": [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        },\n",
    "        \"location\": {  # encoding of different locations\n",
    "            \"nan\":  [0, 0, 0, 0, 0],\n",
    "            \"a\":    [1, 0, 0, 0, 0],\n",
    "            \"b\":    [0, 1, 0, 0, 0],\n",
    "            \"c\":    [0, 0, 1, 0, 0],\n",
    "            \"d\":    [0, 0, 0, 1, 0],\n",
    "            \"e\":    [0, 0, 0, 0, 1],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Few-shot parameters (manually set)\n",
    "dest_env = \"empty_room\"       # Destination environment[\"classroom\"], [\"meeting_room\"], [\"empty_room\"]\n",
    "few_shot_epochs = 100         # Number of epochs for few-shot training\n",
    "few_shot_num_samples = 5     # Number of samples to use from the destination test data\n",
    "\n",
    "Confusion_matrix = 1\n",
    "\n",
    "name_run = \"few={},{},{},m={},t={},epoch={},batch={},environment={}\".format(dest_env, few_shot_epochs, few_shot_num_samples, preset[\"model\"], preset[\"task\"], preset[\"nn\"][\"epoch\"], preset[\"nn\"][\"batch_size\"], preset[\"data\"][\"environment\"])\n",
    "print(name_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c379e6e",
   "metadata": {
    "papermill": {
     "duration": 0.00652,
     "end_time": "2025-12-25T22:23:47.381907",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.375387",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 3: load_data.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eec89a",
   "metadata": {
    "papermill": {
     "duration": 0.006377,
     "end_time": "2025-12-25T22:23:47.394889",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.388512",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "last stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e24580d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T22:23:47.409253Z",
     "iopub.status.busy": "2025-12-25T22:23:47.409030Z",
     "iopub.status.idle": "2025-12-25T22:23:47.437571Z",
     "shell.execute_reply": "2025-12-25T22:23:47.437012Z"
    },
    "papermill": {
     "duration": 0.037356,
     "end_time": "2025-12-25T22:23:47.438690",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.401334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          load_data.py\n",
    "[description]   load annotation file and CSI (from .mat), and encode labels\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as scio\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from numpy.fft import ifft\n",
    "\n",
    "# =========================================================\n",
    "#  Settings\n",
    "# =========================================================\n",
    "# \"raw\"     : abs(raw complex CSI)  -> float\n",
    "# \"lowrank\" : abs(L) after RPCA     -> float\n",
    "# \"sparse\"  : abs(S) after RPCA     -> float\n",
    "CSI_INPUT_MODE = \"sparse\"   # raw / lowrank / sparse\n",
    "\n",
    "# expected dimensions\n",
    "TX = 3\n",
    "RX = 3\n",
    "SC = 30\n",
    "\n",
    "# optional IFFT (frequency -> delay)\n",
    "USE_IFFT = True\n",
    "\n",
    "# RPCA iterations (برای شروع کم بگذار؛ اگر لازم شد بیشتر کن)\n",
    "RPCA_MAX_ITER = 50\n",
    "\n",
    "# lambda mode (اختیاری)\n",
    "# \"classic\" : 1/sqrt(max(m,n))\n",
    "# \"median\"  : 1/sqrt(max(m,n)) * median(abs(M))\n",
    "# \"scaled\"  : 1.2/sqrt(max(m,n))\n",
    "RPCA_LAMBDA_MODE = \"classic\"\n",
    "\n",
    "# cache (very important for speed)\n",
    "CACHE_ENABLED = True\n",
    "CACHE_ROOT = \"/kaggle/working/csi_cache_mat_pipeline\"\n",
    "\n",
    "# print once to verify input is amplitude or complex/phase\n",
    "_DEBUG_PRINT_ONCE = True\n",
    "# =========================================================\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Minimal R_pca implementation (no external dependency)\n",
    "# Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "# =========================================================\n",
    "class R_pca:\n",
    "    def __init__(self, D):\n",
    "        self.D = np.asarray(D, dtype=np.float64)\n",
    "\n",
    "    @staticmethod\n",
    "    def _shrink(M, tau):\n",
    "        return np.sign(M) * np.maximum(np.abs(M) - tau, 0.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _svt(M, tau):\n",
    "        U, s, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "        s = np.maximum(s - tau, 0.0)\n",
    "        if np.all(s == 0):\n",
    "            return np.zeros_like(M)\n",
    "        return (U * s) @ Vt\n",
    "\n",
    "    def _lambda(self, D):\n",
    "        m, n = D.shape\n",
    "        base = 1.0 / np.sqrt(max(m, n))\n",
    "        if RPCA_LAMBDA_MODE == \"classic\":\n",
    "            return base\n",
    "        if RPCA_LAMBDA_MODE == \"scaled\":\n",
    "            return 1.2 * base\n",
    "        if RPCA_LAMBDA_MODE == \"median\":\n",
    "            med = np.median(np.abs(D))\n",
    "            med = float(med) if med > 1e-12 else 1e-12\n",
    "            return base * med\n",
    "        raise ValueError(f\"Unknown RPCA_LAMBDA_MODE={RPCA_LAMBDA_MODE}\")\n",
    "\n",
    "    def fit(self, max_iter=200, tol=1e-6, rho=1.5, mu=None):\n",
    "        \"\"\"\n",
    "        Returns L, S such that D ≈ L + S\n",
    "        \"\"\"\n",
    "        D = self.D\n",
    "        m, n = D.shape\n",
    "        lam = self._lambda(D)\n",
    "\n",
    "        # auto mu\n",
    "        if mu is None:\n",
    "            s0 = np.linalg.svd(D, compute_uv=False, full_matrices=False)[0] if D.size else 1.0\n",
    "            mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "        L = np.zeros_like(D)\n",
    "        S = np.zeros_like(D)\n",
    "        Y = np.zeros_like(D)\n",
    "\n",
    "        normD = np.linalg.norm(D, ord=\"fro\") + 1e-12\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            L = self._svt(D - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "            S = self._shrink(D - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "            R = D - L - S\n",
    "            Y = Y + mu * R\n",
    "\n",
    "            if (np.linalg.norm(R, ord=\"fro\") / normD) < tol:\n",
    "                break\n",
    "\n",
    "            mu *= rho\n",
    "\n",
    "        return L.astype(np.float64), S.astype(np.float64)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1) Phase calibration (sanitize) - vectorized\n",
    "# --------------------------------------------------\n",
    "def phase_sanitize_matrix(X):\n",
    "    \"\"\"\n",
    "    X: complex matrix (N_subcarriers, T)\n",
    "    \"\"\"\n",
    "    phase = np.unwrap(np.angle(X), axis=0)  # (N,T)\n",
    "    N, T = phase.shape\n",
    "    k = np.arange(N, dtype=np.float64)[:, None]  # (N,1)\n",
    "\n",
    "    A = np.concatenate([k, np.ones((N, 1), dtype=np.float64)], axis=1)  # (N,2)\n",
    "    pinvA = np.linalg.pinv(A)  # (2,N)\n",
    "    coeff = pinvA @ phase      # (2,T)\n",
    "    a = coeff[0:1, :]\n",
    "    b = coeff[1:2, :]\n",
    "\n",
    "    phase_corr = phase - (k @ a + b)\n",
    "    return np.abs(X) * np.exp(1j * phase_corr)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2) Preprocess CSI matrix\n",
    "# --------------------------------------------------\n",
    "def preprocess_csi(X):\n",
    "    \"\"\"\n",
    "    X : complex CSI matrix (N_subcarriers, T)\n",
    "    \"\"\"\n",
    "    X_corr = phase_sanitize_matrix(X).astype(np.complex128, copy=False)\n",
    "    fro = np.linalg.norm(X_corr, \"fro\")\n",
    "    if fro > 0:\n",
    "        X_corr /= fro\n",
    "    return X_corr\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3) Optional IFFT\n",
    "# --------------------------------------------------\n",
    "def csi_to_cir(X):\n",
    "    return ifft(X, axis=0)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4) RPCA on complex matrix (real & imag separately)\n",
    "# --------------------------------------------------\n",
    "def rpca_complex(X, max_iter=200):\n",
    "    Xr = np.real(X)\n",
    "    Xi = np.imag(X)\n",
    "\n",
    "    rpca_r = R_pca(Xr)\n",
    "    Lr, Sr = rpca_r.fit(max_iter=max_iter)\n",
    "\n",
    "    rpca_i = R_pca(Xi)\n",
    "    Li, Si = rpca_i.fit(max_iter=max_iter)\n",
    "\n",
    "    L = Lr + 1j * Li\n",
    "    S = Sr + 1j * Si\n",
    "    return L, S\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5) Full pipeline\n",
    "# --------------------------------------------------\n",
    "def csi_lowrank_sparse_pipeline(X, use_ifft=True, max_iter=200):\n",
    "    Xp = preprocess_csi(X)\n",
    "    if use_ifft:\n",
    "        Xp = csi_to_cir(Xp)\n",
    "    L, S = rpca_complex(Xp, max_iter=max_iter)\n",
    "    return L, S\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# MAT loader -> (T,3,3,30) complex\n",
    "# --------------------------------------------------\n",
    "def load_csi_from_mat(mat_path):\n",
    "    m = scio.loadmat(mat_path)\n",
    "    if \"trace\" not in m:\n",
    "        raise KeyError(f\"'trace' not found in {mat_path}\")\n",
    "\n",
    "    trace = m[\"trace\"]  # (T,1) object array\n",
    "    T = trace.shape[0]\n",
    "    out = np.empty((T, TX, RX, SC), dtype=np.complex128)\n",
    "\n",
    "    for t in range(T):\n",
    "        out[t] = trace[t, 0][\"csi\"][0, 0]  # (3,3,30) complex\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _debug_print_once(label, csi_4d):\n",
    "    global _DEBUG_PRINT_ONCE\n",
    "    if not _DEBUG_PRINT_ONCE:\n",
    "        return\n",
    "    _DEBUG_PRINT_ONCE = False\n",
    "\n",
    "    is_cplx = np.iscomplexobj(csi_4d)\n",
    "    print(f\"\\n[DEBUG] First MAT sample: {label}\")\n",
    "    print(f\"[DEBUG] shape={csi_4d.shape}, dtype={csi_4d.dtype}, complex={is_cplx}\")\n",
    "\n",
    "    x0 = csi_4d[0, 0, 0, :]\n",
    "    if is_cplx:\n",
    "        ang = np.angle(x0)\n",
    "        print(f\"[DEBUG] abs range:  min={np.min(np.abs(x0)):.6f}, max={np.max(np.abs(x0)):.6f}\")\n",
    "        print(f\"[DEBUG] phase stats: mean={np.mean(ang):.6f}, std={np.std(ang):.6f}\")\n",
    "        print(\"[DEBUG] ==> Input is COMPLEX (phase exists).\")\n",
    "    else:\n",
    "        print(f\"[DEBUG] value range: min={np.min(x0):.6f}, max={np.max(x0):.6f}\")\n",
    "        print(\"[DEBUG] ==> Input is REAL (likely amplitude-only).\")\n",
    "\n",
    "\n",
    "def _cache_path(label, mode):\n",
    "    return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "def _apply_pipeline_pairwise_9links(csi_4d_complex, label):\n",
    "    \"\"\"\n",
    "    csi_4d_complex: (T,3,3,30) complex\n",
    "    Run pipeline on each link separately: (30,T) -> RPCA -> abs -> back to (T,3,3,30) float32\n",
    "    \"\"\"\n",
    "    _debug_print_once(label, csi_4d_complex)\n",
    "\n",
    "    if CSI_INPUT_MODE == \"raw\":\n",
    "        return np.abs(csi_4d_complex).astype(np.float32)\n",
    "\n",
    "    mode = CSI_INPUT_MODE  # lowrank / sparse\n",
    "\n",
    "    if CACHE_ENABLED:\n",
    "        os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "        p = _cache_path(label, mode)\n",
    "        if os.path.exists(p):\n",
    "            return np.load(p).astype(np.float32, copy=False)\n",
    "\n",
    "    T = csi_4d_complex.shape[0]\n",
    "    out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "    for tx in range(TX):\n",
    "        for rx in range(RX):\n",
    "            X = csi_4d_complex[:, tx, rx, :].T  # (30,T) complex\n",
    "            L, S = csi_lowrank_sparse_pipeline(X, use_ifft=USE_IFFT, max_iter=RPCA_MAX_ITER)\n",
    "            Y = L if mode == \"lowrank\" else S\n",
    "            out[:, tx, rx, :] = np.abs(Y.T).astype(np.float32)\n",
    "\n",
    "    if CACHE_ENABLED:\n",
    "        np.save(p, out)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Existing label loading/encoding\n",
    "# --------------------------------------------------\n",
    "def load_data_y(var_path_data_y,\n",
    "                var_environment=None,\n",
    "                var_wifi_band=None,\n",
    "                var_num_users=None):\n",
    "    data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "    if var_environment is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "    if var_wifi_band is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "    if var_num_users is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "    return data_pd_y\n",
    "\n",
    "\n",
    "def load_data_x(var_path_data_x, var_label_list):\n",
    "    \"\"\"\n",
    "    var_path_data_x should point to MAT directory, e.g. /kaggle/input/wimans/wifi_csi/mat\n",
    "    Each label corresponds to <label>.mat\n",
    "    \"\"\"\n",
    "    data_x = []\n",
    "    target_len = preset[\"data\"][\"length\"]\n",
    "\n",
    "    for label in var_label_list:\n",
    "        mat_path = os.path.join(var_path_data_x, label + \".mat\")\n",
    "        if not os.path.exists(mat_path):\n",
    "            raise FileNotFoundError(f\"MAT file not found: {mat_path}\")\n",
    "\n",
    "        csi_complex = load_csi_from_mat(mat_path)  # (T,3,3,30) complex\n",
    "\n",
    "        # if longer than target_len, keep last target_len frames\n",
    "        if csi_complex.shape[0] > target_len:\n",
    "            csi_complex = csi_complex[-target_len:, :, :, :]\n",
    "\n",
    "        csi_feat = _apply_pipeline_pairwise_9links(csi_complex, label)\n",
    "\n",
    "        # pad if shorter\n",
    "        pad_len = target_len - csi_feat.shape[0]\n",
    "        if pad_len > 0:\n",
    "            csi_feat = np.pad(csi_feat, ((pad_len, 0), (0, 0), (0, 0), (0, 0)))\n",
    "\n",
    "        data_x.append(csi_feat)\n",
    "\n",
    "    return np.array(data_x)\n",
    "\n",
    "\n",
    "def encode_data_y(data_pd_y, var_task):\n",
    "    if var_task == \"identity\":\n",
    "        data_y = encode_identity(data_pd_y)\n",
    "    elif var_task == \"activity\":\n",
    "        data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "    elif var_task == \"location\":\n",
    "        data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "    elif var_task == \"count\":\n",
    "        data_y = encode_count(data_pd_y)\n",
    "    return data_y\n",
    "\n",
    "\n",
    "def encode_identity(data_pd_y):\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "                                    \"user_3_location\", \"user_4_location\",\n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "    data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "    data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "    return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "def encode_activity(data_pd_y, var_encoding):\n",
    "    data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "                                    \"user_3_activity\", \"user_4_activity\",\n",
    "                                    \"user_5_activity\", \"user_6_activity\"]]\n",
    "    data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "    return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "def encode_location(data_pd_y, var_encoding):\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "                                    \"user_3_location\", \"user_4_location\",\n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "    return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "def encode_count(data_pd_y):\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "                                    \"user_3_location\", \"user_4_location\",\n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "    data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "    data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "    data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "    count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0464939",
   "metadata": {
    "papermill": {
     "duration": 0.006553,
     "end_time": "2025-12-25T22:23:47.451824",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.445271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "rpca 30 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48f237a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T22:23:47.466246Z",
     "iopub.status.busy": "2025-12-25T22:23:47.466050Z",
     "iopub.status.idle": "2025-12-25T22:23:47.472746Z",
     "shell.execute_reply": "2025-12-25T22:23:47.472105Z"
    },
    "papermill": {
     "duration": 0.015283,
     "end_time": "2025-12-25T22:23:47.473845",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.458562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # =========================================================\n",
    "# # انتخاب نوع دیتای ورودی مدل:\n",
    "# # \"raw\"     : amp خام\n",
    "# # \"lowrank\" : خروجی Low-rank (L) از RPCA\n",
    "# # \"sparse\"  : خروجی Sparse (S) از RPCA\n",
    "# CSI_INPUT_MODE = \"lowrank\"   # <-- raw / lowrank / sparse\n",
    "\n",
    "# # ابعاد مورد انتظار CSI\n",
    "# TX = 3\n",
    "# RX = 3\n",
    "# SC = 30\n",
    "\n",
    "# # RPCA (IALM) تنظیمات سریع‌تر\n",
    "# RPCA_MAX_ITER = 60\n",
    "# RPCA_TOL      = 1e-5\n",
    "# RPCA_RHO      = 1.5\n",
    "# RPCA_MU_INIT  = None\n",
    "\n",
    "# # ✅ انتخاب لامبدا:\n",
    "# # \"median\" : lam = 1/sqrt(max(m,n)) * median(abs(M))\n",
    "# # \"scaled\" : lam = 1.2/sqrt(max(m,n))\n",
    "# RPCA_LAMBDA_MODE = \"median\"  # <-- \"median\" یا \"scaled\"\n",
    "\n",
    "# # Cache (خیلی مهم برای سرعت)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT    = \"/kaggle/working/csi_cache_pairwise\"\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _soft_threshold(X, tau):\n",
    "#     return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "# def _svt(X, tau):\n",
    "#     # Singular Value Thresholding\n",
    "#     U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "#     s = np.maximum(s - tau, 0.0)\n",
    "#     if np.all(s == 0):\n",
    "#         return np.zeros_like(X)\n",
    "#     return (U * s) @ Vt\n",
    "\n",
    "# def _compute_lambda(M, mode):\n",
    "#     \"\"\"\n",
    "#     mode:\n",
    "#       - 'median': 1/sqrt(max(m,n)) * median(abs(M))\n",
    "#       - 'scaled': 1.2/sqrt(max(m,n))\n",
    "#     \"\"\"\n",
    "#     m, n = M.shape\n",
    "#     base = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "#     if mode == \"median\":\n",
    "#         med = np.median(np.abs(M))\n",
    "#         # اگر med خیلی کوچک بود برای پایداری:\n",
    "#         med = float(med) if med > 1e-12 else 1e-12\n",
    "#         return base * med\n",
    "\n",
    "#     if mode == \"scaled\":\n",
    "#         return 1.2 * base\n",
    "\n",
    "#     raise ValueError(f\"Unknown RPCA_LAMBDA_MODE: {mode}. Use 'median' or 'scaled'.\")\n",
    "\n",
    "\n",
    "# def _rpca_ialm(M, mu=None, rho=1.5, max_iter=60, tol=1e-5):\n",
    "#     \"\"\"\n",
    "#     Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "#     Decompose: M = L + S\n",
    "#     M shape: (T, SC) = (3000, 30)\n",
    "#     \"\"\"\n",
    "#     M = M.astype(np.float64, copy=False)\n",
    "#     m, n = M.shape\n",
    "\n",
    "#     # ✅ lambda طبق انتخاب کاربر\n",
    "#     lam = _compute_lambda(M, RPCA_LAMBDA_MODE)\n",
    "\n",
    "#     # mu خودکار یا دستی\n",
    "#     if mu is None:\n",
    "#         s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "#         mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     Y = np.zeros_like(M)\n",
    "\n",
    "#     normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#         S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#         R = M - L - S\n",
    "#         Y = Y + mu * R\n",
    "\n",
    "#         if (np.linalg.norm(R, ord=\"fro\") / normM) < tol:\n",
    "#             break\n",
    "\n",
    "#         mu *= rho\n",
    "\n",
    "#     return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# def _ensure_shape_4d(data_csi):\n",
    "#     \"\"\"\n",
    "#     Ensure CSI shape is (T, TX, RX, SC).\n",
    "#     If input is (T, 270) we reshape to (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     if data_csi.ndim == 4:\n",
    "#         return data_csi\n",
    "\n",
    "#     if data_csi.ndim == 2 and data_csi.shape[1] == TX * RX * SC:\n",
    "#         T = data_csi.shape[0]\n",
    "#         return data_csi.reshape(T, TX, RX, SC)\n",
    "\n",
    "#     raise ValueError(\n",
    "#         f\"Unexpected CSI shape {data_csi.shape}. Expected (T,{TX},{RX},{SC}) or (T,{TX*RX*SC}).\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _rpca_pairwise_9links(data_csi_4d):\n",
    "#     \"\"\"\n",
    "#     data_csi_4d: (T, TX, RX, SC)\n",
    "#     Run RPCA on each (tx,rx) separately on matrix (T, SC) and reassemble.\n",
    "#     Output shape stays (T, TX, RX, SC).\n",
    "#     \"\"\"\n",
    "#     T = data_csi_4d.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "#     for tx in range(TX):\n",
    "#         for rx in range(RX):\n",
    "#             M = data_csi_4d[:, tx, rx, :]  # (T,30)\n",
    "#             L, S = _rpca_ialm(\n",
    "#                 M,\n",
    "#                 mu=RPCA_MU_INIT,\n",
    "#                 rho=RPCA_RHO,\n",
    "#                 max_iter=RPCA_MAX_ITER,\n",
    "#                 tol=RPCA_TOL\n",
    "#             )\n",
    "#             out[:, tx, rx, :] = L if CSI_INPUT_MODE == \"lowrank\" else S\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def _apply_mode_with_cache(data_csi, label):\n",
    "#     \"\"\"\n",
    "#     Apply raw/lowrank/sparse. For lowrank/sparse use pairwise RPCA (9 links of 3000x30).\n",
    "#     Keeps the original 4D shape (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     data_csi = _ensure_shape_4d(np.asarray(data_csi, dtype=np.float32))\n",
    "\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     mode = CSI_INPUT_MODE\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             cached = np.load(p)\n",
    "#             return _ensure_shape_4d(cached).astype(np.float32, copy=False)\n",
    "\n",
    "#     out = _rpca_pairwise_9links(data_csi)\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out.astype(np.float32))\n",
    "\n",
    "#     return out.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None,\n",
    "#                 var_wifi_band=None,\n",
    "#                 var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ RPCA روی 9 لینک جدا (3000x30) و بعد اتصال\n",
    "#         data_csi = _apply_mode_with_cache(data_csi, var_label)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "#                                     \"user_3_activity\", \"user_4_activity\",\n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec27bfd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T22:23:47.488423Z",
     "iopub.status.busy": "2025-12-25T22:23:47.488192Z",
     "iopub.status.idle": "2025-12-25T22:23:47.494901Z",
     "shell.execute_reply": "2025-12-25T22:23:47.494337Z"
    },
    "papermill": {
     "duration": 0.015333,
     "end_time": "2025-12-25T22:23:47.495932",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.480599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # =========================================================\n",
    "# # انتخاب نوع دیتای ورودی مدل:\n",
    "# # \"raw\"     : amp خام\n",
    "# # \"lowrank\" : خروجی Low-rank (L) از RPCA\n",
    "# # \"sparse\"  : خروجی Sparse (S) از RPCA\n",
    "# CSI_INPUT_MODE = \"sparse\"   # <-- raw / lowrank / sparse\n",
    "\n",
    "# # ابعاد مورد انتظار CSI\n",
    "# TX = 3\n",
    "# RX = 3\n",
    "# SC = 30\n",
    "\n",
    "# # RPCA (IALM) تنظیمات سریع‌تر\n",
    "# RPCA_MAX_ITER = 60\n",
    "# RPCA_TOL      = 1e-5\n",
    "# RPCA_RHO      = 1.5\n",
    "# RPCA_MU_INIT  = None\n",
    "# RPCA_LAMBDA   = None     # None -> 1/sqrt(max(m,n))\n",
    "\n",
    "# # Cache (خیلی مهم برای سرعت)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT    = \"/kaggle/working/csi_cache_pairwise\"  # خروجی‌ها اینجا ذخیره میشن\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _soft_threshold(X, tau):\n",
    "#     return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "# def _svt(X, tau):\n",
    "#     # Singular Value Thresholding\n",
    "#     U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "#     s = np.maximum(s - tau, 0.0)\n",
    "#     if np.all(s == 0):\n",
    "#         return np.zeros_like(X)\n",
    "#     return (U * s) @ Vt\n",
    "\n",
    "# def _rpca_ialm(M, lam=None, mu=None, rho=1.5, max_iter=60, tol=1e-5):\n",
    "#     \"\"\"\n",
    "#     Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "#     Decompose: M = L + S\n",
    "#     M shape: (T, SC) = (3000, 30)\n",
    "#     \"\"\"\n",
    "#     M = M.astype(np.float64, copy=False)\n",
    "#     m, n = M.shape\n",
    "\n",
    "#     if lam is None:\n",
    "#         lam = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "#     if mu is None:\n",
    "#         # top singular value as spectral norm approximation\n",
    "#         s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "#         mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     Y = np.zeros_like(M)\n",
    "\n",
    "#     normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#         S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#         R = M - L - S\n",
    "#         Y = Y + mu * R\n",
    "\n",
    "#         if (np.linalg.norm(R, ord=\"fro\") / normM) < tol:\n",
    "#             break\n",
    "\n",
    "#         mu *= rho\n",
    "\n",
    "#     return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# def _ensure_shape_4d(data_csi):\n",
    "#     \"\"\"\n",
    "#     Ensure CSI shape is (T, TX, RX, SC).\n",
    "#     If input is (T, 270) we reshape to (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     if data_csi.ndim == 4:\n",
    "#         return data_csi\n",
    "\n",
    "#     if data_csi.ndim == 2 and data_csi.shape[1] == TX * RX * SC:\n",
    "#         T = data_csi.shape[0]\n",
    "#         return data_csi.reshape(T, TX, RX, SC)\n",
    "\n",
    "#     raise ValueError(\n",
    "#         f\"Unexpected CSI shape {data_csi.shape}. Expected (T,{TX},{RX},{SC}) or (T,{TX*RX*SC}).\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     # mode: \"lowrank\" or \"sparse\"\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _rpca_pairwise_9links(data_csi_4d):\n",
    "#     \"\"\"\n",
    "#     data_csi_4d: (T, TX, RX, SC)\n",
    "#     Run RPCA on each (tx,rx) separately on matrix (T, SC) and reassemble.\n",
    "#     Output shape stays (T, TX, RX, SC).\n",
    "#     \"\"\"\n",
    "#     T = data_csi_4d.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "#     # 9 times RPCA: for each tx-rx link\n",
    "#     for tx in range(TX):\n",
    "#         for rx in range(RX):\n",
    "#             M = data_csi_4d[:, tx, rx, :]  # (T, SC) => (3000,30)\n",
    "#             L, S = _rpca_ialm(\n",
    "#                 M,\n",
    "#                 lam=RPCA_LAMBDA,\n",
    "#                 mu=RPCA_MU_INIT,\n",
    "#                 rho=RPCA_RHO,\n",
    "#                 max_iter=RPCA_MAX_ITER,\n",
    "#                 tol=RPCA_TOL\n",
    "#             )\n",
    "#             if CSI_INPUT_MODE == \"lowrank\":\n",
    "#                 out[:, tx, rx, :] = L\n",
    "#             else:  # \"sparse\"\n",
    "#                 out[:, tx, rx, :] = S\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def _apply_mode_with_cache(data_csi, label):\n",
    "#     \"\"\"\n",
    "#     Apply raw/lowrank/sparse. For lowrank/sparse use pairwise RPCA (9 links of 3000x30).\n",
    "#     Keeps the original 4D shape (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     data_csi = _ensure_shape_4d(np.asarray(data_csi, dtype=np.float32))\n",
    "\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     mode = CSI_INPUT_MODE\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             cached = np.load(p)\n",
    "#             return _ensure_shape_4d(cached).astype(np.float32, copy=False)\n",
    "\n",
    "#     out = _rpca_pairwise_9links(data_csi)\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out.astype(np.float32))\n",
    "\n",
    "#     return out.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     \"\"\"\n",
    "#     Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "#     \"\"\"\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     Load CSI amplitude (*.npy) files based on a label list.\n",
    "#     \"\"\"\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ NEW: RPCA روی 9 لینک جداگانه (3000x30) و بعد چسباندن\n",
    "#         data_csi = _apply_mode_with_cache(data_csi, var_label)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     \"\"\"\n",
    "#     Encode labels according to specific task.\n",
    "#     \"\"\"\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     return data_identity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for activity labels.\n",
    "#     \"\"\"\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "#                                     \"user_3_activity\", \"user_4_activity\", \n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "#     return data_activity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for location labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "#     return data_location_onehot_y\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1)\n",
    "#     count_data = count_data.reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     count_data_onehot = encoder.fit_transform(count_data).astype(\"int8\")\n",
    "#     return count_data_onehot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dca3fd",
   "metadata": {
    "papermill": {
     "duration": 0.006483,
     "end_time": "2025-12-25T22:23:47.509436",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.502953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "rpca\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9907cbce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T22:23:47.524737Z",
     "iopub.status.busy": "2025-12-25T22:23:47.524460Z",
     "iopub.status.idle": "2025-12-25T22:23:47.531011Z",
     "shell.execute_reply": "2025-12-25T22:23:47.530366Z"
    },
    "papermill": {
     "duration": 0.015896,
     "end_time": "2025-12-25T22:23:47.532119",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.516223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # =========================================================\n",
    "# # انتخاب ورودی مدل:\n",
    "# # \"raw\"     : همون amp خام\n",
    "# # \"lowrank\" : مؤلفه Low-rank از RPCA (L)\n",
    "# # \"sparse\"  : مؤلفه Sparse از RPCA (S)\n",
    "# CSI_INPUT_MODE = \"lowrank\"   # <-- raw / lowrank / sparse\n",
    "\n",
    "# # RPCA (IALM) تنظیمات سریع‌تر\n",
    "# RPCA_MAX_ITER = 80\n",
    "# RPCA_TOL      = 1e-5\n",
    "# RPCA_RHO      = 1.5\n",
    "# RPCA_MU_INIT  = None\n",
    "# RPCA_LAMBDA   = None     # None -> 1/sqrt(max(m,n))\n",
    "\n",
    "# # Cache (خیلی مهم برای سرعت)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT    = \"/kaggle/working/csi_cache\"  # خروجی‌ها اینجا ذخیره میشن\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _soft_threshold(X, tau):\n",
    "#     return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "# def _svt(X, tau):\n",
    "#     U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "#     s = np.maximum(s - tau, 0.0)\n",
    "#     if np.all(s == 0):\n",
    "#         return np.zeros_like(X)\n",
    "#     return (U * s) @ Vt\n",
    "\n",
    "# def _rpca_ialm(M, lam=None, mu=None, rho=1.5, max_iter=80, tol=1e-5):\n",
    "#     M = M.astype(np.float64, copy=False)\n",
    "#     m, n = M.shape\n",
    "\n",
    "#     if lam is None:\n",
    "#         lam = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "#     if mu is None:\n",
    "#         s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "#         mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     Y = np.zeros_like(M)\n",
    "\n",
    "#     normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#         S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#         R = M - L - S\n",
    "#         Y = Y + mu * R\n",
    "\n",
    "#         if (np.linalg.norm(R, ord=\"fro\") / normM) < tol:\n",
    "#             break\n",
    "#         mu *= rho\n",
    "\n",
    "#     return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# def _rpca_keep_shape(X):\n",
    "#     \"\"\"RPCA روی (T,F) و بازگرداندن دقیقاً به shape اولیه\"\"\"\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "#     if X.ndim == 1:\n",
    "#         M = X[:, None]\n",
    "#         L, S = _rpca_ialm(M, RPCA_LAMBDA, RPCA_MU_INIT, RPCA_RHO, RPCA_MAX_ITER, RPCA_TOL)\n",
    "#         return L[:, 0], S[:, 0]\n",
    "\n",
    "#     T = X.shape[0]\n",
    "#     F = int(np.prod(X.shape[1:]))\n",
    "#     M = X.reshape(T, F)\n",
    "\n",
    "#     L, S = _rpca_ialm(M, RPCA_LAMBDA, RPCA_MU_INIT, RPCA_RHO, RPCA_MAX_ITER, RPCA_TOL)\n",
    "#     return L.reshape(X.shape), S.reshape(X.shape)\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     # mode: \"lowrank\" or \"sparse\"\n",
    "#     # فایل خروجی: /kaggle/working/csi_cache/lowrank/<label>.npy\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _apply_mode_with_cache(data_csi, label):\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     # lowrank یا sparse\n",
    "#     mode = CSI_INPUT_MODE\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             return np.load(p).astype(np.float32, copy=False)\n",
    "\n",
    "#     L, S = _rpca_keep_shape(data_csi)\n",
    "#     out = L if mode == \"lowrank\" else S\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out.astype(np.float32))\n",
    "\n",
    "#     return out.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ RPCA lowrank/sparse بدون تغییر shape + با cache\n",
    "#         data_csi = _apply_mode_with_cache(data_csi, var_label)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "#                                     \"user_3_activity\", \"user_4_activity\",\n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a447b1",
   "metadata": {
    "papermill": {
     "duration": 0.007304,
     "end_time": "2025-12-25T22:23:47.546665",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.539361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "svd\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99b9ecab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T22:23:47.562412Z",
     "iopub.status.busy": "2025-12-25T22:23:47.562216Z",
     "iopub.status.idle": "2025-12-25T22:23:47.570091Z",
     "shell.execute_reply": "2025-12-25T22:23:47.569312Z"
    },
    "papermill": {
     "duration": 0.017251,
     "end_time": "2025-12-25T22:23:47.571248",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.553997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # Note: All necessary libraries (os, numpy, pandas, etc.) are imported in Cell 1.\n",
    "# # from preset import preset   --> preset is already defined in Cell 2.\n",
    "\n",
    "# # =========================================================\n",
    "# # 🔧 NEW: Choose CSI representation mode here (ONLY EDIT THIS)\n",
    "# # ---------------------------------------------------------\n",
    "# # \"raw\"     : use original CSI amplitude as-is\n",
    "# # \"lowrank\" : use low-rank approximation (SVD)\n",
    "# # \"sparse\"  : keep only large-magnitude entries (dense array with many zeros)\n",
    "# CSI_INPUT_MODE = \"sparse\"     # <-- set to: \"raw\" / \"lowrank\" / \"sparse\"\n",
    "\n",
    "# # Low-rank settings\n",
    "# LOW_RANK_ENERGY = 0.95     # keep enough singular values to preserve this energy\n",
    "# LOW_RANK_RANK   = None     # if set to an int (e.g., 10), it overrides ENERGY\n",
    "\n",
    "# # Sparse settings\n",
    "# SPARSE_KEEP_RATIO = 0.10   # keep top 10% magnitudes (globally per sample)\n",
    "# SPARSE_MIN_ABS    = None   # if set (e.g., 0.5), keeps |x|>=threshold instead of keep_ratio\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _low_rank_approx_keep_shape(X, rank=None, energy=0.95):\n",
    "#     \"\"\"\n",
    "#     Low-rank approximation using SVD while preserving the original shape.\n",
    "#     Works for 1D/2D/ND by flattening all non-time dims into features.\n",
    "#     Assumes first axis is time.\n",
    "#     \"\"\"\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "\n",
    "#     if X.ndim == 1:\n",
    "#         M = X[:, None]  # (T,1)\n",
    "#         U, S, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "#         if rank is None:\n",
    "#             s2 = S**2\n",
    "#             cum = np.cumsum(s2) / (np.sum(s2) + 1e-12)\n",
    "#             rank = int(np.searchsorted(cum, energy) + 1)\n",
    "#         rank = max(1, min(rank, S.shape[0]))\n",
    "#         M_lr = (U[:, :rank] * S[:rank]) @ Vt[:rank, :]\n",
    "#         return M_lr[:, 0].astype(np.float32)\n",
    "\n",
    "#     # ND: reshape to (T, F)\n",
    "#     T = X.shape[0]\n",
    "#     F = int(np.prod(X.shape[1:]))\n",
    "#     M = X.reshape(T, F)\n",
    "\n",
    "#     U, S, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "\n",
    "#     if rank is None:\n",
    "#         s2 = S**2\n",
    "#         cum = np.cumsum(s2) / (np.sum(s2) + 1e-12)\n",
    "#         rank = int(np.searchsorted(cum, energy) + 1)\n",
    "\n",
    "#     rank = max(1, min(rank, S.shape[0]))\n",
    "#     M_lr = (U[:, :rank] * S[:rank]) @ Vt[:rank, :]\n",
    "\n",
    "#     return M_lr.reshape(X.shape).astype(np.float32)\n",
    "\n",
    "\n",
    "# def _to_sparse_dense_keep_shape(X, keep_ratio=0.10, min_abs=None):\n",
    "#     \"\"\"\n",
    "#     Makes X sparse-in-content (many zeros) but keeps it as a dense numpy array\n",
    "#     so the rest of the pipeline (np.save/np.load/pad/model) doesn't change.\n",
    "#     Keeps the same shape.\n",
    "#     \"\"\"\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "#     flat = X.ravel()\n",
    "#     if flat.size == 0:\n",
    "#         return X.astype(np.float32)\n",
    "\n",
    "#     absflat = np.abs(flat)\n",
    "\n",
    "#     if min_abs is not None:\n",
    "#         thr = float(min_abs)\n",
    "#         mask = absflat >= thr\n",
    "#     else:\n",
    "#         k = int(np.ceil(keep_ratio * flat.size))\n",
    "#         k = max(1, min(k, flat.size))\n",
    "#         if k == flat.size:\n",
    "#             mask = np.ones_like(absflat, dtype=bool)\n",
    "#         else:\n",
    "#             thr = np.partition(absflat, -k)[-k]\n",
    "#             mask = absflat >= thr\n",
    "\n",
    "#     out = np.zeros_like(flat, dtype=np.float32)\n",
    "#     out[mask] = flat[mask]\n",
    "#     return out.reshape(X.shape).astype(np.float32)\n",
    "\n",
    "\n",
    "# def _apply_csi_mode(data_csi):\n",
    "#     \"\"\"\n",
    "#     Apply selected CSI_INPUT_MODE to a single sample array.\n",
    "#     \"\"\"\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     elif CSI_INPUT_MODE == \"lowrank\":\n",
    "#         return _low_rank_approx_keep_shape(\n",
    "#             data_csi,\n",
    "#             rank=LOW_RANK_RANK,\n",
    "#             energy=LOW_RANK_ENERGY\n",
    "#         )\n",
    "\n",
    "#     elif CSI_INPUT_MODE == \"sparse\":\n",
    "#         return _to_sparse_dense_keep_shape(\n",
    "#             data_csi,\n",
    "#             keep_ratio=SPARSE_KEEP_RATIO,\n",
    "#             min_abs=SPARSE_MIN_ABS\n",
    "#         )\n",
    "\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown CSI_INPUT_MODE: {CSI_INPUT_MODE}. Use 'raw', 'lowrank', or 'sparse'.\")\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     \"\"\"\n",
    "#     Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "#     \"\"\"\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     Load CSI amplitude (*.npy) files based on a label list.\n",
    "#     \"\"\"\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_path in var_path_list:\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ NEW: convert input CSI according to selected mode (raw/lowrank/sparse)\n",
    "#         data_csi = _apply_csi_mode(data_csi)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "#     data_x = np.array(data_x)\n",
    "#     return data_x\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     \"\"\"\n",
    "#     Encode labels according to specific task.\n",
    "#     \"\"\"\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     return data_identity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for activity labels.\n",
    "#     \"\"\"\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "#                                     \"user_3_activity\", \"user_4_activity\", \n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "#     return data_activity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for location labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "#     return data_location_onehot_y\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     print(\"data_identity_onehot_y\",data_identity_onehot_y.shape)\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1)\n",
    "#     print(\"count_data\",count_data.shape)\n",
    "#     count_data = count_data.reshape(-1, 1)  # shape = (11286, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)  \n",
    "#     count_data_onehot = encoder.fit_transform(count_data)\n",
    "#     print(count_data_onehot.shape)  \n",
    "#     count_data_onehot = count_data_onehot.astype(\"int8\")\n",
    "\n",
    "#     return count_data_onehot\n",
    "\n",
    "\n",
    "# # Test functions (optional)\n",
    "# def test_load_data_y():\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"classroom\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=[\"1\", \"2\", \"3\"]).describe())\n",
    "\n",
    "# def test_load_data_x():\n",
    "#     data_pd_y = load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=None)\n",
    "#     var_label_list = data_pd_y[\"label\"].to_list()\n",
    "#     data_x = load_data_x(preset[\"path\"][\"data_x\"], var_label_list)\n",
    "#     print(data_x.shape)\n",
    "\n",
    "# def test_encode_identity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_identity_onehot_y = encode_identity(data_pd_y)\n",
    "#     print(data_identity_onehot_y.shape)\n",
    "#     print(data_identity_onehot_y[2000])\n",
    "\n",
    "# def test_encode_activity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_activity_onehot_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     print(data_activity_onehot_y.shape)\n",
    "#     print(data_activity_onehot_y[1560])\n",
    "\n",
    "# def test_encode_location():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_location_onehot_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     print(data_location_onehot_y.shape)\n",
    "#     print(data_location_onehot_y[1560])\n",
    "\n",
    "# def test_encode_count():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_count_onehot_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     print(data_count_onehot_y.shape)\n",
    "#     print(data_count_onehot_y[20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e682eadb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T22:23:47.587116Z",
     "iopub.status.busy": "2025-12-25T22:23:47.586922Z",
     "iopub.status.idle": "2025-12-25T22:23:47.593093Z",
     "shell.execute_reply": "2025-12-25T22:23:47.592168Z"
    },
    "papermill": {
     "duration": 0.015875,
     "end_time": "2025-12-25T22:23:47.594226",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.578351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # Note: All necessary libraries (os, numpy, pandas, etc.) are imported in Cell 1.\n",
    "# # from preset import preset   --> preset is already defined in Cell 2.\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     \"\"\"\n",
    "#     Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "#     \"\"\"\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     Load CSI amplitude (*.npy) files based on a label list.\n",
    "#     \"\"\"\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_path in var_path_list:\n",
    "#         data_csi = np.load(var_path)\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "#     data_x = np.array(data_x)\n",
    "#     return data_x\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     \"\"\"\n",
    "#     Encode labels according to specific task.\n",
    "#     \"\"\"\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     return data_identity_onehot_y\n",
    "\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for activity labels.\n",
    "#     \"\"\"\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "#                                     \"user_3_activity\", \"user_4_activity\", \n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "#     return data_activity_onehot_y\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for location labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "#     return data_location_onehot_y\n",
    "\n",
    "# # Test functions (optional)\n",
    "# def test_load_data_y():\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"classroom\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=[\"1\", \"2\", \"3\"]).describe())\n",
    "\n",
    "# def test_load_data_x():\n",
    "#     data_pd_y = load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=None)\n",
    "#     var_label_list = data_pd_y[\"label\"].to_list()\n",
    "#     data_x = load_data_x(preset[\"path\"][\"data_x\"], var_label_list)\n",
    "#     print(data_x.shape)\n",
    "\n",
    "# def test_encode_identity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_identity_onehot_y = encode_identity(data_pd_y)\n",
    "#     print(data_identity_onehot_y.shape)\n",
    "#     print(data_identity_onehot_y[2000])\n",
    "\n",
    "# def test_encode_activity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_activity_onehot_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     print(data_activity_onehot_y.shape)\n",
    "#     print(data_activity_onehot_y[1560])\n",
    "\n",
    "# def test_encode_location():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_location_onehot_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     print(data_location_onehot_y.shape)\n",
    "#     print(data_location_onehot_y[1560])\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     print(\"data_identity_onehot_y\",data_identity_onehot_y.shape)\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1)\n",
    "#     print(\"count_data\",count_data.shape)\n",
    "#     count_data = count_data.reshape(-1, 1)  # shape = (11286, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)  \n",
    "#     count_data_onehot = encoder.fit_transform(count_data)\n",
    "#     print(count_data_onehot.shape)  \n",
    "#     count_data_onehot = count_data_onehot.astype(\"int8\")\n",
    "\n",
    "#     return count_data_onehot\n",
    "\n",
    "\n",
    "# def test_encode_count():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_count_onehot_y = encode_count(data_pd_y)\n",
    "#     print(data_count_onehot_y.shape)\n",
    "#     print(data_count_onehot_y[20])\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     test_encode_count()\n",
    "# #     test_load_data_y()\n",
    "# #     test_load_data_x()\n",
    "# #     test_encode_identity()\n",
    "# #     test_encode_activity()\n",
    "# #     test_encode_location()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49438c",
   "metadata": {
    "papermill": {
     "duration": 0.006663,
     "end_time": "2025-12-25T22:23:47.607848",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.601185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 4: preprocess.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1171bdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T22:23:47.622537Z",
     "iopub.status.busy": "2025-12-25T22:23:47.622271Z",
     "iopub.status.idle": "2025-12-25T22:23:47.636787Z",
     "shell.execute_reply": "2025-12-25T22:23:47.636205Z"
    },
    "papermill": {
     "duration": 0.023412,
     "end_time": "2025-12-25T22:23:47.637973",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.614561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          preprocess.py\n",
    "[description]   preprocess WiFi CSI data\n",
    "\"\"\"\n",
    "\n",
    "# All necessary libraries are already imported in Cell 1.\n",
    "\n",
    "# def mat_to_amp(data_mat):\n",
    "#     \"\"\"\n",
    "#     Calculate amplitude of raw WiFi CSI data.\n",
    "#     \"\"\"\n",
    "#     var_length = data_mat[\"trace\"].shape[0]\n",
    "#     data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "#     data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "#     return data_csi_amp\n",
    "\n",
    "def extract_csi_amp(var_dir_mat, var_dir_amp):\n",
    "    \"\"\"\n",
    "    Read raw WiFi CSI (*.mat) files, calculate CSI amplitude, and save as (*.npy).\n",
    "    \"\"\"\n",
    "    var_path_mat = os.listdir(var_dir_mat)\n",
    "    for var_c, var_path in enumerate(var_path_mat):\n",
    "        data_mat = scio.loadmat(os.path.join(var_dir_mat, var_path))\n",
    "        data_csi_amp = mat_to_amp(data_mat)\n",
    "        # print(var_c, data_csi_amp.shape)\n",
    "        var_path_save = os.path.join(var_dir_amp, var_path.replace(\".mat\", \".npy\"))\n",
    "        with open(var_path_save, \"wb\") as var_file:\n",
    "            np.save(var_file, data_csi_amp)\n",
    "\n",
    "\n",
    "\n",
    "# # تنظیمات low-rank (بدون تغییر ورودی mat_to_amp)\n",
    "# LOW_RANK_ENERGY = 0.95   # مثلاً 95% انرژی\n",
    "# LOW_RANK_RANK = None     # اگر عدد بذاری (مثلاً 5)، به جای ENERGY از rank ثابت استفاده میشه\n",
    "\n",
    "# def _low_rank_approx(X, rank=None, energy=0.95):\n",
    "#     X = np.asarray(X)\n",
    "\n",
    "#     was_1d = (X.ndim == 1)\n",
    "#     if was_1d:\n",
    "#         X = X[:, None]\n",
    "\n",
    "#     U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "#     if rank is None:\n",
    "#         s2 = S**2\n",
    "#         cum = np.cumsum(s2) / (np.sum(s2) + 1e-12)\n",
    "#         rank = int(np.searchsorted(cum, energy) + 1)\n",
    "\n",
    "#     rank = max(1, min(rank, S.shape[0]))\n",
    "#     X_lr = (U[:, :rank] * S[:rank]) @ Vt[:rank, :]\n",
    "\n",
    "#     if was_1d:\n",
    "#         X_lr = X_lr[:, 0]\n",
    "\n",
    "#     return X_lr.astype(np.float32)\n",
    "\n",
    "# def mat_to_amp(data_mat):\n",
    "#     \"\"\"\n",
    "#     Calculate amplitude of raw WiFi CSI data, then return its low-rank approximation.\n",
    "#     (ورودی تابع تغییر نکرده)\n",
    "#     \"\"\"\n",
    "#     var_length = data_mat[\"trace\"].shape[0]\n",
    "#     data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "#     data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "#     # خروجی low-rank با همان ابعاد\n",
    "#     data_csi_amp_lr = _low_rank_approx(\n",
    "#         data_csi_amp,\n",
    "#         rank=LOW_RANK_RANK,\n",
    "#         energy=LOW_RANK_ENERGY\n",
    "#     )\n",
    "#     return data_csi_amp_lr\n",
    "\n",
    "\n",
    "\n",
    "# # تنظیمات sparsity (بدون تغییر ورودی mat_to_amp)\n",
    "# SPARSE_KEEP_RATIO = 0.10   # مثلا فقط 10% بزرگترین مقادیر نگه داشته بشن\n",
    "# SPARSE_MIN_ABS = None      # اگر عدد بذاری (مثلا 0.5)، به جای keep_ratio آستانه ثابت میشه\n",
    "\n",
    "# def _to_sparse(X, keep_ratio=0.10, min_abs=None):\n",
    "#     \"\"\"\n",
    "#     Convert X to a sparse representation by keeping only large-magnitude entries.\n",
    "#     Returns:\n",
    "#       - scipy.sparse.csr_matrix if SciPy is available\n",
    "#       - otherwise returns a dense array with many zeros (still \"sparse\" in content)\n",
    "#     \"\"\"\n",
    "#     X = np.asarray(X)\n",
    "#     flat = X.ravel()\n",
    "#     absflat = np.abs(flat)\n",
    "\n",
    "#     if flat.size == 0:\n",
    "#         return X.astype(np.float32)\n",
    "\n",
    "#     # انتخاب آستانه\n",
    "#     if min_abs is not None:\n",
    "#         thr = float(min_abs)\n",
    "#         mask = absflat >= thr\n",
    "#     else:\n",
    "#         k = int(np.ceil(keep_ratio * flat.size))\n",
    "#         k = max(1, min(k, flat.size))\n",
    "#         if k == flat.size:\n",
    "#             mask = np.ones_like(absflat, dtype=bool)\n",
    "#         else:\n",
    "#             thr = np.partition(absflat, -k)[-k]  # kth largest magnitude\n",
    "#             mask = absflat >= thr\n",
    "\n",
    "#     idx = np.nonzero(mask)[0]\n",
    "#     data = flat[idx].astype(np.float32)\n",
    "\n",
    "#     # اگر SciPy هست: sparse واقعی بساز\n",
    "#     try:\n",
    "#         # معمولاً تو Cell1 یا از قبل import شده؛ اگر هم نشده باشه اینجا تلاش می‌کنه.\n",
    "#         import scipy.sparse as sp\n",
    "\n",
    "#         if X.ndim == 1:\n",
    "#             rows = idx\n",
    "#             cols = np.zeros_like(rows)\n",
    "#             shape = (X.shape[0], 1)\n",
    "#         else:\n",
    "#             rows, cols = np.unravel_index(idx, X.shape)\n",
    "#             shape = X.shape\n",
    "\n",
    "#         return sp.coo_matrix((data, (rows, cols)), shape=shape).tocsr()\n",
    "\n",
    "#     except Exception:\n",
    "#         # fallback: آرایه‌ی dense با صفرهای زیاد\n",
    "#         out = np.zeros_like(flat, dtype=np.float32)\n",
    "#         out[idx] = data\n",
    "#         return out.reshape(X.shape)\n",
    "\n",
    "def mat_to_amp(data_mat):\n",
    "    \"\"\"\n",
    "    Calculate amplitude of raw WiFi CSI data, then return its sparse version.\n",
    "    (ورودی تابع تغییر نکرده)\n",
    "    \"\"\"\n",
    "    var_length = data_mat[\"trace\"].shape[0]\n",
    "    data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "    data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "    # خروجی sparse (CSR اگر SciPy باشد)\n",
    "    return _to_sparse(data_csi_amp, keep_ratio=SPARSE_KEEP_RATIO, min_abs=SPARSE_MIN_ABS)\n",
    "\n",
    "# تنظیمات RPCA (می‌تونی عوضشون کنی)\n",
    "RPCA_MAX_ITER = 500\n",
    "RPCA_TOL = 1e-7\n",
    "RPCA_RHO = 1.5\n",
    "RPCA_MU_INIT = None     # None یعنی خودکار\n",
    "RPCA_LAMBDA = None      # None یعنی 1/sqrt(max(m,n))\n",
    "\n",
    "def _soft_threshold(X, tau):\n",
    "    return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "def _svt(X, tau):\n",
    "    # Singular Value Thresholding\n",
    "    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    s_thr = np.maximum(s - tau, 0.0)\n",
    "    # اگر همه صفر شد، سریع برگرد\n",
    "    if np.all(s_thr == 0):\n",
    "        return np.zeros_like(X)\n",
    "    return (U * s_thr) @ Vt\n",
    "\n",
    "def _rpca_ialm(M, lam=None, mu=None, rho=1.5, max_iter=500, tol=1e-7):\n",
    "    \"\"\"\n",
    "    Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "    Decompose: M = L + S\n",
    "    Returns: L, S (same shape as M)\n",
    "    \"\"\"\n",
    "    M = M.astype(np.float64, copy=False)\n",
    "    m, n = M.shape\n",
    "\n",
    "    if lam is None:\n",
    "        lam = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "    # mu پیشنهادی (خودکار)\n",
    "    if mu is None:\n",
    "        # ||M||_2 تقریباً بزرگ‌ترین singular value است\n",
    "        norm2 = np.linalg.svd(M, compute_uv=False)[0] if M.size else 1.0\n",
    "        mu = 1.25 / (norm2 + 1e-12)\n",
    "\n",
    "    L = np.zeros_like(M)\n",
    "    S = np.zeros_like(M)\n",
    "    Y = np.zeros_like(M)\n",
    "\n",
    "    normM = np.linalg.norm(M, ord='fro') + 1e-12\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # L update\n",
    "        L = _svt(M - S + (1.0/mu)*Y, 1.0/mu)\n",
    "\n",
    "        # S update (sparse)\n",
    "        S = _soft_threshold(M - L + (1.0/mu)*Y, lam/mu)\n",
    "\n",
    "        # dual update\n",
    "        R = M - L - S\n",
    "        Y = Y + mu * R\n",
    "\n",
    "        # stop\n",
    "        err = np.linalg.norm(R, ord='fro') / normM\n",
    "        if err < tol:\n",
    "            break\n",
    "\n",
    "        mu *= rho\n",
    "\n",
    "    return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "# def mat_to_amp(data_mat):\n",
    "#     \"\"\"\n",
    "#     Calculate amplitude of raw WiFi CSI data, then return RPCA sparse component S.\n",
    "#     (ورودی تابع تغییر نکرده)\n",
    "#     \"\"\"\n",
    "#     var_length = data_mat[\"trace\"].shape[0]\n",
    "#     data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "#     data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "#     was_1d = (data_csi_amp.ndim == 1)\n",
    "#     M = data_csi_amp[:, None] if was_1d else data_csi_amp\n",
    "\n",
    "#     _, S = _rpca_ialm(\n",
    "#         M,\n",
    "#         lam=RPCA_LAMBDA,\n",
    "#         mu=RPCA_MU_INIT,\n",
    "#         rho=RPCA_RHO,\n",
    "#         max_iter=RPCA_MAX_ITER,\n",
    "#         tol=RPCA_TOL\n",
    "#     )\n",
    "\n",
    "#     if was_1d:\n",
    "#         S = S[:, 0]\n",
    "\n",
    "#     return S\n",
    "\n",
    "def mat_to_amp(data_mat):\n",
    "    \"\"\"\n",
    "    Calculate amplitude of raw WiFi CSI data, then return RPCA low-rank component L.\n",
    "    (ورودی تابع تغییر نکرده)\n",
    "    \"\"\"\n",
    "    var_length = data_mat[\"trace\"].shape[0]\n",
    "    data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "    data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "    was_1d = (data_csi_amp.ndim == 1)\n",
    "    M = data_csi_amp[:, None] if was_1d else data_csi_amp\n",
    "\n",
    "    L, _ = _rpca_ialm(\n",
    "        M,\n",
    "        lam=RPCA_LAMBDA,\n",
    "        mu=RPCA_MU_INIT,\n",
    "        rho=RPCA_RHO,\n",
    "        max_iter=RPCA_MAX_ITER,\n",
    "        tol=RPCA_TOL\n",
    "    )\n",
    "\n",
    "    if was_1d:\n",
    "        L = L[:, 0]\n",
    "\n",
    "    return L\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parse arguments from input.\n",
    "    \"\"\"\n",
    "    var_args = argparse.ArgumentParser()\n",
    "    var_args.add_argument(\"--dir_mat\", default=\"/kaggle/input/wimans/wifi_csi/mat\", type=str)\n",
    "    var_args.add_argument(\"--dir_amp\", default=\"/kaggle/input/wimans/wifi_csi/amp\", type=str)\n",
    "    return var_args.parse_args()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     var_args = parse_args()\n",
    "#     extract_csi_amp(var_dir_mat=var_args.dir_mat, var_dir_amp=var_args.dir_amp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80245b9",
   "metadata": {
    "papermill": {
     "duration": 0.006702,
     "end_time": "2025-12-25T22:23:47.651768",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.645066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 5: that.py (WiFi-based Model THAT)\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2f84a25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T22:23:47.665833Z",
     "iopub.status.busy": "2025-12-25T22:23:47.665658Z",
     "iopub.status.idle": "2025-12-25T22:23:47.690458Z",
     "shell.execute_reply": "2025-12-25T22:23:47.689982Z"
    },
    "papermill": {
     "duration": 0.033409,
     "end_time": "2025-12-25T22:23:47.691476",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.658067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          that.py\n",
    "[description]   implement and evaluate WiFi-based model THAT\n",
    "                https://github.com/windofshadow/THAT\n",
    "\"\"\"\n",
    "\n",
    "# All necessary libraries are imported in Cell 1.\n",
    "# from train import train   --> Defined in Cell 6.\n",
    "# from preset import preset --> Defined in Cell 2.\n",
    "\n",
    "class Gaussian_Position(torch.nn.Module):\n",
    "    def __init__(self, var_dim_feature, var_dim_time, var_num_gaussian=10):\n",
    "        super(Gaussian_Position, self).__init__()\n",
    "        var_embedding = torch.zeros([var_num_gaussian, var_dim_feature], dtype=torch.float)\n",
    "        self.var_embedding = torch.nn.Parameter(var_embedding, requires_grad=True)\n",
    "        torch.nn.init.xavier_uniform_(self.var_embedding)\n",
    "        var_position = torch.arange(0.0, var_dim_time).unsqueeze(1).repeat(1, var_num_gaussian)\n",
    "        self.var_position = torch.nn.Parameter(var_position, requires_grad=False)\n",
    "        var_mu = torch.arange(0.0, var_dim_time, var_dim_time/var_num_gaussian).unsqueeze(0)\n",
    "        self.var_mu = torch.nn.Parameter(var_mu, requires_grad=True)\n",
    "        var_sigma = torch.tensor([50.0] * var_num_gaussian).unsqueeze(0)\n",
    "        self.var_sigma = torch.nn.Parameter(var_sigma, requires_grad=True)\n",
    "\n",
    "    def calculate_pdf(self, var_position, var_mu, var_sigma):\n",
    "        var_pdf = var_position - var_mu\n",
    "        var_pdf = - var_pdf * var_pdf\n",
    "        var_pdf = var_pdf / var_sigma / var_sigma / 2\n",
    "        var_pdf = var_pdf - torch.log(var_sigma)\n",
    "        return var_pdf\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        var_pdf = self.calculate_pdf(self.var_position, self.var_mu, self.var_sigma)\n",
    "        var_pdf = torch.softmax(var_pdf, dim=-1)\n",
    "        var_position_encoding = torch.matmul(var_pdf, self.var_embedding)\n",
    "        var_output = var_input + var_position_encoding.unsqueeze(0)\n",
    "        return var_output\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, var_dim_feature, var_num_head=10, var_size_cnn=[1, 3, 5]):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layer_norm_0 = torch.nn.LayerNorm(var_dim_feature, eps=1e-6)\n",
    "        self.layer_attention = torch.nn.MultiheadAttention(var_dim_feature, var_num_head, batch_first=True)\n",
    "        self.layer_dropout_0 = torch.nn.Dropout(0.1)\n",
    "        self.layer_norm_1 = torch.nn.LayerNorm(var_dim_feature, 1e-6)\n",
    "        layer_cnn = []\n",
    "        for var_size in var_size_cnn:\n",
    "            layer = torch.nn.Sequential(\n",
    "                torch.nn.Conv1d(var_dim_feature, var_dim_feature, var_size, padding=\"same\"),\n",
    "                torch.nn.BatchNorm1d(var_dim_feature),\n",
    "                torch.nn.Dropout(0.1),\n",
    "                torch.nn.LeakyReLU()\n",
    "            )\n",
    "            layer_cnn.append(layer)\n",
    "        self.layer_cnn = torch.nn.ModuleList(layer_cnn)\n",
    "        self.layer_dropout_1 = torch.nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        var_t = var_input\n",
    "        var_t = self.layer_norm_0(var_t)\n",
    "        var_t, _ = self.layer_attention(var_t, var_t, var_t)\n",
    "        var_t = self.layer_dropout_0(var_t)\n",
    "        var_t = var_t + var_input\n",
    "        var_s = self.layer_norm_1(var_t)\n",
    "        var_s = torch.permute(var_s, (0, 2, 1))\n",
    "        var_c = torch.stack([layer(var_s) for layer in self.layer_cnn], dim=0)\n",
    "        var_s = torch.sum(var_c, dim=0) / len(self.layer_cnn)\n",
    "        var_s = self.layer_dropout_1(var_s)\n",
    "        var_s = torch.permute(var_s, (0, 2, 1))\n",
    "        var_output = var_s + var_t\n",
    "        return var_output\n",
    "\n",
    "class THAT(torch.nn.Module):\n",
    "    def __init__(self, var_x_shape, var_y_shape):\n",
    "        super(THAT, self).__init__()\n",
    "        var_dim_feature = var_x_shape[-1]\n",
    "        var_dim_time = var_x_shape[-2]\n",
    "        var_dim_output = var_y_shape[-1]\n",
    "        # Left branch\n",
    "        self.layer_left_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "        self.layer_left_gaussian = Gaussian_Position(var_dim_feature, var_dim_time // 20)\n",
    "        var_num_left = 4\n",
    "        var_dim_left = var_dim_feature\n",
    "        self.layer_left_encoder = torch.nn.ModuleList([\n",
    "            Encoder(var_dim_feature=var_dim_left, var_num_head=10, var_size_cnn=[1, 3, 5])\n",
    "            for _ in range(var_num_left)\n",
    "        ])\n",
    "        self.layer_left_norm = torch.nn.LayerNorm(var_dim_left, eps=1e-6)\n",
    "        self.layer_left_cnn_0 = torch.nn.Conv1d(in_channels=var_dim_left, out_channels=128, kernel_size=8)\n",
    "        self.layer_left_cnn_1 = torch.nn.Conv1d(in_channels=var_dim_left, out_channels=128, kernel_size=16)\n",
    "        self.layer_left_dropout = torch.nn.Dropout(0.5)\n",
    "        # Right branch\n",
    "        self.layer_right_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "        var_num_right = 1\n",
    "        var_dim_right = var_dim_time // 20\n",
    "        self.layer_right_encoder = torch.nn.ModuleList([\n",
    "            Encoder(var_dim_feature=var_dim_right, var_num_head=10, var_size_cnn=[1, 2, 3])\n",
    "            for _ in range(var_num_right)\n",
    "        ])\n",
    "        self.layer_right_norm = torch.nn.LayerNorm(var_dim_right, eps=1e-6)\n",
    "        self.layer_right_cnn_0 = torch.nn.Conv1d(in_channels=var_dim_right, out_channels=16, kernel_size=2)\n",
    "        self.layer_right_cnn_1 = torch.nn.Conv1d(in_channels=var_dim_right, out_channels=16, kernel_size=4)\n",
    "        self.layer_right_dropout = torch.nn.Dropout(0.5)\n",
    "        self.layer_leakyrelu = torch.nn.LeakyReLU()\n",
    "        self.layer_output = torch.nn.Linear(256 + 32, var_dim_output)\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        var_t = var_input  # shape: (batch_size, time_steps, features)\n",
    "        # Left branch\n",
    "        var_left = torch.permute(var_t, (0, 2, 1))\n",
    "        var_left = self.layer_left_pooling(var_left)\n",
    "        var_left = torch.permute(var_left, (0, 2, 1))\n",
    "        var_left = self.layer_left_gaussian(var_left)\n",
    "        for layer in self.layer_left_encoder:\n",
    "            var_left = layer(var_left)\n",
    "        var_left = self.layer_left_norm(var_left)\n",
    "        var_left = torch.permute(var_left, (0, 2, 1))\n",
    "        var_left_0 = self.layer_leakyrelu(self.layer_left_cnn_0(var_left))\n",
    "        var_left_1 = self.layer_leakyrelu(self.layer_left_cnn_1(var_left))\n",
    "        var_left_0 = torch.sum(var_left_0, dim=-1)\n",
    "        var_left_1 = torch.sum(var_left_1, dim=-1)\n",
    "        var_left = torch.concat([var_left_0, var_left_1], dim=-1)\n",
    "        var_left = self.layer_left_dropout(var_left)\n",
    "        # Right branch\n",
    "        var_right = torch.permute(var_t, (0, 2, 1))\n",
    "        var_right = self.layer_right_pooling(var_right)\n",
    "        for layer in self.layer_right_encoder:\n",
    "            var_right = layer(var_right)\n",
    "        var_right = self.layer_right_norm(var_right)\n",
    "        var_right = torch.permute(var_right, (0, 2, 1))\n",
    "        var_right_0 = self.layer_leakyrelu(self.layer_right_cnn_0(var_right))\n",
    "        var_right_1 = self.layer_leakyrelu(self.layer_right_cnn_1(var_right))\n",
    "        var_right_0 = torch.sum(var_right_0, dim=-1)\n",
    "        var_right_1 = torch.sum(var_right_1, dim=-1)\n",
    "        var_right = torch.concat([var_right_0, var_right_1], dim=-1)\n",
    "        var_right = self.layer_right_dropout(var_right)\n",
    "        # Concatenate branches\n",
    "        var_t = torch.concat([var_left, var_right], dim=-1)\n",
    "        var_output = self.layer_output(var_t)\n",
    "        return var_output\n",
    "\n",
    "def run_that(data_train_x, data_train_y, data_test_x, data_test_y, var_repeat=10, init_model=None):\n",
    "    \"\"\"\n",
    "    Run WiFi-based model THAT.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data_train_x = data_train_x.reshape(data_train_x.shape[0], data_train_x.shape[1], -1)\n",
    "    data_test_x = data_test_x.reshape(data_test_x.shape[0], data_test_x.shape[1], -1)\n",
    "    var_x_shape, var_y_shape = data_train_x[0].shape, data_train_y[0].reshape(-1).shape\n",
    "    data_train_set = TensorDataset(torch.from_numpy(data_train_x), torch.from_numpy(data_train_y))\n",
    "    data_test_set = TensorDataset(torch.from_numpy(data_test_x), torch.from_numpy(data_test_y))\n",
    "    \n",
    "    result = {}\n",
    "    result_accuracy = []\n",
    "    result_time_train = []\n",
    "    result_time_test = []\n",
    "    \n",
    "    # var_macs, var_params = get_model_complexity_info(THAT(var_x_shape, var_y_shape), var_x_shape, as_strings=False)\n",
    "    # print(\"Parameters:\", var_params, \"- FLOPs:\", var_macs * 2)\n",
    "    \n",
    "    for var_r in range(var_repeat):\n",
    "        print(\"Repeat\", var_r)\n",
    "        torch.random.manual_seed(var_r + 39)\n",
    "        if init_model is not None:\n",
    "            model_that = init_model\n",
    "            lr2 = preset[\"nn\"][\"lr\"] /10\n",
    "        else:\n",
    "            model_that = THAT(var_x_shape, var_y_shape).to(device)\n",
    "            lr2 = preset[\"nn\"][\"lr\"]\n",
    "\n",
    "        optimizer = torch.optim.Adam(model_that.parameters(), lr=lr2, weight_decay=0)\n",
    "        loss = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([4] * var_y_shape[-1]).to(device))\n",
    "        var_time_0 = time.time()\n",
    "        \n",
    "        # Train\n",
    "        var_best_weight = train(model=model_that, optimizer=optimizer, loss=loss, \n",
    "                                  data_train_set=data_train_set, data_test_set=data_test_set,\n",
    "                                  var_threshold=preset[\"nn\"][\"threshold\"],\n",
    "                                  var_batch_size=preset[\"nn\"][\"batch_size\"],\n",
    "                                  var_epochs=preset[\"nn\"][\"epoch\"],\n",
    "                                  device=device)\n",
    "        var_time_1 = time.time()\n",
    "        \n",
    "        # Test\n",
    "        model_that.load_state_dict(var_best_weight)\n",
    "        with torch.no_grad():\n",
    "            predict_test_y = model_that(torch.from_numpy(data_test_x).to(device))\n",
    "        predict_test_y = (torch.sigmoid(predict_test_y) > preset[\"nn\"][\"threshold\"]).float()\n",
    "        predict_test_y = predict_test_y.detach().cpu().numpy()\n",
    "        var_time_2 = time.time()\n",
    "        \n",
    "        # Evaluate\n",
    "        data_test_y_c = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "        predict_test_y_c = predict_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "        result_acc = accuracy_score(data_test_y_c.astype(int), predict_test_y_c.astype(int))\n",
    "        result_dict = classification_report(data_test_y_c, predict_test_y_c, digits=6, zero_division=0, output_dict=True)\n",
    "        result[\"repeat_\" + str(var_r)] = result_dict\n",
    "        result_accuracy.append(result_acc)\n",
    "        result_time_train.append(var_time_1 - var_time_0)\n",
    "        result_time_test.append(var_time_2 - var_time_1)\n",
    "        print(\"repeat_\" + str(var_r), result_accuracy)\n",
    "        print(result)\n",
    "    \n",
    "    result[\"accuracy\"] = {\"avg\": np.mean(result_accuracy), \"std\": np.std(result_accuracy)}\n",
    "    result[\"time_train\"] = {\"avg\": np.mean(result_time_train), \"std\": np.std(result_time_train)}\n",
    "    result[\"time_test\"] = {\"avg\": np.mean(result_time_test), \"std\": np.std(result_time_test)}\n",
    "    # result[\"complexity\"] = {\"parameter\": var_params, \"flops\": var_macs * 2}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab43a81",
   "metadata": {
    "papermill": {
     "duration": 0.006933,
     "end_time": "2025-12-25T22:23:47.705010",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.698077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc57b2a7",
   "metadata": {
    "papermill": {
     "duration": 0.006245,
     "end_time": "2025-12-25T22:23:47.717877",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.711632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell7: for RESNET18 Model\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f5798cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T22:23:47.732443Z",
     "iopub.status.busy": "2025-12-25T22:23:47.732211Z",
     "iopub.status.idle": "2025-12-25T22:23:47.738572Z",
     "shell.execute_reply": "2025-12-25T22:23:47.737697Z"
    },
    "papermill": {
     "duration": 0.015045,
     "end_time": "2025-12-25T22:23:47.739790",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.724745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# import torch._dynamo\n",
    "# torch._dynamo.config.suppress_errors = True\n",
    "# import time\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "# import numpy as np\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# import torchvision.models as models\n",
    "# from copy import deepcopy\n",
    "\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "# torch._dynamo.config.cache_size_limit = 65536\n",
    "\n",
    "# # فرض می‌کنیم preset قبلاً تعریف شده باشه\n",
    "# # preset = { \"nn\": {\"lr\": 1e-3, \"epoch\": 10, \"batch_size\": 4, \"threshold\": 0.5}, ... }\n",
    "\n",
    "# class ResNet18Model(torch.nn.Module):\n",
    "#     def __init__(self, var_x_shape, var_y_shape):\n",
    "#         super(ResNet18Model, self).__init__()\n",
    "#         model_resnet = models.resnet18(weights=None)\n",
    "#         model_resnet.conv1 = torch.nn.Conv2d(1, 64, 7, 3, 2, bias=False)\n",
    "#         in_features_fc = model_resnet.fc.in_features  # معمولاً 512\n",
    "#         out_features_fc = var_y_shape[-1]\n",
    "#         model_resnet.fc = torch.nn.Linear(in_features_fc, out_features_fc)\n",
    "#         self.resnet = model_resnet\n",
    "\n",
    "#     def forward(self, var_input):\n",
    "#         var_input = var_input.reshape(var_input.size(0), 1, 3000, 270)\n",
    "#         return self.resnet(var_input)\n",
    "\n",
    "# def run_resnet(data_train_x, data_train_y, data_test_x, data_test_y, var_repeat=10, init_model=None):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     var_x_shape = data_train_x[0].shape\n",
    "#     var_y_shape = data_train_y[0].reshape(-1).shape\n",
    "\n",
    "#     # تغییر شکل داده‌ها روی CPU\n",
    "#     data_train_x = data_train_x.reshape(data_train_x.shape[0], 1, data_train_x.shape[1],\n",
    "#                                         data_train_x.shape[2]*data_train_x.shape[3]*data_train_x.shape[4])\n",
    "#     data_test_x  = data_test_x.reshape(data_test_x.shape[0], 1, data_test_x.shape[1],\n",
    "#                                        data_test_x.shape[2]*data_test_x.shape[3]*data_test_x.shape[4])\n",
    "    \n",
    "#     # دیتاست‌ها روی CPU\n",
    "#     data_train_set = TensorDataset(torch.from_numpy(data_train_x).float(),\n",
    "#                                    torch.from_numpy(data_train_y).float())\n",
    "#     data_test_set  = TensorDataset(torch.from_numpy(data_test_x).float(),\n",
    "#                                    torch.from_numpy(data_test_y).float())\n",
    "    \n",
    "#     result = {}\n",
    "#     result_accuracy = []\n",
    "#     result_time_train = []\n",
    "#     result_time_test = []\n",
    "    \n",
    "#     for var_r in range(var_repeat):\n",
    "#         print(\"Repeat\", var_r)\n",
    "#         torch.random.manual_seed(var_r + 39)\n",
    "        \n",
    "#         # ساخت مدل و انتقال به GPU\n",
    "#         if init_model is not None:\n",
    "#             model_resnet = init_model\n",
    "#             lr2 = preset[\"nn\"][\"lr\"] /10\n",
    "            \n",
    "#         else:\n",
    "#             model_resnet = ResNet18Model(var_x_shape, var_y_shape).to(device)\n",
    "#             lr2 = preset[\"nn\"][\"lr\"]\n",
    "\n",
    "#         optimizer = torch.optim.Adam(model_resnet.parameters(), lr=lr2, weight_decay=0)\n",
    "#         loss_func = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([6] * var_y_shape[-1]).to(device))\n",
    "        \n",
    "#         # تابع آموزش داخلی؛ دیتا روی CPU باقی می‌مونه و فقط هنگام محاسبه batch به GPU میره\n",
    "#         def train_inner():\n",
    "#             train_loader = DataLoader(data_train_set, preset[\"nn\"][\"batch_size\"], shuffle=True, pin_memory=False)\n",
    "#             test_loader = DataLoader(data_test_set, preset[\"nn\"][\"batch_size\"], shuffle=False, pin_memory=False)\n",
    "#             best_accuracy = 0\n",
    "#             best_weight = None\n",
    "            \n",
    "#             for epoch in range(preset[\"nn\"][\"epoch\"]):\n",
    "#                 t0 = time.time()\n",
    "#                 model_resnet.train()\n",
    "#                 # متغیرهای مربوط به آخرین batch آموزش\n",
    "#                 last_train_loss = None\n",
    "#                 last_train_acc = None\n",
    "#                 for batch in train_loader:\n",
    "#                     batch_x, batch_y = batch\n",
    "#                     batch_x = batch_x.to(device)\n",
    "#                     batch_y = batch_y.to(device)\n",
    "#                     outputs = model_resnet(batch_x)\n",
    "#                     loss_val = loss_func(outputs, batch_y.reshape(batch_y.shape[0], -1).float())\n",
    "#                     optimizer.zero_grad()\n",
    "#                     loss_val.backward()\n",
    "#                     optimizer.step()\n",
    "#                     last_train_loss = loss_val.item()\n",
    "#                     # محاسبه دقت آخرین batch آموزش\n",
    "#                     train_preds = (torch.sigmoid(outputs) > preset[\"nn\"][\"threshold\"]).float()\n",
    "#                     last_train_acc = accuracy_score(batch_y.reshape(batch_y.shape[0], -1).detach().cpu().numpy().astype(int),\n",
    "#                                                     train_preds.detach().cpu().numpy().astype(int))\n",
    "                \n",
    "#                 # ارزیابی روی دیتاست تست به صورت batch به batch\n",
    "#                 model_resnet.eval()\n",
    "#                 all_preds = []\n",
    "#                 all_labels = []\n",
    "#                 test_loss_val = None\n",
    "#                 with torch.no_grad():\n",
    "#                     for t_batch in test_loader:\n",
    "#                         t_x, t_y = t_batch\n",
    "#                         t_x = t_x.to(device)\n",
    "#                         outputs_test = model_resnet(t_x)\n",
    "#                         outputs_test = (torch.sigmoid(outputs_test) > preset[\"nn\"][\"threshold\"]).float()\n",
    "#                         all_preds.append(outputs_test.detach().cpu().numpy())\n",
    "#                         all_labels.append(t_y.cpu().numpy())  # اینجا تغییر دادیم\n",
    "#                 preds_cat = np.vstack(all_preds)\n",
    "#                 labels_cat = np.vstack(all_labels)\n",
    "#                 print(\"preds_cat\",preds_cat.shape)\n",
    "#                 # تبدیل به شکل (n, 6, 5)\n",
    "                \n",
    "#                 # preds_cat = preds_cat.reshape(-1, 6, 5)\n",
    "#                 # labels_cat = labels_cat.reshape(-1, 6, 5)\n",
    "\n",
    "#                 preds_cat = preds_cat.reshape(-1, 6)\n",
    "#                 labels_cat = labels_cat.reshape(-1, 6)\n",
    "                \n",
    "#                 # برای محاسبه دقت، مسطح می‌کنیم\n",
    "#                 test_acc = accuracy_score(labels_cat.reshape(labels_cat.shape[0], -1).astype(int),\n",
    "#                                           preds_cat.reshape(preds_cat.shape[0], -1).astype(int))\n",
    "#                 epoch_time = time.time() - t0\n",
    "#                 print(f\"Epoch {epoch}/{preset['nn']['epoch']} - \"\n",
    "#                       f\"Train Loss: {(last_train_loss if last_train_loss is not None else 0.0):.6f}, \"\n",
    "#                       f\"Train Acc: {(last_train_acc if last_train_acc is not None else 0.0):.6f}, \"\n",
    "#                       f\"Test Loss: {(test_loss_val if test_loss_val is not None else 0.0):.6f}, \"\n",
    "#                       f\"Test Acc: {(test_acc if test_acc is not None else 0.0):.6f} - \"\n",
    "#                       f\"Time: {epoch_time:.4f}s\")\n",
    "\n",
    "#                 if test_acc > best_accuracy:\n",
    "#                     best_accuracy = test_acc\n",
    "#                     print('-----***-----')\n",
    "#                     print(best_accuracy)\n",
    "#                     best_weight = deepcopy(model_resnet.state_dict())\n",
    "#             return best_weight\n",
    "        \n",
    "#         t0_run = time.time()\n",
    "#         best_weight = train_inner()\n",
    "#         t1_run = time.time()\n",
    "        \n",
    "#         torch.save(model_resnet.state_dict(), f\"{name_run}_model_final.pt\")\n",
    "#         model_resnet.load_state_dict(best_weight)\n",
    "#         torch.save(model_resnet.state_dict(), f\"{name_run}_best_model.pt\")\n",
    "\n",
    "#         # bad age niaz bod load koni\n",
    "#         # model_resnet = ResNet18Model(var_x_shape, var_y_shape).to(device)\n",
    "#         # model_resnet.load_state_dict(torch.load(\"resnet_model_repeat0.pt\"))\n",
    "#         # model_resnet.eval()\n",
    "\n",
    "        \n",
    "#         # ارزیابی نهایی مدل روی دیتاست تست (استفاده از batchهای کوچک)\n",
    "#         model_resnet.eval()\n",
    "#         all_preds = []\n",
    "#         test_loader_final = DataLoader(data_test_set, preset[\"nn\"][\"batch_size\"], shuffle=False, pin_memory=False)\n",
    "#         with torch.no_grad():\n",
    "#             for batch in test_loader_final:\n",
    "#                 batch_x, _ = batch\n",
    "#                 batch_x = batch_x.to(device)\n",
    "#                 all_preds.append(model_resnet(batch_x))\n",
    "#         preds_all = torch.cat(all_preds, dim=0)\n",
    "#         preds_final = (torch.sigmoid(preds_all) > preset[\"nn\"][\"threshold\"]).float().detach().cpu().numpy()\n",
    "#         t2_run = time.time()\n",
    "        \n",
    "#         data_test_y_np = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "#         preds_final = preds_final.reshape(-1, data_test_y.shape[-1])\n",
    "#         acc_final = accuracy_score(data_test_y_np.astype(int), preds_final.astype(int))\n",
    "#         result[f\"repeat_{var_r}\"] = {\"accuracy\": acc_final}\n",
    "#         result_accuracy.append(acc_final)\n",
    "#         result_time_train.append(t1_run - t0_run)\n",
    "#         result_time_test.append(t2_run - t1_run)\n",
    "#         print(\"Repeat\", var_r, \"Final Test Accuracy:\", acc_final)\n",
    "    \n",
    "#     result[\"accuracy\"] = {\"avg\": np.mean(result_accuracy), \"std\": np.std(result_accuracy)}\n",
    "#     result[\"time_train\"] = {\"avg\": np.mean(result_time_train), \"std\": np.std(result_time_train)}\n",
    "#     result[\"time_test\"] = {\"avg\": np.mean(result_time_test), \"std\": np.std(result_time_test)}\n",
    "#     return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68b915",
   "metadata": {
    "papermill": {
     "duration": 0.006545,
     "end_time": "2025-12-25T22:23:47.753064",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.746519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 9: train.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3f90f12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T22:23:47.767058Z",
     "iopub.status.busy": "2025-12-25T22:23:47.766875Z",
     "iopub.status.idle": "2025-12-25T22:23:48.277633Z",
     "shell.execute_reply": "2025-12-25T22:23:48.277048Z"
    },
    "papermill": {
     "duration": 0.519426,
     "end_time": "2025-12-25T22:23:48.279075",
     "exception": false,
     "start_time": "2025-12-25T22:23:47.759649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          train.py\n",
    "[description]   function to train WiFi-based models\n",
    "\"\"\"\n",
    "\n",
    "# All necessary libraries are imported in Cell 1.\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch._dynamo.config.cache_size_limit = 65536\n",
    "\n",
    "def train(model, optimizer, loss, data_train_set, data_test_set, var_threshold, var_batch_size, var_epochs, device):\n",
    "    \"\"\"\n",
    "    Generic training function for WiFi-based models.\n",
    "    \"\"\"\n",
    "    # دیتا رو روی CPU نگه می‌داریم (pin_memory=False)\n",
    "    data_train_loader = DataLoader(data_train_set, var_batch_size, shuffle=True, pin_memory=False)\n",
    "    data_test_loader = DataLoader(data_test_set, batch_size=len(data_test_set), shuffle=False, pin_memory=False)\n",
    "    \n",
    "    var_best_accuracy = -1.0\n",
    "    var_best_weight   = deepcopy(model.state_dict())\n",
    "    \n",
    "    \n",
    "    for var_epoch in range(var_epochs):\n",
    "        var_time_e0 = time.time()\n",
    "        model.train()\n",
    "        for data_batch in data_train_loader:\n",
    "            data_batch_x, data_batch_y = data_batch\n",
    "            # انتقال موقتی داده به GPU فقط برای forward pass\n",
    "            data_batch_x = data_batch_x.to(device)\n",
    "            data_batch_y = data_batch_y.to(device)\n",
    "            predict_train_y = model(data_batch_x)\n",
    "            var_loss_train = loss(predict_train_y, data_batch_y.reshape(data_batch_y.shape[0], -1).float())\n",
    "            optimizer.zero_grad()\n",
    "            var_loss_train.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # محاسبه دقت روی آخرین batch و انتقال نتایج به CPU\n",
    "        predict_train_y = (torch.sigmoid(predict_train_y) > var_threshold).float()\n",
    "        data_batch_y = data_batch_y.detach().cpu().numpy()\n",
    "        predict_train_y = predict_train_y.detach().cpu().numpy()\n",
    "        \n",
    "        predict_train_y = predict_train_y.reshape(-1, data_batch_y.shape[-1])\n",
    "        data_batch_y = data_batch_y.reshape(-1, data_batch_y.shape[-1])\n",
    "        var_accuracy_train = accuracy_score(data_batch_y.astype(int), predict_train_y.astype(int))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            data_test_x, data_test_y = next(iter(data_test_loader))\n",
    "            # انتقال موقتی دیتا تست به GPU برای محاسبات\n",
    "            data_test_x = data_test_x.to(device)\n",
    "            data_test_y = data_test_y.to(device)\n",
    "            \n",
    "            predict_test_y = model(data_test_x)\n",
    "            var_loss_test = loss(predict_test_y, data_test_y.reshape(data_test_y.shape[0], -1).float())\n",
    "            \n",
    "            predict_test_y = (torch.sigmoid(predict_test_y) > var_threshold).float()\n",
    "            \n",
    "            # انتقال نتایج به CPU برای ارزیابی\n",
    "            data_test_y = data_test_y.detach().cpu().numpy()\n",
    "            predict_test_y = predict_test_y.detach().cpu().numpy()\n",
    "            \n",
    "            predict_test_y = predict_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            data_test_y = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            var_accuracy_test = accuracy_score(data_test_y.astype(int), predict_test_y.astype(int))\n",
    "        \n",
    "        print(f\"Epoch {var_epoch}/{var_epochs}\",\n",
    "              \"- %.6fs\"%(time.time() - var_time_e0),\n",
    "              \"- Loss %.6f\"%var_loss_train.cpu(),\n",
    "              \"- Accuracy %.6f\"%var_accuracy_train,\n",
    "              \"- Test Loss %.6f\"%var_loss_test.cpu(),\n",
    "              \"- Test Accuracy %.6f\"%var_accuracy_test)\n",
    "            \n",
    "        if var_accuracy_test > var_best_accuracy:\n",
    "            var_best_accuracy = var_accuracy_test\n",
    "            print('-----***-----')\n",
    "            print(var_best_accuracy)\n",
    "            var_best_weight = deepcopy(model.state_dict())\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{name_run}_model_final.pt\")\n",
    "    torch.save(var_best_weight, f\"{name_run}_best_model.pt\")\n",
    "\n",
    "    \n",
    "    return var_best_weight\n",
    "\n",
    "\n",
    "\n",
    "# === importsِ لازم را یک‌بار بالای فایل اضافه کن ===\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- تابع کمکی ----------\n",
    "def save_confusion_matrix(model, data_loader, threshold, device, pdf_path):\n",
    "    \"\"\"\n",
    "    Runs the model on `data_loader`, builds a confusion matrix and writes it to `pdf_path`.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "\n",
    "            preds = (torch.sigmoid(logits) > threshold).float().cpu().numpy().ravel()\n",
    "            yb    = yb.cpu().numpy().ravel()\n",
    "\n",
    "            y_true.extend(yb)\n",
    "            y_pred.extend(preds)\n",
    "\n",
    "    cm  = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    ConfusionMatrixDisplay(cm).plot(ax=ax)\n",
    "    ax.set_title(\"Confusion Matrix – Test\")\n",
    "\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "# ---------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f068cc",
   "metadata": {
    "papermill": {
     "duration": 0.006651,
     "end_time": "2025-12-25T22:23:48.293099",
     "exception": false,
     "start_time": "2025-12-25T22:23:48.286448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 11: run.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59b7cbfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T22:23:48.307477Z",
     "iopub.status.busy": "2025-12-25T22:23:48.307150Z",
     "iopub.status.idle": "2025-12-26T01:26:35.669849Z",
     "shell.execute_reply": "2025-12-26T01:26:35.668999Z"
    },
    "papermill": {
     "duration": 10967.371661,
     "end_time": "2025-12-26T01:26:35.671340",
     "exception": false,
     "start_time": "2025-12-25T22:23:48.299679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "\n",
      "[DEBUG] First MAT sample: act_1_1\n",
      "[DEBUG] shape=(2835, 3, 3, 30), dtype=complex128, complex=True\n",
      "[DEBUG] abs range:  min=1.414214, max=18.027756\n",
      "[DEBUG] phase stats: mean=0.906475, std=1.591276\n",
      "[DEBUG] ==> Input is COMPLEX (phase exists).\n",
      "Running model: THAT\n",
      "Repeat 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py:370: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1036.)\n",
      "  return F.conv1d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000 - 5.532526s - Loss 2.637712 - Accuracy 0.104167 - Test Loss 2.130124 - Test Accuracy 0.027851\n",
      "-----***-----\n",
      "0.027851458885941646\n",
      "Epoch 1/1000 - 4.655242s - Loss 1.270083 - Accuracy 0.161458 - Test Loss 1.216973 - Test Accuracy 0.057913\n",
      "-----***-----\n",
      "0.05791335101679929\n",
      "Epoch 2/1000 - 5.084772s - Loss 0.884406 - Accuracy 0.177083 - Test Loss 0.858684 - Test Accuracy 0.045977\n",
      "Epoch 3/1000 - 5.068352s - Loss 0.649677 - Accuracy 0.343750 - Test Loss 1.163451 - Test Accuracy 0.003979\n",
      "Epoch 4/1000 - 5.109291s - Loss 0.590172 - Accuracy 0.317708 - Test Loss 0.646205 - Test Accuracy 0.242263\n",
      "-----***-----\n",
      "0.242263483642794\n",
      "Epoch 5/1000 - 5.062421s - Loss 0.617145 - Accuracy 0.307292 - Test Loss 0.695516 - Test Accuracy 0.082670\n",
      "Epoch 6/1000 - 5.094190s - Loss 0.536146 - Accuracy 0.364583 - Test Loss 0.612230 - Test Accuracy 0.309019\n",
      "-----***-----\n",
      "0.3090185676392573\n",
      "Epoch 7/1000 - 5.070422s - Loss 0.583220 - Accuracy 0.380208 - Test Loss 0.571680 - Test Accuracy 0.414677\n",
      "-----***-----\n",
      "0.4146772767462423\n",
      "Epoch 8/1000 - 5.101509s - Loss 0.633297 - Accuracy 0.421875 - Test Loss 0.545857 - Test Accuracy 0.544209\n",
      "-----***-----\n",
      "0.5442086648983201\n",
      "Epoch 9/1000 - 5.077489s - Loss 0.524776 - Accuracy 0.395833 - Test Loss 0.554169 - Test Accuracy 0.523873\n",
      "Epoch 10/1000 - 5.086674s - Loss 0.566202 - Accuracy 0.421875 - Test Loss 0.577547 - Test Accuracy 0.536693\n",
      "Epoch 11/1000 - 5.056117s - Loss 0.497167 - Accuracy 0.500000 - Test Loss 0.753767 - Test Accuracy 0.092838\n",
      "Epoch 12/1000 - 5.088280s - Loss 0.616880 - Accuracy 0.369792 - Test Loss 0.561288 - Test Accuracy 0.511936\n",
      "Epoch 13/1000 - 5.065838s - Loss 0.488577 - Accuracy 0.515625 - Test Loss 0.564502 - Test Accuracy 0.535367\n",
      "Epoch 14/1000 - 5.078914s - Loss 0.539992 - Accuracy 0.453125 - Test Loss 0.589795 - Test Accuracy 0.447834\n",
      "Epoch 15/1000 - 5.119218s - Loss 0.600883 - Accuracy 0.427083 - Test Loss 0.638602 - Test Accuracy 0.455349\n",
      "Epoch 16/1000 - 5.071535s - Loss 0.530920 - Accuracy 0.458333 - Test Loss 0.603546 - Test Accuracy 0.480548\n",
      "Epoch 17/1000 - 5.058469s - Loss 0.556368 - Accuracy 0.453125 - Test Loss 0.579017 - Test Accuracy 0.518126\n",
      "Epoch 18/1000 - 5.057608s - Loss 0.564099 - Accuracy 0.505208 - Test Loss 0.570990 - Test Accuracy 0.482759\n",
      "Epoch 19/1000 - 5.094991s - Loss 0.479249 - Accuracy 0.557292 - Test Loss 0.570864 - Test Accuracy 0.538019\n",
      "Epoch 20/1000 - 5.063756s - Loss 0.552054 - Accuracy 0.427083 - Test Loss 0.600143 - Test Accuracy 0.498674\n",
      "Epoch 21/1000 - 5.080644s - Loss 0.445928 - Accuracy 0.598958 - Test Loss 0.589137 - Test Accuracy 0.471264\n",
      "Epoch 22/1000 - 5.115126s - Loss 0.459816 - Accuracy 0.562500 - Test Loss 0.627571 - Test Accuracy 0.254642\n",
      "Epoch 23/1000 - 5.221469s - Loss 0.447037 - Accuracy 0.578125 - Test Loss 0.583368 - Test Accuracy 0.435455\n",
      "Epoch 24/1000 - 5.100787s - Loss 0.414346 - Accuracy 0.625000 - Test Loss 0.559000 - Test Accuracy 0.529178\n",
      "Epoch 25/1000 - 5.063455s - Loss 0.481369 - Accuracy 0.526042 - Test Loss 0.576819 - Test Accuracy 0.462865\n",
      "Epoch 26/1000 - 5.090965s - Loss 0.483222 - Accuracy 0.562500 - Test Loss 0.560855 - Test Accuracy 0.508842\n",
      "Epoch 27/1000 - 5.066449s - Loss 0.458526 - Accuracy 0.515625 - Test Loss 0.560063 - Test Accuracy 0.502210\n",
      "Epoch 28/1000 - 5.102258s - Loss 0.441199 - Accuracy 0.541667 - Test Loss 0.586063 - Test Accuracy 0.442087\n",
      "Epoch 29/1000 - 5.050004s - Loss 0.452490 - Accuracy 0.484375 - Test Loss 0.601982 - Test Accuracy 0.400531\n",
      "Epoch 30/1000 - 5.068743s - Loss 0.469053 - Accuracy 0.489583 - Test Loss 0.551410 - Test Accuracy 0.488948\n",
      "Epoch 31/1000 - 5.076845s - Loss 0.418138 - Accuracy 0.562500 - Test Loss 0.604080 - Test Accuracy 0.354996\n",
      "Epoch 32/1000 - 5.071612s - Loss 0.430994 - Accuracy 0.505208 - Test Loss 0.610106 - Test Accuracy 0.335544\n",
      "Epoch 33/1000 - 5.094071s - Loss 0.447834 - Accuracy 0.531250 - Test Loss 0.562581 - Test Accuracy 0.480106\n",
      "Epoch 34/1000 - 5.085876s - Loss 0.492459 - Accuracy 0.453125 - Test Loss 0.612255 - Test Accuracy 0.387268\n",
      "Epoch 35/1000 - 5.071908s - Loss 0.470651 - Accuracy 0.531250 - Test Loss 0.572232 - Test Accuracy 0.454465\n",
      "Epoch 36/1000 - 5.076538s - Loss 0.416250 - Accuracy 0.552083 - Test Loss 0.576356 - Test Accuracy 0.414677\n",
      "Epoch 37/1000 - 5.096052s - Loss 0.431236 - Accuracy 0.531250 - Test Loss 0.591725 - Test Accuracy 0.344385\n",
      "Epoch 38/1000 - 5.066820s - Loss 0.471570 - Accuracy 0.468750 - Test Loss 0.570151 - Test Accuracy 0.432803\n",
      "Epoch 39/1000 - 5.085147s - Loss 0.391571 - Accuracy 0.536458 - Test Loss 0.582981 - Test Accuracy 0.383289\n",
      "Epoch 40/1000 - 5.069935s - Loss 0.438846 - Accuracy 0.489583 - Test Loss 0.591480 - Test Accuracy 0.361627\n",
      "Epoch 41/1000 - 5.064705s - Loss 0.386536 - Accuracy 0.510417 - Test Loss 0.591005 - Test Accuracy 0.421751\n",
      "Epoch 42/1000 - 5.066320s - Loss 0.482237 - Accuracy 0.411458 - Test Loss 0.616721 - Test Accuracy 0.332449\n",
      "Epoch 43/1000 - 5.089220s - Loss 0.388865 - Accuracy 0.578125 - Test Loss 0.601541 - Test Accuracy 0.287356\n",
      "Epoch 44/1000 - 5.107481s - Loss 0.379327 - Accuracy 0.567708 - Test Loss 0.584651 - Test Accuracy 0.358532\n",
      "Epoch 45/1000 - 5.084771s - Loss 0.419331 - Accuracy 0.515625 - Test Loss 0.585891 - Test Accuracy 0.334218\n",
      "Epoch 46/1000 - 5.069005s - Loss 0.357890 - Accuracy 0.630208 - Test Loss 0.597095 - Test Accuracy 0.372237\n",
      "Epoch 47/1000 - 5.051825s - Loss 0.438998 - Accuracy 0.567708 - Test Loss 0.644119 - Test Accuracy 0.306808\n",
      "Epoch 48/1000 - 5.087234s - Loss 0.401777 - Accuracy 0.500000 - Test Loss 0.626975 - Test Accuracy 0.278073\n",
      "Epoch 49/1000 - 5.077137s - Loss 0.373603 - Accuracy 0.531250 - Test Loss 0.578187 - Test Accuracy 0.336428\n",
      "Epoch 50/1000 - 5.094952s - Loss 0.390035 - Accuracy 0.494792 - Test Loss 0.598840 - Test Accuracy 0.291777\n",
      "Epoch 51/1000 - 5.076663s - Loss 0.321082 - Accuracy 0.500000 - Test Loss 0.628939 - Test Accuracy 0.269231\n",
      "Epoch 52/1000 - 5.078698s - Loss 0.414627 - Accuracy 0.526042 - Test Loss 0.596409 - Test Accuracy 0.320955\n",
      "Epoch 53/1000 - 5.075158s - Loss 0.391194 - Accuracy 0.505208 - Test Loss 0.626534 - Test Accuracy 0.289567\n",
      "Epoch 54/1000 - 5.060087s - Loss 0.377002 - Accuracy 0.515625 - Test Loss 0.683249 - Test Accuracy 0.326260\n",
      "Epoch 55/1000 - 5.046688s - Loss 0.357536 - Accuracy 0.567708 - Test Loss 0.601387 - Test Accuracy 0.308576\n",
      "Epoch 56/1000 - 5.098601s - Loss 0.424227 - Accuracy 0.552083 - Test Loss 0.587054 - Test Accuracy 0.275862\n",
      "Epoch 57/1000 - 5.078899s - Loss 0.371878 - Accuracy 0.500000 - Test Loss 0.620623 - Test Accuracy 0.240053\n",
      "Epoch 58/1000 - 5.048299s - Loss 0.323377 - Accuracy 0.651042 - Test Loss 0.591515 - Test Accuracy 0.281609\n",
      "Epoch 59/1000 - 5.053220s - Loss 0.386190 - Accuracy 0.531250 - Test Loss 0.607819 - Test Accuracy 0.313882\n",
      "Epoch 60/1000 - 5.099340s - Loss 0.297592 - Accuracy 0.557292 - Test Loss 0.616056 - Test Accuracy 0.230769\n",
      "Epoch 61/1000 - 5.094717s - Loss 0.309198 - Accuracy 0.567708 - Test Loss 0.657749 - Test Accuracy 0.329797\n",
      "Epoch 62/1000 - 5.073134s - Loss 0.316850 - Accuracy 0.593750 - Test Loss 0.635296 - Test Accuracy 0.204244\n",
      "Epoch 63/1000 - 5.109441s - Loss 0.331421 - Accuracy 0.531250 - Test Loss 0.712360 - Test Accuracy 0.223254\n",
      "Epoch 64/1000 - 5.065370s - Loss 0.321507 - Accuracy 0.567708 - Test Loss 0.746837 - Test Accuracy 0.227675\n",
      "Epoch 65/1000 - 5.128298s - Loss 0.336753 - Accuracy 0.500000 - Test Loss 0.687867 - Test Accuracy 0.293103\n",
      "Epoch 66/1000 - 5.062762s - Loss 0.300640 - Accuracy 0.505208 - Test Loss 0.674332 - Test Accuracy 0.250663\n",
      "Epoch 67/1000 - 5.093986s - Loss 0.349999 - Accuracy 0.614583 - Test Loss 0.682854 - Test Accuracy 0.224138\n",
      "Epoch 68/1000 - 5.095332s - Loss 0.354852 - Accuracy 0.515625 - Test Loss 0.645291 - Test Accuracy 0.253316\n",
      "Epoch 69/1000 - 5.082750s - Loss 0.315815 - Accuracy 0.546875 - Test Loss 0.679458 - Test Accuracy 0.193192\n",
      "Epoch 70/1000 - 5.082355s - Loss 0.332320 - Accuracy 0.489583 - Test Loss 0.693687 - Test Accuracy 0.221927\n",
      "Epoch 71/1000 - 5.092527s - Loss 0.294772 - Accuracy 0.515625 - Test Loss 0.658276 - Test Accuracy 0.188771\n",
      "Epoch 72/1000 - 5.068453s - Loss 0.304175 - Accuracy 0.552083 - Test Loss 0.703267 - Test Accuracy 0.174182\n",
      "Epoch 73/1000 - 5.091057s - Loss 0.254344 - Accuracy 0.661458 - Test Loss 0.699195 - Test Accuracy 0.256410\n",
      "Epoch 74/1000 - 5.071770s - Loss 0.216546 - Accuracy 0.614583 - Test Loss 0.669196 - Test Accuracy 0.175950\n",
      "Epoch 75/1000 - 5.077652s - Loss 0.298153 - Accuracy 0.536458 - Test Loss 0.738891 - Test Accuracy 0.179487\n",
      "Epoch 76/1000 - 5.087277s - Loss 0.264406 - Accuracy 0.588542 - Test Loss 0.733763 - Test Accuracy 0.215738\n",
      "Epoch 77/1000 - 5.083273s - Loss 0.251522 - Accuracy 0.583333 - Test Loss 0.726723 - Test Accuracy 0.222370\n",
      "Epoch 78/1000 - 5.047035s - Loss 0.327376 - Accuracy 0.510417 - Test Loss 0.721757 - Test Accuracy 0.225464\n",
      "Epoch 79/1000 - 5.071945s - Loss 0.328819 - Accuracy 0.531250 - Test Loss 0.720586 - Test Accuracy 0.179487\n",
      "Epoch 80/1000 - 5.051314s - Loss 0.223653 - Accuracy 0.645833 - Test Loss 0.758374 - Test Accuracy 0.194518\n",
      "Epoch 81/1000 - 5.065081s - Loss 0.232195 - Accuracy 0.588542 - Test Loss 0.738974 - Test Accuracy 0.210433\n",
      "Epoch 82/1000 - 5.061489s - Loss 0.239222 - Accuracy 0.635417 - Test Loss 0.791424 - Test Accuracy 0.220159\n",
      "Epoch 83/1000 - 5.084780s - Loss 0.227101 - Accuracy 0.583333 - Test Loss 0.828477 - Test Accuracy 0.230327\n",
      "Epoch 84/1000 - 5.086172s - Loss 0.292024 - Accuracy 0.588542 - Test Loss 0.818054 - Test Accuracy 0.220159\n",
      "Epoch 85/1000 - 5.075394s - Loss 0.276884 - Accuracy 0.536458 - Test Loss 0.881714 - Test Accuracy 0.258621\n",
      "Epoch 86/1000 - 5.077546s - Loss 0.226897 - Accuracy 0.572917 - Test Loss 0.857457 - Test Accuracy 0.256410\n",
      "Epoch 87/1000 - 5.073107s - Loss 0.240216 - Accuracy 0.640625 - Test Loss 0.831260 - Test Accuracy 0.270557\n",
      "Epoch 88/1000 - 5.064562s - Loss 0.265132 - Accuracy 0.510417 - Test Loss 0.844174 - Test Accuracy 0.239611\n",
      "Epoch 89/1000 - 5.083018s - Loss 0.276032 - Accuracy 0.536458 - Test Loss 0.846873 - Test Accuracy 0.229443\n",
      "Epoch 90/1000 - 5.058557s - Loss 0.221532 - Accuracy 0.635417 - Test Loss 0.932252 - Test Accuracy 0.214412\n",
      "Epoch 91/1000 - 5.057033s - Loss 0.203228 - Accuracy 0.630208 - Test Loss 1.094545 - Test Accuracy 0.254200\n",
      "Epoch 92/1000 - 5.056187s - Loss 0.206670 - Accuracy 0.598958 - Test Loss 0.965994 - Test Accuracy 0.256852\n",
      "Epoch 93/1000 - 5.103328s - Loss 0.219116 - Accuracy 0.625000 - Test Loss 1.063512 - Test Accuracy 0.241379\n",
      "Epoch 94/1000 - 5.082694s - Loss 0.254135 - Accuracy 0.531250 - Test Loss 0.807451 - Test Accuracy 0.199823\n",
      "Epoch 95/1000 - 5.103052s - Loss 0.188653 - Accuracy 0.645833 - Test Loss 1.015569 - Test Accuracy 0.221927\n",
      "Epoch 96/1000 - 5.079189s - Loss 0.211851 - Accuracy 0.609375 - Test Loss 0.990969 - Test Accuracy 0.212644\n",
      "Epoch 97/1000 - 5.066157s - Loss 0.251077 - Accuracy 0.609375 - Test Loss 0.920651 - Test Accuracy 0.212202\n",
      "Epoch 98/1000 - 5.096437s - Loss 0.189777 - Accuracy 0.651042 - Test Loss 1.004741 - Test Accuracy 0.245358\n",
      "Epoch 99/1000 - 5.071736s - Loss 0.224070 - Accuracy 0.666667 - Test Loss 0.909776 - Test Accuracy 0.237401\n",
      "Epoch 100/1000 - 5.052140s - Loss 0.276140 - Accuracy 0.604167 - Test Loss 1.051226 - Test Accuracy 0.218833\n",
      "Epoch 101/1000 - 5.076294s - Loss 0.267692 - Accuracy 0.583333 - Test Loss 0.936480 - Test Accuracy 0.242263\n",
      "Epoch 102/1000 - 5.065879s - Loss 0.311291 - Accuracy 0.578125 - Test Loss 0.837005 - Test Accuracy 0.201149\n",
      "Epoch 103/1000 - 5.066276s - Loss 0.204784 - Accuracy 0.614583 - Test Loss 0.959808 - Test Accuracy 0.274536\n",
      "Epoch 104/1000 - 5.061800s - Loss 0.232174 - Accuracy 0.557292 - Test Loss 1.022127 - Test Accuracy 0.207781\n",
      "Epoch 105/1000 - 5.067680s - Loss 0.241364 - Accuracy 0.515625 - Test Loss 0.980414 - Test Accuracy 0.231211\n",
      "Epoch 106/1000 - 5.060666s - Loss 0.196282 - Accuracy 0.640625 - Test Loss 0.958440 - Test Accuracy 0.207339\n",
      "Epoch 107/1000 - 5.064209s - Loss 0.202529 - Accuracy 0.609375 - Test Loss 1.075324 - Test Accuracy 0.274536\n",
      "Epoch 108/1000 - 5.067141s - Loss 0.251006 - Accuracy 0.505208 - Test Loss 0.995602 - Test Accuracy 0.220159\n",
      "Epoch 109/1000 - 5.095828s - Loss 0.195179 - Accuracy 0.640625 - Test Loss 1.007632 - Test Accuracy 0.239169\n",
      "Epoch 110/1000 - 5.048910s - Loss 0.293347 - Accuracy 0.500000 - Test Loss 1.036273 - Test Accuracy 0.251547\n",
      "Epoch 111/1000 - 5.097810s - Loss 0.168883 - Accuracy 0.687500 - Test Loss 1.060537 - Test Accuracy 0.301503\n",
      "Epoch 112/1000 - 5.070236s - Loss 0.179147 - Accuracy 0.640625 - Test Loss 0.934743 - Test Accuracy 0.176835\n",
      "Epoch 113/1000 - 5.102412s - Loss 0.210262 - Accuracy 0.614583 - Test Loss 1.116348 - Test Accuracy 0.241379\n",
      "Epoch 114/1000 - 5.074170s - Loss 0.226128 - Accuracy 0.567708 - Test Loss 1.085376 - Test Accuracy 0.276746\n",
      "Epoch 115/1000 - 5.083913s - Loss 0.161714 - Accuracy 0.661458 - Test Loss 1.107473 - Test Accuracy 0.261715\n",
      "Epoch 116/1000 - 5.077635s - Loss 0.227772 - Accuracy 0.619792 - Test Loss 1.030158 - Test Accuracy 0.258179\n",
      "Epoch 117/1000 - 5.093309s - Loss 0.211702 - Accuracy 0.625000 - Test Loss 1.101608 - Test Accuracy 0.239611\n",
      "Epoch 118/1000 - 5.060757s - Loss 0.235121 - Accuracy 0.619792 - Test Loss 1.143449 - Test Accuracy 0.275862\n",
      "Epoch 119/1000 - 5.068024s - Loss 0.236986 - Accuracy 0.635417 - Test Loss 1.061852 - Test Accuracy 0.216622\n",
      "Epoch 120/1000 - 5.058868s - Loss 0.184592 - Accuracy 0.588542 - Test Loss 1.060740 - Test Accuracy 0.242263\n",
      "Epoch 121/1000 - 5.073667s - Loss 0.180222 - Accuracy 0.656250 - Test Loss 1.195041 - Test Accuracy 0.247569\n",
      "Epoch 122/1000 - 5.082757s - Loss 0.196605 - Accuracy 0.645833 - Test Loss 1.243932 - Test Accuracy 0.261273\n",
      "Epoch 123/1000 - 5.073034s - Loss 0.194890 - Accuracy 0.656250 - Test Loss 1.026709 - Test Accuracy 0.250663\n",
      "Epoch 124/1000 - 5.085966s - Loss 0.195358 - Accuracy 0.609375 - Test Loss 1.088055 - Test Accuracy 0.223696\n",
      "Epoch 125/1000 - 5.090354s - Loss 0.211365 - Accuracy 0.578125 - Test Loss 1.189474 - Test Accuracy 0.251105\n",
      "Epoch 126/1000 - 5.069677s - Loss 0.163103 - Accuracy 0.671875 - Test Loss 1.231332 - Test Accuracy 0.252874\n",
      "Epoch 127/1000 - 5.071579s - Loss 0.197605 - Accuracy 0.583333 - Test Loss 1.150932 - Test Accuracy 0.246684\n",
      "Epoch 128/1000 - 5.109121s - Loss 0.202259 - Accuracy 0.593750 - Test Loss 1.154615 - Test Accuracy 0.248453\n",
      "Epoch 129/1000 - 5.083726s - Loss 0.178890 - Accuracy 0.635417 - Test Loss 1.057632 - Test Accuracy 0.249779\n",
      "Epoch 130/1000 - 5.097309s - Loss 0.206117 - Accuracy 0.562500 - Test Loss 1.074459 - Test Accuracy 0.226790\n",
      "Epoch 131/1000 - 5.065706s - Loss 0.153627 - Accuracy 0.692708 - Test Loss 1.112123 - Test Accuracy 0.217065\n",
      "Epoch 132/1000 - 5.088282s - Loss 0.187600 - Accuracy 0.651042 - Test Loss 1.222377 - Test Accuracy 0.252874\n",
      "Epoch 133/1000 - 5.081622s - Loss 0.163034 - Accuracy 0.708333 - Test Loss 1.181226 - Test Accuracy 0.260831\n",
      "Epoch 134/1000 - 5.073583s - Loss 0.183911 - Accuracy 0.645833 - Test Loss 1.099306 - Test Accuracy 0.260831\n",
      "Epoch 135/1000 - 5.085033s - Loss 0.176166 - Accuracy 0.656250 - Test Loss 1.395575 - Test Accuracy 0.262599\n",
      "Epoch 136/1000 - 5.098333s - Loss 0.157246 - Accuracy 0.718750 - Test Loss 1.310784 - Test Accuracy 0.271883\n",
      "Epoch 137/1000 - 5.075065s - Loss 0.132700 - Accuracy 0.671875 - Test Loss 1.213931 - Test Accuracy 0.229885\n",
      "Epoch 138/1000 - 5.084012s - Loss 0.229528 - Accuracy 0.619792 - Test Loss 1.212289 - Test Accuracy 0.242706\n",
      "Epoch 139/1000 - 5.094592s - Loss 0.259188 - Accuracy 0.583333 - Test Loss 1.328799 - Test Accuracy 0.229001\n",
      "Epoch 140/1000 - 5.074806s - Loss 0.198999 - Accuracy 0.640625 - Test Loss 1.512677 - Test Accuracy 0.312555\n",
      "Epoch 141/1000 - 5.060450s - Loss 0.185826 - Accuracy 0.625000 - Test Loss 1.177701 - Test Accuracy 0.211760\n",
      "Epoch 142/1000 - 5.073281s - Loss 0.151055 - Accuracy 0.750000 - Test Loss 1.269281 - Test Accuracy 0.272767\n",
      "Epoch 143/1000 - 5.067289s - Loss 0.203615 - Accuracy 0.661458 - Test Loss 1.391899 - Test Accuracy 0.251105\n",
      "Epoch 144/1000 - 5.073099s - Loss 0.138230 - Accuracy 0.750000 - Test Loss 1.281293 - Test Accuracy 0.236516\n",
      "Epoch 145/1000 - 5.058953s - Loss 0.178097 - Accuracy 0.703125 - Test Loss 1.323766 - Test Accuracy 0.255526\n",
      "Epoch 146/1000 - 5.062956s - Loss 0.242380 - Accuracy 0.614583 - Test Loss 1.391065 - Test Accuracy 0.243148\n",
      "Epoch 147/1000 - 5.081105s - Loss 0.196749 - Accuracy 0.614583 - Test Loss 1.524705 - Test Accuracy 0.289125\n",
      "Epoch 148/1000 - 5.108245s - Loss 0.204091 - Accuracy 0.604167 - Test Loss 1.350764 - Test Accuracy 0.263484\n",
      "Epoch 149/1000 - 5.072422s - Loss 0.162164 - Accuracy 0.677083 - Test Loss 1.259190 - Test Accuracy 0.220159\n",
      "Epoch 150/1000 - 5.100930s - Loss 0.174283 - Accuracy 0.625000 - Test Loss 1.248010 - Test Accuracy 0.282051\n",
      "Epoch 151/1000 - 5.071606s - Loss 0.167300 - Accuracy 0.723958 - Test Loss 1.211173 - Test Accuracy 0.262599\n",
      "Epoch 152/1000 - 5.081872s - Loss 0.158531 - Accuracy 0.687500 - Test Loss 1.380261 - Test Accuracy 0.260389\n",
      "Epoch 153/1000 - 5.072329s - Loss 0.189121 - Accuracy 0.625000 - Test Loss 1.311715 - Test Accuracy 0.255968\n",
      "Epoch 154/1000 - 5.061797s - Loss 0.170308 - Accuracy 0.635417 - Test Loss 1.335338 - Test Accuracy 0.282051\n",
      "Epoch 155/1000 - 5.055651s - Loss 0.180325 - Accuracy 0.713542 - Test Loss 1.395209 - Test Accuracy 0.294430\n",
      "Epoch 156/1000 - 5.063199s - Loss 0.181158 - Accuracy 0.666667 - Test Loss 1.287513 - Test Accuracy 0.240937\n",
      "Epoch 157/1000 - 5.060108s - Loss 0.205402 - Accuracy 0.614583 - Test Loss 1.484863 - Test Accuracy 0.275420\n",
      "Epoch 158/1000 - 5.096322s - Loss 0.258579 - Accuracy 0.619792 - Test Loss 1.423416 - Test Accuracy 0.265694\n",
      "Epoch 159/1000 - 5.054182s - Loss 0.127454 - Accuracy 0.677083 - Test Loss 1.513687 - Test Accuracy 0.276304\n",
      "Epoch 160/1000 - 5.122443s - Loss 0.167090 - Accuracy 0.697917 - Test Loss 1.285396 - Test Accuracy 0.262599\n",
      "Epoch 161/1000 - 5.095131s - Loss 0.164828 - Accuracy 0.723958 - Test Loss 1.437201 - Test Accuracy 0.249779\n",
      "Epoch 162/1000 - 5.105127s - Loss 0.205929 - Accuracy 0.625000 - Test Loss 1.244616 - Test Accuracy 0.232095\n",
      "Epoch 163/1000 - 5.076633s - Loss 0.175492 - Accuracy 0.598958 - Test Loss 1.556344 - Test Accuracy 0.297524\n",
      "Epoch 164/1000 - 5.078478s - Loss 0.149407 - Accuracy 0.661458 - Test Loss 1.410338 - Test Accuracy 0.289125\n",
      "Epoch 165/1000 - 5.073501s - Loss 0.174827 - Accuracy 0.656250 - Test Loss 1.403429 - Test Accuracy 0.272325\n",
      "Epoch 166/1000 - 5.061268s - Loss 0.164211 - Accuracy 0.635417 - Test Loss 1.396557 - Test Accuracy 0.285146\n",
      "Epoch 167/1000 - 5.071420s - Loss 0.155248 - Accuracy 0.619792 - Test Loss 1.316565 - Test Accuracy 0.258179\n",
      "Epoch 168/1000 - 5.079345s - Loss 0.183410 - Accuracy 0.635417 - Test Loss 1.368802 - Test Accuracy 0.261273\n",
      "Epoch 169/1000 - 5.058422s - Loss 0.136342 - Accuracy 0.697917 - Test Loss 1.500580 - Test Accuracy 0.283820\n",
      "Epoch 170/1000 - 5.059116s - Loss 0.177166 - Accuracy 0.635417 - Test Loss 1.515498 - Test Accuracy 0.277188\n",
      "Epoch 171/1000 - 5.055290s - Loss 0.184778 - Accuracy 0.671875 - Test Loss 1.441980 - Test Accuracy 0.281167\n",
      "Epoch 172/1000 - 5.067921s - Loss 0.196711 - Accuracy 0.630208 - Test Loss 1.359216 - Test Accuracy 0.271883\n",
      "Epoch 173/1000 - 5.063855s - Loss 0.188045 - Accuracy 0.635417 - Test Loss 1.454217 - Test Accuracy 0.299735\n",
      "Epoch 174/1000 - 5.058437s - Loss 0.188705 - Accuracy 0.619792 - Test Loss 1.348858 - Test Accuracy 0.266136\n",
      "Epoch 175/1000 - 5.081168s - Loss 0.160262 - Accuracy 0.708333 - Test Loss 1.379601 - Test Accuracy 0.279841\n",
      "Epoch 176/1000 - 5.109358s - Loss 0.148410 - Accuracy 0.703125 - Test Loss 1.479272 - Test Accuracy 0.259947\n",
      "Epoch 177/1000 - 5.071576s - Loss 0.195260 - Accuracy 0.671875 - Test Loss 1.417943 - Test Accuracy 0.240053\n",
      "Epoch 178/1000 - 5.088259s - Loss 0.190419 - Accuracy 0.609375 - Test Loss 1.575878 - Test Accuracy 0.316534\n",
      "Epoch 179/1000 - 5.072042s - Loss 0.157199 - Accuracy 0.703125 - Test Loss 1.387560 - Test Accuracy 0.250221\n",
      "Epoch 180/1000 - 5.107447s - Loss 0.149644 - Accuracy 0.635417 - Test Loss 1.410378 - Test Accuracy 0.268789\n",
      "Epoch 181/1000 - 5.077966s - Loss 0.200542 - Accuracy 0.562500 - Test Loss 1.477137 - Test Accuracy 0.283378\n",
      "Epoch 182/1000 - 5.081867s - Loss 0.168138 - Accuracy 0.687500 - Test Loss 1.559224 - Test Accuracy 0.259505\n",
      "Epoch 183/1000 - 5.060994s - Loss 0.283958 - Accuracy 0.671875 - Test Loss 1.534454 - Test Accuracy 0.258621\n",
      "Epoch 184/1000 - 5.088375s - Loss 0.204409 - Accuracy 0.651042 - Test Loss 1.522648 - Test Accuracy 0.257294\n",
      "Epoch 185/1000 - 5.069705s - Loss 0.171035 - Accuracy 0.692708 - Test Loss 1.419505 - Test Accuracy 0.320071\n",
      "Epoch 186/1000 - 5.099808s - Loss 0.158154 - Accuracy 0.651042 - Test Loss 1.434963 - Test Accuracy 0.262599\n",
      "Epoch 187/1000 - 5.071006s - Loss 0.153067 - Accuracy 0.661458 - Test Loss 1.401478 - Test Accuracy 0.248453\n",
      "Epoch 188/1000 - 5.097408s - Loss 0.183481 - Accuracy 0.692708 - Test Loss 1.435877 - Test Accuracy 0.277630\n",
      "Epoch 189/1000 - 5.100277s - Loss 0.116780 - Accuracy 0.760417 - Test Loss 1.583874 - Test Accuracy 0.291777\n",
      "Epoch 190/1000 - 5.080397s - Loss 0.146064 - Accuracy 0.708333 - Test Loss 1.395720 - Test Accuracy 0.261715\n",
      "Epoch 191/1000 - 5.087147s - Loss 0.182732 - Accuracy 0.692708 - Test Loss 1.506674 - Test Accuracy 0.286472\n",
      "Epoch 192/1000 - 5.063684s - Loss 0.177840 - Accuracy 0.645833 - Test Loss 1.711588 - Test Accuracy 0.330681\n",
      "Epoch 193/1000 - 5.098379s - Loss 0.144383 - Accuracy 0.718750 - Test Loss 1.390549 - Test Accuracy 0.252431\n",
      "Epoch 194/1000 - 5.080704s - Loss 0.149215 - Accuracy 0.718750 - Test Loss 1.500612 - Test Accuracy 0.286914\n",
      "Epoch 195/1000 - 5.104915s - Loss 0.176683 - Accuracy 0.729167 - Test Loss 1.501250 - Test Accuracy 0.265694\n",
      "Epoch 196/1000 - 5.061271s - Loss 0.150786 - Accuracy 0.734375 - Test Loss 1.512601 - Test Accuracy 0.273652\n",
      "Epoch 197/1000 - 5.072158s - Loss 0.186797 - Accuracy 0.677083 - Test Loss 1.413221 - Test Accuracy 0.232980\n",
      "Epoch 198/1000 - 5.090207s - Loss 0.176580 - Accuracy 0.609375 - Test Loss 1.651906 - Test Accuracy 0.312997\n",
      "Epoch 199/1000 - 5.097901s - Loss 0.167678 - Accuracy 0.656250 - Test Loss 1.460497 - Test Accuracy 0.243590\n",
      "Epoch 200/1000 - 5.092747s - Loss 0.148081 - Accuracy 0.765625 - Test Loss 1.436056 - Test Accuracy 0.295314\n",
      "Epoch 201/1000 - 5.094526s - Loss 0.186811 - Accuracy 0.630208 - Test Loss 1.412218 - Test Accuracy 0.269231\n",
      "Epoch 202/1000 - 5.106914s - Loss 0.159186 - Accuracy 0.703125 - Test Loss 1.551547 - Test Accuracy 0.272767\n",
      "Epoch 203/1000 - 5.084855s - Loss 0.128507 - Accuracy 0.755208 - Test Loss 1.458507 - Test Accuracy 0.263042\n",
      "Epoch 204/1000 - 5.066503s - Loss 0.124996 - Accuracy 0.708333 - Test Loss 1.667654 - Test Accuracy 0.302829\n",
      "Epoch 205/1000 - 5.070399s - Loss 0.152776 - Accuracy 0.692708 - Test Loss 1.473390 - Test Accuracy 0.281167\n",
      "Epoch 206/1000 - 5.069830s - Loss 0.193519 - Accuracy 0.598958 - Test Loss 1.568149 - Test Accuracy 0.271883\n",
      "Epoch 207/1000 - 5.076782s - Loss 0.151471 - Accuracy 0.708333 - Test Loss 1.500609 - Test Accuracy 0.259947\n",
      "Epoch 208/1000 - 5.088178s - Loss 0.122379 - Accuracy 0.812500 - Test Loss 1.568463 - Test Accuracy 0.283378\n",
      "Epoch 209/1000 - 5.095178s - Loss 0.109282 - Accuracy 0.723958 - Test Loss 1.631983 - Test Accuracy 0.291777\n",
      "Epoch 210/1000 - 5.081842s - Loss 0.126206 - Accuracy 0.692708 - Test Loss 1.655582 - Test Accuracy 0.295756\n",
      "Epoch 211/1000 - 5.080623s - Loss 0.203024 - Accuracy 0.677083 - Test Loss 1.595500 - Test Accuracy 0.275420\n",
      "Epoch 212/1000 - 5.072552s - Loss 0.109779 - Accuracy 0.750000 - Test Loss 1.635024 - Test Accuracy 0.283378\n",
      "Epoch 213/1000 - 5.078284s - Loss 0.167014 - Accuracy 0.661458 - Test Loss 1.667988 - Test Accuracy 0.262599\n",
      "Epoch 214/1000 - 5.075475s - Loss 0.157171 - Accuracy 0.651042 - Test Loss 1.516925 - Test Accuracy 0.270115\n",
      "Epoch 215/1000 - 5.113171s - Loss 0.178180 - Accuracy 0.625000 - Test Loss 1.703347 - Test Accuracy 0.265252\n",
      "Epoch 216/1000 - 5.072361s - Loss 0.137804 - Accuracy 0.729167 - Test Loss 1.570809 - Test Accuracy 0.280725\n",
      "Epoch 217/1000 - 5.079937s - Loss 0.110730 - Accuracy 0.729167 - Test Loss 1.798990 - Test Accuracy 0.283820\n",
      "Epoch 218/1000 - 5.059951s - Loss 0.172722 - Accuracy 0.682292 - Test Loss 1.654126 - Test Accuracy 0.256410\n",
      "Epoch 219/1000 - 5.101813s - Loss 0.151743 - Accuracy 0.708333 - Test Loss 1.667872 - Test Accuracy 0.261273\n",
      "Epoch 220/1000 - 5.072948s - Loss 0.172756 - Accuracy 0.682292 - Test Loss 1.535609 - Test Accuracy 0.244032\n",
      "Epoch 221/1000 - 5.092722s - Loss 0.189724 - Accuracy 0.708333 - Test Loss 1.584205 - Test Accuracy 0.254200\n",
      "Epoch 222/1000 - 5.074427s - Loss 0.158118 - Accuracy 0.656250 - Test Loss 1.747083 - Test Accuracy 0.304156\n",
      "Epoch 223/1000 - 5.085870s - Loss 0.113631 - Accuracy 0.760417 - Test Loss 1.686044 - Test Accuracy 0.250221\n",
      "Epoch 224/1000 - 5.049374s - Loss 0.145306 - Accuracy 0.692708 - Test Loss 1.703094 - Test Accuracy 0.293103\n",
      "Epoch 225/1000 - 5.078265s - Loss 0.115003 - Accuracy 0.755208 - Test Loss 1.772067 - Test Accuracy 0.279841\n",
      "Epoch 226/1000 - 5.086263s - Loss 0.201875 - Accuracy 0.656250 - Test Loss 1.677637 - Test Accuracy 0.280725\n",
      "Epoch 227/1000 - 5.072787s - Loss 0.147571 - Accuracy 0.734375 - Test Loss 1.766651 - Test Accuracy 0.276746\n",
      "Epoch 228/1000 - 5.068542s - Loss 0.132000 - Accuracy 0.776042 - Test Loss 1.781912 - Test Accuracy 0.308134\n",
      "Epoch 229/1000 - 5.085348s - Loss 0.145341 - Accuracy 0.713542 - Test Loss 1.795405 - Test Accuracy 0.278073\n",
      "Epoch 230/1000 - 5.068223s - Loss 0.159062 - Accuracy 0.708333 - Test Loss 1.620602 - Test Accuracy 0.270557\n",
      "Epoch 231/1000 - 5.059621s - Loss 0.159081 - Accuracy 0.687500 - Test Loss 1.799797 - Test Accuracy 0.273652\n",
      "Epoch 232/1000 - 5.060289s - Loss 0.193058 - Accuracy 0.682292 - Test Loss 1.756908 - Test Accuracy 0.291335\n",
      "Epoch 233/1000 - 5.083812s - Loss 0.163171 - Accuracy 0.687500 - Test Loss 1.811924 - Test Accuracy 0.287798\n",
      "Epoch 234/1000 - 5.053498s - Loss 0.260603 - Accuracy 0.677083 - Test Loss 2.184385 - Test Accuracy 0.104775\n",
      "Epoch 235/1000 - 5.076699s - Loss 3.443511 - Accuracy 0.369792 - Test Loss 7.416737 - Test Accuracy 0.187445\n",
      "Epoch 236/1000 - 5.046908s - Loss 1.042180 - Accuracy 0.296875 - Test Loss 1.484137 - Test Accuracy 0.207781\n",
      "Epoch 237/1000 - 5.063747s - Loss 0.706491 - Accuracy 0.348958 - Test Loss 1.613240 - Test Accuracy 0.255526\n",
      "Epoch 238/1000 - 5.072483s - Loss 0.403445 - Accuracy 0.432292 - Test Loss 1.152929 - Test Accuracy 0.268347\n",
      "Epoch 239/1000 - 5.082549s - Loss 0.353462 - Accuracy 0.489583 - Test Loss 1.063353 - Test Accuracy 0.233422\n",
      "Epoch 240/1000 - 5.060856s - Loss 0.387805 - Accuracy 0.302083 - Test Loss 0.955839 - Test Accuracy 0.201149\n",
      "Epoch 241/1000 - 5.078321s - Loss 0.237925 - Accuracy 0.578125 - Test Loss 1.106351 - Test Accuracy 0.212644\n",
      "Epoch 242/1000 - 5.052926s - Loss 0.286152 - Accuracy 0.557292 - Test Loss 0.974795 - Test Accuracy 0.187445\n",
      "Epoch 243/1000 - 5.109943s - Loss 0.292708 - Accuracy 0.494792 - Test Loss 1.058883 - Test Accuracy 0.171530\n",
      "Epoch 244/1000 - 5.064180s - Loss 0.319238 - Accuracy 0.505208 - Test Loss 1.247728 - Test Accuracy 0.181256\n",
      "Epoch 245/1000 - 5.077950s - Loss 0.240157 - Accuracy 0.583333 - Test Loss 1.176375 - Test Accuracy 0.207339\n",
      "Epoch 246/1000 - 5.070049s - Loss 0.186110 - Accuracy 0.609375 - Test Loss 1.175827 - Test Accuracy 0.167993\n",
      "Epoch 247/1000 - 5.083386s - Loss 0.217794 - Accuracy 0.619792 - Test Loss 1.218982 - Test Accuracy 0.176835\n",
      "Epoch 248/1000 - 5.061709s - Loss 0.206975 - Accuracy 0.630208 - Test Loss 1.250927 - Test Accuracy 0.209549\n",
      "Epoch 249/1000 - 5.081527s - Loss 0.226143 - Accuracy 0.526042 - Test Loss 1.307526 - Test Accuracy 0.207781\n",
      "Epoch 250/1000 - 5.056015s - Loss 0.169605 - Accuracy 0.677083 - Test Loss 1.196821 - Test Accuracy 0.212644\n",
      "Epoch 251/1000 - 5.096528s - Loss 0.203877 - Accuracy 0.609375 - Test Loss 1.322113 - Test Accuracy 0.190981\n",
      "Epoch 252/1000 - 5.094442s - Loss 0.236735 - Accuracy 0.671875 - Test Loss 1.418936 - Test Accuracy 0.274978\n",
      "Epoch 253/1000 - 5.086011s - Loss 0.201751 - Accuracy 0.614583 - Test Loss 1.408336 - Test Accuracy 0.258179\n",
      "Epoch 254/1000 - 5.095944s - Loss 0.147227 - Accuracy 0.697917 - Test Loss 1.357155 - Test Accuracy 0.218833\n",
      "Epoch 255/1000 - 5.071962s - Loss 0.230652 - Accuracy 0.666667 - Test Loss 1.527927 - Test Accuracy 0.238727\n",
      "Epoch 256/1000 - 5.096630s - Loss 0.170253 - Accuracy 0.677083 - Test Loss 1.454204 - Test Accuracy 0.217949\n",
      "Epoch 257/1000 - 5.049384s - Loss 0.152417 - Accuracy 0.630208 - Test Loss 1.565516 - Test Accuracy 0.247126\n",
      "Epoch 258/1000 - 5.084017s - Loss 0.161877 - Accuracy 0.708333 - Test Loss 1.490489 - Test Accuracy 0.220601\n",
      "Epoch 259/1000 - 5.058824s - Loss 0.169780 - Accuracy 0.713542 - Test Loss 1.640130 - Test Accuracy 0.237843\n",
      "Epoch 260/1000 - 5.082466s - Loss 0.173059 - Accuracy 0.640625 - Test Loss 1.597433 - Test Accuracy 0.250221\n",
      "Epoch 261/1000 - 5.062250s - Loss 0.184218 - Accuracy 0.661458 - Test Loss 1.579597 - Test Accuracy 0.245358\n",
      "Epoch 262/1000 - 5.086372s - Loss 0.122225 - Accuracy 0.718750 - Test Loss 1.790634 - Test Accuracy 0.260389\n",
      "Epoch 263/1000 - 5.084451s - Loss 0.165891 - Accuracy 0.614583 - Test Loss 1.605726 - Test Accuracy 0.248011\n",
      "Epoch 264/1000 - 5.107055s - Loss 0.185978 - Accuracy 0.682292 - Test Loss 1.613864 - Test Accuracy 0.262157\n",
      "Epoch 265/1000 - 5.089973s - Loss 0.142346 - Accuracy 0.744792 - Test Loss 1.530264 - Test Accuracy 0.244474\n",
      "Epoch 266/1000 - 5.097420s - Loss 0.090568 - Accuracy 0.791667 - Test Loss 1.720545 - Test Accuracy 0.284262\n",
      "Epoch 267/1000 - 5.078472s - Loss 0.164629 - Accuracy 0.635417 - Test Loss 1.764657 - Test Accuracy 0.289125\n",
      "Epoch 268/1000 - 5.080361s - Loss 0.159537 - Accuracy 0.713542 - Test Loss 1.881053 - Test Accuracy 0.302829\n",
      "Epoch 269/1000 - 5.062123s - Loss 0.148887 - Accuracy 0.666667 - Test Loss 1.753359 - Test Accuracy 0.288683\n",
      "Epoch 270/1000 - 5.074926s - Loss 0.121170 - Accuracy 0.734375 - Test Loss 1.642825 - Test Accuracy 0.276746\n",
      "Epoch 271/1000 - 5.062861s - Loss 0.139515 - Accuracy 0.786458 - Test Loss 1.826128 - Test Accuracy 0.279841\n",
      "Epoch 272/1000 - 5.059916s - Loss 0.171012 - Accuracy 0.687500 - Test Loss 1.946536 - Test Accuracy 0.282493\n",
      "Epoch 273/1000 - 5.080036s - Loss 0.124698 - Accuracy 0.786458 - Test Loss 1.706283 - Test Accuracy 0.252431\n",
      "Epoch 274/1000 - 5.103881s - Loss 0.117029 - Accuracy 0.739583 - Test Loss 1.836819 - Test Accuracy 0.287356\n",
      "Epoch 275/1000 - 5.055162s - Loss 0.180294 - Accuracy 0.640625 - Test Loss 1.902410 - Test Accuracy 0.294872\n",
      "Epoch 276/1000 - 5.082341s - Loss 0.114289 - Accuracy 0.708333 - Test Loss 1.737025 - Test Accuracy 0.295756\n",
      "Epoch 277/1000 - 5.079481s - Loss 0.126990 - Accuracy 0.734375 - Test Loss 1.879582 - Test Accuracy 0.299735\n",
      "Epoch 278/1000 - 5.117200s - Loss 0.118089 - Accuracy 0.755208 - Test Loss 1.945222 - Test Accuracy 0.315650\n",
      "Epoch 279/1000 - 5.069524s - Loss 0.107299 - Accuracy 0.765625 - Test Loss 1.850916 - Test Accuracy 0.295756\n",
      "Epoch 280/1000 - 5.080342s - Loss 0.147660 - Accuracy 0.729167 - Test Loss 1.913693 - Test Accuracy 0.324492\n",
      "Epoch 281/1000 - 5.044444s - Loss 0.149016 - Accuracy 0.713542 - Test Loss 1.982835 - Test Accuracy 0.331123\n",
      "Epoch 282/1000 - 5.083491s - Loss 0.104806 - Accuracy 0.791667 - Test Loss 2.041126 - Test Accuracy 0.327586\n",
      "Epoch 283/1000 - 5.064852s - Loss 0.104139 - Accuracy 0.781250 - Test Loss 1.947876 - Test Accuracy 0.291777\n",
      "Epoch 284/1000 - 5.063328s - Loss 0.169102 - Accuracy 0.656250 - Test Loss 1.857304 - Test Accuracy 0.301945\n",
      "Epoch 285/1000 - 5.096659s - Loss 0.194903 - Accuracy 0.671875 - Test Loss 1.918138 - Test Accuracy 0.316976\n",
      "Epoch 286/1000 - 5.063246s - Loss 0.124762 - Accuracy 0.765625 - Test Loss 2.099335 - Test Accuracy 0.299735\n",
      "Epoch 287/1000 - 5.054875s - Loss 0.123425 - Accuracy 0.781250 - Test Loss 1.903767 - Test Accuracy 0.294872\n",
      "Epoch 288/1000 - 5.065598s - Loss 0.125039 - Accuracy 0.776042 - Test Loss 2.050533 - Test Accuracy 0.355880\n",
      "Epoch 289/1000 - 5.061137s - Loss 0.112436 - Accuracy 0.755208 - Test Loss 1.871217 - Test Accuracy 0.312555\n",
      "Epoch 290/1000 - 5.079956s - Loss 0.150597 - Accuracy 0.734375 - Test Loss 1.921227 - Test Accuracy 0.322723\n",
      "Epoch 291/1000 - 5.079863s - Loss 0.132180 - Accuracy 0.776042 - Test Loss 1.899497 - Test Accuracy 0.307250\n",
      "Epoch 292/1000 - 5.119244s - Loss 0.153853 - Accuracy 0.671875 - Test Loss 1.923488 - Test Accuracy 0.339080\n",
      "Epoch 293/1000 - 5.087577s - Loss 0.136837 - Accuracy 0.666667 - Test Loss 1.990795 - Test Accuracy 0.301503\n",
      "Epoch 294/1000 - 5.103048s - Loss 0.120213 - Accuracy 0.791667 - Test Loss 2.197043 - Test Accuracy 0.330239\n",
      "Epoch 295/1000 - 5.074513s - Loss 0.120221 - Accuracy 0.802083 - Test Loss 2.027255 - Test Accuracy 0.321839\n",
      "Epoch 296/1000 - 5.082827s - Loss 0.108986 - Accuracy 0.729167 - Test Loss 2.043997 - Test Accuracy 0.289567\n",
      "Epoch 297/1000 - 5.064324s - Loss 0.128484 - Accuracy 0.734375 - Test Loss 1.984808 - Test Accuracy 0.318302\n",
      "Epoch 298/1000 - 5.086303s - Loss 0.134140 - Accuracy 0.729167 - Test Loss 2.004688 - Test Accuracy 0.341291\n",
      "Epoch 299/1000 - 5.065922s - Loss 0.118806 - Accuracy 0.744792 - Test Loss 2.064183 - Test Accuracy 0.323165\n",
      "Epoch 300/1000 - 5.074461s - Loss 0.165990 - Accuracy 0.677083 - Test Loss 2.145929 - Test Accuracy 0.318302\n",
      "Epoch 301/1000 - 5.070405s - Loss 0.159330 - Accuracy 0.656250 - Test Loss 2.096828 - Test Accuracy 0.318302\n",
      "Epoch 302/1000 - 5.078698s - Loss 0.139999 - Accuracy 0.776042 - Test Loss 1.954172 - Test Accuracy 0.309461\n",
      "Epoch 303/1000 - 5.087322s - Loss 0.160365 - Accuracy 0.703125 - Test Loss 1.991140 - Test Accuracy 0.307250\n",
      "Epoch 304/1000 - 5.090959s - Loss 0.129769 - Accuracy 0.723958 - Test Loss 2.179265 - Test Accuracy 0.331123\n",
      "Epoch 305/1000 - 5.085530s - Loss 0.087583 - Accuracy 0.786458 - Test Loss 2.130498 - Test Accuracy 0.311671\n",
      "Epoch 306/1000 - 5.103098s - Loss 0.142301 - Accuracy 0.703125 - Test Loss 2.255785 - Test Accuracy 0.341291\n",
      "Epoch 307/1000 - 5.084083s - Loss 0.138156 - Accuracy 0.687500 - Test Loss 1.958928 - Test Accuracy 0.282493\n",
      "Epoch 308/1000 - 5.087512s - Loss 0.170567 - Accuracy 0.682292 - Test Loss 2.185922 - Test Accuracy 0.351901\n",
      "Epoch 309/1000 - 5.074506s - Loss 0.132676 - Accuracy 0.739583 - Test Loss 2.084524 - Test Accuracy 0.306366\n",
      "Epoch 310/1000 - 5.103852s - Loss 0.140874 - Accuracy 0.682292 - Test Loss 1.955807 - Test Accuracy 0.314766\n",
      "Epoch 311/1000 - 5.093394s - Loss 0.133685 - Accuracy 0.750000 - Test Loss 1.903232 - Test Accuracy 0.302387\n",
      "Epoch 312/1000 - 5.082351s - Loss 0.133462 - Accuracy 0.697917 - Test Loss 2.076945 - Test Accuracy 0.311671\n",
      "Epoch 313/1000 - 5.106081s - Loss 0.145722 - Accuracy 0.739583 - Test Loss 2.086247 - Test Accuracy 0.305924\n",
      "Epoch 314/1000 - 5.095563s - Loss 0.107087 - Accuracy 0.781250 - Test Loss 2.117816 - Test Accuracy 0.333775\n",
      "Epoch 315/1000 - 5.100749s - Loss 0.138733 - Accuracy 0.739583 - Test Loss 2.125436 - Test Accuracy 0.322281\n",
      "Epoch 316/1000 - 5.079238s - Loss 0.139015 - Accuracy 0.718750 - Test Loss 2.007978 - Test Accuracy 0.268347\n",
      "Epoch 317/1000 - 5.132542s - Loss 0.100592 - Accuracy 0.760417 - Test Loss 2.172258 - Test Accuracy 0.329355\n",
      "Epoch 318/1000 - 5.067688s - Loss 0.119542 - Accuracy 0.770833 - Test Loss 2.078818 - Test Accuracy 0.324934\n",
      "Epoch 319/1000 - 5.101086s - Loss 0.120766 - Accuracy 0.723958 - Test Loss 2.121804 - Test Accuracy 0.324492\n",
      "Epoch 320/1000 - 5.070971s - Loss 0.076345 - Accuracy 0.812500 - Test Loss 2.183654 - Test Accuracy 0.316534\n",
      "Epoch 321/1000 - 5.106682s - Loss 0.123067 - Accuracy 0.739583 - Test Loss 2.393058 - Test Accuracy 0.343059\n",
      "Epoch 322/1000 - 5.090076s - Loss 0.152352 - Accuracy 0.713542 - Test Loss 2.107089 - Test Accuracy 0.290893\n",
      "Epoch 323/1000 - 5.107988s - Loss 0.129191 - Accuracy 0.765625 - Test Loss 2.324720 - Test Accuracy 0.356322\n",
      "Epoch 324/1000 - 5.085147s - Loss 0.120559 - Accuracy 0.755208 - Test Loss 2.118665 - Test Accuracy 0.296198\n",
      "Epoch 325/1000 - 5.098238s - Loss 0.100145 - Accuracy 0.791667 - Test Loss 2.201482 - Test Accuracy 0.307250\n",
      "Epoch 326/1000 - 5.075520s - Loss 0.126704 - Accuracy 0.781250 - Test Loss 2.069442 - Test Accuracy 0.308576\n",
      "Epoch 327/1000 - 5.109177s - Loss 0.156290 - Accuracy 0.708333 - Test Loss 2.193445 - Test Accuracy 0.304598\n",
      "Epoch 328/1000 - 5.074899s - Loss 0.128305 - Accuracy 0.760417 - Test Loss 2.112317 - Test Accuracy 0.312113\n",
      "Epoch 329/1000 - 5.086004s - Loss 0.091755 - Accuracy 0.791667 - Test Loss 2.199330 - Test Accuracy 0.305482\n",
      "Epoch 330/1000 - 5.074522s - Loss 0.122769 - Accuracy 0.776042 - Test Loss 2.471801 - Test Accuracy 0.340849\n",
      "Epoch 331/1000 - 5.089236s - Loss 0.116831 - Accuracy 0.692708 - Test Loss 2.220262 - Test Accuracy 0.311671\n",
      "Epoch 332/1000 - 5.073624s - Loss 0.175466 - Accuracy 0.677083 - Test Loss 2.310440 - Test Accuracy 0.305924\n",
      "Epoch 333/1000 - 5.119301s - Loss 0.191618 - Accuracy 0.651042 - Test Loss 2.288113 - Test Accuracy 0.299293\n",
      "Epoch 334/1000 - 5.083959s - Loss 0.270226 - Accuracy 0.614583 - Test Loss 3.401667 - Test Accuracy 0.322281\n",
      "Epoch 335/1000 - 5.103772s - Loss 0.307422 - Accuracy 0.572917 - Test Loss 2.072024 - Test Accuracy 0.225464\n",
      "Epoch 336/1000 - 5.100658s - Loss 0.171965 - Accuracy 0.682292 - Test Loss 2.154685 - Test Accuracy 0.257737\n",
      "Epoch 337/1000 - 5.081193s - Loss 0.219670 - Accuracy 0.619792 - Test Loss 1.999913 - Test Accuracy 0.269673\n",
      "Epoch 338/1000 - 5.079334s - Loss 0.179703 - Accuracy 0.687500 - Test Loss 2.257324 - Test Accuracy 0.297966\n",
      "Epoch 339/1000 - 5.082917s - Loss 0.187531 - Accuracy 0.661458 - Test Loss 2.381771 - Test Accuracy 0.311229\n",
      "Epoch 340/1000 - 5.074102s - Loss 0.125557 - Accuracy 0.765625 - Test Loss 2.223284 - Test Accuracy 0.263926\n",
      "Epoch 341/1000 - 5.082790s - Loss 0.139938 - Accuracy 0.703125 - Test Loss 2.242214 - Test Accuracy 0.263926\n",
      "Epoch 342/1000 - 5.072244s - Loss 0.186660 - Accuracy 0.677083 - Test Loss 2.263292 - Test Accuracy 0.305924\n",
      "Epoch 343/1000 - 5.097939s - Loss 0.175244 - Accuracy 0.614583 - Test Loss 2.346962 - Test Accuracy 0.330681\n",
      "Epoch 344/1000 - 5.071218s - Loss 0.137200 - Accuracy 0.692708 - Test Loss 2.082154 - Test Accuracy 0.282493\n",
      "Epoch 345/1000 - 5.107781s - Loss 0.125866 - Accuracy 0.729167 - Test Loss 2.100307 - Test Accuracy 0.309461\n",
      "Epoch 346/1000 - 5.055151s - Loss 0.130508 - Accuracy 0.781250 - Test Loss 2.303755 - Test Accuracy 0.292661\n",
      "Epoch 347/1000 - 5.073093s - Loss 0.190310 - Accuracy 0.661458 - Test Loss 2.437013 - Test Accuracy 0.333775\n",
      "Epoch 348/1000 - 5.088752s - Loss 0.129522 - Accuracy 0.755208 - Test Loss 2.269184 - Test Accuracy 0.323607\n",
      "Epoch 349/1000 - 5.095068s - Loss 0.137013 - Accuracy 0.723958 - Test Loss 2.261977 - Test Accuracy 0.291777\n",
      "Epoch 350/1000 - 5.068765s - Loss 0.100766 - Accuracy 0.750000 - Test Loss 2.027657 - Test Accuracy 0.251105\n",
      "Epoch 351/1000 - 5.108136s - Loss 0.108386 - Accuracy 0.781250 - Test Loss 1.936741 - Test Accuracy 0.239169\n",
      "Epoch 352/1000 - 5.066859s - Loss 0.117500 - Accuracy 0.802083 - Test Loss 2.145705 - Test Accuracy 0.310345\n",
      "Epoch 353/1000 - 5.085477s - Loss 0.139029 - Accuracy 0.734375 - Test Loss 2.251759 - Test Accuracy 0.289567\n",
      "Epoch 354/1000 - 5.056164s - Loss 0.133772 - Accuracy 0.729167 - Test Loss 2.202132 - Test Accuracy 0.300177\n",
      "Epoch 355/1000 - 5.074808s - Loss 0.171179 - Accuracy 0.687500 - Test Loss 2.226855 - Test Accuracy 0.325818\n",
      "Epoch 356/1000 - 5.079315s - Loss 0.185498 - Accuracy 0.677083 - Test Loss 2.119247 - Test Accuracy 0.276746\n",
      "Epoch 357/1000 - 5.065464s - Loss 0.163141 - Accuracy 0.692708 - Test Loss 2.217160 - Test Accuracy 0.331123\n",
      "Epoch 358/1000 - 5.093197s - Loss 0.115423 - Accuracy 0.718750 - Test Loss 2.056536 - Test Accuracy 0.263926\n",
      "Epoch 359/1000 - 5.122961s - Loss 0.111760 - Accuracy 0.744792 - Test Loss 2.318033 - Test Accuracy 0.311671\n",
      "Epoch 360/1000 - 5.088548s - Loss 0.150703 - Accuracy 0.645833 - Test Loss 2.317155 - Test Accuracy 0.331123\n",
      "Epoch 361/1000 - 5.104718s - Loss 0.149308 - Accuracy 0.687500 - Test Loss 2.261682 - Test Accuracy 0.300619\n",
      "Epoch 362/1000 - 5.095776s - Loss 0.145087 - Accuracy 0.734375 - Test Loss 2.226081 - Test Accuracy 0.297524\n",
      "Epoch 363/1000 - 5.085601s - Loss 0.162983 - Accuracy 0.656250 - Test Loss 2.431658 - Test Accuracy 0.329355\n",
      "Epoch 364/1000 - 5.054530s - Loss 0.146869 - Accuracy 0.718750 - Test Loss 2.319159 - Test Accuracy 0.318744\n",
      "Epoch 365/1000 - 5.080563s - Loss 0.104621 - Accuracy 0.770833 - Test Loss 2.419020 - Test Accuracy 0.363395\n",
      "Epoch 366/1000 - 5.084776s - Loss 0.120228 - Accuracy 0.734375 - Test Loss 2.165949 - Test Accuracy 0.287356\n",
      "Epoch 367/1000 - 5.085533s - Loss 0.152150 - Accuracy 0.677083 - Test Loss 2.369292 - Test Accuracy 0.335986\n",
      "Epoch 368/1000 - 5.059066s - Loss 0.167609 - Accuracy 0.677083 - Test Loss 2.303418 - Test Accuracy 0.330681\n",
      "Epoch 369/1000 - 5.106741s - Loss 0.226731 - Accuracy 0.713542 - Test Loss 2.285049 - Test Accuracy 0.310787\n",
      "Epoch 370/1000 - 5.081459s - Loss 0.129120 - Accuracy 0.744792 - Test Loss 1.982273 - Test Accuracy 0.300177\n",
      "Epoch 371/1000 - 5.058999s - Loss 0.126444 - Accuracy 0.734375 - Test Loss 2.128014 - Test Accuracy 0.317860\n",
      "Epoch 372/1000 - 5.094523s - Loss 0.152350 - Accuracy 0.729167 - Test Loss 2.098767 - Test Accuracy 0.309019\n",
      "Epoch 373/1000 - 5.071889s - Loss 0.117436 - Accuracy 0.739583 - Test Loss 2.156671 - Test Accuracy 0.285588\n",
      "Epoch 374/1000 - 5.099629s - Loss 0.096435 - Accuracy 0.822917 - Test Loss 2.130709 - Test Accuracy 0.324492\n",
      "Epoch 375/1000 - 5.077878s - Loss 0.117984 - Accuracy 0.755208 - Test Loss 1.996501 - Test Accuracy 0.301061\n",
      "Epoch 376/1000 - 5.091047s - Loss 0.124279 - Accuracy 0.786458 - Test Loss 2.293298 - Test Accuracy 0.320071\n",
      "Epoch 377/1000 - 5.054279s - Loss 0.117894 - Accuracy 0.786458 - Test Loss 2.081562 - Test Accuracy 0.286030\n",
      "Epoch 378/1000 - 5.084500s - Loss 0.155937 - Accuracy 0.760417 - Test Loss 2.344929 - Test Accuracy 0.350133\n",
      "Epoch 379/1000 - 5.063477s - Loss 0.177838 - Accuracy 0.640625 - Test Loss 2.412929 - Test Accuracy 0.319629\n",
      "Epoch 380/1000 - 5.124741s - Loss 0.151566 - Accuracy 0.697917 - Test Loss 2.254810 - Test Accuracy 0.327586\n",
      "Epoch 381/1000 - 5.076511s - Loss 0.122127 - Accuracy 0.708333 - Test Loss 2.242867 - Test Accuracy 0.306366\n",
      "Epoch 382/1000 - 5.090766s - Loss 0.104844 - Accuracy 0.770833 - Test Loss 2.286614 - Test Accuracy 0.296198\n",
      "Epoch 383/1000 - 5.070075s - Loss 0.124038 - Accuracy 0.734375 - Test Loss 2.453360 - Test Accuracy 0.334218\n",
      "Epoch 384/1000 - 5.115981s - Loss 0.137464 - Accuracy 0.734375 - Test Loss 2.346172 - Test Accuracy 0.308134\n",
      "Epoch 385/1000 - 5.089135s - Loss 0.140846 - Accuracy 0.734375 - Test Loss 2.174866 - Test Accuracy 0.291777\n",
      "Epoch 386/1000 - 5.091176s - Loss 0.095532 - Accuracy 0.791667 - Test Loss 2.423050 - Test Accuracy 0.332449\n",
      "Epoch 387/1000 - 5.076236s - Loss 0.127414 - Accuracy 0.750000 - Test Loss 2.318958 - Test Accuracy 0.300619\n",
      "Epoch 388/1000 - 5.086451s - Loss 0.102177 - Accuracy 0.802083 - Test Loss 2.244130 - Test Accuracy 0.303271\n",
      "Epoch 389/1000 - 5.076440s - Loss 0.098313 - Accuracy 0.744792 - Test Loss 2.258628 - Test Accuracy 0.310787\n",
      "Epoch 390/1000 - 5.089387s - Loss 0.113047 - Accuracy 0.781250 - Test Loss 2.077589 - Test Accuracy 0.287798\n",
      "Epoch 391/1000 - 5.056915s - Loss 0.147308 - Accuracy 0.729167 - Test Loss 2.156871 - Test Accuracy 0.318744\n",
      "Epoch 392/1000 - 5.060265s - Loss 0.137059 - Accuracy 0.677083 - Test Loss 2.210402 - Test Accuracy 0.313439\n",
      "Epoch 393/1000 - 5.045903s - Loss 0.132124 - Accuracy 0.734375 - Test Loss 2.177355 - Test Accuracy 0.286472\n",
      "Epoch 394/1000 - 5.078594s - Loss 0.148122 - Accuracy 0.703125 - Test Loss 2.310300 - Test Accuracy 0.323607\n",
      "Epoch 395/1000 - 5.058813s - Loss 0.137741 - Accuracy 0.708333 - Test Loss 2.207851 - Test Accuracy 0.301503\n",
      "Epoch 396/1000 - 5.092194s - Loss 0.111075 - Accuracy 0.744792 - Test Loss 2.338270 - Test Accuracy 0.324934\n",
      "Epoch 397/1000 - 5.059875s - Loss 0.131006 - Accuracy 0.739583 - Test Loss 2.204217 - Test Accuracy 0.282493\n",
      "Epoch 398/1000 - 5.088924s - Loss 0.133144 - Accuracy 0.770833 - Test Loss 2.337292 - Test Accuracy 0.305040\n",
      "Epoch 399/1000 - 5.073925s - Loss 0.124963 - Accuracy 0.713542 - Test Loss 2.341663 - Test Accuracy 0.322281\n",
      "Epoch 400/1000 - 5.073587s - Loss 0.087299 - Accuracy 0.791667 - Test Loss 2.336339 - Test Accuracy 0.315650\n",
      "Epoch 401/1000 - 5.063614s - Loss 0.096961 - Accuracy 0.791667 - Test Loss 2.208108 - Test Accuracy 0.285146\n",
      "Epoch 402/1000 - 5.104561s - Loss 0.097783 - Accuracy 0.828125 - Test Loss 1.995944 - Test Accuracy 0.282493\n",
      "Epoch 403/1000 - 5.076890s - Loss 0.155074 - Accuracy 0.703125 - Test Loss 2.338717 - Test Accuracy 0.328912\n",
      "Epoch 404/1000 - 5.105347s - Loss 0.159112 - Accuracy 0.760417 - Test Loss 2.224018 - Test Accuracy 0.310787\n",
      "Epoch 405/1000 - 5.077836s - Loss 0.132491 - Accuracy 0.718750 - Test Loss 2.200755 - Test Accuracy 0.323165\n",
      "Epoch 406/1000 - 5.101302s - Loss 0.137933 - Accuracy 0.708333 - Test Loss 2.230207 - Test Accuracy 0.303271\n",
      "Epoch 407/1000 - 5.072514s - Loss 0.103521 - Accuracy 0.729167 - Test Loss 2.512516 - Test Accuracy 0.330681\n",
      "Epoch 408/1000 - 5.077272s - Loss 0.127357 - Accuracy 0.765625 - Test Loss 2.278286 - Test Accuracy 0.282493\n",
      "Epoch 409/1000 - 5.070512s - Loss 0.116622 - Accuracy 0.760417 - Test Loss 2.095687 - Test Accuracy 0.244916\n",
      "Epoch 410/1000 - 5.081122s - Loss 0.085171 - Accuracy 0.833333 - Test Loss 2.241403 - Test Accuracy 0.316092\n",
      "Epoch 411/1000 - 5.068607s - Loss 0.124006 - Accuracy 0.786458 - Test Loss 2.541805 - Test Accuracy 0.362953\n",
      "Epoch 412/1000 - 5.081143s - Loss 0.127068 - Accuracy 0.708333 - Test Loss 2.318463 - Test Accuracy 0.310787\n",
      "Epoch 413/1000 - 5.083431s - Loss 0.098317 - Accuracy 0.786458 - Test Loss 2.286485 - Test Accuracy 0.305924\n",
      "Epoch 414/1000 - 5.099305s - Loss 0.090546 - Accuracy 0.833333 - Test Loss 2.513694 - Test Accuracy 0.312555\n",
      "Epoch 415/1000 - 5.066395s - Loss 0.093183 - Accuracy 0.786458 - Test Loss 2.329371 - Test Accuracy 0.303714\n",
      "Epoch 416/1000 - 5.076924s - Loss 0.123559 - Accuracy 0.812500 - Test Loss 2.452053 - Test Accuracy 0.293988\n",
      "Epoch 417/1000 - 5.058350s - Loss 0.157443 - Accuracy 0.755208 - Test Loss 2.376921 - Test Accuracy 0.311671\n",
      "Epoch 418/1000 - 5.066323s - Loss 0.141247 - Accuracy 0.739583 - Test Loss 2.429264 - Test Accuracy 0.326260\n",
      "Epoch 419/1000 - 5.077332s - Loss 0.099116 - Accuracy 0.802083 - Test Loss 2.315920 - Test Accuracy 0.312113\n",
      "Epoch 420/1000 - 5.101590s - Loss 0.113842 - Accuracy 0.765625 - Test Loss 2.308072 - Test Accuracy 0.301503\n",
      "Epoch 421/1000 - 5.062986s - Loss 0.111114 - Accuracy 0.770833 - Test Loss 2.518500 - Test Accuracy 0.344828\n",
      "Epoch 422/1000 - 5.102588s - Loss 0.136324 - Accuracy 0.744792 - Test Loss 2.381158 - Test Accuracy 0.328470\n",
      "Epoch 423/1000 - 5.082883s - Loss 0.112878 - Accuracy 0.744792 - Test Loss 2.166116 - Test Accuracy 0.290009\n",
      "Epoch 424/1000 - 5.082697s - Loss 0.089294 - Accuracy 0.796875 - Test Loss 2.364551 - Test Accuracy 0.340849\n",
      "Epoch 425/1000 - 5.060751s - Loss 0.119019 - Accuracy 0.833333 - Test Loss 2.256930 - Test Accuracy 0.307692\n",
      "Epoch 426/1000 - 5.069504s - Loss 0.080402 - Accuracy 0.817708 - Test Loss 2.464053 - Test Accuracy 0.310787\n",
      "Epoch 427/1000 - 5.083267s - Loss 0.127207 - Accuracy 0.750000 - Test Loss 2.192673 - Test Accuracy 0.320071\n",
      "Epoch 428/1000 - 5.077968s - Loss 0.072842 - Accuracy 0.812500 - Test Loss 2.497919 - Test Accuracy 0.338196\n",
      "Epoch 429/1000 - 5.064002s - Loss 0.157204 - Accuracy 0.760417 - Test Loss 2.411958 - Test Accuracy 0.332007\n",
      "Epoch 430/1000 - 5.100927s - Loss 0.176763 - Accuracy 0.697917 - Test Loss 2.172217 - Test Accuracy 0.284704\n",
      "Epoch 431/1000 - 5.067060s - Loss 0.132269 - Accuracy 0.739583 - Test Loss 2.394935 - Test Accuracy 0.295756\n",
      "Epoch 432/1000 - 5.068950s - Loss 0.177829 - Accuracy 0.656250 - Test Loss 2.224789 - Test Accuracy 0.282051\n",
      "Epoch 433/1000 - 5.106658s - Loss 0.125068 - Accuracy 0.734375 - Test Loss 2.672524 - Test Accuracy 0.354553\n",
      "Epoch 434/1000 - 5.087681s - Loss 0.136392 - Accuracy 0.734375 - Test Loss 2.210666 - Test Accuracy 0.289567\n",
      "Epoch 435/1000 - 5.069897s - Loss 0.177314 - Accuracy 0.703125 - Test Loss 2.420389 - Test Accuracy 0.282935\n",
      "Epoch 436/1000 - 5.073377s - Loss 0.107209 - Accuracy 0.760417 - Test Loss 2.250135 - Test Accuracy 0.304598\n",
      "Epoch 437/1000 - 5.086124s - Loss 0.135182 - Accuracy 0.750000 - Test Loss 2.362692 - Test Accuracy 0.326260\n",
      "Epoch 438/1000 - 5.086961s - Loss 0.125329 - Accuracy 0.776042 - Test Loss 2.322254 - Test Accuracy 0.316534\n",
      "Epoch 439/1000 - 5.104378s - Loss 0.122574 - Accuracy 0.770833 - Test Loss 2.385875 - Test Accuracy 0.303714\n",
      "Epoch 440/1000 - 5.082690s - Loss 0.107161 - Accuracy 0.791667 - Test Loss 2.109181 - Test Accuracy 0.271883\n",
      "Epoch 441/1000 - 5.090934s - Loss 0.129333 - Accuracy 0.786458 - Test Loss 2.275383 - Test Accuracy 0.309461\n",
      "Epoch 442/1000 - 5.064685s - Loss 0.140421 - Accuracy 0.713542 - Test Loss 2.400732 - Test Accuracy 0.307250\n",
      "Epoch 443/1000 - 5.093834s - Loss 0.116018 - Accuracy 0.744792 - Test Loss 2.325969 - Test Accuracy 0.293988\n",
      "Epoch 444/1000 - 5.071926s - Loss 0.132459 - Accuracy 0.744792 - Test Loss 2.423988 - Test Accuracy 0.294430\n",
      "Epoch 445/1000 - 5.098389s - Loss 0.069710 - Accuracy 0.854167 - Test Loss 2.321050 - Test Accuracy 0.261273\n",
      "Epoch 446/1000 - 5.082764s - Loss 0.163250 - Accuracy 0.687500 - Test Loss 2.213475 - Test Accuracy 0.238727\n",
      "Epoch 447/1000 - 5.080250s - Loss 0.110190 - Accuracy 0.796875 - Test Loss 2.587267 - Test Accuracy 0.304156\n",
      "Epoch 448/1000 - 5.085485s - Loss 0.141663 - Accuracy 0.703125 - Test Loss 2.415435 - Test Accuracy 0.280283\n",
      "Epoch 449/1000 - 5.097157s - Loss 0.121284 - Accuracy 0.765625 - Test Loss 2.324043 - Test Accuracy 0.286914\n",
      "Epoch 450/1000 - 5.068215s - Loss 0.110458 - Accuracy 0.807292 - Test Loss 2.421011 - Test Accuracy 0.325376\n",
      "Epoch 451/1000 - 5.093120s - Loss 0.130583 - Accuracy 0.755208 - Test Loss 2.365807 - Test Accuracy 0.298851\n",
      "Epoch 452/1000 - 5.070383s - Loss 0.149508 - Accuracy 0.703125 - Test Loss 2.490813 - Test Accuracy 0.321839\n",
      "Epoch 453/1000 - 5.078308s - Loss 0.127061 - Accuracy 0.750000 - Test Loss 2.406900 - Test Accuracy 0.293103\n",
      "Epoch 454/1000 - 5.056231s - Loss 0.139359 - Accuracy 0.739583 - Test Loss 2.359585 - Test Accuracy 0.269231\n",
      "Epoch 455/1000 - 5.082436s - Loss 0.101476 - Accuracy 0.781250 - Test Loss 2.426422 - Test Accuracy 0.304598\n",
      "Epoch 456/1000 - 5.083621s - Loss 0.115374 - Accuracy 0.755208 - Test Loss 2.397081 - Test Accuracy 0.314766\n",
      "Epoch 457/1000 - 5.087142s - Loss 0.095647 - Accuracy 0.739583 - Test Loss 2.470017 - Test Accuracy 0.345712\n",
      "Epoch 458/1000 - 5.067874s - Loss 0.121895 - Accuracy 0.786458 - Test Loss 2.349956 - Test Accuracy 0.300177\n",
      "Epoch 459/1000 - 5.086018s - Loss 0.100017 - Accuracy 0.765625 - Test Loss 2.421923 - Test Accuracy 0.312113\n",
      "Epoch 460/1000 - 5.080288s - Loss 0.101233 - Accuracy 0.807292 - Test Loss 2.304647 - Test Accuracy 0.301061\n",
      "Epoch 461/1000 - 5.094002s - Loss 0.151549 - Accuracy 0.666667 - Test Loss 2.443307 - Test Accuracy 0.283820\n",
      "Epoch 462/1000 - 5.087188s - Loss 0.122036 - Accuracy 0.729167 - Test Loss 2.230041 - Test Accuracy 0.231653\n",
      "Epoch 463/1000 - 5.079788s - Loss 0.169576 - Accuracy 0.687500 - Test Loss 2.572869 - Test Accuracy 0.314766\n",
      "Epoch 464/1000 - 5.088244s - Loss 0.139465 - Accuracy 0.744792 - Test Loss 2.372916 - Test Accuracy 0.250663\n",
      "Epoch 465/1000 - 5.142568s - Loss 0.129365 - Accuracy 0.750000 - Test Loss 2.423143 - Test Accuracy 0.312997\n",
      "Epoch 466/1000 - 5.064089s - Loss 0.126808 - Accuracy 0.729167 - Test Loss 2.182701 - Test Accuracy 0.292219\n",
      "Epoch 467/1000 - 5.100663s - Loss 0.109312 - Accuracy 0.739583 - Test Loss 2.388723 - Test Accuracy 0.287798\n",
      "Epoch 468/1000 - 5.101640s - Loss 0.133880 - Accuracy 0.781250 - Test Loss 2.352418 - Test Accuracy 0.279399\n",
      "Epoch 469/1000 - 5.103042s - Loss 0.103484 - Accuracy 0.765625 - Test Loss 2.310128 - Test Accuracy 0.289567\n",
      "Epoch 470/1000 - 5.077357s - Loss 0.140997 - Accuracy 0.744792 - Test Loss 2.555054 - Test Accuracy 0.315208\n",
      "Epoch 471/1000 - 5.080184s - Loss 0.095246 - Accuracy 0.812500 - Test Loss 2.427369 - Test Accuracy 0.307692\n",
      "Epoch 472/1000 - 5.077892s - Loss 0.128427 - Accuracy 0.739583 - Test Loss 2.255553 - Test Accuracy 0.287798\n",
      "Epoch 473/1000 - 5.085495s - Loss 0.125825 - Accuracy 0.739583 - Test Loss 2.506699 - Test Accuracy 0.331123\n",
      "Epoch 474/1000 - 5.073121s - Loss 0.146304 - Accuracy 0.729167 - Test Loss 2.264542 - Test Accuracy 0.245358\n",
      "Epoch 475/1000 - 5.071096s - Loss 0.112147 - Accuracy 0.755208 - Test Loss 2.516103 - Test Accuracy 0.292219\n",
      "Epoch 476/1000 - 5.085129s - Loss 0.156467 - Accuracy 0.734375 - Test Loss 2.510198 - Test Accuracy 0.329355\n",
      "Epoch 477/1000 - 5.082172s - Loss 0.086679 - Accuracy 0.791667 - Test Loss 2.360833 - Test Accuracy 0.297082\n",
      "Epoch 478/1000 - 5.062427s - Loss 0.135034 - Accuracy 0.718750 - Test Loss 2.476803 - Test Accuracy 0.302829\n",
      "Epoch 479/1000 - 5.087034s - Loss 0.125736 - Accuracy 0.734375 - Test Loss 2.225233 - Test Accuracy 0.261273\n",
      "Epoch 480/1000 - 5.069558s - Loss 0.139316 - Accuracy 0.755208 - Test Loss 2.284058 - Test Accuracy 0.304156\n",
      "Epoch 481/1000 - 5.085272s - Loss 0.107127 - Accuracy 0.765625 - Test Loss 2.470653 - Test Accuracy 0.326260\n",
      "Epoch 482/1000 - 5.062825s - Loss 0.110328 - Accuracy 0.765625 - Test Loss 2.354472 - Test Accuracy 0.321839\n",
      "Epoch 483/1000 - 5.086926s - Loss 0.124622 - Accuracy 0.770833 - Test Loss 2.409482 - Test Accuracy 0.285588\n",
      "Epoch 484/1000 - 5.078354s - Loss 0.107850 - Accuracy 0.776042 - Test Loss 2.359919 - Test Accuracy 0.281167\n",
      "Epoch 485/1000 - 5.099079s - Loss 0.103764 - Accuracy 0.786458 - Test Loss 2.260947 - Test Accuracy 0.264810\n",
      "Epoch 486/1000 - 5.061662s - Loss 0.125395 - Accuracy 0.770833 - Test Loss 2.628340 - Test Accuracy 0.301503\n",
      "Epoch 487/1000 - 5.081343s - Loss 0.139395 - Accuracy 0.739583 - Test Loss 2.367903 - Test Accuracy 0.295314\n",
      "Epoch 488/1000 - 5.058590s - Loss 0.105154 - Accuracy 0.817708 - Test Loss 2.404003 - Test Accuracy 0.297082\n",
      "Epoch 489/1000 - 5.087853s - Loss 0.159886 - Accuracy 0.729167 - Test Loss 2.354059 - Test Accuracy 0.302387\n",
      "Epoch 490/1000 - 5.064157s - Loss 0.087105 - Accuracy 0.812500 - Test Loss 2.300856 - Test Accuracy 0.275420\n",
      "Epoch 491/1000 - 5.087139s - Loss 0.160219 - Accuracy 0.703125 - Test Loss 2.483652 - Test Accuracy 0.293546\n",
      "Epoch 492/1000 - 5.084454s - Loss 0.096205 - Accuracy 0.828125 - Test Loss 2.453059 - Test Accuracy 0.309903\n",
      "Epoch 493/1000 - 5.086978s - Loss 0.117222 - Accuracy 0.755208 - Test Loss 2.446492 - Test Accuracy 0.279841\n",
      "Epoch 494/1000 - 5.100507s - Loss 0.124619 - Accuracy 0.781250 - Test Loss 2.312395 - Test Accuracy 0.272767\n",
      "Epoch 495/1000 - 5.084493s - Loss 0.102735 - Accuracy 0.786458 - Test Loss 2.389474 - Test Accuracy 0.314766\n",
      "Epoch 496/1000 - 5.099017s - Loss 0.122918 - Accuracy 0.781250 - Test Loss 2.375288 - Test Accuracy 0.306366\n",
      "Epoch 497/1000 - 5.077552s - Loss 0.094322 - Accuracy 0.791667 - Test Loss 2.486810 - Test Accuracy 0.284704\n",
      "Epoch 498/1000 - 5.104047s - Loss 0.089620 - Accuracy 0.817708 - Test Loss 2.406095 - Test Accuracy 0.310345\n",
      "Epoch 499/1000 - 5.072892s - Loss 0.164757 - Accuracy 0.776042 - Test Loss 2.398428 - Test Accuracy 0.310345\n",
      "Epoch 500/1000 - 5.096119s - Loss 0.126022 - Accuracy 0.729167 - Test Loss 2.647672 - Test Accuracy 0.342617\n",
      "Epoch 501/1000 - 5.066413s - Loss 0.162606 - Accuracy 0.734375 - Test Loss 2.558383 - Test Accuracy 0.312113\n",
      "Epoch 502/1000 - 5.088524s - Loss 0.140013 - Accuracy 0.776042 - Test Loss 2.483905 - Test Accuracy 0.283378\n",
      "Epoch 503/1000 - 5.062035s - Loss 0.117051 - Accuracy 0.734375 - Test Loss 2.467390 - Test Accuracy 0.319629\n",
      "Epoch 504/1000 - 5.061099s - Loss 0.122418 - Accuracy 0.739583 - Test Loss 2.412149 - Test Accuracy 0.302829\n",
      "Epoch 505/1000 - 5.083847s - Loss 0.165074 - Accuracy 0.776042 - Test Loss 2.531623 - Test Accuracy 0.282493\n",
      "Epoch 506/1000 - 5.091456s - Loss 0.121220 - Accuracy 0.791667 - Test Loss 2.527386 - Test Accuracy 0.286472\n",
      "Epoch 507/1000 - 5.093890s - Loss 0.106716 - Accuracy 0.776042 - Test Loss 2.491170 - Test Accuracy 0.296640\n",
      "Epoch 508/1000 - 5.083982s - Loss 0.104941 - Accuracy 0.781250 - Test Loss 2.331553 - Test Accuracy 0.288240\n",
      "Epoch 509/1000 - 5.067675s - Loss 0.127860 - Accuracy 0.760417 - Test Loss 2.519145 - Test Accuracy 0.277188\n",
      "Epoch 510/1000 - 5.089098s - Loss 0.107782 - Accuracy 0.697917 - Test Loss 2.131198 - Test Accuracy 0.250663\n",
      "Epoch 511/1000 - 5.071133s - Loss 0.126563 - Accuracy 0.750000 - Test Loss 2.534406 - Test Accuracy 0.293988\n",
      "Epoch 512/1000 - 5.067644s - Loss 0.110519 - Accuracy 0.802083 - Test Loss 2.335547 - Test Accuracy 0.259505\n",
      "Epoch 513/1000 - 5.071237s - Loss 0.129736 - Accuracy 0.760417 - Test Loss 2.526353 - Test Accuracy 0.297524\n",
      "Epoch 514/1000 - 5.093547s - Loss 0.118492 - Accuracy 0.734375 - Test Loss 2.438843 - Test Accuracy 0.293546\n",
      "Epoch 515/1000 - 5.068577s - Loss 0.103870 - Accuracy 0.817708 - Test Loss 2.348047 - Test Accuracy 0.246242\n",
      "Epoch 516/1000 - 5.086852s - Loss 0.137294 - Accuracy 0.755208 - Test Loss 2.268675 - Test Accuracy 0.266136\n",
      "Epoch 517/1000 - 5.075744s - Loss 0.071323 - Accuracy 0.864583 - Test Loss 2.359611 - Test Accuracy 0.297966\n",
      "Epoch 518/1000 - 5.085514s - Loss 0.146068 - Accuracy 0.692708 - Test Loss 2.337742 - Test Accuracy 0.312113\n",
      "Epoch 519/1000 - 5.083970s - Loss 0.120462 - Accuracy 0.744792 - Test Loss 2.367859 - Test Accuracy 0.290451\n",
      "Epoch 520/1000 - 5.081952s - Loss 0.082145 - Accuracy 0.812500 - Test Loss 2.442917 - Test Accuracy 0.291335\n",
      "Epoch 521/1000 - 5.064051s - Loss 0.088340 - Accuracy 0.807292 - Test Loss 2.322546 - Test Accuracy 0.292219\n",
      "Epoch 522/1000 - 5.115951s - Loss 0.146678 - Accuracy 0.697917 - Test Loss 2.359307 - Test Accuracy 0.300619\n",
      "Epoch 523/1000 - 5.068794s - Loss 0.142673 - Accuracy 0.697917 - Test Loss 2.301289 - Test Accuracy 0.283378\n",
      "Epoch 524/1000 - 5.083866s - Loss 0.153574 - Accuracy 0.760417 - Test Loss 2.307237 - Test Accuracy 0.300619\n",
      "Epoch 525/1000 - 5.064500s - Loss 0.121074 - Accuracy 0.703125 - Test Loss 2.401151 - Test Accuracy 0.225022\n",
      "Epoch 526/1000 - 5.115897s - Loss 0.115540 - Accuracy 0.786458 - Test Loss 2.690925 - Test Accuracy 0.368700\n",
      "Epoch 527/1000 - 5.061134s - Loss 0.300878 - Accuracy 0.625000 - Test Loss 3.940534 - Test Accuracy 0.338196\n",
      "Epoch 528/1000 - 5.101683s - Loss 0.210489 - Accuracy 0.687500 - Test Loss 2.098436 - Test Accuracy 0.234748\n",
      "Epoch 529/1000 - 5.088630s - Loss 0.240758 - Accuracy 0.598958 - Test Loss 2.656288 - Test Accuracy 0.309019\n",
      "Epoch 530/1000 - 5.102776s - Loss 0.146622 - Accuracy 0.729167 - Test Loss 2.663615 - Test Accuracy 0.298408\n",
      "Epoch 531/1000 - 5.069949s - Loss 0.211084 - Accuracy 0.656250 - Test Loss 2.471180 - Test Accuracy 0.259947\n",
      "Epoch 532/1000 - 5.071716s - Loss 0.125463 - Accuracy 0.729167 - Test Loss 2.333662 - Test Accuracy 0.256410\n",
      "Epoch 533/1000 - 5.077884s - Loss 0.106012 - Accuracy 0.765625 - Test Loss 2.174461 - Test Accuracy 0.286472\n",
      "Epoch 534/1000 - 5.080237s - Loss 0.120063 - Accuracy 0.770833 - Test Loss 2.569322 - Test Accuracy 0.308134\n",
      "Epoch 535/1000 - 5.073609s - Loss 0.118551 - Accuracy 0.703125 - Test Loss 2.768739 - Test Accuracy 0.309019\n",
      "Epoch 536/1000 - 5.064471s - Loss 0.139433 - Accuracy 0.687500 - Test Loss 2.345315 - Test Accuracy 0.301945\n",
      "Epoch 537/1000 - 5.058445s - Loss 0.113039 - Accuracy 0.765625 - Test Loss 2.654365 - Test Accuracy 0.311229\n",
      "Epoch 538/1000 - 5.069718s - Loss 0.082511 - Accuracy 0.791667 - Test Loss 2.364577 - Test Accuracy 0.267020\n",
      "Epoch 539/1000 - 5.069455s - Loss 0.113697 - Accuracy 0.729167 - Test Loss 2.531867 - Test Accuracy 0.272767\n",
      "Epoch 540/1000 - 5.097062s - Loss 0.084811 - Accuracy 0.822917 - Test Loss 2.574113 - Test Accuracy 0.295314\n",
      "Epoch 541/1000 - 5.054766s - Loss 0.110201 - Accuracy 0.781250 - Test Loss 2.517011 - Test Accuracy 0.262599\n",
      "Epoch 542/1000 - 5.095379s - Loss 0.117339 - Accuracy 0.770833 - Test Loss 2.568043 - Test Accuracy 0.287798\n",
      "Epoch 543/1000 - 5.088161s - Loss 0.111180 - Accuracy 0.781250 - Test Loss 2.495356 - Test Accuracy 0.293546\n",
      "Epoch 544/1000 - 5.089930s - Loss 0.057748 - Accuracy 0.854167 - Test Loss 2.444920 - Test Accuracy 0.303714\n",
      "Epoch 545/1000 - 5.068541s - Loss 0.084602 - Accuracy 0.880208 - Test Loss 2.604444 - Test Accuracy 0.327144\n",
      "Epoch 546/1000 - 5.093190s - Loss 0.112229 - Accuracy 0.760417 - Test Loss 2.513936 - Test Accuracy 0.297082\n",
      "Epoch 547/1000 - 5.069394s - Loss 0.116915 - Accuracy 0.807292 - Test Loss 2.761504 - Test Accuracy 0.315208\n",
      "Epoch 548/1000 - 5.063957s - Loss 0.109265 - Accuracy 0.776042 - Test Loss 2.534466 - Test Accuracy 0.280725\n",
      "Epoch 549/1000 - 5.056634s - Loss 0.099382 - Accuracy 0.781250 - Test Loss 2.673390 - Test Accuracy 0.304598\n",
      "Epoch 550/1000 - 5.079674s - Loss 0.097906 - Accuracy 0.822917 - Test Loss 2.613728 - Test Accuracy 0.306366\n",
      "Epoch 551/1000 - 5.059943s - Loss 0.116681 - Accuracy 0.802083 - Test Loss 2.710801 - Test Accuracy 0.310787\n",
      "Epoch 552/1000 - 5.083354s - Loss 0.159584 - Accuracy 0.687500 - Test Loss 2.551838 - Test Accuracy 0.305040\n",
      "Epoch 553/1000 - 5.060719s - Loss 0.100789 - Accuracy 0.822917 - Test Loss 2.676627 - Test Accuracy 0.304598\n",
      "Epoch 554/1000 - 5.098458s - Loss 0.091621 - Accuracy 0.822917 - Test Loss 2.583376 - Test Accuracy 0.308576\n",
      "Epoch 555/1000 - 5.065594s - Loss 0.128459 - Accuracy 0.765625 - Test Loss 2.582370 - Test Accuracy 0.317418\n",
      "Epoch 556/1000 - 5.104227s - Loss 0.108221 - Accuracy 0.817708 - Test Loss 2.803439 - Test Accuracy 0.301503\n",
      "Epoch 557/1000 - 5.086580s - Loss 0.153838 - Accuracy 0.697917 - Test Loss 2.273568 - Test Accuracy 0.276746\n",
      "Epoch 558/1000 - 5.069365s - Loss 0.153749 - Accuracy 0.765625 - Test Loss 2.582342 - Test Accuracy 0.316092\n",
      "Epoch 559/1000 - 5.085738s - Loss 0.128223 - Accuracy 0.781250 - Test Loss 2.368936 - Test Accuracy 0.279399\n",
      "Epoch 560/1000 - 5.078134s - Loss 0.094038 - Accuracy 0.750000 - Test Loss 2.624337 - Test Accuracy 0.304598\n",
      "Epoch 561/1000 - 5.101820s - Loss 0.089008 - Accuracy 0.822917 - Test Loss 2.557806 - Test Accuracy 0.310345\n",
      "Epoch 562/1000 - 5.087178s - Loss 0.094366 - Accuracy 0.796875 - Test Loss 2.675751 - Test Accuracy 0.314324\n",
      "Epoch 563/1000 - 5.081858s - Loss 0.105053 - Accuracy 0.786458 - Test Loss 2.582556 - Test Accuracy 0.329355\n",
      "Epoch 564/1000 - 5.080266s - Loss 0.176516 - Accuracy 0.687500 - Test Loss 2.376340 - Test Accuracy 0.290009\n",
      "Epoch 565/1000 - 5.089562s - Loss 0.098093 - Accuracy 0.796875 - Test Loss 2.468933 - Test Accuracy 0.300619\n",
      "Epoch 566/1000 - 5.086268s - Loss 0.096486 - Accuracy 0.781250 - Test Loss 2.502234 - Test Accuracy 0.319629\n",
      "Epoch 567/1000 - 5.093361s - Loss 0.089923 - Accuracy 0.807292 - Test Loss 2.456249 - Test Accuracy 0.289567\n",
      "Epoch 568/1000 - 5.050787s - Loss 0.138246 - Accuracy 0.755208 - Test Loss 2.732695 - Test Accuracy 0.321839\n",
      "Epoch 569/1000 - 5.084987s - Loss 0.122603 - Accuracy 0.770833 - Test Loss 2.613032 - Test Accuracy 0.297524\n",
      "Epoch 570/1000 - 5.089358s - Loss 0.108018 - Accuracy 0.760417 - Test Loss 2.610941 - Test Accuracy 0.289567\n",
      "Epoch 571/1000 - 5.100578s - Loss 0.187158 - Accuracy 0.687500 - Test Loss 2.604898 - Test Accuracy 0.290451\n",
      "Epoch 572/1000 - 5.066775s - Loss 0.089056 - Accuracy 0.828125 - Test Loss 2.660846 - Test Accuracy 0.302829\n",
      "Epoch 573/1000 - 5.086376s - Loss 0.097568 - Accuracy 0.781250 - Test Loss 2.502283 - Test Accuracy 0.301945\n",
      "Epoch 574/1000 - 5.078315s - Loss 0.131515 - Accuracy 0.781250 - Test Loss 2.692143 - Test Accuracy 0.316534\n",
      "Epoch 575/1000 - 5.077627s - Loss 0.118304 - Accuracy 0.791667 - Test Loss 2.501724 - Test Accuracy 0.276746\n",
      "Epoch 576/1000 - 5.055499s - Loss 0.120818 - Accuracy 0.760417 - Test Loss 2.425354 - Test Accuracy 0.296198\n",
      "Epoch 577/1000 - 5.062350s - Loss 0.142622 - Accuracy 0.739583 - Test Loss 2.474208 - Test Accuracy 0.289567\n",
      "Epoch 578/1000 - 5.083711s - Loss 0.139536 - Accuracy 0.697917 - Test Loss 2.683594 - Test Accuracy 0.314766\n",
      "Epoch 579/1000 - 5.064717s - Loss 0.191688 - Accuracy 0.713542 - Test Loss 2.456722 - Test Accuracy 0.278957\n",
      "Epoch 580/1000 - 5.076991s - Loss 0.086242 - Accuracy 0.755208 - Test Loss 2.253787 - Test Accuracy 0.284704\n",
      "Epoch 581/1000 - 5.073689s - Loss 0.102993 - Accuracy 0.739583 - Test Loss 2.516740 - Test Accuracy 0.313882\n",
      "Epoch 582/1000 - 5.056257s - Loss 0.095921 - Accuracy 0.776042 - Test Loss 2.488811 - Test Accuracy 0.312555\n",
      "Epoch 583/1000 - 5.073785s - Loss 0.100283 - Accuracy 0.796875 - Test Loss 2.515526 - Test Accuracy 0.297082\n",
      "Epoch 584/1000 - 5.076381s - Loss 0.088908 - Accuracy 0.807292 - Test Loss 2.466009 - Test Accuracy 0.290451\n",
      "Epoch 585/1000 - 5.095091s - Loss 0.102095 - Accuracy 0.812500 - Test Loss 2.654100 - Test Accuracy 0.323165\n",
      "Epoch 586/1000 - 5.074946s - Loss 0.097295 - Accuracy 0.807292 - Test Loss 2.548043 - Test Accuracy 0.280725\n",
      "Epoch 587/1000 - 5.074954s - Loss 0.135352 - Accuracy 0.750000 - Test Loss 2.649705 - Test Accuracy 0.290893\n",
      "Epoch 588/1000 - 5.085636s - Loss 0.109585 - Accuracy 0.812500 - Test Loss 2.708974 - Test Accuracy 0.334660\n",
      "Epoch 589/1000 - 5.078816s - Loss 0.116617 - Accuracy 0.776042 - Test Loss 2.516092 - Test Accuracy 0.315208\n",
      "Epoch 590/1000 - 5.059666s - Loss 0.098836 - Accuracy 0.833333 - Test Loss 2.696090 - Test Accuracy 0.316092\n",
      "Epoch 591/1000 - 5.070811s - Loss 0.117040 - Accuracy 0.765625 - Test Loss 2.578402 - Test Accuracy 0.327586\n",
      "Epoch 592/1000 - 5.061230s - Loss 0.126870 - Accuracy 0.755208 - Test Loss 2.539422 - Test Accuracy 0.304598\n",
      "Epoch 593/1000 - 5.084798s - Loss 0.129989 - Accuracy 0.697917 - Test Loss 2.706563 - Test Accuracy 0.334660\n",
      "Epoch 594/1000 - 5.067802s - Loss 0.079926 - Accuracy 0.838542 - Test Loss 2.743784 - Test Accuracy 0.335102\n",
      "Epoch 595/1000 - 5.106016s - Loss 0.072927 - Accuracy 0.807292 - Test Loss 2.448853 - Test Accuracy 0.309903\n",
      "Epoch 596/1000 - 5.092116s - Loss 0.137189 - Accuracy 0.760417 - Test Loss 2.511413 - Test Accuracy 0.321839\n",
      "Epoch 597/1000 - 5.125196s - Loss 0.092064 - Accuracy 0.765625 - Test Loss 2.549537 - Test Accuracy 0.317418\n",
      "Epoch 598/1000 - 5.090372s - Loss 0.156553 - Accuracy 0.708333 - Test Loss 2.631668 - Test Accuracy 0.315650\n",
      "Epoch 599/1000 - 5.094085s - Loss 0.093044 - Accuracy 0.776042 - Test Loss 2.555421 - Test Accuracy 0.324492\n",
      "Epoch 600/1000 - 5.090643s - Loss 0.117768 - Accuracy 0.765625 - Test Loss 2.443707 - Test Accuracy 0.290451\n",
      "Epoch 601/1000 - 5.085342s - Loss 0.106325 - Accuracy 0.734375 - Test Loss 2.453584 - Test Accuracy 0.289125\n",
      "Epoch 602/1000 - 5.099596s - Loss 0.083644 - Accuracy 0.838542 - Test Loss 2.564615 - Test Accuracy 0.300619\n",
      "Epoch 603/1000 - 5.078736s - Loss 0.115839 - Accuracy 0.723958 - Test Loss 2.456988 - Test Accuracy 0.294430\n",
      "Epoch 604/1000 - 5.068824s - Loss 0.101646 - Accuracy 0.786458 - Test Loss 2.520768 - Test Accuracy 0.299293\n",
      "Epoch 605/1000 - 5.107916s - Loss 0.152798 - Accuracy 0.734375 - Test Loss 2.884157 - Test Accuracy 0.307250\n",
      "Epoch 606/1000 - 5.106548s - Loss 0.090683 - Accuracy 0.750000 - Test Loss 2.492870 - Test Accuracy 0.299293\n",
      "Epoch 607/1000 - 5.109836s - Loss 0.098583 - Accuracy 0.807292 - Test Loss 2.546423 - Test Accuracy 0.312113\n",
      "Epoch 608/1000 - 5.070309s - Loss 0.091926 - Accuracy 0.786458 - Test Loss 2.525411 - Test Accuracy 0.301503\n",
      "Epoch 609/1000 - 5.105952s - Loss 0.132141 - Accuracy 0.713542 - Test Loss 2.665084 - Test Accuracy 0.296640\n",
      "Epoch 610/1000 - 5.071346s - Loss 0.098121 - Accuracy 0.755208 - Test Loss 2.544446 - Test Accuracy 0.313439\n",
      "Epoch 611/1000 - 5.077409s - Loss 0.118007 - Accuracy 0.817708 - Test Loss 2.667640 - Test Accuracy 0.322281\n",
      "Epoch 612/1000 - 5.067840s - Loss 0.117610 - Accuracy 0.828125 - Test Loss 2.668073 - Test Accuracy 0.317860\n",
      "Epoch 613/1000 - 5.112253s - Loss 0.122027 - Accuracy 0.822917 - Test Loss 2.605125 - Test Accuracy 0.321839\n",
      "Epoch 614/1000 - 5.077158s - Loss 0.092052 - Accuracy 0.770833 - Test Loss 2.655340 - Test Accuracy 0.319187\n",
      "Epoch 615/1000 - 5.068033s - Loss 0.102686 - Accuracy 0.770833 - Test Loss 2.476291 - Test Accuracy 0.263926\n",
      "Epoch 616/1000 - 5.059257s - Loss 0.185882 - Accuracy 0.765625 - Test Loss 2.765853 - Test Accuracy 0.307692\n",
      "Epoch 617/1000 - 5.066970s - Loss 0.074070 - Accuracy 0.843750 - Test Loss 2.438854 - Test Accuracy 0.285146\n",
      "Epoch 618/1000 - 5.069975s - Loss 0.100480 - Accuracy 0.802083 - Test Loss 2.343776 - Test Accuracy 0.244916\n",
      "Epoch 619/1000 - 5.058223s - Loss 0.112794 - Accuracy 0.776042 - Test Loss 2.680838 - Test Accuracy 0.293103\n",
      "Epoch 620/1000 - 5.097521s - Loss 0.072244 - Accuracy 0.864583 - Test Loss 2.447614 - Test Accuracy 0.280283\n",
      "Epoch 621/1000 - 5.064114s - Loss 0.102186 - Accuracy 0.817708 - Test Loss 2.556943 - Test Accuracy 0.263042\n",
      "Epoch 622/1000 - 5.104410s - Loss 0.129349 - Accuracy 0.786458 - Test Loss 2.708000 - Test Accuracy 0.309019\n",
      "Epoch 623/1000 - 5.064930s - Loss 0.177320 - Accuracy 0.671875 - Test Loss 2.416448 - Test Accuracy 0.288683\n",
      "Epoch 624/1000 - 5.102676s - Loss 0.101478 - Accuracy 0.765625 - Test Loss 2.424291 - Test Accuracy 0.289567\n",
      "Epoch 625/1000 - 5.062935s - Loss 0.108240 - Accuracy 0.750000 - Test Loss 2.711889 - Test Accuracy 0.294872\n",
      "Epoch 626/1000 - 5.112448s - Loss 0.113640 - Accuracy 0.781250 - Test Loss 2.588975 - Test Accuracy 0.301945\n",
      "Epoch 627/1000 - 5.052219s - Loss 0.134307 - Accuracy 0.682292 - Test Loss 2.482662 - Test Accuracy 0.291777\n",
      "Epoch 628/1000 - 5.086902s - Loss 0.112158 - Accuracy 0.817708 - Test Loss 2.578590 - Test Accuracy 0.294430\n",
      "Epoch 629/1000 - 5.099081s - Loss 0.154606 - Accuracy 0.750000 - Test Loss 2.692939 - Test Accuracy 0.324050\n",
      "Epoch 630/1000 - 5.101695s - Loss 0.148727 - Accuracy 0.708333 - Test Loss 2.325606 - Test Accuracy 0.287356\n",
      "Epoch 631/1000 - 5.055057s - Loss 0.129086 - Accuracy 0.755208 - Test Loss 2.599526 - Test Accuracy 0.286914\n",
      "Epoch 632/1000 - 5.089248s - Loss 0.157101 - Accuracy 0.718750 - Test Loss 2.531423 - Test Accuracy 0.305482\n",
      "Epoch 633/1000 - 5.073416s - Loss 0.084693 - Accuracy 0.822917 - Test Loss 2.703898 - Test Accuracy 0.297966\n",
      "Epoch 634/1000 - 5.086544s - Loss 0.099994 - Accuracy 0.786458 - Test Loss 2.399670 - Test Accuracy 0.323165\n",
      "Epoch 635/1000 - 5.054966s - Loss 0.114155 - Accuracy 0.791667 - Test Loss 2.566000 - Test Accuracy 0.325818\n",
      "Epoch 636/1000 - 5.073775s - Loss 0.101967 - Accuracy 0.781250 - Test Loss 2.615192 - Test Accuracy 0.327586\n",
      "Epoch 637/1000 - 5.069806s - Loss 0.135219 - Accuracy 0.708333 - Test Loss 2.628258 - Test Accuracy 0.294430\n",
      "Epoch 638/1000 - 5.068966s - Loss 0.092138 - Accuracy 0.786458 - Test Loss 2.596148 - Test Accuracy 0.328028\n",
      "Epoch 639/1000 - 5.053300s - Loss 0.099979 - Accuracy 0.786458 - Test Loss 2.733425 - Test Accuracy 0.319187\n",
      "Epoch 640/1000 - 5.059496s - Loss 0.083119 - Accuracy 0.776042 - Test Loss 2.504965 - Test Accuracy 0.297082\n",
      "Epoch 641/1000 - 5.059977s - Loss 0.117274 - Accuracy 0.770833 - Test Loss 2.578943 - Test Accuracy 0.301061\n",
      "Epoch 642/1000 - 5.065840s - Loss 0.146024 - Accuracy 0.708333 - Test Loss 2.634836 - Test Accuracy 0.314324\n",
      "Epoch 643/1000 - 5.059556s - Loss 0.142307 - Accuracy 0.765625 - Test Loss 2.623361 - Test Accuracy 0.301945\n",
      "Epoch 644/1000 - 5.078988s - Loss 0.078030 - Accuracy 0.791667 - Test Loss 2.549321 - Test Accuracy 0.310345\n",
      "Epoch 645/1000 - 5.061115s - Loss 0.078556 - Accuracy 0.854167 - Test Loss 2.503490 - Test Accuracy 0.309461\n",
      "Epoch 646/1000 - 5.092437s - Loss 0.122542 - Accuracy 0.770833 - Test Loss 2.718250 - Test Accuracy 0.307692\n",
      "Epoch 647/1000 - 5.060535s - Loss 0.126231 - Accuracy 0.796875 - Test Loss 2.561508 - Test Accuracy 0.321839\n",
      "Epoch 648/1000 - 5.104144s - Loss 0.151969 - Accuracy 0.708333 - Test Loss 2.438483 - Test Accuracy 0.282051\n",
      "Epoch 649/1000 - 5.084400s - Loss 0.108737 - Accuracy 0.807292 - Test Loss 2.620857 - Test Accuracy 0.303271\n",
      "Epoch 650/1000 - 5.076310s - Loss 0.146922 - Accuracy 0.718750 - Test Loss 2.629223 - Test Accuracy 0.348364\n",
      "Epoch 651/1000 - 5.067193s - Loss 0.105147 - Accuracy 0.786458 - Test Loss 2.728102 - Test Accuracy 0.308576\n",
      "Epoch 652/1000 - 5.078774s - Loss 0.085311 - Accuracy 0.843750 - Test Loss 2.519863 - Test Accuracy 0.292219\n",
      "Epoch 653/1000 - 5.092240s - Loss 0.133642 - Accuracy 0.755208 - Test Loss 2.932409 - Test Accuracy 0.319187\n",
      "Epoch 654/1000 - 5.069805s - Loss 0.101903 - Accuracy 0.833333 - Test Loss 2.770137 - Test Accuracy 0.309019\n",
      "Epoch 655/1000 - 5.049911s - Loss 0.132453 - Accuracy 0.770833 - Test Loss 2.540709 - Test Accuracy 0.288683\n",
      "Epoch 656/1000 - 5.063493s - Loss 0.099574 - Accuracy 0.765625 - Test Loss 2.338129 - Test Accuracy 0.275862\n",
      "Epoch 657/1000 - 5.065058s - Loss 0.132345 - Accuracy 0.729167 - Test Loss 2.641414 - Test Accuracy 0.323607\n",
      "Epoch 658/1000 - 5.079221s - Loss 0.132354 - Accuracy 0.770833 - Test Loss 2.520698 - Test Accuracy 0.338196\n",
      "Epoch 659/1000 - 5.063471s - Loss 0.111610 - Accuracy 0.791667 - Test Loss 2.622377 - Test Accuracy 0.334660\n",
      "Epoch 660/1000 - 5.058357s - Loss 0.121111 - Accuracy 0.781250 - Test Loss 2.612286 - Test Accuracy 0.318302\n",
      "Epoch 661/1000 - 5.074504s - Loss 0.104776 - Accuracy 0.776042 - Test Loss 2.626947 - Test Accuracy 0.328912\n",
      "Epoch 662/1000 - 5.062995s - Loss 0.098244 - Accuracy 0.807292 - Test Loss 2.510612 - Test Accuracy 0.274094\n",
      "Epoch 663/1000 - 5.072011s - Loss 0.077137 - Accuracy 0.828125 - Test Loss 2.838951 - Test Accuracy 0.331565\n",
      "Epoch 664/1000 - 5.077099s - Loss 0.104717 - Accuracy 0.802083 - Test Loss 2.540451 - Test Accuracy 0.293988\n",
      "Epoch 665/1000 - 5.091574s - Loss 0.123303 - Accuracy 0.786458 - Test Loss 2.803033 - Test Accuracy 0.334218\n",
      "Epoch 666/1000 - 5.058373s - Loss 0.072838 - Accuracy 0.802083 - Test Loss 2.772761 - Test Accuracy 0.329797\n",
      "Epoch 667/1000 - 5.073667s - Loss 0.119373 - Accuracy 0.755208 - Test Loss 2.336794 - Test Accuracy 0.274978\n",
      "Epoch 668/1000 - 5.067483s - Loss 0.068484 - Accuracy 0.864583 - Test Loss 2.729184 - Test Accuracy 0.322723\n",
      "Epoch 669/1000 - 5.058481s - Loss 0.096013 - Accuracy 0.786458 - Test Loss 2.708825 - Test Accuracy 0.296640\n",
      "Epoch 670/1000 - 5.073331s - Loss 0.144320 - Accuracy 0.656250 - Test Loss 2.655854 - Test Accuracy 0.299293\n",
      "Epoch 671/1000 - 5.060373s - Loss 0.135282 - Accuracy 0.739583 - Test Loss 2.676573 - Test Accuracy 0.305924\n",
      "Epoch 672/1000 - 5.086338s - Loss 0.095400 - Accuracy 0.791667 - Test Loss 2.512939 - Test Accuracy 0.289125\n",
      "Epoch 673/1000 - 5.060443s - Loss 0.109631 - Accuracy 0.807292 - Test Loss 2.736418 - Test Accuracy 0.331565\n",
      "Epoch 674/1000 - 5.097298s - Loss 0.081953 - Accuracy 0.807292 - Test Loss 2.518805 - Test Accuracy 0.293546\n",
      "Epoch 675/1000 - 5.070136s - Loss 0.094889 - Accuracy 0.786458 - Test Loss 2.789397 - Test Accuracy 0.308134\n",
      "Epoch 676/1000 - 5.094693s - Loss 0.089346 - Accuracy 0.848958 - Test Loss 2.838209 - Test Accuracy 0.326702\n",
      "Epoch 677/1000 - 5.072257s - Loss 0.069567 - Accuracy 0.859375 - Test Loss 2.747024 - Test Accuracy 0.304598\n",
      "Epoch 678/1000 - 5.066406s - Loss 0.087382 - Accuracy 0.828125 - Test Loss 2.644076 - Test Accuracy 0.282051\n",
      "Epoch 679/1000 - 5.063415s - Loss 0.140603 - Accuracy 0.786458 - Test Loss 2.587960 - Test Accuracy 0.296640\n",
      "Epoch 680/1000 - 5.100810s - Loss 0.126875 - Accuracy 0.750000 - Test Loss 2.739451 - Test Accuracy 0.317860\n",
      "Epoch 681/1000 - 5.061784s - Loss 0.107639 - Accuracy 0.776042 - Test Loss 2.933813 - Test Accuracy 0.312997\n",
      "Epoch 682/1000 - 5.109231s - Loss 0.115136 - Accuracy 0.812500 - Test Loss 2.782943 - Test Accuracy 0.312997\n",
      "Epoch 683/1000 - 5.090607s - Loss 0.103103 - Accuracy 0.796875 - Test Loss 2.774505 - Test Accuracy 0.312113\n",
      "Epoch 684/1000 - 5.065614s - Loss 0.130375 - Accuracy 0.723958 - Test Loss 2.757596 - Test Accuracy 0.304156\n",
      "Epoch 685/1000 - 5.107335s - Loss 0.095758 - Accuracy 0.796875 - Test Loss 2.878355 - Test Accuracy 0.320071\n",
      "Epoch 686/1000 - 5.077009s - Loss 0.090230 - Accuracy 0.802083 - Test Loss 2.664567 - Test Accuracy 0.278515\n",
      "Epoch 687/1000 - 5.099407s - Loss 0.113350 - Accuracy 0.781250 - Test Loss 2.591202 - Test Accuracy 0.283378\n",
      "Epoch 688/1000 - 5.079377s - Loss 0.103743 - Accuracy 0.843750 - Test Loss 2.799600 - Test Accuracy 0.325376\n",
      "Epoch 689/1000 - 5.117694s - Loss 0.117288 - Accuracy 0.729167 - Test Loss 2.731688 - Test Accuracy 0.326702\n",
      "Epoch 690/1000 - 5.066319s - Loss 0.099749 - Accuracy 0.807292 - Test Loss 2.522126 - Test Accuracy 0.246684\n",
      "Epoch 691/1000 - 5.085703s - Loss 0.155940 - Accuracy 0.755208 - Test Loss 2.573497 - Test Accuracy 0.348364\n",
      "Epoch 692/1000 - 5.060541s - Loss 0.210286 - Accuracy 0.786458 - Test Loss 3.059137 - Test Accuracy 0.210433\n",
      "Epoch 693/1000 - 5.072383s - Loss 0.149033 - Accuracy 0.744792 - Test Loss 2.979756 - Test Accuracy 0.336870\n",
      "Epoch 694/1000 - 5.068659s - Loss 0.141862 - Accuracy 0.786458 - Test Loss 2.634685 - Test Accuracy 0.266136\n",
      "Epoch 695/1000 - 5.074610s - Loss 0.119079 - Accuracy 0.796875 - Test Loss 2.782880 - Test Accuracy 0.275862\n",
      "Epoch 696/1000 - 5.086064s - Loss 0.107154 - Accuracy 0.765625 - Test Loss 2.291240 - Test Accuracy 0.235190\n",
      "Epoch 697/1000 - 5.076500s - Loss 0.124115 - Accuracy 0.734375 - Test Loss 2.885794 - Test Accuracy 0.324492\n",
      "Epoch 698/1000 - 5.064642s - Loss 0.098107 - Accuracy 0.750000 - Test Loss 2.594835 - Test Accuracy 0.283378\n",
      "Epoch 699/1000 - 5.100530s - Loss 0.108755 - Accuracy 0.755208 - Test Loss 2.387918 - Test Accuracy 0.275420\n",
      "Epoch 700/1000 - 5.061325s - Loss 0.110363 - Accuracy 0.786458 - Test Loss 2.633909 - Test Accuracy 0.298408\n",
      "Epoch 701/1000 - 5.081841s - Loss 0.086240 - Accuracy 0.807292 - Test Loss 2.581391 - Test Accuracy 0.286030\n",
      "Epoch 702/1000 - 5.059036s - Loss 0.104516 - Accuracy 0.807292 - Test Loss 2.802270 - Test Accuracy 0.296198\n",
      "Epoch 703/1000 - 5.065198s - Loss 0.122790 - Accuracy 0.755208 - Test Loss 2.654787 - Test Accuracy 0.271883\n",
      "Epoch 704/1000 - 5.069592s - Loss 0.105714 - Accuracy 0.786458 - Test Loss 2.446189 - Test Accuracy 0.290893\n",
      "Epoch 705/1000 - 5.066079s - Loss 0.074668 - Accuracy 0.854167 - Test Loss 2.402147 - Test Accuracy 0.273210\n",
      "Epoch 706/1000 - 5.057343s - Loss 0.133612 - Accuracy 0.765625 - Test Loss 2.742143 - Test Accuracy 0.310787\n",
      "Epoch 707/1000 - 5.113650s - Loss 0.112615 - Accuracy 0.848958 - Test Loss 2.488988 - Test Accuracy 0.284262\n",
      "Epoch 708/1000 - 5.078632s - Loss 0.133705 - Accuracy 0.807292 - Test Loss 2.747221 - Test Accuracy 0.304598\n",
      "Epoch 709/1000 - 5.100452s - Loss 0.072909 - Accuracy 0.822917 - Test Loss 2.543287 - Test Accuracy 0.281167\n",
      "Epoch 710/1000 - 5.089990s - Loss 0.098447 - Accuracy 0.817708 - Test Loss 2.642606 - Test Accuracy 0.293988\n",
      "Epoch 711/1000 - 5.089679s - Loss 0.143405 - Accuracy 0.729167 - Test Loss 2.692271 - Test Accuracy 0.242263\n",
      "Epoch 712/1000 - 5.059668s - Loss 0.139352 - Accuracy 0.723958 - Test Loss 2.619124 - Test Accuracy 0.268789\n",
      "Epoch 713/1000 - 5.088547s - Loss 0.086677 - Accuracy 0.760417 - Test Loss 2.956692 - Test Accuracy 0.322723\n",
      "Epoch 714/1000 - 5.070751s - Loss 0.080677 - Accuracy 0.791667 - Test Loss 2.664067 - Test Accuracy 0.298851\n",
      "Epoch 715/1000 - 5.083343s - Loss 0.077763 - Accuracy 0.822917 - Test Loss 2.620092 - Test Accuracy 0.293988\n",
      "Epoch 716/1000 - 5.056056s - Loss 0.085131 - Accuracy 0.838542 - Test Loss 2.613385 - Test Accuracy 0.287356\n",
      "Epoch 717/1000 - 5.072624s - Loss 0.100848 - Accuracy 0.786458 - Test Loss 2.601726 - Test Accuracy 0.275862\n",
      "Epoch 718/1000 - 5.054190s - Loss 0.064412 - Accuracy 0.880208 - Test Loss 2.693062 - Test Accuracy 0.308134\n",
      "Epoch 719/1000 - 5.069581s - Loss 0.116935 - Accuracy 0.781250 - Test Loss 2.744890 - Test Accuracy 0.302387\n",
      "Epoch 720/1000 - 5.071399s - Loss 0.090103 - Accuracy 0.828125 - Test Loss 2.685086 - Test Accuracy 0.286472\n",
      "Epoch 721/1000 - 5.059054s - Loss 0.099924 - Accuracy 0.770833 - Test Loss 2.532820 - Test Accuracy 0.276304\n",
      "Epoch 722/1000 - 5.064085s - Loss 0.060727 - Accuracy 0.875000 - Test Loss 2.479265 - Test Accuracy 0.291777\n",
      "Epoch 723/1000 - 5.096405s - Loss 0.097393 - Accuracy 0.770833 - Test Loss 2.414327 - Test Accuracy 0.249779\n",
      "Epoch 724/1000 - 5.074946s - Loss 0.095463 - Accuracy 0.744792 - Test Loss 2.850658 - Test Accuracy 0.288240\n",
      "Epoch 725/1000 - 5.094107s - Loss 0.115898 - Accuracy 0.755208 - Test Loss 2.814646 - Test Accuracy 0.281167\n",
      "Epoch 726/1000 - 5.057026s - Loss 0.092874 - Accuracy 0.828125 - Test Loss 2.676926 - Test Accuracy 0.290009\n",
      "Epoch 727/1000 - 5.089507s - Loss 0.112543 - Accuracy 0.786458 - Test Loss 2.784256 - Test Accuracy 0.302829\n",
      "Epoch 728/1000 - 5.088358s - Loss 0.103871 - Accuracy 0.791667 - Test Loss 2.523567 - Test Accuracy 0.267020\n",
      "Epoch 729/1000 - 5.070608s - Loss 0.133590 - Accuracy 0.765625 - Test Loss 2.695798 - Test Accuracy 0.299293\n",
      "Epoch 730/1000 - 5.063429s - Loss 0.076668 - Accuracy 0.828125 - Test Loss 2.554364 - Test Accuracy 0.274536\n",
      "Epoch 731/1000 - 5.074427s - Loss 0.088983 - Accuracy 0.802083 - Test Loss 2.674927 - Test Accuracy 0.301061\n",
      "Epoch 732/1000 - 5.073734s - Loss 0.114863 - Accuracy 0.786458 - Test Loss 2.577227 - Test Accuracy 0.293546\n",
      "Epoch 733/1000 - 5.087847s - Loss 0.090454 - Accuracy 0.838542 - Test Loss 2.591804 - Test Accuracy 0.288240\n",
      "Epoch 734/1000 - 5.062160s - Loss 0.131455 - Accuracy 0.796875 - Test Loss 2.730041 - Test Accuracy 0.301503\n",
      "Epoch 735/1000 - 5.089747s - Loss 0.120438 - Accuracy 0.776042 - Test Loss 2.764240 - Test Accuracy 0.304156\n",
      "Epoch 736/1000 - 5.072912s - Loss 0.145375 - Accuracy 0.755208 - Test Loss 2.626139 - Test Accuracy 0.283378\n",
      "Epoch 737/1000 - 5.084893s - Loss 0.084866 - Accuracy 0.807292 - Test Loss 2.486292 - Test Accuracy 0.256852\n",
      "Epoch 738/1000 - 5.064330s - Loss 0.101828 - Accuracy 0.791667 - Test Loss 2.719972 - Test Accuracy 0.294872\n",
      "Epoch 739/1000 - 5.077798s - Loss 0.091124 - Accuracy 0.817708 - Test Loss 2.767751 - Test Accuracy 0.290009\n",
      "Epoch 740/1000 - 5.067089s - Loss 0.111514 - Accuracy 0.802083 - Test Loss 2.615134 - Test Accuracy 0.279841\n",
      "Epoch 741/1000 - 5.075796s - Loss 0.109705 - Accuracy 0.760417 - Test Loss 2.700508 - Test Accuracy 0.324050\n",
      "Epoch 742/1000 - 5.061831s - Loss 0.110391 - Accuracy 0.781250 - Test Loss 2.587446 - Test Accuracy 0.288683\n",
      "Epoch 743/1000 - 5.071062s - Loss 0.137558 - Accuracy 0.703125 - Test Loss 2.719920 - Test Accuracy 0.304156\n",
      "Epoch 744/1000 - 5.062810s - Loss 0.096791 - Accuracy 0.776042 - Test Loss 2.657290 - Test Accuracy 0.298408\n",
      "Epoch 745/1000 - 5.094896s - Loss 0.108946 - Accuracy 0.786458 - Test Loss 2.626039 - Test Accuracy 0.290893\n",
      "Epoch 746/1000 - 5.077162s - Loss 0.112895 - Accuracy 0.760417 - Test Loss 2.757865 - Test Accuracy 0.297082\n",
      "Epoch 747/1000 - 5.070925s - Loss 0.085705 - Accuracy 0.791667 - Test Loss 2.784250 - Test Accuracy 0.274978\n",
      "Epoch 748/1000 - 5.067846s - Loss 0.091814 - Accuracy 0.828125 - Test Loss 2.726939 - Test Accuracy 0.278957\n",
      "Epoch 749/1000 - 5.077826s - Loss 0.090522 - Accuracy 0.807292 - Test Loss 2.661379 - Test Accuracy 0.301503\n",
      "Epoch 750/1000 - 5.093731s - Loss 0.123878 - Accuracy 0.765625 - Test Loss 2.754132 - Test Accuracy 0.327144\n",
      "Epoch 751/1000 - 5.066782s - Loss 0.132856 - Accuracy 0.796875 - Test Loss 2.493312 - Test Accuracy 0.292661\n",
      "Epoch 752/1000 - 5.078937s - Loss 0.076760 - Accuracy 0.802083 - Test Loss 2.584040 - Test Accuracy 0.304598\n",
      "Epoch 753/1000 - 5.075297s - Loss 0.088767 - Accuracy 0.859375 - Test Loss 2.572996 - Test Accuracy 0.296640\n",
      "Epoch 754/1000 - 5.089497s - Loss 0.089339 - Accuracy 0.781250 - Test Loss 2.703387 - Test Accuracy 0.305924\n",
      "Epoch 755/1000 - 5.065313s - Loss 0.072797 - Accuracy 0.838542 - Test Loss 2.685198 - Test Accuracy 0.284704\n",
      "Epoch 756/1000 - 5.095476s - Loss 0.100357 - Accuracy 0.817708 - Test Loss 2.649442 - Test Accuracy 0.293988\n",
      "Epoch 757/1000 - 5.055466s - Loss 0.103492 - Accuracy 0.750000 - Test Loss 2.658197 - Test Accuracy 0.292661\n",
      "Epoch 758/1000 - 5.119342s - Loss 0.100325 - Accuracy 0.812500 - Test Loss 2.723051 - Test Accuracy 0.308134\n",
      "Epoch 759/1000 - 5.071438s - Loss 0.080211 - Accuracy 0.812500 - Test Loss 2.743174 - Test Accuracy 0.319187\n",
      "Epoch 760/1000 - 5.072785s - Loss 0.089947 - Accuracy 0.822917 - Test Loss 2.713361 - Test Accuracy 0.328912\n",
      "Epoch 761/1000 - 5.058897s - Loss 0.092201 - Accuracy 0.781250 - Test Loss 2.628922 - Test Accuracy 0.276304\n",
      "Epoch 762/1000 - 5.078549s - Loss 0.057436 - Accuracy 0.838542 - Test Loss 2.616238 - Test Accuracy 0.296198\n",
      "Epoch 763/1000 - 5.060349s - Loss 0.068313 - Accuracy 0.833333 - Test Loss 2.939410 - Test Accuracy 0.302829\n",
      "Epoch 764/1000 - 5.096362s - Loss 0.088361 - Accuracy 0.828125 - Test Loss 2.791356 - Test Accuracy 0.283378\n",
      "Epoch 765/1000 - 5.067104s - Loss 0.139085 - Accuracy 0.744792 - Test Loss 2.763428 - Test Accuracy 0.284704\n",
      "Epoch 766/1000 - 5.087669s - Loss 0.129912 - Accuracy 0.776042 - Test Loss 2.762689 - Test Accuracy 0.295314\n",
      "Epoch 767/1000 - 5.063057s - Loss 0.102242 - Accuracy 0.781250 - Test Loss 2.490854 - Test Accuracy 0.258621\n",
      "Epoch 768/1000 - 5.099402s - Loss 0.104031 - Accuracy 0.869792 - Test Loss 2.581322 - Test Accuracy 0.301503\n",
      "Epoch 769/1000 - 5.072848s - Loss 0.118186 - Accuracy 0.734375 - Test Loss 2.625682 - Test Accuracy 0.282935\n",
      "Epoch 770/1000 - 5.079166s - Loss 0.094749 - Accuracy 0.755208 - Test Loss 2.561323 - Test Accuracy 0.297082\n",
      "Epoch 771/1000 - 5.085416s - Loss 0.090183 - Accuracy 0.838542 - Test Loss 2.693969 - Test Accuracy 0.295314\n",
      "Epoch 772/1000 - 5.065616s - Loss 0.096671 - Accuracy 0.776042 - Test Loss 2.611592 - Test Accuracy 0.309461\n",
      "Epoch 773/1000 - 5.068968s - Loss 0.114026 - Accuracy 0.817708 - Test Loss 2.650028 - Test Accuracy 0.282051\n",
      "Epoch 774/1000 - 5.096591s - Loss 0.102933 - Accuracy 0.770833 - Test Loss 2.561288 - Test Accuracy 0.286030\n",
      "Epoch 775/1000 - 5.061894s - Loss 0.106402 - Accuracy 0.807292 - Test Loss 2.973433 - Test Accuracy 0.312997\n",
      "Epoch 776/1000 - 5.069986s - Loss 0.107756 - Accuracy 0.744792 - Test Loss 2.849309 - Test Accuracy 0.300619\n",
      "Epoch 777/1000 - 5.072878s - Loss 0.102560 - Accuracy 0.812500 - Test Loss 2.502899 - Test Accuracy 0.274094\n",
      "Epoch 778/1000 - 5.079518s - Loss 0.090619 - Accuracy 0.786458 - Test Loss 3.005553 - Test Accuracy 0.312113\n",
      "Epoch 779/1000 - 5.067173s - Loss 0.102098 - Accuracy 0.781250 - Test Loss 2.624538 - Test Accuracy 0.272767\n",
      "Epoch 780/1000 - 5.099799s - Loss 0.144301 - Accuracy 0.750000 - Test Loss 2.617879 - Test Accuracy 0.292219\n",
      "Epoch 781/1000 - 5.059895s - Loss 0.136046 - Accuracy 0.744792 - Test Loss 2.612961 - Test Accuracy 0.291777\n",
      "Epoch 782/1000 - 5.062732s - Loss 0.109920 - Accuracy 0.765625 - Test Loss 2.419738 - Test Accuracy 0.267020\n",
      "Epoch 783/1000 - 5.051104s - Loss 0.095494 - Accuracy 0.791667 - Test Loss 2.846656 - Test Accuracy 0.312997\n",
      "Epoch 784/1000 - 5.075734s - Loss 0.072073 - Accuracy 0.848958 - Test Loss 2.621798 - Test Accuracy 0.296198\n",
      "Epoch 785/1000 - 5.063442s - Loss 0.113742 - Accuracy 0.770833 - Test Loss 2.736597 - Test Accuracy 0.300619\n",
      "Epoch 786/1000 - 5.071315s - Loss 0.089336 - Accuracy 0.828125 - Test Loss 2.422797 - Test Accuracy 0.270115\n",
      "Epoch 787/1000 - 5.079245s - Loss 0.113931 - Accuracy 0.744792 - Test Loss 2.674577 - Test Accuracy 0.289567\n",
      "Epoch 788/1000 - 5.080171s - Loss 0.077396 - Accuracy 0.822917 - Test Loss 2.542272 - Test Accuracy 0.278957\n",
      "Epoch 789/1000 - 5.067764s - Loss 0.076631 - Accuracy 0.812500 - Test Loss 2.842312 - Test Accuracy 0.318302\n",
      "Epoch 790/1000 - 5.073390s - Loss 0.078034 - Accuracy 0.885417 - Test Loss 2.695387 - Test Accuracy 0.324934\n",
      "Epoch 791/1000 - 5.074674s - Loss 0.054761 - Accuracy 0.875000 - Test Loss 2.734677 - Test Accuracy 0.321397\n",
      "Epoch 792/1000 - 5.064213s - Loss 0.061740 - Accuracy 0.890625 - Test Loss 2.677529 - Test Accuracy 0.324492\n",
      "Epoch 793/1000 - 5.091025s - Loss 0.089307 - Accuracy 0.812500 - Test Loss 3.019089 - Test Accuracy 0.333333\n",
      "Epoch 794/1000 - 5.066857s - Loss 0.116337 - Accuracy 0.791667 - Test Loss 2.767427 - Test Accuracy 0.294430\n",
      "Epoch 795/1000 - 5.071272s - Loss 0.086967 - Accuracy 0.812500 - Test Loss 2.594667 - Test Accuracy 0.305482\n",
      "Epoch 796/1000 - 5.061459s - Loss 0.091042 - Accuracy 0.786458 - Test Loss 2.833652 - Test Accuracy 0.295314\n",
      "Epoch 797/1000 - 5.061347s - Loss 0.103156 - Accuracy 0.817708 - Test Loss 2.730726 - Test Accuracy 0.292219\n",
      "Epoch 798/1000 - 5.117470s - Loss 0.082057 - Accuracy 0.843750 - Test Loss 2.795496 - Test Accuracy 0.317418\n",
      "Epoch 799/1000 - 5.051804s - Loss 0.096726 - Accuracy 0.786458 - Test Loss 2.905418 - Test Accuracy 0.291335\n",
      "Epoch 800/1000 - 5.104227s - Loss 0.093592 - Accuracy 0.755208 - Test Loss 2.884751 - Test Accuracy 0.312997\n",
      "Epoch 801/1000 - 5.057498s - Loss 0.110832 - Accuracy 0.765625 - Test Loss 2.830103 - Test Accuracy 0.306366\n",
      "Epoch 802/1000 - 5.097207s - Loss 0.118831 - Accuracy 0.734375 - Test Loss 2.615446 - Test Accuracy 0.304156\n",
      "Epoch 803/1000 - 5.078500s - Loss 0.093758 - Accuracy 0.817708 - Test Loss 2.805411 - Test Accuracy 0.305924\n",
      "Epoch 804/1000 - 5.083841s - Loss 0.116775 - Accuracy 0.776042 - Test Loss 2.889501 - Test Accuracy 0.352343\n",
      "Epoch 805/1000 - 5.056927s - Loss 0.104181 - Accuracy 0.765625 - Test Loss 2.793375 - Test Accuracy 0.288683\n",
      "Epoch 806/1000 - 5.101594s - Loss 0.082044 - Accuracy 0.828125 - Test Loss 2.803154 - Test Accuracy 0.297524\n",
      "Epoch 807/1000 - 5.068704s - Loss 0.078922 - Accuracy 0.885417 - Test Loss 2.804004 - Test Accuracy 0.319187\n",
      "Epoch 808/1000 - 5.091725s - Loss 0.129308 - Accuracy 0.703125 - Test Loss 2.793183 - Test Accuracy 0.286030\n",
      "Epoch 809/1000 - 5.057348s - Loss 0.070024 - Accuracy 0.822917 - Test Loss 2.800045 - Test Accuracy 0.294430\n",
      "Epoch 810/1000 - 5.086344s - Loss 0.070678 - Accuracy 0.828125 - Test Loss 2.835792 - Test Accuracy 0.299735\n",
      "Epoch 811/1000 - 5.051397s - Loss 0.088817 - Accuracy 0.817708 - Test Loss 2.828991 - Test Accuracy 0.293103\n",
      "Epoch 812/1000 - 5.070480s - Loss 0.098941 - Accuracy 0.796875 - Test Loss 2.959797 - Test Accuracy 0.332891\n",
      "Epoch 813/1000 - 5.055026s - Loss 0.093422 - Accuracy 0.791667 - Test Loss 2.871188 - Test Accuracy 0.326260\n",
      "Epoch 814/1000 - 5.068422s - Loss 0.127727 - Accuracy 0.770833 - Test Loss 2.675537 - Test Accuracy 0.311671\n",
      "Epoch 815/1000 - 5.081842s - Loss 0.117566 - Accuracy 0.765625 - Test Loss 2.429235 - Test Accuracy 0.294872\n",
      "Epoch 816/1000 - 5.086915s - Loss 0.126218 - Accuracy 0.776042 - Test Loss 2.960883 - Test Accuracy 0.302829\n",
      "Epoch 817/1000 - 5.084384s - Loss 0.083449 - Accuracy 0.786458 - Test Loss 2.752969 - Test Accuracy 0.316976\n",
      "Epoch 818/1000 - 5.074549s - Loss 0.071929 - Accuracy 0.812500 - Test Loss 2.641950 - Test Accuracy 0.298851\n",
      "Epoch 819/1000 - 5.087981s - Loss 0.102613 - Accuracy 0.765625 - Test Loss 2.696270 - Test Accuracy 0.283820\n",
      "Epoch 820/1000 - 5.071730s - Loss 0.053158 - Accuracy 0.885417 - Test Loss 2.923668 - Test Accuracy 0.316092\n",
      "Epoch 821/1000 - 5.105175s - Loss 0.106676 - Accuracy 0.838542 - Test Loss 2.862460 - Test Accuracy 0.319629\n",
      "Epoch 822/1000 - 5.066698s - Loss 0.140866 - Accuracy 0.703125 - Test Loss 2.788816 - Test Accuracy 0.308576\n",
      "Epoch 823/1000 - 5.083822s - Loss 0.149719 - Accuracy 0.734375 - Test Loss 2.611941 - Test Accuracy 0.303714\n",
      "Epoch 824/1000 - 5.066332s - Loss 0.076587 - Accuracy 0.812500 - Test Loss 2.644718 - Test Accuracy 0.302387\n",
      "Epoch 825/1000 - 5.103148s - Loss 0.083960 - Accuracy 0.802083 - Test Loss 2.756141 - Test Accuracy 0.313882\n",
      "Epoch 826/1000 - 5.064306s - Loss 0.097506 - Accuracy 0.765625 - Test Loss 2.427940 - Test Accuracy 0.282935\n",
      "Epoch 827/1000 - 5.092245s - Loss 0.098624 - Accuracy 0.817708 - Test Loss 2.641186 - Test Accuracy 0.278515\n",
      "Epoch 828/1000 - 5.059887s - Loss 0.113555 - Accuracy 0.739583 - Test Loss 2.750234 - Test Accuracy 0.315208\n",
      "Epoch 829/1000 - 5.104447s - Loss 0.081915 - Accuracy 0.807292 - Test Loss 2.676050 - Test Accuracy 0.297966\n",
      "Epoch 830/1000 - 5.080743s - Loss 0.071766 - Accuracy 0.796875 - Test Loss 2.728765 - Test Accuracy 0.306808\n",
      "Epoch 831/1000 - 5.114657s - Loss 0.131080 - Accuracy 0.776042 - Test Loss 2.576104 - Test Accuracy 0.299293\n",
      "Epoch 832/1000 - 5.064349s - Loss 0.161183 - Accuracy 0.760417 - Test Loss 2.954586 - Test Accuracy 0.326702\n",
      "Epoch 833/1000 - 5.083628s - Loss 0.071813 - Accuracy 0.864583 - Test Loss 2.606181 - Test Accuracy 0.302829\n",
      "Epoch 834/1000 - 5.061668s - Loss 0.177758 - Accuracy 0.687500 - Test Loss 2.581874 - Test Accuracy 0.301945\n",
      "Epoch 835/1000 - 5.073133s - Loss 0.121353 - Accuracy 0.744792 - Test Loss 2.694335 - Test Accuracy 0.299735\n",
      "Epoch 836/1000 - 5.063880s - Loss 0.086094 - Accuracy 0.812500 - Test Loss 2.514755 - Test Accuracy 0.280283\n",
      "Epoch 837/1000 - 5.062821s - Loss 0.092383 - Accuracy 0.796875 - Test Loss 2.590221 - Test Accuracy 0.287798\n",
      "Epoch 838/1000 - 5.049956s - Loss 0.126091 - Accuracy 0.744792 - Test Loss 2.658997 - Test Accuracy 0.282935\n",
      "Epoch 839/1000 - 5.052966s - Loss 0.094401 - Accuracy 0.796875 - Test Loss 2.628289 - Test Accuracy 0.305040\n",
      "Epoch 840/1000 - 5.057425s - Loss 0.099393 - Accuracy 0.776042 - Test Loss 2.710509 - Test Accuracy 0.312113\n",
      "Epoch 841/1000 - 5.087788s - Loss 0.118812 - Accuracy 0.796875 - Test Loss 2.479266 - Test Accuracy 0.280725\n",
      "Epoch 842/1000 - 5.052662s - Loss 0.078849 - Accuracy 0.828125 - Test Loss 2.516915 - Test Accuracy 0.268789\n",
      "Epoch 843/1000 - 5.094609s - Loss 0.088599 - Accuracy 0.817708 - Test Loss 2.773583 - Test Accuracy 0.287798\n",
      "Epoch 844/1000 - 5.045082s - Loss 0.072219 - Accuracy 0.854167 - Test Loss 2.753373 - Test Accuracy 0.309019\n",
      "Epoch 845/1000 - 5.068138s - Loss 0.083241 - Accuracy 0.869792 - Test Loss 2.982549 - Test Accuracy 0.256852\n",
      "Epoch 846/1000 - 5.067758s - Loss 0.189483 - Accuracy 0.734375 - Test Loss 3.177240 - Test Accuracy 0.254642\n",
      "Epoch 847/1000 - 5.070572s - Loss 0.135774 - Accuracy 0.734375 - Test Loss 2.794387 - Test Accuracy 0.322281\n",
      "Epoch 848/1000 - 5.067915s - Loss 0.132835 - Accuracy 0.791667 - Test Loss 2.764269 - Test Accuracy 0.305040\n",
      "Epoch 849/1000 - 5.064905s - Loss 0.132858 - Accuracy 0.734375 - Test Loss 2.856670 - Test Accuracy 0.317418\n",
      "Epoch 850/1000 - 5.072338s - Loss 0.130811 - Accuracy 0.770833 - Test Loss 2.519631 - Test Accuracy 0.289125\n",
      "Epoch 851/1000 - 5.078591s - Loss 0.065154 - Accuracy 0.864583 - Test Loss 2.832220 - Test Accuracy 0.296640\n",
      "Epoch 852/1000 - 5.065431s - Loss 0.082565 - Accuracy 0.828125 - Test Loss 2.726818 - Test Accuracy 0.290009\n",
      "Epoch 853/1000 - 5.065656s - Loss 0.100274 - Accuracy 0.765625 - Test Loss 2.613103 - Test Accuracy 0.287798\n",
      "Epoch 854/1000 - 5.092442s - Loss 0.091699 - Accuracy 0.833333 - Test Loss 2.809911 - Test Accuracy 0.293546\n",
      "Epoch 855/1000 - 5.093523s - Loss 0.093254 - Accuracy 0.765625 - Test Loss 2.816518 - Test Accuracy 0.298851\n",
      "Epoch 856/1000 - 5.054706s - Loss 0.093882 - Accuracy 0.781250 - Test Loss 2.724439 - Test Accuracy 0.266578\n",
      "Epoch 857/1000 - 5.090649s - Loss 0.119897 - Accuracy 0.750000 - Test Loss 2.671854 - Test Accuracy 0.278515\n",
      "Epoch 858/1000 - 5.061589s - Loss 0.097469 - Accuracy 0.770833 - Test Loss 2.764610 - Test Accuracy 0.316976\n",
      "Epoch 859/1000 - 5.081055s - Loss 0.137085 - Accuracy 0.765625 - Test Loss 2.898393 - Test Accuracy 0.320513\n",
      "Epoch 860/1000 - 5.067881s - Loss 0.108739 - Accuracy 0.729167 - Test Loss 2.654593 - Test Accuracy 0.271441\n",
      "Epoch 861/1000 - 5.068149s - Loss 0.137518 - Accuracy 0.750000 - Test Loss 2.799298 - Test Accuracy 0.286914\n",
      "Epoch 862/1000 - 5.063707s - Loss 0.102274 - Accuracy 0.781250 - Test Loss 2.961936 - Test Accuracy 0.333775\n",
      "Epoch 863/1000 - 5.057148s - Loss 0.138503 - Accuracy 0.796875 - Test Loss 2.757231 - Test Accuracy 0.285588\n",
      "Epoch 864/1000 - 5.059640s - Loss 0.135116 - Accuracy 0.744792 - Test Loss 2.923556 - Test Accuracy 0.313439\n",
      "Epoch 865/1000 - 5.075318s - Loss 0.099581 - Accuracy 0.812500 - Test Loss 3.063475 - Test Accuracy 0.307692\n",
      "Epoch 866/1000 - 5.055560s - Loss 0.074794 - Accuracy 0.817708 - Test Loss 2.704787 - Test Accuracy 0.286472\n",
      "Epoch 867/1000 - 5.080269s - Loss 0.081732 - Accuracy 0.833333 - Test Loss 2.631298 - Test Accuracy 0.271441\n",
      "Epoch 868/1000 - 5.063425s - Loss 0.107391 - Accuracy 0.770833 - Test Loss 3.048455 - Test Accuracy 0.336870\n",
      "Epoch 869/1000 - 5.067596s - Loss 0.070121 - Accuracy 0.843750 - Test Loss 2.801296 - Test Accuracy 0.287356\n",
      "Epoch 870/1000 - 5.073322s - Loss 0.073183 - Accuracy 0.812500 - Test Loss 2.734367 - Test Accuracy 0.270115\n",
      "Epoch 871/1000 - 5.082174s - Loss 0.088555 - Accuracy 0.854167 - Test Loss 2.960141 - Test Accuracy 0.315208\n",
      "Epoch 872/1000 - 5.076266s - Loss 0.071927 - Accuracy 0.838542 - Test Loss 2.992493 - Test Accuracy 0.316976\n",
      "Epoch 873/1000 - 5.101207s - Loss 0.092145 - Accuracy 0.791667 - Test Loss 2.872355 - Test Accuracy 0.296640\n",
      "Epoch 874/1000 - 5.053021s - Loss 0.080189 - Accuracy 0.812500 - Test Loss 3.086690 - Test Accuracy 0.330681\n",
      "Epoch 875/1000 - 5.075690s - Loss 0.088901 - Accuracy 0.859375 - Test Loss 2.967778 - Test Accuracy 0.317418\n",
      "Epoch 876/1000 - 5.060157s - Loss 0.075762 - Accuracy 0.817708 - Test Loss 2.703350 - Test Accuracy 0.280725\n",
      "Epoch 877/1000 - 5.088238s - Loss 0.111191 - Accuracy 0.796875 - Test Loss 3.103219 - Test Accuracy 0.301503\n",
      "Epoch 878/1000 - 5.060777s - Loss 0.117515 - Accuracy 0.750000 - Test Loss 2.900671 - Test Accuracy 0.286472\n",
      "Epoch 879/1000 - 5.077211s - Loss 0.111912 - Accuracy 0.817708 - Test Loss 2.795356 - Test Accuracy 0.296640\n",
      "Epoch 880/1000 - 5.235563s - Loss 0.101564 - Accuracy 0.838542 - Test Loss 2.816443 - Test Accuracy 0.277630\n",
      "Epoch 881/1000 - 5.088075s - Loss 0.131635 - Accuracy 0.791667 - Test Loss 3.267277 - Test Accuracy 0.323607\n",
      "Epoch 882/1000 - 5.090328s - Loss 0.128463 - Accuracy 0.755208 - Test Loss 2.772953 - Test Accuracy 0.280283\n",
      "Epoch 883/1000 - 5.081589s - Loss 0.090490 - Accuracy 0.817708 - Test Loss 2.979231 - Test Accuracy 0.303271\n",
      "Epoch 884/1000 - 5.074154s - Loss 0.104882 - Accuracy 0.807292 - Test Loss 2.818244 - Test Accuracy 0.304598\n",
      "Epoch 885/1000 - 5.072127s - Loss 0.097926 - Accuracy 0.791667 - Test Loss 2.672765 - Test Accuracy 0.284704\n",
      "Epoch 886/1000 - 5.095476s - Loss 0.115191 - Accuracy 0.750000 - Test Loss 2.799875 - Test Accuracy 0.300177\n",
      "Epoch 887/1000 - 5.071521s - Loss 0.101515 - Accuracy 0.828125 - Test Loss 2.680273 - Test Accuracy 0.277188\n",
      "Epoch 888/1000 - 5.098189s - Loss 0.067283 - Accuracy 0.854167 - Test Loss 2.867799 - Test Accuracy 0.300619\n",
      "Epoch 889/1000 - 5.060233s - Loss 0.115450 - Accuracy 0.776042 - Test Loss 2.763469 - Test Accuracy 0.298408\n",
      "Epoch 890/1000 - 5.084604s - Loss 0.105486 - Accuracy 0.786458 - Test Loss 2.646859 - Test Accuracy 0.286472\n",
      "Epoch 891/1000 - 5.068163s - Loss 0.068596 - Accuracy 0.812500 - Test Loss 2.915981 - Test Accuracy 0.301503\n",
      "Epoch 892/1000 - 5.073716s - Loss 0.057420 - Accuracy 0.848958 - Test Loss 2.832870 - Test Accuracy 0.302387\n",
      "Epoch 893/1000 - 5.065915s - Loss 0.092683 - Accuracy 0.755208 - Test Loss 2.824252 - Test Accuracy 0.294430\n",
      "Epoch 894/1000 - 5.105483s - Loss 0.129389 - Accuracy 0.765625 - Test Loss 2.832036 - Test Accuracy 0.312997\n",
      "Epoch 895/1000 - 5.086313s - Loss 0.067846 - Accuracy 0.848958 - Test Loss 2.888100 - Test Accuracy 0.308134\n",
      "Epoch 896/1000 - 5.090955s - Loss 0.091581 - Accuracy 0.791667 - Test Loss 2.747763 - Test Accuracy 0.280725\n",
      "Epoch 897/1000 - 5.074751s - Loss 0.096720 - Accuracy 0.791667 - Test Loss 2.649943 - Test Accuracy 0.309903\n",
      "Epoch 898/1000 - 5.083458s - Loss 0.102904 - Accuracy 0.770833 - Test Loss 2.598355 - Test Accuracy 0.274536\n",
      "Epoch 899/1000 - 5.085886s - Loss 0.104518 - Accuracy 0.786458 - Test Loss 2.742836 - Test Accuracy 0.291777\n",
      "Epoch 900/1000 - 5.076401s - Loss 0.098533 - Accuracy 0.786458 - Test Loss 2.793132 - Test Accuracy 0.302387\n",
      "Epoch 901/1000 - 5.059228s - Loss 0.076917 - Accuracy 0.838542 - Test Loss 2.748050 - Test Accuracy 0.283820\n",
      "Epoch 902/1000 - 5.060029s - Loss 0.108963 - Accuracy 0.828125 - Test Loss 3.095233 - Test Accuracy 0.302387\n",
      "Epoch 903/1000 - 5.081110s - Loss 0.089870 - Accuracy 0.807292 - Test Loss 2.833362 - Test Accuracy 0.297082\n",
      "Epoch 904/1000 - 5.091877s - Loss 0.086709 - Accuracy 0.838542 - Test Loss 2.728826 - Test Accuracy 0.280283\n",
      "Epoch 905/1000 - 5.060450s - Loss 0.063456 - Accuracy 0.859375 - Test Loss 2.841091 - Test Accuracy 0.311671\n",
      "Epoch 906/1000 - 5.074118s - Loss 0.108632 - Accuracy 0.760417 - Test Loss 3.001773 - Test Accuracy 0.321397\n",
      "Epoch 907/1000 - 5.067750s - Loss 0.073314 - Accuracy 0.854167 - Test Loss 2.954640 - Test Accuracy 0.307250\n",
      "Epoch 908/1000 - 5.087616s - Loss 0.130643 - Accuracy 0.781250 - Test Loss 2.816445 - Test Accuracy 0.324050\n",
      "Epoch 909/1000 - 5.071952s - Loss 0.091744 - Accuracy 0.828125 - Test Loss 2.717331 - Test Accuracy 0.286914\n",
      "Epoch 910/1000 - 5.104252s - Loss 0.125891 - Accuracy 0.781250 - Test Loss 2.789384 - Test Accuracy 0.311671\n",
      "Epoch 911/1000 - 5.089181s - Loss 0.090496 - Accuracy 0.833333 - Test Loss 3.091250 - Test Accuracy 0.330681\n",
      "Epoch 912/1000 - 5.097701s - Loss 0.071211 - Accuracy 0.833333 - Test Loss 2.974878 - Test Accuracy 0.322281\n",
      "Epoch 913/1000 - 5.085399s - Loss 0.102574 - Accuracy 0.770833 - Test Loss 2.877221 - Test Accuracy 0.297082\n",
      "Epoch 914/1000 - 5.116040s - Loss 0.185083 - Accuracy 0.744792 - Test Loss 2.823781 - Test Accuracy 0.291777\n",
      "Epoch 915/1000 - 5.058443s - Loss 0.100115 - Accuracy 0.791667 - Test Loss 2.935040 - Test Accuracy 0.297082\n",
      "Epoch 916/1000 - 5.103084s - Loss 0.117514 - Accuracy 0.765625 - Test Loss 2.954202 - Test Accuracy 0.316092\n",
      "Epoch 917/1000 - 5.067792s - Loss 0.086718 - Accuracy 0.854167 - Test Loss 2.735230 - Test Accuracy 0.298408\n",
      "Epoch 918/1000 - 5.090151s - Loss 0.083231 - Accuracy 0.843750 - Test Loss 2.800952 - Test Accuracy 0.321839\n",
      "Epoch 919/1000 - 5.080810s - Loss 0.065354 - Accuracy 0.848958 - Test Loss 2.798122 - Test Accuracy 0.300619\n",
      "Epoch 920/1000 - 5.095883s - Loss 0.104539 - Accuracy 0.828125 - Test Loss 2.796906 - Test Accuracy 0.317418\n",
      "Epoch 921/1000 - 5.080403s - Loss 0.138653 - Accuracy 0.786458 - Test Loss 2.878621 - Test Accuracy 0.312113\n",
      "Epoch 922/1000 - 5.068293s - Loss 0.127742 - Accuracy 0.755208 - Test Loss 2.763035 - Test Accuracy 0.318302\n",
      "Epoch 923/1000 - 5.087045s - Loss 0.081174 - Accuracy 0.843750 - Test Loss 2.745671 - Test Accuracy 0.292661\n",
      "Epoch 924/1000 - 5.078413s - Loss 0.080770 - Accuracy 0.812500 - Test Loss 2.994122 - Test Accuracy 0.359859\n",
      "Epoch 925/1000 - 5.070282s - Loss 0.152463 - Accuracy 0.750000 - Test Loss 3.148083 - Test Accuracy 0.335102\n",
      "Epoch 926/1000 - 5.068065s - Loss 0.075909 - Accuracy 0.833333 - Test Loss 2.917949 - Test Accuracy 0.332891\n",
      "Epoch 927/1000 - 5.058521s - Loss 0.124045 - Accuracy 0.755208 - Test Loss 2.634152 - Test Accuracy 0.269231\n",
      "Epoch 928/1000 - 5.088168s - Loss 0.080303 - Accuracy 0.796875 - Test Loss 2.781411 - Test Accuracy 0.301945\n",
      "Epoch 929/1000 - 5.082890s - Loss 0.086572 - Accuracy 0.854167 - Test Loss 3.064179 - Test Accuracy 0.319187\n",
      "Epoch 930/1000 - 5.084051s - Loss 0.085414 - Accuracy 0.812500 - Test Loss 2.847851 - Test Accuracy 0.280283\n",
      "Epoch 931/1000 - 5.069702s - Loss 0.097781 - Accuracy 0.781250 - Test Loss 2.751966 - Test Accuracy 0.307692\n",
      "Epoch 932/1000 - 5.089793s - Loss 0.102665 - Accuracy 0.812500 - Test Loss 2.732906 - Test Accuracy 0.298408\n",
      "Epoch 933/1000 - 5.074261s - Loss 0.081495 - Accuracy 0.802083 - Test Loss 2.827031 - Test Accuracy 0.305924\n",
      "Epoch 934/1000 - 5.083758s - Loss 0.089259 - Accuracy 0.848958 - Test Loss 3.028867 - Test Accuracy 0.326702\n",
      "Epoch 935/1000 - 5.089428s - Loss 0.057057 - Accuracy 0.875000 - Test Loss 2.798756 - Test Accuracy 0.289567\n",
      "Epoch 936/1000 - 5.086406s - Loss 0.124540 - Accuracy 0.807292 - Test Loss 2.801968 - Test Accuracy 0.308576\n",
      "Epoch 937/1000 - 5.056315s - Loss 0.129593 - Accuracy 0.791667 - Test Loss 2.860104 - Test Accuracy 0.304156\n",
      "Epoch 938/1000 - 5.068326s - Loss 0.082112 - Accuracy 0.838542 - Test Loss 2.800159 - Test Accuracy 0.310787\n",
      "Epoch 939/1000 - 5.065861s - Loss 0.069463 - Accuracy 0.833333 - Test Loss 2.906402 - Test Accuracy 0.335544\n",
      "Epoch 940/1000 - 5.072946s - Loss 0.110281 - Accuracy 0.807292 - Test Loss 2.968458 - Test Accuracy 0.319629\n",
      "Epoch 941/1000 - 5.067354s - Loss 0.126998 - Accuracy 0.796875 - Test Loss 2.983663 - Test Accuracy 0.315650\n",
      "Epoch 942/1000 - 5.094802s - Loss 0.048891 - Accuracy 0.890625 - Test Loss 2.809142 - Test Accuracy 0.296640\n",
      "Epoch 943/1000 - 5.083057s - Loss 0.091643 - Accuracy 0.822917 - Test Loss 2.592123 - Test Accuracy 0.257737\n",
      "Epoch 944/1000 - 5.085855s - Loss 0.150148 - Accuracy 0.713542 - Test Loss 2.734564 - Test Accuracy 0.270999\n",
      "Epoch 945/1000 - 5.075377s - Loss 0.132145 - Accuracy 0.765625 - Test Loss 2.661509 - Test Accuracy 0.289567\n",
      "Epoch 946/1000 - 5.080755s - Loss 0.113404 - Accuracy 0.807292 - Test Loss 2.714539 - Test Accuracy 0.290009\n",
      "Epoch 947/1000 - 5.088248s - Loss 0.075789 - Accuracy 0.828125 - Test Loss 2.598140 - Test Accuracy 0.282493\n",
      "Epoch 948/1000 - 5.062099s - Loss 0.104272 - Accuracy 0.744792 - Test Loss 2.768248 - Test Accuracy 0.270999\n",
      "Epoch 949/1000 - 5.084969s - Loss 0.092474 - Accuracy 0.781250 - Test Loss 2.897729 - Test Accuracy 0.327586\n",
      "Epoch 950/1000 - 5.059424s - Loss 0.088121 - Accuracy 0.822917 - Test Loss 2.780776 - Test Accuracy 0.303271\n",
      "Epoch 951/1000 - 5.103533s - Loss 0.088967 - Accuracy 0.854167 - Test Loss 2.884017 - Test Accuracy 0.325818\n",
      "Epoch 952/1000 - 5.052794s - Loss 0.067394 - Accuracy 0.807292 - Test Loss 2.770565 - Test Accuracy 0.317860\n",
      "Epoch 953/1000 - 5.078110s - Loss 0.086991 - Accuracy 0.781250 - Test Loss 2.683028 - Test Accuracy 0.287356\n",
      "Epoch 954/1000 - 5.089049s - Loss 0.085066 - Accuracy 0.817708 - Test Loss 2.759134 - Test Accuracy 0.270115\n",
      "Epoch 955/1000 - 5.089077s - Loss 0.136428 - Accuracy 0.755208 - Test Loss 2.894738 - Test Accuracy 0.281167\n",
      "Epoch 956/1000 - 5.086479s - Loss 0.108608 - Accuracy 0.776042 - Test Loss 2.827238 - Test Accuracy 0.306808\n",
      "Epoch 957/1000 - 5.095312s - Loss 0.077839 - Accuracy 0.875000 - Test Loss 2.965308 - Test Accuracy 0.324934\n",
      "Epoch 958/1000 - 5.095309s - Loss 0.114578 - Accuracy 0.791667 - Test Loss 2.987346 - Test Accuracy 0.340849\n",
      "Epoch 959/1000 - 5.101479s - Loss 0.103341 - Accuracy 0.770833 - Test Loss 2.745784 - Test Accuracy 0.280283\n",
      "Epoch 960/1000 - 5.066268s - Loss 0.146516 - Accuracy 0.760417 - Test Loss 2.812463 - Test Accuracy 0.298408\n",
      "Epoch 961/1000 - 5.074200s - Loss 0.099291 - Accuracy 0.796875 - Test Loss 2.717860 - Test Accuracy 0.285588\n",
      "Epoch 962/1000 - 5.087168s - Loss 0.101298 - Accuracy 0.807292 - Test Loss 2.827551 - Test Accuracy 0.298408\n",
      "Epoch 963/1000 - 5.078423s - Loss 0.091885 - Accuracy 0.796875 - Test Loss 3.076623 - Test Accuracy 0.333333\n",
      "Epoch 964/1000 - 5.084565s - Loss 0.070019 - Accuracy 0.854167 - Test Loss 2.835791 - Test Accuracy 0.315208\n",
      "Epoch 965/1000 - 5.082896s - Loss 0.078988 - Accuracy 0.854167 - Test Loss 2.791322 - Test Accuracy 0.301945\n",
      "Epoch 966/1000 - 5.074442s - Loss 0.084850 - Accuracy 0.838542 - Test Loss 2.792340 - Test Accuracy 0.319187\n",
      "Epoch 967/1000 - 5.074165s - Loss 0.108190 - Accuracy 0.807292 - Test Loss 2.779231 - Test Accuracy 0.305482\n",
      "Epoch 968/1000 - 5.060746s - Loss 0.068426 - Accuracy 0.838542 - Test Loss 2.950958 - Test Accuracy 0.310787\n",
      "Epoch 969/1000 - 5.067770s - Loss 0.108328 - Accuracy 0.807292 - Test Loss 3.035930 - Test Accuracy 0.320071\n",
      "Epoch 970/1000 - 5.074569s - Loss 0.125111 - Accuracy 0.776042 - Test Loss 2.942942 - Test Accuracy 0.308134\n",
      "Epoch 971/1000 - 5.081757s - Loss 0.080282 - Accuracy 0.802083 - Test Loss 2.886189 - Test Accuracy 0.293988\n",
      "Epoch 972/1000 - 5.071437s - Loss 0.110116 - Accuracy 0.770833 - Test Loss 2.963185 - Test Accuracy 0.321839\n",
      "Epoch 973/1000 - 5.074587s - Loss 0.080903 - Accuracy 0.848958 - Test Loss 2.882091 - Test Accuracy 0.300177\n",
      "Epoch 974/1000 - 5.074300s - Loss 0.112796 - Accuracy 0.760417 - Test Loss 2.791105 - Test Accuracy 0.287798\n",
      "Epoch 975/1000 - 5.075875s - Loss 0.082285 - Accuracy 0.781250 - Test Loss 3.052221 - Test Accuracy 0.317860\n",
      "Epoch 976/1000 - 5.070030s - Loss 0.077226 - Accuracy 0.812500 - Test Loss 2.839190 - Test Accuracy 0.292219\n",
      "Epoch 977/1000 - 5.087425s - Loss 0.083027 - Accuracy 0.812500 - Test Loss 2.917299 - Test Accuracy 0.295314\n",
      "Epoch 978/1000 - 5.068468s - Loss 0.088159 - Accuracy 0.776042 - Test Loss 3.168573 - Test Accuracy 0.304598\n",
      "Epoch 979/1000 - 5.110552s - Loss 0.103240 - Accuracy 0.828125 - Test Loss 2.758425 - Test Accuracy 0.302829\n",
      "Epoch 980/1000 - 5.075146s - Loss 0.070035 - Accuracy 0.828125 - Test Loss 2.864268 - Test Accuracy 0.330239\n",
      "Epoch 981/1000 - 5.071311s - Loss 0.076286 - Accuracy 0.822917 - Test Loss 2.689822 - Test Accuracy 0.299735\n",
      "Epoch 982/1000 - 5.067230s - Loss 0.100950 - Accuracy 0.843750 - Test Loss 2.733493 - Test Accuracy 0.305924\n",
      "Epoch 983/1000 - 5.074032s - Loss 0.133885 - Accuracy 0.786458 - Test Loss 2.603479 - Test Accuracy 0.307250\n",
      "Epoch 984/1000 - 5.073855s - Loss 0.095942 - Accuracy 0.776042 - Test Loss 2.852904 - Test Accuracy 0.332007\n",
      "Epoch 985/1000 - 5.093767s - Loss 0.067739 - Accuracy 0.802083 - Test Loss 2.940616 - Test Accuracy 0.305924\n",
      "Epoch 986/1000 - 5.072992s - Loss 0.094510 - Accuracy 0.802083 - Test Loss 2.685727 - Test Accuracy 0.280725\n",
      "Epoch 987/1000 - 5.084033s - Loss 0.123173 - Accuracy 0.770833 - Test Loss 2.889698 - Test Accuracy 0.292661\n",
      "Epoch 988/1000 - 5.080981s - Loss 0.073460 - Accuracy 0.822917 - Test Loss 2.692650 - Test Accuracy 0.287356\n",
      "Epoch 989/1000 - 5.076673s - Loss 0.091700 - Accuracy 0.791667 - Test Loss 2.910842 - Test Accuracy 0.309461\n",
      "Epoch 990/1000 - 5.089439s - Loss 0.099830 - Accuracy 0.822917 - Test Loss 2.767202 - Test Accuracy 0.301945\n",
      "Epoch 991/1000 - 5.116033s - Loss 0.093183 - Accuracy 0.828125 - Test Loss 2.755828 - Test Accuracy 0.302387\n",
      "Epoch 992/1000 - 5.066075s - Loss 0.138756 - Accuracy 0.765625 - Test Loss 2.986115 - Test Accuracy 0.316976\n",
      "Epoch 993/1000 - 5.091265s - Loss 0.111671 - Accuracy 0.833333 - Test Loss 3.028101 - Test Accuracy 0.338196\n",
      "Epoch 994/1000 - 5.063194s - Loss 0.083183 - Accuracy 0.817708 - Test Loss 2.763764 - Test Accuracy 0.300619\n",
      "Epoch 995/1000 - 5.091496s - Loss 0.095883 - Accuracy 0.838542 - Test Loss 2.867923 - Test Accuracy 0.316092\n",
      "Epoch 996/1000 - 5.066613s - Loss 0.065978 - Accuracy 0.833333 - Test Loss 2.922261 - Test Accuracy 0.313439\n",
      "Epoch 997/1000 - 5.106394s - Loss 0.092987 - Accuracy 0.796875 - Test Loss 2.533104 - Test Accuracy 0.271441\n",
      "Epoch 998/1000 - 5.071983s - Loss 0.091054 - Accuracy 0.854167 - Test Loss 2.819171 - Test Accuracy 0.303271\n",
      "Epoch 999/1000 - 5.071737s - Loss 0.059539 - Accuracy 0.869792 - Test Loss 2.897738 - Test Accuracy 0.299293\n",
      "repeat_0 [0.5442086648983201]\n",
      "{'repeat_0': {'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 102}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 133}, '2': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 116}, '3': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 94}, '4': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 107}, '5': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 110}, '6': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 99}, '7': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 110}, '8': {'precision': 0.08441558441558442, 'recall': 0.11711711711711711, 'f1-score': 0.0981132075471698, 'support': 111}, 'micro avg': {'precision': 0.08125, 'recall': 0.013238289205702648, 'f1-score': 0.022767075306479864, 'support': 982}, 'macro avg': {'precision': 0.00937950937950938, 'recall': 0.013013013013013013, 'f1-score': 0.01090146750524109, 'support': 982}, 'weighted avg': {'precision': 0.009541883778136324, 'recall': 0.013238289205702648, 'f1-score': 0.011090189447796179, 'support': 982}, 'samples avg': {'precision': 0.005747126436781609, 'recall': 0.005747126436781609, 'f1-score': 0.005747126436781609, 'support': 982}}}\n",
      "{'repeat_0': {'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 102}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 133}, '2': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 116}, '3': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 94}, '4': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 107}, '5': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 110}, '6': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 99}, '7': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 110}, '8': {'precision': 0.08441558441558442, 'recall': 0.11711711711711711, 'f1-score': 0.0981132075471698, 'support': 111}, 'micro avg': {'precision': 0.08125, 'recall': 0.013238289205702648, 'f1-score': 0.022767075306479864, 'support': 982}, 'macro avg': {'precision': 0.00937950937950938, 'recall': 0.013013013013013013, 'f1-score': 0.01090146750524109, 'support': 982}, 'weighted avg': {'precision': 0.009541883778136324, 'recall': 0.013238289205702648, 'f1-score': 0.011090189447796179, 'support': 982}, 'samples avg': {'precision': 0.005747126436781609, 'recall': 0.005747126436781609, 'f1-score': 0.005747126436781609, 'support': 982}}, 'accuracy': {'avg': 0.5442086648983201, 'std': 0.0}, 'time_train': {'avg': 5079.5013926029205, 'std': 0.0}, 'time_test': {'avg': 0.4783790111541748, 'std': 0.0}, 'model': 'THAT', 'task': 'activity', 'data': {'num_users': ['0', '1', '2', '3', '4', '5'], 'wifi_band': ['2.4'], 'environment': ['classroom'], 'length': 3000}, 'nn': {'lr': 0.0005, 'epoch': 1000, 'batch_size': 64, 'threshold': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()           \n",
    "torch.cuda.empty_cache()  \n",
    "torch.cuda.ipc_collect()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "[file]          run.py\n",
    "[description]   run WiFi-based models and optionally save a multiclass confusion matrix\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# from preset import preset, name_run\n",
    "# from load_data import load_data_x, load_data_y, encode_data_y\n",
    "# from lstm import run_lstm, LSTMM\n",
    "# from bilstm import run_bilstm, BiLSTMM\n",
    "# from that import run_that, THAT\n",
    "# from resnet import run_resnet, ResNet18Model\n",
    "# from strf import run_strf  # if you have the ST-RF implementation\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model\",   default=preset[\"model\"],  type=str)\n",
    "    parser.add_argument(\"--task\",    default=preset[\"task\"],   type=str)\n",
    "    parser.add_argument(\"--repeat\",  default=preset[\"repeat\"], type=int)\n",
    "    parser.add_argument(\"--save_cm\", action=\"store_true\",\n",
    "                        help=\"Save a multiclass confusion matrix of the best model to PDF\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def save_multiclass_confusion_matrix(model, data_loader, device, pdf_path, num_classes):\n",
    "    \"\"\"\n",
    "    Given a model that outputs one-hot logits for a multiclass task,\n",
    "    convert to predicted classes via argmax, then plot and save a\n",
    "    num_classes × num_classes confusion matrix to pdf_path.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            # predicted class is index of max logit\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            trues = torch.argmax(yb, dim=1).cpu().numpy()\n",
    "            y_pred.extend(preds.tolist())\n",
    "            y_true.extend(trues.tolist())\n",
    "\n",
    "    labels = list(range(num_classes))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    disp.plot(ax=ax, xticks_rotation=\"vertical\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "def run():\n",
    "    args       = parse_args()\n",
    "    var_model  = args.model\n",
    "    var_task   = args.task\n",
    "    var_repeat = args.repeat\n",
    "\n",
    "    # --- Load and encode the data ---\n",
    "    data_pd_y = load_data_y(\n",
    "        preset[\"path\"][\"data_y\"],\n",
    "        var_environment=preset[\"data\"][\"environment\"],\n",
    "        var_wifi_band=preset[\"data\"][\"wifi_band\"],\n",
    "        var_num_users=preset[\"data\"][\"num_users\"]\n",
    "    )\n",
    "    labels = data_pd_y[\"label\"].tolist()\n",
    "    data_x = load_data_x(preset[\"path\"][\"data_x\"], labels)\n",
    "    data_y = encode_data_y(data_pd_y, var_task)\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(\n",
    "        data_x, data_y, test_size=0.2, shuffle=True, random_state=39\n",
    "    )\n",
    "\n",
    "    # --- Select which model runner to use ---\n",
    "    if var_model == \"ST-RF\":\n",
    "        from strf import run_strf\n",
    "        run_model = run_strf\n",
    "    elif var_model == \"LSTM\":\n",
    "        run_model = run_lstm\n",
    "    elif var_model == \"bi-LSTM\":\n",
    "        run_model = run_bilstm\n",
    "    elif var_model == \"THAT\":\n",
    "        run_model = run_that\n",
    "    elif var_model == \"ResNet18\":\n",
    "        run_model = run_resnet\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {var_model}\")\n",
    "\n",
    "    # --- Train and evaluate ---\n",
    "    print(f\"Running model: {var_model}\")\n",
    "    result = run_model(train_x, train_y, test_x, test_y, var_repeat)\n",
    "    result[\"model\"] = var_model\n",
    "    result[\"task\"]  = var_task\n",
    "    result[\"data\"]  = preset[\"data\"]\n",
    "    result[\"nn\"]    = preset[\"nn\"]\n",
    "    print(result)\n",
    "\n",
    "    # --- Save results to JSON ---\n",
    "    # with open(preset[\"path\"][\"save\"], \"w\") as f:\n",
    "    #     json.dump(result, f, indent=4)\n",
    "\n",
    "    # # --- Optionally save a multiclass confusion matrix ---\n",
    "    # # if args.save_cm:\n",
    "    # if Confusion_matrix == 1:\n",
    "    #     # 1) completely release GPU memory used for training\n",
    "    #     del run_model                      # if 'model' from training is still in scope\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     torch.cuda.ipc_collect()\n",
    "    \n",
    "    #     # 2) reshape input only if the network is sequence‑based\n",
    "    #     if var_model in (\"LSTM\", \"bi-LSTM\", \"THAT\"):\n",
    "    #         test_x_cm = test_x.reshape(test_x.shape[0], test_x.shape[1], -1)\n",
    "    #     else:                           # ResNet18, ST‑RF\n",
    "    #         test_x_cm = test_x\n",
    "    \n",
    "    #     # 3) build the *same* architecture on CPU and load its weights\n",
    "    #     device_cm = torch.device(\"cpu\")\n",
    "    #     if var_model == \"LSTM\":\n",
    "    #         model_cm = LSTMM(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     elif var_model == \"bi-LSTM\":\n",
    "    #         model_cm = BiLSTMM(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     elif var_model == \"THAT\":\n",
    "    #         model_cm = THAT(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     elif var_model == \"ResNet18\":\n",
    "    #         model_cm = ResNet18Model(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Confusion matrix not supported for {var_model}\")\n",
    "    \n",
    "    #     # best_path = f\"/kaggle/working/{name_run}_best_model.pt\"\n",
    "    #     # model_cm.load_state_dict(torch.load(best_path, map_location=device_cm))\n",
    "    #     # model_cm.eval()\n",
    "    \n",
    "    #     # 4) DataLoader on CPU with a safe batch size\n",
    "    #     test_ds = TensorDataset(torch.from_numpy(test_x_cm).float(),\n",
    "    #                             torch.from_numpy(test_y).float())\n",
    "    #     test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "    \n",
    "    #     # 5) save the confusion matrix PDF\n",
    "    #     num_classes = test_y.shape[1]\n",
    "    #     pdf_name = f\"{name_run}_confusion_matrix.pdf\"\n",
    "    #     save_multiclass_confusion_matrix(model_cm,test_loader,device_cm,pdf_name,num_classes)\n",
    "    #     print(f\"✅ Saved confusion matrix (classes 0–{num_classes-1}) to {pdf_name}\")\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"start\")\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec612f16",
   "metadata": {
    "papermill": {
     "duration": 0.057269,
     "end_time": "2025-12-26T01:26:35.781324",
     "exception": false,
     "start_time": "2025-12-26T01:26:35.724055",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 12: Few-shot Learning\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98da3dcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T01:26:35.958651Z",
     "iopub.status.busy": "2025-12-26T01:26:35.958370Z",
     "iopub.status.idle": "2025-12-26T01:26:35.964158Z",
     "shell.execute_reply": "2025-12-26T01:26:35.963536Z"
    },
    "papermill": {
     "duration": 0.125692,
     "end_time": "2025-12-26T01:26:35.965252",
     "exception": false,
     "start_time": "2025-12-26T01:26:35.839560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import torch\n",
    "# import shutil\n",
    "# import json\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# gc.collect()           \n",
    "# torch.cuda.empty_cache()  \n",
    "# torch.cuda.ipc_collect()\n",
    "\n",
    "# # ---------- helper: save multiclass confusion matrix ------------------\n",
    "# def save_multiclass_confusion_matrix(model, data_loader, pdf_path, num_classes):\n",
    "#     \"\"\"\n",
    "#     Forward‑pass on CPU, collect predictions, and write an N×N confusion matrix\n",
    "#     to a single‑page PDF (pdf_path).\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     y_true, y_pred = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in data_loader:\n",
    "#             logits = model(xb.cpu())                       # ensure CPU\n",
    "#             preds  = torch.argmax(logits, dim=1).numpy()\n",
    "#             trues  = torch.argmax(yb, dim=1).numpy()\n",
    "#             y_pred.extend(preds.tolist())\n",
    "#             y_true.extend(trues.tolist())\n",
    "\n",
    "#     labels = list(range(num_classes))\n",
    "#     cm  = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "#     disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "#     fig, ax = plt.subplots(figsize=(8, 8))\n",
    "#     disp.plot(ax=ax, xticks_rotation=\"vertical\")\n",
    "#     ax.set_title(\"Few‑shot Confusion Matrix\")\n",
    "#     with PdfPages(pdf_path) as pdf:\n",
    "#         pdf.savefig(fig)\n",
    "#     plt.close(fig)\n",
    "\n",
    "# # -------------------- pick run_* function ------------------------------\n",
    "# if preset[\"model\"] == \"ST-RF\":\n",
    "#     run_model = run_strf\n",
    "# elif preset[\"model\"] == \"LSTM\":\n",
    "#     run_model = run_lstm\n",
    "# elif preset[\"model\"] == \"bi-LSTM\":\n",
    "#     run_model = run_bilstm\n",
    "# elif preset[\"model\"] == \"THAT\":\n",
    "#     run_model = run_that\n",
    "# elif preset[\"model\"] == \"ResNet18\":\n",
    "#     run_model = run_resnet\n",
    "# else:\n",
    "#     raise ValueError(f\"No few‑shot implementation for {preset['model']}.\")\n",
    "\n",
    "# # ------------------------ load / split data ----------------------------\n",
    "# data_pd_y = load_data_y(preset[\"path\"][\"data_y\"],\n",
    "#                         var_environment=[dest_env],\n",
    "#                         var_wifi_band=preset[\"data\"][\"wifi_band\"],\n",
    "#                         var_num_users=preset[\"data\"][\"num_users\"])\n",
    "\n",
    "# labels_list = data_pd_y[\"label\"].tolist()\n",
    "# data_x = load_data_x(preset[\"path\"][\"data_x\"], labels_list)\n",
    "# data_y = encode_data_y(data_pd_y, preset[\"task\"])\n",
    "\n",
    "# train_x, test_x, train_y, test_y = train_test_split(\n",
    "#     data_x, data_y, test_size=0.2, shuffle=True, random_state=39)\n",
    "\n",
    "# # Few-shot sample size\n",
    "# train_x = train_x[:few_shot_num_samples]\n",
    "# train_y = train_y[:few_shot_num_samples]\n",
    "\n",
    "# # ----------------------- few‑shot training -----------------------------\n",
    "# original_epochs = preset[\"nn\"][\"epoch\"]\n",
    "# preset[\"nn\"][\"epoch\"] = few_shot_epochs\n",
    "\n",
    "# # Load the best model weights\n",
    "# best_model_path = f\"{name_run}_best_model.pt\"\n",
    "\n",
    "# # Initialize the model \n",
    "# if preset[\"model\"] == \"LSTM\":\n",
    "#     model = LSTMM(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "#     # print('train_y_[0].shape:', train_y[0].shape)\n",
    "#     # print('train_x_[0].shape:', train_x[0].reshape(train_x[0].shape[0], -1).shape)\n",
    "# elif preset[\"model\"] == \"bi-LSTM\":\n",
    "#     model = BiLSTMM(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "# elif preset[\"model\"] == \"THAT\":\n",
    "#     model = THAT(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "# elif preset[\"model\"] == \"ResNet18\":\n",
    "#     model = ResNet18Model(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "# else:\n",
    "#     raise ValueError(f\"Model {preset['model']} not supported!\")\n",
    "\n",
    "# # Load the weights into the model\n",
    "# model.load_state_dict(torch.load(best_model_path, map_location=\"cpu\"))\n",
    "# model = model.to('cuda')\n",
    "\n",
    "# # Fine-tune the model on few-shot data (note: `run_model` should now return only the result)\n",
    "# result = run_model(train_x, train_y, test_x, test_y, var_repeat=1, init_model=model)\n",
    "# print(result)\n",
    "\n",
    "# # --------------------- save few‑shot checkpoints -----------------------\n",
    "# # After fine-tuning, save the model\n",
    "# torch.save(model.state_dict(), f\"{name_run}_fewshot_final_model.pt\")\n",
    "# torch.save(model.state_dict(), f\"{name_run}_fewshot_best_model.pt\")\n",
    "\n",
    "# # ------------------- confusion matrix on CPU ---------------------------\n",
    "# if Confusion_matrix == 1 and preset[\"model\"] != \"ST-RF\":\n",
    "\n",
    "#     # reshape for sequence models\n",
    "#     test_x_rs = (test_x.reshape(test_x.shape[0], test_x.shape[1], -1)\n",
    "#                  if preset[\"model\"] in (\"LSTM\", \"bi-LSTM\", \"THAT\") else test_x)\n",
    "\n",
    "#     # instantiate identical architecture on CPU\n",
    "#     if preset[\"model\"] == \"LSTM\":\n",
    "#         model_cpu = LSTMM(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "#     elif preset[\"model\"] == \"bi-LSTM\":\n",
    "#         model_cpu = BiLSTMM(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "#     elif preset[\"model\"] == \"THAT\":\n",
    "#         model_cpu = THAT(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "#     else:  # ResNet18\n",
    "#         model_cpu = ResNet18Model(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "\n",
    "#     # load weights\n",
    "#     model_cpu.load_state_dict(torch.load(f\"{name_run}_fewshot_best_model.pt\", map_location=\"cpu\"))\n",
    "\n",
    "#     # CPU DataLoader with a safe batch size\n",
    "#     test_ds = TensorDataset(torch.from_numpy(test_x_rs).float(),\n",
    "#                             torch.from_numpy(test_y).float())\n",
    "#     test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "#     pdf_name = f\"{name_run}_fewshot_confusion_matrix.pdf\"\n",
    "#     num_classes = test_y.shape[1]\n",
    "#     save_multiclass_confusion_matrix(model_cpu, test_loader, pdf_name, num_classes)\n",
    "#     print(f\"✅ Saved few‑shot confusion matrix (classes 0–{num_classes-1}) to {pdf_name}\")\n",
    "\n",
    "# # ----------------------- restore & persist -----------------------------\n",
    "# preset[\"nn\"][\"epoch\"] = original_epochs\n",
    "\n",
    "# # Save the final result to JSON\n",
    "# with open(\"result_fewshot.json\", \"w\") as f:\n",
    "#     json.dump(result, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fdc9498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T01:26:36.056931Z",
     "iopub.status.busy": "2025-12-26T01:26:36.056736Z",
     "iopub.status.idle": "2025-12-26T01:26:36.066444Z",
     "shell.execute_reply": "2025-12-26T01:26:36.065939Z"
    },
    "papermill": {
     "duration": 0.056941,
     "end_time": "2025-12-26T01:26:36.067575",
     "exception": false,
     "start_time": "2025-12-26T01:26:36.010634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import scipy.io as scio\n",
    "# import time\n",
    "# import torch\n",
    "# import gc\n",
    "# from numpy.linalg import svd\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "# from copy import deepcopy\n",
    "# import json\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# import torch._dynamo\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # --- تنظیمات سیستمی ---\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# # --------------------------\n",
    "# # 1. تنظیمات (Configuration)\n",
    "# # --------------------------\n",
    "# preset = {\n",
    "#     \"model\": \"THAT\",          \n",
    "#     \"task\": \"activity\",       \n",
    "#     \"repeat\": 1,\n",
    "#     \"path\": {\n",
    "#         \"data_x\": \"/kaggle/input/wimans/wifi_csi/amp\",   \n",
    "#         \"data_y\": \"/kaggle/input/wimans/annotation.csv\", \n",
    "#     },\n",
    "#     \"data\": {\n",
    "#         \"num_users\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"],  \n",
    "#         \"wifi_band\": [\"2.4\"],                         \n",
    "#         \"environment\": [\"classroom\"],                 \n",
    "#         \"length\": 3000,\n",
    "        \n",
    "#         # 1.0 = 100% data (Full run) | 0.1 = 10% data (Quick test)\n",
    "#         \"subset_ratio\": 0.5,  \n",
    "#     },\n",
    "#     \"nn\": {\n",
    "#         \"lr\": 1e-3,           \n",
    "#         \"epoch\": 80,          \n",
    "#         \"batch_size\": 32,    \n",
    "#         \"threshold\": 0.5,\n",
    "#         \"patience\": 5,        \n",
    "#         \"factor\": 0.5,        \n",
    "#         \"min_lr\": 1e-6        \n",
    "#     },\n",
    "#     \"encoding\": {\n",
    "#         \"activity\": {\n",
    "#             \"nan\":      [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#             \"nothing\":  [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#             \"walk\":     [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "#             \"rotation\": [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "#             \"jump\":     [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "#             \"wave\":     [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "#             \"lie_down\": [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "#             \"pick_up\":  [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "#             \"sit_down\": [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "#             \"stand_up\": [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "#         },\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# # --------------------------\n",
    "# # 2. توابع RPCA و لود دیتا\n",
    "# # --------------------------\n",
    "# def soft_threshold(x, epsilon):\n",
    "#     return np.maximum(np.abs(x) - epsilon, 0) * np.sign(x)\n",
    "\n",
    "# def robust_pca(M, max_iter=10, tol=1e-4):\n",
    "#     n1, n2 = M.shape\n",
    "#     lambda_param = 1 / np.sqrt(max(n1, n2))\n",
    "#     Y = M / np.maximum(np.linalg.norm(M, 2), np.linalg.norm(M, np.inf) / lambda_param)\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     mu = 1.25 / np.linalg.norm(M, 2)\n",
    "#     rho = 1.5\n",
    "#     for i in range(max_iter):\n",
    "#         temp_L = M - S + (1/mu) * Y\n",
    "#         U, Sigma, Vt = svd(temp_L, full_matrices=False)\n",
    "#         Sigma_thresh = soft_threshold(Sigma, 1/mu)\n",
    "#         L_new = np.dot(U * Sigma_thresh, Vt)\n",
    "#         temp_S = M - L_new + (1/mu) * Y\n",
    "#         S_new = soft_threshold(temp_S, lambda_param/mu)\n",
    "#         error = np.linalg.norm(M - L_new - S_new, 'fro') / np.linalg.norm(M, 'fro')\n",
    "#         L = L_new; S = S_new\n",
    "#         if error < tol: break\n",
    "#         Y = Y + mu * (M - L - S)\n",
    "#         mu = min(mu * rho, 1e7)\n",
    "#     return L, S\n",
    "\n",
    "# def load_data_y(var_path_data_y, var_environment=None, var_wifi_band=None, var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None: data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None: data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None: data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list, use_rpca=True):\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     mode_str = \"WITH RPCA\" if use_rpca else \"RAW DATA (No RPCA)\"\n",
    "#     print(f\"Loading {len(var_path_list)} samples - Mode: {mode_str}...\")\n",
    "#     for i, var_path in enumerate(var_path_list):\n",
    "#         if i % 100 == 0 and i > 0: print(f\"Processing {i}/{len(var_path_list)}...\")\n",
    "#         data_csi = np.load(var_path) \n",
    "#         data_csi_2d = data_csi.reshape(data_csi.shape[0], -1)\n",
    "#         target_len = preset[\"data\"][\"length\"]\n",
    "#         current_len = data_csi_2d.shape[0]\n",
    "#         var_pad_length = target_len - current_len\n",
    "#         if var_pad_length > 0: data_csi_pad = np.pad(data_csi_2d, ((0, var_pad_length), (0, 0)), mode='constant')\n",
    "#         else: data_csi_pad = data_csi_2d[:target_len, :]\n",
    "#         if use_rpca:\n",
    "#             L, S = robust_pca(data_csi_pad)\n",
    "#             final_sample = np.concatenate([L, S], axis=1) \n",
    "#         else:\n",
    "#             final_sample = data_csi_pad\n",
    "#         data_x.append(final_sample)\n",
    "#     data_x = np.array(data_x)\n",
    "#     return data_x\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"activity\": return encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     return encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     cols = [f\"user_{i}_activity\" for i in range(1, 7)]\n",
    "#     data = data_pd_y[cols].to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[y] for y in sample] for sample in data])\n",
    "\n",
    "# # --------------------------\n",
    "# # 3. مدل THAT\n",
    "# # --------------------------\n",
    "# class Gaussian_Position(torch.nn.Module):\n",
    "#     def __init__(self, var_dim_feature, var_dim_time, var_num_gaussian=10):\n",
    "#         super(Gaussian_Position, self).__init__()\n",
    "#         self.var_embedding = torch.nn.Parameter(torch.zeros([var_num_gaussian, var_dim_feature]), requires_grad=True)\n",
    "#         torch.nn.init.xavier_uniform_(self.var_embedding)\n",
    "#         self.var_position = torch.nn.Parameter(torch.arange(0.0, var_dim_time).unsqueeze(1).repeat(1, var_num_gaussian), requires_grad=False)\n",
    "#         self.var_mu = torch.nn.Parameter(torch.arange(0.0, var_dim_time, var_dim_time/var_num_gaussian).unsqueeze(0), requires_grad=True)\n",
    "#         self.var_sigma = torch.nn.Parameter(torch.tensor([50.0] * var_num_gaussian).unsqueeze(0), requires_grad=True)\n",
    "#     def forward(self, var_input):\n",
    "#         var_pdf = - (self.var_position - self.var_mu)**2 / (2 * self.var_sigma**2) - torch.log(self.var_sigma)\n",
    "#         var_pdf = torch.softmax(var_pdf, dim=-1)\n",
    "#         return var_input + torch.matmul(var_pdf, self.var_embedding).unsqueeze(0)\n",
    "\n",
    "# class Encoder(torch.nn.Module):\n",
    "#     def __init__(self, var_dim_feature, var_num_head=10, var_size_cnn=[1, 3, 5]):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.layer_norm_0 = torch.nn.LayerNorm(var_dim_feature, eps=1e-6)\n",
    "#         self.layer_attention = torch.nn.MultiheadAttention(var_dim_feature, var_num_head, batch_first=True)\n",
    "#         self.layer_dropout_0 = torch.nn.Dropout(0.1)\n",
    "#         self.layer_norm_1 = torch.nn.LayerNorm(var_dim_feature, 1e-6)\n",
    "#         self.layer_cnn = torch.nn.ModuleList([torch.nn.Sequential(torch.nn.Conv1d(var_dim_feature, var_dim_feature, s, padding=\"same\"), torch.nn.BatchNorm1d(var_dim_feature), torch.nn.Dropout(0.1), torch.nn.LeakyReLU()) for s in var_size_cnn])\n",
    "#         self.layer_dropout_1 = torch.nn.Dropout(0.1)\n",
    "#     def forward(self, var_input):\n",
    "#         var_t = self.layer_norm_0(var_input)\n",
    "#         var_t, _ = self.layer_attention(var_t, var_t, var_t)\n",
    "#         var_t = self.layer_dropout_0(var_t) + var_input\n",
    "#         var_s = self.layer_norm_1(var_t).permute(0, 2, 1)\n",
    "#         var_c = torch.stack([l(var_s) for l in self.layer_cnn], dim=0)\n",
    "#         var_s = self.layer_dropout_1((torch.sum(var_c, dim=0) / len(self.layer_cnn)).permute(0, 2, 1))\n",
    "#         return var_s + var_t\n",
    "\n",
    "# class THAT(torch.nn.Module):\n",
    "#     def __init__(self, var_x_shape, var_y_shape):\n",
    "#         super(THAT, self).__init__()\n",
    "#         var_dim_feature, var_dim_time = var_x_shape[-1], var_x_shape[-2]\n",
    "#         var_dim_output = var_y_shape[-1]\n",
    "#         self.layer_left_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "#         self.layer_left_gaussian = Gaussian_Position(var_dim_feature, var_dim_time // 20)\n",
    "#         self.layer_left_encoder = torch.nn.ModuleList([Encoder(var_dim_feature, 10, [1, 3, 5]) for _ in range(4)])\n",
    "#         self.layer_left_norm = torch.nn.LayerNorm(var_dim_feature, eps=1e-6)\n",
    "#         self.layer_left_cnn = torch.nn.ModuleList([torch.nn.Conv1d(var_dim_feature, 128, k) for k in [8, 16]])\n",
    "#         self.layer_left_dropout = torch.nn.Dropout(0.5)\n",
    "#         var_dim_right = var_dim_time // 20\n",
    "#         self.layer_right_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "#         self.layer_right_encoder = torch.nn.ModuleList([Encoder(var_dim_right, 10, [1, 2, 3])])\n",
    "#         self.layer_right_norm = torch.nn.LayerNorm(var_dim_right, eps=1e-6)\n",
    "#         self.layer_right_cnn = torch.nn.ModuleList([torch.nn.Conv1d(var_dim_right, 16, k) for k in [2, 4]])\n",
    "#         self.layer_right_dropout = torch.nn.Dropout(0.5)\n",
    "#         self.layer_leakyrelu = torch.nn.LeakyReLU()\n",
    "#         self.layer_output = torch.nn.Linear(256 + 32, var_dim_output)\n",
    "#     def forward(self, var_input):\n",
    "#         v_l = self.layer_left_gaussian(self.layer_left_pooling(var_input.permute(0, 2, 1)).permute(0, 2, 1))\n",
    "#         for l in self.layer_left_encoder: v_l = l(v_l)\n",
    "#         v_l = self.layer_left_norm(v_l).permute(0, 2, 1)\n",
    "#         v_l = torch.cat([torch.sum(self.layer_leakyrelu(cnn(v_l)), dim=-1) for cnn in self.layer_left_cnn], dim=-1)\n",
    "#         v_l = self.layer_left_dropout(v_l)\n",
    "#         v_r = self.layer_right_pooling(var_input.permute(0, 2, 1))\n",
    "#         for l in self.layer_right_encoder: v_r = l(v_r)\n",
    "#         v_r = self.layer_right_norm(v_r).permute(0, 2, 1)\n",
    "#         v_r = torch.cat([torch.sum(self.layer_leakyrelu(cnn(v_r)), dim=-1) for cnn in self.layer_right_cnn], dim=-1)\n",
    "#         v_r = self.layer_right_dropout(v_r)\n",
    "#         return self.layer_output(torch.cat([v_l, v_r], dim=-1))\n",
    "\n",
    "# # --------------------------\n",
    "# # 4. Training Loop\n",
    "# # --------------------------\n",
    "# def train(model, optimizer, loss_fn, train_loader, test_loader, threshold, epochs, device, model_path):\n",
    "#     best_acc = -1.0\n",
    "#     best_w = deepcopy(model.state_dict())\n",
    "    \n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#         optimizer, mode='max', factor=preset[\"nn\"][\"factor\"], patience=preset[\"nn\"][\"patience\"],\n",
    "#         min_lr=preset[\"nn\"][\"min_lr\"], verbose=True\n",
    "#     )\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         t0 = time.time()\n",
    "#         model.train()\n",
    "        \n",
    "#         # --- [MODIFIED] Using requested variable names ---\n",
    "#         for data_batch_x, data_batch_y in train_loader:\n",
    "#             data_batch_x = data_batch_x.to(device)\n",
    "#             data_batch_y = data_batch_y.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             predict_train_y = model(data_batch_x)\n",
    "            \n",
    "#             # --- [REQUESTED LINE] ---\n",
    "#             loss_value = loss_fn(predict_train_y, data_batch_y.reshape(data_batch_y.shape[0], -1).float())\n",
    "            \n",
    "#             loss_value.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             tx, ty = next(iter(test_loader))\n",
    "#             tx, ty = tx.to(device), ty.to(device)\n",
    "#             pred_t = model(tx)\n",
    "            \n",
    "#             p_cls = (torch.sigmoid(pred_t) > threshold).float().cpu().numpy()\n",
    "#             t_cls = ty.cpu().numpy()\n",
    "#             acc = accuracy_score(t_cls.reshape(-1, t_cls.shape[-1]), p_cls.reshape(-1, t_cls.shape[-1]))\n",
    "            \n",
    "#         scheduler.step(acc)\n",
    "#         current_lr = optimizer.param_groups[0]['lr']\n",
    "#         print(f\"Ep {epoch+1}/{epochs} | LR: {current_lr:.6f} | L_tr: {loss_value.item():.4f} | Acc: {acc:.4f}\")\n",
    "        \n",
    "#         if acc > best_acc:\n",
    "#             best_acc = acc\n",
    "#             best_w = deepcopy(model.state_dict())\n",
    "            \n",
    "#     torch.save(best_w, model_path)\n",
    "#     return best_w\n",
    "\n",
    "# def save_multiclass_confusion_matrix(model, data_loader, device, pdf_path, num_classes, title_text):\n",
    "#     model.eval()\n",
    "#     y_true, y_pred = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in data_loader:\n",
    "#             xb = xb.to(device)\n",
    "#             logits = model(xb) \n",
    "#             logits = logits.reshape(-1, num_classes) \n",
    "#             yb = yb.reshape(-1, num_classes)        \n",
    "#             y_pred.extend(torch.argmax(logits, dim=1).cpu().numpy().tolist())\n",
    "#             y_true.extend(torch.argmax(yb, dim=1).cpu().numpy().tolist())\n",
    "    \n",
    "#     labels = list(range(num_classes))\n",
    "#     cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "#     disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "#     fig, ax = plt.subplots(figsize=(12, 12))\n",
    "#     disp.plot(ax=ax, xticks_rotation=\"vertical\", cmap='Blues')\n",
    "#     ax.set_title(title_text)\n",
    "#     with PdfPages(pdf_path) as pdf: pdf.savefig(fig)\n",
    "#     plt.close(fig)\n",
    "\n",
    "# # --------------------------\n",
    "# # 5. اجرا\n",
    "# # --------------------------\n",
    "# def run_experiment(scenario_name, use_rpca):\n",
    "#     print(f\"\\n################################################\")\n",
    "#     print(f\"STARTING SCENARIO: {scenario_name}\")\n",
    "#     print(f\"################################################\")\n",
    "    \n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     current_run_name = f\"{preset['model']}_{preset['task']}_{scenario_name}\"\n",
    "#     model_save_path = f\"{current_run_name}_best_model.pt\"\n",
    "#     json_save_path = f\"result_{current_run_name}.json\"\n",
    "#     pdf_save_path = f\"Confusion_{current_run_name}.pdf\"\n",
    "    \n",
    "#     # 1. Load Labels\n",
    "#     data_pd_y = load_data_y(preset[\"path\"][\"data_y\"], preset[\"data\"][\"environment\"], preset[\"data\"][\"wifi_band\"], preset[\"data\"][\"num_users\"])\n",
    "    \n",
    "#     # Apply Subset Ratio\n",
    "#     subset_ratio = preset[\"data\"][\"subset_ratio\"]\n",
    "#     if subset_ratio < 1.0:\n",
    "#         data_pd_y = data_pd_y.sample(frac=subset_ratio, random_state=42).reset_index(drop=True)\n",
    "#         print(f\"*** DEBUG MODE: Using {subset_ratio*100}% of data ({len(data_pd_y)} samples) ***\")\n",
    "    \n",
    "#     # 2. Load X\n",
    "#     data_x = load_data_x(preset[\"path\"][\"data_x\"], data_pd_y[\"label\"].tolist(), use_rpca=use_rpca)\n",
    "#     data_y = encode_data_y(data_pd_y, preset[\"task\"])\n",
    "    \n",
    "#     # 3. Split\n",
    "#     train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2, shuffle=True, random_state=39)\n",
    "#     train_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "#     test_ds = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "#     train_loader = DataLoader(train_ds, batch_size=preset[\"nn\"][\"batch_size\"], shuffle=True)\n",
    "#     test_loader = DataLoader(test_ds, batch_size=len(test_ds), shuffle=False)\n",
    "    \n",
    "#     result = {\"accuracy\": []}\n",
    "    \n",
    "#     for r in range(preset[\"repeat\"]):\n",
    "#         print(f\"--- Repeat {r+1}/{preset['repeat']} ---\")\n",
    "#         torch.random.manual_seed(r + 39)\n",
    "        \n",
    "#         model = THAT(train_x[0].shape, train_y[0].reshape(-1).shape).to(device)\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=preset[\"nn\"][\"lr\"])\n",
    "#         loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "#         best_w = train(model, optimizer, loss_fn, train_loader, test_loader, \n",
    "#                        preset[\"nn\"][\"threshold\"], preset[\"nn\"][\"epoch\"], device, model_save_path)\n",
    "        \n",
    "#         model.load_state_dict(best_w)\n",
    "#         with torch.no_grad():\n",
    "#             preds = model(torch.from_numpy(test_x).to(device))\n",
    "#             preds_reshaped = (torch.sigmoid(preds) > preset[\"nn\"][\"threshold\"]).float().cpu().numpy().reshape(-1, 9)\n",
    "#             targets_reshaped = test_y.reshape(-1, 9)\n",
    "#             acc = accuracy_score(targets_reshaped, preds_reshaped)\n",
    "#             result[\"accuracy\"].append(acc)\n",
    "            \n",
    "#     print(f\"Final Accuracy ({scenario_name}): {np.mean(result['accuracy']):.4f}\")\n",
    "#     with open(json_save_path, \"w\") as f: json.dump(result, f, indent=4)\n",
    "    \n",
    "#     print(\"Generating Confusion Matrix...\")\n",
    "#     model_cm = THAT(test_x[0].shape, test_y[0].reshape(-1).shape).to(\"cpu\")\n",
    "#     model_cm.load_state_dict(torch.load(model_save_path, map_location=\"cpu\"))\n",
    "#     cm_loader = DataLoader(TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y)), batch_size=32)\n",
    "#     num_classes = test_y.shape[2] \n",
    "#     title = f\"Confusion Matrix: {scenario_name} (Acc: {np.mean(result['accuracy']):.2f} - {subset_ratio*100}% Data)\"\n",
    "#     save_multiclass_confusion_matrix(model_cm, cm_loader, \"cpu\", pdf_save_path, num_classes, title)\n",
    "    \n",
    "#     del model, model_cm, train_x, test_x, data_x\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     print(f\"Done with {scenario_name}.\\n\")\n",
    "\n",
    "# def run():\n",
    "#     scenarios = [\n",
    "#         (\"RPCA\", True),\n",
    "#         (\"RAW\", False)\n",
    "#     ]\n",
    "#     for name, rpca_flag in scenarios:\n",
    "#         run_experiment(name, rpca_flag)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d330eb9",
   "metadata": {
    "papermill": {
     "duration": 0.045888,
     "end_time": "2025-12-26T01:26:36.158763",
     "exception": false,
     "start_time": "2025-12-26T01:26:36.112875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4451316,
     "sourceId": 7638081,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7503373,
     "sourceId": 11934698,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10985.414638,
   "end_time": "2025-12-26T01:26:38.931050",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-25T22:23:33.516412",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
