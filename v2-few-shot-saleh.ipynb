{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939d288f",
   "metadata": {
    "papermill": {
     "duration": 0.007838,
     "end_time": "2025-12-25T15:19:08.383354",
     "exception": false,
     "start_time": "2025-12-25T15:19:08.375516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 1: Library Imports\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b35827",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:19:08.397343Z",
     "iopub.status.busy": "2025-12-25T15:19:08.397094Z",
     "iopub.status.idle": "2025-12-25T15:19:18.575627Z",
     "shell.execute_reply": "2025-12-25T15:19:18.574737Z"
    },
    "papermill": {
     "duration": 10.18728,
     "end_time": "2025-12-25T15:19:18.577249",
     "exception": false,
     "start_time": "2025-12-25T15:19:08.389969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 1: Library Imports\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as scio\n",
    "import time\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch._dynamo\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# from ptflops import get_model_complexity_info\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from copy import deepcopy\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5193be6f",
   "metadata": {
    "papermill": {
     "duration": 0.007446,
     "end_time": "2025-12-25T15:19:18.592719",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.585273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 2: preset.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65a0b336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:19:18.609057Z",
     "iopub.status.busy": "2025-12-25T15:19:18.608636Z",
     "iopub.status.idle": "2025-12-25T15:19:18.618973Z",
     "shell.execute_reply": "2025-12-25T15:19:18.618273Z"
    },
    "papermill": {
     "duration": 0.01987,
     "end_time": "2025-12-25T15:19:18.620123",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.600253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "few=empty_room,100,5,m=THAT,t=activity,epoch=100,batch=64,environment=['classroom']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "[file]          preset.py\n",
    "[description]   default settings of WiFi-based models\n",
    "\"\"\"\n",
    "minidata_set = 1\n",
    "preset = {\n",
    "    # define model\n",
    "    \"model\": \"THAT\",  # \"ST-RF\", \"MLP\", \"LSTM\", \"CNN-1D\", \"CNN-2D\", \"CLSTM\", \"ABLSTM\", \"THAT\", \"bi-LSTM\", \"ResNet18\"\n",
    "    # define task\n",
    "    \"task\": \"activity\",  # \"identity\", \"activity\", \"location\", \"count\"\n",
    "    # number of repeated experiments\n",
    "    \"repeat\": 1,\n",
    "    # path of data\n",
    "    \"path\": {\n",
    "        \"data_x\": \"/kaggle/input/wimans/wifi_csi/mat\",   # directory of CSI amplitude files \n",
    "        # \"data_x\": \"/kaggle/input/wimans/wifi_csi/amp\",   # directory of CSI amplitude files \n",
    "        \"data_y\": \"/kaggle/input/wimans/annotation.csv\", # path of annotation file\n",
    "        \"save\": \"result_lstm_epoch=80_batchsize=32_envs=empty_room_wifiband=2.4.json\"               # path to save results\n",
    "    },\n",
    "    # data selection for experiments\n",
    "    \"data\": {\n",
    "        \"num_users\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"],  # select number(s) of users\n",
    "        \"wifi_band\": [\"2.4\"],                         # select WiFi band(s)\n",
    "        \"environment\": [\"classroom\"],                 # select environment(s) [\"classroom\"], [\"meeting_room\"], [\"empty_room\"]\n",
    "        \"length\": 3000,                               # default length of CSI\n",
    "    },\n",
    "    # hyperparameters of models\n",
    "    \"nn\": {\n",
    "        \"lr\": 1e-3,           # learning rate\n",
    "        \"epoch\": 100,         # number of epochs\n",
    "        \"batch_size\": 64,    # batch size\n",
    "        \"threshold\": 0.5,     # threshold to binarize sigmoid outputs\n",
    "    },\n",
    "    # encoding of activities and locations\n",
    "    \"encoding\": {\n",
    "        \"activity\": {  # encoding of different activities\n",
    "            \"nan\":      [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"nothing\":  [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"walk\":     [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            \"rotation\": [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            \"jump\":     [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            \"wave\":     [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "            \"lie_down\": [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "            \"pick_up\":  [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "            \"sit_down\": [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "            \"stand_up\": [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        },\n",
    "        \"location\": {  # encoding of different locations\n",
    "            \"nan\":  [0, 0, 0, 0, 0],\n",
    "            \"a\":    [1, 0, 0, 0, 0],\n",
    "            \"b\":    [0, 1, 0, 0, 0],\n",
    "            \"c\":    [0, 0, 1, 0, 0],\n",
    "            \"d\":    [0, 0, 0, 1, 0],\n",
    "            \"e\":    [0, 0, 0, 0, 1],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Few-shot parameters (manually set)\n",
    "dest_env = \"empty_room\"       # Destination environment[\"classroom\"], [\"meeting_room\"], [\"empty_room\"]\n",
    "few_shot_epochs = 100         # Number of epochs for few-shot training\n",
    "few_shot_num_samples = 5     # Number of samples to use from the destination test data\n",
    "\n",
    "Confusion_matrix = 1\n",
    "\n",
    "name_run = \"few={},{},{},m={},t={},epoch={},batch={},environment={}\".format(dest_env, few_shot_epochs, few_shot_num_samples, preset[\"model\"], preset[\"task\"], preset[\"nn\"][\"epoch\"], preset[\"nn\"][\"batch_size\"], preset[\"data\"][\"environment\"])\n",
    "print(name_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9348bdc",
   "metadata": {
    "papermill": {
     "duration": 0.00701,
     "end_time": "2025-12-25T15:19:18.634063",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.627053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 3: load_data.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914caf9",
   "metadata": {
    "papermill": {
     "duration": 0.006605,
     "end_time": "2025-12-25T15:19:18.647122",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.640517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "last stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d014a9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:19:18.661749Z",
     "iopub.status.busy": "2025-12-25T15:19:18.661465Z",
     "iopub.status.idle": "2025-12-25T15:19:18.691617Z",
     "shell.execute_reply": "2025-12-25T15:19:18.690805Z"
    },
    "papermill": {
     "duration": 0.039337,
     "end_time": "2025-12-25T15:19:18.692969",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.653632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          load_data.py\n",
    "[description]   load annotation file and CSI (from .mat), and encode labels\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as scio\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from numpy.fft import ifft\n",
    "\n",
    "# =========================================================\n",
    "#  Settings\n",
    "# =========================================================\n",
    "# \"raw\"     : abs(raw complex CSI)  -> float\n",
    "# \"lowrank\" : abs(L) after RPCA     -> float\n",
    "# \"sparse\"  : abs(S) after RPCA     -> float\n",
    "CSI_INPUT_MODE = \"sparse\"   # raw / lowrank / sparse\n",
    "\n",
    "# expected dimensions\n",
    "TX = 3\n",
    "RX = 3\n",
    "SC = 30\n",
    "\n",
    "# optional IFFT (frequency -> delay)\n",
    "USE_IFFT = True\n",
    "\n",
    "# RPCA iterations (برای شروع کم بگذار؛ اگر لازم شد بیشتر کن)\n",
    "RPCA_MAX_ITER = 50\n",
    "\n",
    "# lambda mode (اختیاری)\n",
    "# \"classic\" : 1/sqrt(max(m,n))\n",
    "# \"median\"  : 1/sqrt(max(m,n)) * median(abs(M))\n",
    "# \"scaled\"  : 1.2/sqrt(max(m,n))\n",
    "RPCA_LAMBDA_MODE = \"classic\"\n",
    "\n",
    "# cache (very important for speed)\n",
    "CACHE_ENABLED = True\n",
    "CACHE_ROOT = \"/kaggle/working/csi_cache_mat_pipeline\"\n",
    "\n",
    "# print once to verify input is amplitude or complex/phase\n",
    "_DEBUG_PRINT_ONCE = True\n",
    "# =========================================================\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Minimal R_pca implementation (no external dependency)\n",
    "# Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "# =========================================================\n",
    "class R_pca:\n",
    "    def __init__(self, D):\n",
    "        self.D = np.asarray(D, dtype=np.float64)\n",
    "\n",
    "    @staticmethod\n",
    "    def _shrink(M, tau):\n",
    "        return np.sign(M) * np.maximum(np.abs(M) - tau, 0.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _svt(M, tau):\n",
    "        U, s, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "        s = np.maximum(s - tau, 0.0)\n",
    "        if np.all(s == 0):\n",
    "            return np.zeros_like(M)\n",
    "        return (U * s) @ Vt\n",
    "\n",
    "    def _lambda(self, D):\n",
    "        m, n = D.shape\n",
    "        base = 1.0 / np.sqrt(max(m, n))\n",
    "        if RPCA_LAMBDA_MODE == \"classic\":\n",
    "            return base\n",
    "        if RPCA_LAMBDA_MODE == \"scaled\":\n",
    "            return 1.2 * base\n",
    "        if RPCA_LAMBDA_MODE == \"median\":\n",
    "            med = np.median(np.abs(D))\n",
    "            med = float(med) if med > 1e-12 else 1e-12\n",
    "            return base * med\n",
    "        raise ValueError(f\"Unknown RPCA_LAMBDA_MODE={RPCA_LAMBDA_MODE}\")\n",
    "\n",
    "    def fit(self, max_iter=200, tol=1e-6, rho=1.5, mu=None):\n",
    "        \"\"\"\n",
    "        Returns L, S such that D ≈ L + S\n",
    "        \"\"\"\n",
    "        D = self.D\n",
    "        m, n = D.shape\n",
    "        lam = self._lambda(D)\n",
    "\n",
    "        # auto mu\n",
    "        if mu is None:\n",
    "            s0 = np.linalg.svd(D, compute_uv=False, full_matrices=False)[0] if D.size else 1.0\n",
    "            mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "        L = np.zeros_like(D)\n",
    "        S = np.zeros_like(D)\n",
    "        Y = np.zeros_like(D)\n",
    "\n",
    "        normD = np.linalg.norm(D, ord=\"fro\") + 1e-12\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            L = self._svt(D - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "            S = self._shrink(D - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "            R = D - L - S\n",
    "            Y = Y + mu * R\n",
    "\n",
    "            if (np.linalg.norm(R, ord=\"fro\") / normD) < tol:\n",
    "                break\n",
    "\n",
    "            mu *= rho\n",
    "\n",
    "        return L.astype(np.float64), S.astype(np.float64)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1) Phase calibration (sanitize) - vectorized\n",
    "# --------------------------------------------------\n",
    "def phase_sanitize_matrix(X):\n",
    "    \"\"\"\n",
    "    X: complex matrix (N_subcarriers, T)\n",
    "    \"\"\"\n",
    "    phase = np.unwrap(np.angle(X), axis=0)  # (N,T)\n",
    "    N, T = phase.shape\n",
    "    k = np.arange(N, dtype=np.float64)[:, None]  # (N,1)\n",
    "\n",
    "    A = np.concatenate([k, np.ones((N, 1), dtype=np.float64)], axis=1)  # (N,2)\n",
    "    pinvA = np.linalg.pinv(A)  # (2,N)\n",
    "    coeff = pinvA @ phase      # (2,T)\n",
    "    a = coeff[0:1, :]\n",
    "    b = coeff[1:2, :]\n",
    "\n",
    "    phase_corr = phase - (k @ a + b)\n",
    "    return np.abs(X) * np.exp(1j * phase_corr)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2) Preprocess CSI matrix\n",
    "# --------------------------------------------------\n",
    "def preprocess_csi(X):\n",
    "    \"\"\"\n",
    "    X : complex CSI matrix (N_subcarriers, T)\n",
    "    \"\"\"\n",
    "    X_corr = phase_sanitize_matrix(X).astype(np.complex128, copy=False)\n",
    "    fro = np.linalg.norm(X_corr, \"fro\")\n",
    "    if fro > 0:\n",
    "        X_corr /= fro\n",
    "    return X_corr\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3) Optional IFFT\n",
    "# --------------------------------------------------\n",
    "def csi_to_cir(X):\n",
    "    return ifft(X, axis=0)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4) RPCA on complex matrix (real & imag separately)\n",
    "# --------------------------------------------------\n",
    "def rpca_complex(X, max_iter=200):\n",
    "    Xr = np.real(X)\n",
    "    Xi = np.imag(X)\n",
    "\n",
    "    rpca_r = R_pca(Xr)\n",
    "    Lr, Sr = rpca_r.fit(max_iter=max_iter)\n",
    "\n",
    "    rpca_i = R_pca(Xi)\n",
    "    Li, Si = rpca_i.fit(max_iter=max_iter)\n",
    "\n",
    "    L = Lr + 1j * Li\n",
    "    S = Sr + 1j * Si\n",
    "    return L, S\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5) Full pipeline\n",
    "# --------------------------------------------------\n",
    "def csi_lowrank_sparse_pipeline(X, use_ifft=True, max_iter=200):\n",
    "    Xp = preprocess_csi(X)\n",
    "    if use_ifft:\n",
    "        Xp = csi_to_cir(Xp)\n",
    "    L, S = rpca_complex(Xp, max_iter=max_iter)\n",
    "    return L, S\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# MAT loader -> (T,3,3,30) complex\n",
    "# --------------------------------------------------\n",
    "def load_csi_from_mat(mat_path):\n",
    "    m = scio.loadmat(mat_path)\n",
    "    if \"trace\" not in m:\n",
    "        raise KeyError(f\"'trace' not found in {mat_path}\")\n",
    "\n",
    "    trace = m[\"trace\"]  # (T,1) object array\n",
    "    T = trace.shape[0]\n",
    "    out = np.empty((T, TX, RX, SC), dtype=np.complex128)\n",
    "\n",
    "    for t in range(T):\n",
    "        out[t] = trace[t, 0][\"csi\"][0, 0]  # (3,3,30) complex\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _debug_print_once(label, csi_4d):\n",
    "    global _DEBUG_PRINT_ONCE\n",
    "    if not _DEBUG_PRINT_ONCE:\n",
    "        return\n",
    "    _DEBUG_PRINT_ONCE = False\n",
    "\n",
    "    is_cplx = np.iscomplexobj(csi_4d)\n",
    "    print(f\"\\n[DEBUG] First MAT sample: {label}\")\n",
    "    print(f\"[DEBUG] shape={csi_4d.shape}, dtype={csi_4d.dtype}, complex={is_cplx}\")\n",
    "\n",
    "    x0 = csi_4d[0, 0, 0, :]\n",
    "    if is_cplx:\n",
    "        ang = np.angle(x0)\n",
    "        print(f\"[DEBUG] abs range:  min={np.min(np.abs(x0)):.6f}, max={np.max(np.abs(x0)):.6f}\")\n",
    "        print(f\"[DEBUG] phase stats: mean={np.mean(ang):.6f}, std={np.std(ang):.6f}\")\n",
    "        print(\"[DEBUG] ==> Input is COMPLEX (phase exists).\")\n",
    "    else:\n",
    "        print(f\"[DEBUG] value range: min={np.min(x0):.6f}, max={np.max(x0):.6f}\")\n",
    "        print(\"[DEBUG] ==> Input is REAL (likely amplitude-only).\")\n",
    "\n",
    "\n",
    "def _cache_path(label, mode):\n",
    "    return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "def _apply_pipeline_pairwise_9links(csi_4d_complex, label):\n",
    "    \"\"\"\n",
    "    csi_4d_complex: (T,3,3,30) complex\n",
    "    Run pipeline on each link separately: (30,T) -> RPCA -> abs -> back to (T,3,3,30) float32\n",
    "    \"\"\"\n",
    "    _debug_print_once(label, csi_4d_complex)\n",
    "\n",
    "    if CSI_INPUT_MODE == \"raw\":\n",
    "        return np.abs(csi_4d_complex).astype(np.float32)\n",
    "\n",
    "    mode = CSI_INPUT_MODE  # lowrank / sparse\n",
    "\n",
    "    if CACHE_ENABLED:\n",
    "        os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "        p = _cache_path(label, mode)\n",
    "        if os.path.exists(p):\n",
    "            return np.load(p).astype(np.float32, copy=False)\n",
    "\n",
    "    T = csi_4d_complex.shape[0]\n",
    "    out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "    for tx in range(TX):\n",
    "        for rx in range(RX):\n",
    "            X = csi_4d_complex[:, tx, rx, :].T  # (30,T) complex\n",
    "            L, S = csi_lowrank_sparse_pipeline(X, use_ifft=USE_IFFT, max_iter=RPCA_MAX_ITER)\n",
    "            Y = L if mode == \"lowrank\" else S\n",
    "            out[:, tx, rx, :] = np.abs(Y.T).astype(np.float32)\n",
    "\n",
    "    if CACHE_ENABLED:\n",
    "        np.save(p, out)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Existing label loading/encoding\n",
    "# --------------------------------------------------\n",
    "def load_data_y(var_path_data_y,\n",
    "                var_environment=None,\n",
    "                var_wifi_band=None,\n",
    "                var_num_users=None):\n",
    "    data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "    if var_environment is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "    if var_wifi_band is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "    if var_num_users is not None:\n",
    "        data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "    return data_pd_y\n",
    "\n",
    "\n",
    "def load_data_x(var_path_data_x, var_label_list):\n",
    "    \"\"\"\n",
    "    var_path_data_x should point to MAT directory, e.g. /kaggle/input/wimans/wifi_csi/mat\n",
    "    Each label corresponds to <label>.mat\n",
    "    \"\"\"\n",
    "    data_x = []\n",
    "    target_len = preset[\"data\"][\"length\"]\n",
    "\n",
    "    for label in var_label_list:\n",
    "        mat_path = os.path.join(var_path_data_x, label + \".mat\")\n",
    "        if not os.path.exists(mat_path):\n",
    "            raise FileNotFoundError(f\"MAT file not found: {mat_path}\")\n",
    "\n",
    "        csi_complex = load_csi_from_mat(mat_path)  # (T,3,3,30) complex\n",
    "\n",
    "        # if longer than target_len, keep last target_len frames\n",
    "        if csi_complex.shape[0] > target_len:\n",
    "            csi_complex = csi_complex[-target_len:, :, :, :]\n",
    "\n",
    "        csi_feat = _apply_pipeline_pairwise_9links(csi_complex, label)\n",
    "\n",
    "        # pad if shorter\n",
    "        pad_len = target_len - csi_feat.shape[0]\n",
    "        if pad_len > 0:\n",
    "            csi_feat = np.pad(csi_feat, ((pad_len, 0), (0, 0), (0, 0), (0, 0)))\n",
    "\n",
    "        data_x.append(csi_feat)\n",
    "\n",
    "    return np.array(data_x)\n",
    "\n",
    "\n",
    "def encode_data_y(data_pd_y, var_task):\n",
    "    if var_task == \"identity\":\n",
    "        data_y = encode_identity(data_pd_y)\n",
    "    elif var_task == \"activity\":\n",
    "        data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "    elif var_task == \"location\":\n",
    "        data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "    elif var_task == \"count\":\n",
    "        data_y = encode_count(data_pd_y)\n",
    "    return data_y\n",
    "\n",
    "\n",
    "def encode_identity(data_pd_y):\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "                                    \"user_3_location\", \"user_4_location\",\n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "    data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "    data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "    return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "def encode_activity(data_pd_y, var_encoding):\n",
    "    data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "                                    \"user_3_activity\", \"user_4_activity\",\n",
    "                                    \"user_5_activity\", \"user_6_activity\"]]\n",
    "    data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "    return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "def encode_location(data_pd_y, var_encoding):\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "                                    \"user_3_location\", \"user_4_location\",\n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "    return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "def encode_count(data_pd_y):\n",
    "    data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "                                    \"user_3_location\", \"user_4_location\",\n",
    "                                    \"user_5_location\", \"user_6_location\"]]\n",
    "    data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "    data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "    data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "    data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "    count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de2a42c",
   "metadata": {
    "papermill": {
     "duration": 0.006689,
     "end_time": "2025-12-25T15:19:18.706923",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.700234",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "rpca 30 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adebeb0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:19:18.721897Z",
     "iopub.status.busy": "2025-12-25T15:19:18.721593Z",
     "iopub.status.idle": "2025-12-25T15:19:18.728667Z",
     "shell.execute_reply": "2025-12-25T15:19:18.728021Z"
    },
    "papermill": {
     "duration": 0.016101,
     "end_time": "2025-12-25T15:19:18.729739",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.713638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # =========================================================\n",
    "# # انتخاب نوع دیتای ورودی مدل:\n",
    "# # \"raw\"     : amp خام\n",
    "# # \"lowrank\" : خروجی Low-rank (L) از RPCA\n",
    "# # \"sparse\"  : خروجی Sparse (S) از RPCA\n",
    "# CSI_INPUT_MODE = \"lowrank\"   # <-- raw / lowrank / sparse\n",
    "\n",
    "# # ابعاد مورد انتظار CSI\n",
    "# TX = 3\n",
    "# RX = 3\n",
    "# SC = 30\n",
    "\n",
    "# # RPCA (IALM) تنظیمات سریع‌تر\n",
    "# RPCA_MAX_ITER = 60\n",
    "# RPCA_TOL      = 1e-5\n",
    "# RPCA_RHO      = 1.5\n",
    "# RPCA_MU_INIT  = None\n",
    "\n",
    "# # ✅ انتخاب لامبدا:\n",
    "# # \"median\" : lam = 1/sqrt(max(m,n)) * median(abs(M))\n",
    "# # \"scaled\" : lam = 1.2/sqrt(max(m,n))\n",
    "# RPCA_LAMBDA_MODE = \"median\"  # <-- \"median\" یا \"scaled\"\n",
    "\n",
    "# # Cache (خیلی مهم برای سرعت)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT    = \"/kaggle/working/csi_cache_pairwise\"\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _soft_threshold(X, tau):\n",
    "#     return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "# def _svt(X, tau):\n",
    "#     # Singular Value Thresholding\n",
    "#     U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "#     s = np.maximum(s - tau, 0.0)\n",
    "#     if np.all(s == 0):\n",
    "#         return np.zeros_like(X)\n",
    "#     return (U * s) @ Vt\n",
    "\n",
    "# def _compute_lambda(M, mode):\n",
    "#     \"\"\"\n",
    "#     mode:\n",
    "#       - 'median': 1/sqrt(max(m,n)) * median(abs(M))\n",
    "#       - 'scaled': 1.2/sqrt(max(m,n))\n",
    "#     \"\"\"\n",
    "#     m, n = M.shape\n",
    "#     base = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "#     if mode == \"median\":\n",
    "#         med = np.median(np.abs(M))\n",
    "#         # اگر med خیلی کوچک بود برای پایداری:\n",
    "#         med = float(med) if med > 1e-12 else 1e-12\n",
    "#         return base * med\n",
    "\n",
    "#     if mode == \"scaled\":\n",
    "#         return 1.2 * base\n",
    "\n",
    "#     raise ValueError(f\"Unknown RPCA_LAMBDA_MODE: {mode}. Use 'median' or 'scaled'.\")\n",
    "\n",
    "\n",
    "# def _rpca_ialm(M, mu=None, rho=1.5, max_iter=60, tol=1e-5):\n",
    "#     \"\"\"\n",
    "#     Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "#     Decompose: M = L + S\n",
    "#     M shape: (T, SC) = (3000, 30)\n",
    "#     \"\"\"\n",
    "#     M = M.astype(np.float64, copy=False)\n",
    "#     m, n = M.shape\n",
    "\n",
    "#     # ✅ lambda طبق انتخاب کاربر\n",
    "#     lam = _compute_lambda(M, RPCA_LAMBDA_MODE)\n",
    "\n",
    "#     # mu خودکار یا دستی\n",
    "#     if mu is None:\n",
    "#         s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "#         mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     Y = np.zeros_like(M)\n",
    "\n",
    "#     normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#         S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#         R = M - L - S\n",
    "#         Y = Y + mu * R\n",
    "\n",
    "#         if (np.linalg.norm(R, ord=\"fro\") / normM) < tol:\n",
    "#             break\n",
    "\n",
    "#         mu *= rho\n",
    "\n",
    "#     return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# def _ensure_shape_4d(data_csi):\n",
    "#     \"\"\"\n",
    "#     Ensure CSI shape is (T, TX, RX, SC).\n",
    "#     If input is (T, 270) we reshape to (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     if data_csi.ndim == 4:\n",
    "#         return data_csi\n",
    "\n",
    "#     if data_csi.ndim == 2 and data_csi.shape[1] == TX * RX * SC:\n",
    "#         T = data_csi.shape[0]\n",
    "#         return data_csi.reshape(T, TX, RX, SC)\n",
    "\n",
    "#     raise ValueError(\n",
    "#         f\"Unexpected CSI shape {data_csi.shape}. Expected (T,{TX},{RX},{SC}) or (T,{TX*RX*SC}).\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _rpca_pairwise_9links(data_csi_4d):\n",
    "#     \"\"\"\n",
    "#     data_csi_4d: (T, TX, RX, SC)\n",
    "#     Run RPCA on each (tx,rx) separately on matrix (T, SC) and reassemble.\n",
    "#     Output shape stays (T, TX, RX, SC).\n",
    "#     \"\"\"\n",
    "#     T = data_csi_4d.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "#     for tx in range(TX):\n",
    "#         for rx in range(RX):\n",
    "#             M = data_csi_4d[:, tx, rx, :]  # (T,30)\n",
    "#             L, S = _rpca_ialm(\n",
    "#                 M,\n",
    "#                 mu=RPCA_MU_INIT,\n",
    "#                 rho=RPCA_RHO,\n",
    "#                 max_iter=RPCA_MAX_ITER,\n",
    "#                 tol=RPCA_TOL\n",
    "#             )\n",
    "#             out[:, tx, rx, :] = L if CSI_INPUT_MODE == \"lowrank\" else S\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def _apply_mode_with_cache(data_csi, label):\n",
    "#     \"\"\"\n",
    "#     Apply raw/lowrank/sparse. For lowrank/sparse use pairwise RPCA (9 links of 3000x30).\n",
    "#     Keeps the original 4D shape (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     data_csi = _ensure_shape_4d(np.asarray(data_csi, dtype=np.float32))\n",
    "\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     mode = CSI_INPUT_MODE\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             cached = np.load(p)\n",
    "#             return _ensure_shape_4d(cached).astype(np.float32, copy=False)\n",
    "\n",
    "#     out = _rpca_pairwise_9links(data_csi)\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out.astype(np.float32))\n",
    "\n",
    "#     return out.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None,\n",
    "#                 var_wifi_band=None,\n",
    "#                 var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ RPCA روی 9 لینک جدا (3000x30) و بعد اتصال\n",
    "#         data_csi = _apply_mode_with_cache(data_csi, var_label)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "#                                     \"user_3_activity\", \"user_4_activity\",\n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3bbba1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:19:18.745094Z",
     "iopub.status.busy": "2025-12-25T15:19:18.744874Z",
     "iopub.status.idle": "2025-12-25T15:19:18.751967Z",
     "shell.execute_reply": "2025-12-25T15:19:18.751350Z"
    },
    "papermill": {
     "duration": 0.016427,
     "end_time": "2025-12-25T15:19:18.753117",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.736690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # =========================================================\n",
    "# # انتخاب نوع دیتای ورودی مدل:\n",
    "# # \"raw\"     : amp خام\n",
    "# # \"lowrank\" : خروجی Low-rank (L) از RPCA\n",
    "# # \"sparse\"  : خروجی Sparse (S) از RPCA\n",
    "# CSI_INPUT_MODE = \"sparse\"   # <-- raw / lowrank / sparse\n",
    "\n",
    "# # ابعاد مورد انتظار CSI\n",
    "# TX = 3\n",
    "# RX = 3\n",
    "# SC = 30\n",
    "\n",
    "# # RPCA (IALM) تنظیمات سریع‌تر\n",
    "# RPCA_MAX_ITER = 60\n",
    "# RPCA_TOL      = 1e-5\n",
    "# RPCA_RHO      = 1.5\n",
    "# RPCA_MU_INIT  = None\n",
    "# RPCA_LAMBDA   = None     # None -> 1/sqrt(max(m,n))\n",
    "\n",
    "# # Cache (خیلی مهم برای سرعت)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT    = \"/kaggle/working/csi_cache_pairwise\"  # خروجی‌ها اینجا ذخیره میشن\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _soft_threshold(X, tau):\n",
    "#     return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "# def _svt(X, tau):\n",
    "#     # Singular Value Thresholding\n",
    "#     U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "#     s = np.maximum(s - tau, 0.0)\n",
    "#     if np.all(s == 0):\n",
    "#         return np.zeros_like(X)\n",
    "#     return (U * s) @ Vt\n",
    "\n",
    "# def _rpca_ialm(M, lam=None, mu=None, rho=1.5, max_iter=60, tol=1e-5):\n",
    "#     \"\"\"\n",
    "#     Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "#     Decompose: M = L + S\n",
    "#     M shape: (T, SC) = (3000, 30)\n",
    "#     \"\"\"\n",
    "#     M = M.astype(np.float64, copy=False)\n",
    "#     m, n = M.shape\n",
    "\n",
    "#     if lam is None:\n",
    "#         lam = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "#     if mu is None:\n",
    "#         # top singular value as spectral norm approximation\n",
    "#         s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "#         mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     Y = np.zeros_like(M)\n",
    "\n",
    "#     normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#         S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#         R = M - L - S\n",
    "#         Y = Y + mu * R\n",
    "\n",
    "#         if (np.linalg.norm(R, ord=\"fro\") / normM) < tol:\n",
    "#             break\n",
    "\n",
    "#         mu *= rho\n",
    "\n",
    "#     return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# def _ensure_shape_4d(data_csi):\n",
    "#     \"\"\"\n",
    "#     Ensure CSI shape is (T, TX, RX, SC).\n",
    "#     If input is (T, 270) we reshape to (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     if data_csi.ndim == 4:\n",
    "#         return data_csi\n",
    "\n",
    "#     if data_csi.ndim == 2 and data_csi.shape[1] == TX * RX * SC:\n",
    "#         T = data_csi.shape[0]\n",
    "#         return data_csi.reshape(T, TX, RX, SC)\n",
    "\n",
    "#     raise ValueError(\n",
    "#         f\"Unexpected CSI shape {data_csi.shape}. Expected (T,{TX},{RX},{SC}) or (T,{TX*RX*SC}).\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     # mode: \"lowrank\" or \"sparse\"\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _rpca_pairwise_9links(data_csi_4d):\n",
    "#     \"\"\"\n",
    "#     data_csi_4d: (T, TX, RX, SC)\n",
    "#     Run RPCA on each (tx,rx) separately on matrix (T, SC) and reassemble.\n",
    "#     Output shape stays (T, TX, RX, SC).\n",
    "#     \"\"\"\n",
    "#     T = data_csi_4d.shape[0]\n",
    "#     out = np.empty((T, TX, RX, SC), dtype=np.float32)\n",
    "\n",
    "#     # 9 times RPCA: for each tx-rx link\n",
    "#     for tx in range(TX):\n",
    "#         for rx in range(RX):\n",
    "#             M = data_csi_4d[:, tx, rx, :]  # (T, SC) => (3000,30)\n",
    "#             L, S = _rpca_ialm(\n",
    "#                 M,\n",
    "#                 lam=RPCA_LAMBDA,\n",
    "#                 mu=RPCA_MU_INIT,\n",
    "#                 rho=RPCA_RHO,\n",
    "#                 max_iter=RPCA_MAX_ITER,\n",
    "#                 tol=RPCA_TOL\n",
    "#             )\n",
    "#             if CSI_INPUT_MODE == \"lowrank\":\n",
    "#                 out[:, tx, rx, :] = L\n",
    "#             else:  # \"sparse\"\n",
    "#                 out[:, tx, rx, :] = S\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def _apply_mode_with_cache(data_csi, label):\n",
    "#     \"\"\"\n",
    "#     Apply raw/lowrank/sparse. For lowrank/sparse use pairwise RPCA (9 links of 3000x30).\n",
    "#     Keeps the original 4D shape (T,3,3,30).\n",
    "#     \"\"\"\n",
    "#     data_csi = _ensure_shape_4d(np.asarray(data_csi, dtype=np.float32))\n",
    "\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     mode = CSI_INPUT_MODE\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             cached = np.load(p)\n",
    "#             return _ensure_shape_4d(cached).astype(np.float32, copy=False)\n",
    "\n",
    "#     out = _rpca_pairwise_9links(data_csi)\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out.astype(np.float32))\n",
    "\n",
    "#     return out.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     \"\"\"\n",
    "#     Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "#     \"\"\"\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     Load CSI amplitude (*.npy) files based on a label list.\n",
    "#     \"\"\"\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ NEW: RPCA روی 9 لینک جداگانه (3000x30) و بعد چسباندن\n",
    "#         data_csi = _apply_mode_with_cache(data_csi, var_label)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     \"\"\"\n",
    "#     Encode labels according to specific task.\n",
    "#     \"\"\"\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     return data_identity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for activity labels.\n",
    "#     \"\"\"\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "#                                     \"user_3_activity\", \"user_4_activity\", \n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "#     return data_activity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for location labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "#     return data_location_onehot_y\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1)\n",
    "#     count_data = count_data.reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     count_data_onehot = encoder.fit_transform(count_data).astype(\"int8\")\n",
    "#     return count_data_onehot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb1196",
   "metadata": {
    "papermill": {
     "duration": 0.006942,
     "end_time": "2025-12-25T15:19:18.766858",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.759916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "rpca\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf18d053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:19:18.781301Z",
     "iopub.status.busy": "2025-12-25T15:19:18.781099Z",
     "iopub.status.idle": "2025-12-25T15:19:18.787030Z",
     "shell.execute_reply": "2025-12-25T15:19:18.786338Z"
    },
    "papermill": {
     "duration": 0.014542,
     "end_time": "2025-12-25T15:19:18.788131",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.773589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # =========================================================\n",
    "# # انتخاب ورودی مدل:\n",
    "# # \"raw\"     : همون amp خام\n",
    "# # \"lowrank\" : مؤلفه Low-rank از RPCA (L)\n",
    "# # \"sparse\"  : مؤلفه Sparse از RPCA (S)\n",
    "# CSI_INPUT_MODE = \"lowrank\"   # <-- raw / lowrank / sparse\n",
    "\n",
    "# # RPCA (IALM) تنظیمات سریع‌تر\n",
    "# RPCA_MAX_ITER = 80\n",
    "# RPCA_TOL      = 1e-5\n",
    "# RPCA_RHO      = 1.5\n",
    "# RPCA_MU_INIT  = None\n",
    "# RPCA_LAMBDA   = None     # None -> 1/sqrt(max(m,n))\n",
    "\n",
    "# # Cache (خیلی مهم برای سرعت)\n",
    "# CACHE_ENABLED = True\n",
    "# CACHE_ROOT    = \"/kaggle/working/csi_cache\"  # خروجی‌ها اینجا ذخیره میشن\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _soft_threshold(X, tau):\n",
    "#     return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "# def _svt(X, tau):\n",
    "#     U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "#     s = np.maximum(s - tau, 0.0)\n",
    "#     if np.all(s == 0):\n",
    "#         return np.zeros_like(X)\n",
    "#     return (U * s) @ Vt\n",
    "\n",
    "# def _rpca_ialm(M, lam=None, mu=None, rho=1.5, max_iter=80, tol=1e-5):\n",
    "#     M = M.astype(np.float64, copy=False)\n",
    "#     m, n = M.shape\n",
    "\n",
    "#     if lam is None:\n",
    "#         lam = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "#     if mu is None:\n",
    "#         s0 = np.linalg.svd(M, compute_uv=False, full_matrices=False)[0] if M.size else 1.0\n",
    "#         mu = 1.25 / (s0 + 1e-12)\n",
    "\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     Y = np.zeros_like(M)\n",
    "\n",
    "#     normM = np.linalg.norm(M, ord=\"fro\") + 1e-12\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         L = _svt(M - S + (1.0 / mu) * Y, 1.0 / mu)\n",
    "#         S = _soft_threshold(M - L + (1.0 / mu) * Y, lam / mu)\n",
    "\n",
    "#         R = M - L - S\n",
    "#         Y = Y + mu * R\n",
    "\n",
    "#         if (np.linalg.norm(R, ord=\"fro\") / normM) < tol:\n",
    "#             break\n",
    "#         mu *= rho\n",
    "\n",
    "#     return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "\n",
    "# def _rpca_keep_shape(X):\n",
    "#     \"\"\"RPCA روی (T,F) و بازگرداندن دقیقاً به shape اولیه\"\"\"\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "#     if X.ndim == 1:\n",
    "#         M = X[:, None]\n",
    "#         L, S = _rpca_ialm(M, RPCA_LAMBDA, RPCA_MU_INIT, RPCA_RHO, RPCA_MAX_ITER, RPCA_TOL)\n",
    "#         return L[:, 0], S[:, 0]\n",
    "\n",
    "#     T = X.shape[0]\n",
    "#     F = int(np.prod(X.shape[1:]))\n",
    "#     M = X.reshape(T, F)\n",
    "\n",
    "#     L, S = _rpca_ialm(M, RPCA_LAMBDA, RPCA_MU_INIT, RPCA_RHO, RPCA_MAX_ITER, RPCA_TOL)\n",
    "#     return L.reshape(X.shape), S.reshape(X.shape)\n",
    "\n",
    "\n",
    "# def _cache_path(label, mode):\n",
    "#     # mode: \"lowrank\" or \"sparse\"\n",
    "#     # فایل خروجی: /kaggle/working/csi_cache/lowrank/<label>.npy\n",
    "#     return os.path.join(CACHE_ROOT, mode, f\"{label}.npy\")\n",
    "\n",
    "\n",
    "# def _apply_mode_with_cache(data_csi, label):\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     # lowrank یا sparse\n",
    "#     mode = CSI_INPUT_MODE\n",
    "#     if CACHE_ENABLED:\n",
    "#         os.makedirs(os.path.join(CACHE_ROOT, mode), exist_ok=True)\n",
    "#         p = _cache_path(label, mode)\n",
    "#         if os.path.exists(p):\n",
    "#             return np.load(p).astype(np.float32, copy=False)\n",
    "\n",
    "#     L, S = _rpca_keep_shape(data_csi)\n",
    "#     out = L if mode == \"lowrank\" else S\n",
    "\n",
    "#     if CACHE_ENABLED:\n",
    "#         np.save(p, out.astype(np.float32))\n",
    "\n",
    "#     return out.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_label, var_path in zip(var_label_list, var_path_list):\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ RPCA lowrank/sparse بدون تغییر shape + با cache\n",
    "#         data_csi = _apply_mode_with_cache(data_csi, var_label)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "\n",
    "#     return np.array(data_x)\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     return data_identity_y.astype(\"int8\")\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\",\n",
    "#                                     \"user_3_activity\", \"user_4_activity\",\n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_activity_y])\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[v] for v in sample] for sample in data_location_y])\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\",\n",
    "#                                     \"user_3_location\", \"user_4_location\",\n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1).reshape(-1, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)\n",
    "#     return encoder.fit_transform(count_data).astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b0e4eb",
   "metadata": {
    "papermill": {
     "duration": 0.007032,
     "end_time": "2025-12-25T15:19:18.802023",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.794991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "svd\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1154529d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:19:18.817579Z",
     "iopub.status.busy": "2025-12-25T15:19:18.817302Z",
     "iopub.status.idle": "2025-12-25T15:19:18.825229Z",
     "shell.execute_reply": "2025-12-25T15:19:18.824531Z"
    },
    "papermill": {
     "duration": 0.017561,
     "end_time": "2025-12-25T15:19:18.826408",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.808847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # Note: All necessary libraries (os, numpy, pandas, etc.) are imported in Cell 1.\n",
    "# # from preset import preset   --> preset is already defined in Cell 2.\n",
    "\n",
    "# # =========================================================\n",
    "# # 🔧 NEW: Choose CSI representation mode here (ONLY EDIT THIS)\n",
    "# # ---------------------------------------------------------\n",
    "# # \"raw\"     : use original CSI amplitude as-is\n",
    "# # \"lowrank\" : use low-rank approximation (SVD)\n",
    "# # \"sparse\"  : keep only large-magnitude entries (dense array with many zeros)\n",
    "# CSI_INPUT_MODE = \"sparse\"     # <-- set to: \"raw\" / \"lowrank\" / \"sparse\"\n",
    "\n",
    "# # Low-rank settings\n",
    "# LOW_RANK_ENERGY = 0.95     # keep enough singular values to preserve this energy\n",
    "# LOW_RANK_RANK   = None     # if set to an int (e.g., 10), it overrides ENERGY\n",
    "\n",
    "# # Sparse settings\n",
    "# SPARSE_KEEP_RATIO = 0.10   # keep top 10% magnitudes (globally per sample)\n",
    "# SPARSE_MIN_ABS    = None   # if set (e.g., 0.5), keeps |x|>=threshold instead of keep_ratio\n",
    "# # =========================================================\n",
    "\n",
    "\n",
    "# def _low_rank_approx_keep_shape(X, rank=None, energy=0.95):\n",
    "#     \"\"\"\n",
    "#     Low-rank approximation using SVD while preserving the original shape.\n",
    "#     Works for 1D/2D/ND by flattening all non-time dims into features.\n",
    "#     Assumes first axis is time.\n",
    "#     \"\"\"\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "\n",
    "#     if X.ndim == 1:\n",
    "#         M = X[:, None]  # (T,1)\n",
    "#         U, S, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "#         if rank is None:\n",
    "#             s2 = S**2\n",
    "#             cum = np.cumsum(s2) / (np.sum(s2) + 1e-12)\n",
    "#             rank = int(np.searchsorted(cum, energy) + 1)\n",
    "#         rank = max(1, min(rank, S.shape[0]))\n",
    "#         M_lr = (U[:, :rank] * S[:rank]) @ Vt[:rank, :]\n",
    "#         return M_lr[:, 0].astype(np.float32)\n",
    "\n",
    "#     # ND: reshape to (T, F)\n",
    "#     T = X.shape[0]\n",
    "#     F = int(np.prod(X.shape[1:]))\n",
    "#     M = X.reshape(T, F)\n",
    "\n",
    "#     U, S, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "\n",
    "#     if rank is None:\n",
    "#         s2 = S**2\n",
    "#         cum = np.cumsum(s2) / (np.sum(s2) + 1e-12)\n",
    "#         rank = int(np.searchsorted(cum, energy) + 1)\n",
    "\n",
    "#     rank = max(1, min(rank, S.shape[0]))\n",
    "#     M_lr = (U[:, :rank] * S[:rank]) @ Vt[:rank, :]\n",
    "\n",
    "#     return M_lr.reshape(X.shape).astype(np.float32)\n",
    "\n",
    "\n",
    "# def _to_sparse_dense_keep_shape(X, keep_ratio=0.10, min_abs=None):\n",
    "#     \"\"\"\n",
    "#     Makes X sparse-in-content (many zeros) but keeps it as a dense numpy array\n",
    "#     so the rest of the pipeline (np.save/np.load/pad/model) doesn't change.\n",
    "#     Keeps the same shape.\n",
    "#     \"\"\"\n",
    "#     X = np.asarray(X, dtype=np.float32)\n",
    "#     flat = X.ravel()\n",
    "#     if flat.size == 0:\n",
    "#         return X.astype(np.float32)\n",
    "\n",
    "#     absflat = np.abs(flat)\n",
    "\n",
    "#     if min_abs is not None:\n",
    "#         thr = float(min_abs)\n",
    "#         mask = absflat >= thr\n",
    "#     else:\n",
    "#         k = int(np.ceil(keep_ratio * flat.size))\n",
    "#         k = max(1, min(k, flat.size))\n",
    "#         if k == flat.size:\n",
    "#             mask = np.ones_like(absflat, dtype=bool)\n",
    "#         else:\n",
    "#             thr = np.partition(absflat, -k)[-k]\n",
    "#             mask = absflat >= thr\n",
    "\n",
    "#     out = np.zeros_like(flat, dtype=np.float32)\n",
    "#     out[mask] = flat[mask]\n",
    "#     return out.reshape(X.shape).astype(np.float32)\n",
    "\n",
    "\n",
    "# def _apply_csi_mode(data_csi):\n",
    "#     \"\"\"\n",
    "#     Apply selected CSI_INPUT_MODE to a single sample array.\n",
    "#     \"\"\"\n",
    "#     if CSI_INPUT_MODE == \"raw\":\n",
    "#         return data_csi.astype(np.float32, copy=False)\n",
    "\n",
    "#     elif CSI_INPUT_MODE == \"lowrank\":\n",
    "#         return _low_rank_approx_keep_shape(\n",
    "#             data_csi,\n",
    "#             rank=LOW_RANK_RANK,\n",
    "#             energy=LOW_RANK_ENERGY\n",
    "#         )\n",
    "\n",
    "#     elif CSI_INPUT_MODE == \"sparse\":\n",
    "#         return _to_sparse_dense_keep_shape(\n",
    "#             data_csi,\n",
    "#             keep_ratio=SPARSE_KEEP_RATIO,\n",
    "#             min_abs=SPARSE_MIN_ABS\n",
    "#         )\n",
    "\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown CSI_INPUT_MODE: {CSI_INPUT_MODE}. Use 'raw', 'lowrank', or 'sparse'.\")\n",
    "\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     \"\"\"\n",
    "#     Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "#     \"\"\"\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     Load CSI amplitude (*.npy) files based on a label list.\n",
    "#     \"\"\"\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_path in var_path_list:\n",
    "#         data_csi = np.load(var_path)\n",
    "\n",
    "#         # ✅ NEW: convert input CSI according to selected mode (raw/lowrank/sparse)\n",
    "#         data_csi = _apply_csi_mode(data_csi)\n",
    "\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "#     data_x = np.array(data_x)\n",
    "#     return data_x\n",
    "\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     \"\"\"\n",
    "#     Encode labels according to specific task.\n",
    "#     \"\"\"\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     return data_identity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for activity labels.\n",
    "#     \"\"\"\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "#                                     \"user_3_activity\", \"user_4_activity\", \n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "#     return data_activity_onehot_y\n",
    "\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for location labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "#     return data_location_onehot_y\n",
    "\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     print(\"data_identity_onehot_y\",data_identity_onehot_y.shape)\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1)\n",
    "#     print(\"count_data\",count_data.shape)\n",
    "#     count_data = count_data.reshape(-1, 1)  # shape = (11286, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)  \n",
    "#     count_data_onehot = encoder.fit_transform(count_data)\n",
    "#     print(count_data_onehot.shape)  \n",
    "#     count_data_onehot = count_data_onehot.astype(\"int8\")\n",
    "\n",
    "#     return count_data_onehot\n",
    "\n",
    "\n",
    "# # Test functions (optional)\n",
    "# def test_load_data_y():\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"classroom\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=[\"1\", \"2\", \"3\"]).describe())\n",
    "\n",
    "# def test_load_data_x():\n",
    "#     data_pd_y = load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=None)\n",
    "#     var_label_list = data_pd_y[\"label\"].to_list()\n",
    "#     data_x = load_data_x(preset[\"path\"][\"data_x\"], var_label_list)\n",
    "#     print(data_x.shape)\n",
    "\n",
    "# def test_encode_identity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_identity_onehot_y = encode_identity(data_pd_y)\n",
    "#     print(data_identity_onehot_y.shape)\n",
    "#     print(data_identity_onehot_y[2000])\n",
    "\n",
    "# def test_encode_activity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_activity_onehot_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     print(data_activity_onehot_y.shape)\n",
    "#     print(data_activity_onehot_y[1560])\n",
    "\n",
    "# def test_encode_location():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_location_onehot_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     print(data_location_onehot_y.shape)\n",
    "#     print(data_location_onehot_y[1560])\n",
    "\n",
    "# def test_encode_count():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_count_onehot_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     print(data_count_onehot_y.shape)\n",
    "#     print(data_count_onehot_y[20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4443bd0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:19:18.841955Z",
     "iopub.status.busy": "2025-12-25T15:19:18.841738Z",
     "iopub.status.idle": "2025-12-25T15:19:18.847607Z",
     "shell.execute_reply": "2025-12-25T15:19:18.846892Z"
    },
    "papermill": {
     "duration": 0.015301,
     "end_time": "2025-12-25T15:19:18.848776",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.833475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# [file]          load_data.py\n",
    "# [description]   load annotation file and CSI amplitude, and encode labels\n",
    "# \"\"\"\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# import numpy as np\n",
    "\n",
    "# # Note: All necessary libraries (os, numpy, pandas, etc.) are imported in Cell 1.\n",
    "# # from preset import preset   --> preset is already defined in Cell 2.\n",
    "\n",
    "# def load_data_y(var_path_data_y,\n",
    "#                 var_environment=None, \n",
    "#                 var_wifi_band=None, \n",
    "#                 var_num_users=None):\n",
    "#     \"\"\"\n",
    "#     Load annotation file (*.csv) as a pandas dataframe and filter by environment, WiFi band, and number of users.\n",
    "#     \"\"\"\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None:\n",
    "#         data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list):\n",
    "#     \"\"\"\n",
    "#     Load CSI amplitude (*.npy) files based on a label list.\n",
    "#     \"\"\"\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     for var_path in var_path_list:\n",
    "#         data_csi = np.load(var_path)\n",
    "#         var_pad_length = preset[\"data\"][\"length\"] - data_csi.shape[0]\n",
    "#         data_csi_pad = np.pad(data_csi, ((var_pad_length, 0), (0, 0), (0, 0), (0, 0)))\n",
    "#         data_x.append(data_csi_pad)\n",
    "#     data_x = np.array(data_x)\n",
    "#     return data_x\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     \"\"\"\n",
    "#     Encode labels according to specific task.\n",
    "#     \"\"\"\n",
    "#     if var_task == \"identity\":\n",
    "#         data_y = encode_identity(data_pd_y)\n",
    "#     elif var_task == \"activity\":\n",
    "#         data_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     elif var_task == \"location\":\n",
    "#         data_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     elif var_task == \"count\":\n",
    "#         data_y = encode_count(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     return data_y\n",
    "\n",
    "# def encode_identity(data_pd_y):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     return data_identity_onehot_y\n",
    "\n",
    "\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for activity labels.\n",
    "#     \"\"\"\n",
    "#     data_activity_pd_y = data_pd_y[[\"user_1_activity\", \"user_2_activity\", \n",
    "#                                     \"user_3_activity\", \"user_4_activity\", \n",
    "#                                     \"user_5_activity\", \"user_6_activity\"]]\n",
    "#     data_activity_y = data_activity_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_activity_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_activity_y])\n",
    "#     return data_activity_onehot_y\n",
    "\n",
    "# def encode_location(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for location labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_location_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_location_onehot_y = np.array([[var_encoding[var_y] for var_y in var_sample] for var_sample in data_location_y])\n",
    "#     return data_location_onehot_y\n",
    "\n",
    "# # Test functions (optional)\n",
    "# def test_load_data_y():\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"classroom\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"]).describe())\n",
    "#     print(load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=[\"1\", \"2\", \"3\"]).describe())\n",
    "\n",
    "# def test_load_data_x():\n",
    "#     data_pd_y = load_data_y(preset[\"path\"][\"data_y\"], var_environment=[\"meeting_room\"], var_wifi_band=[\"2.4\"], var_num_users=None)\n",
    "#     var_label_list = data_pd_y[\"label\"].to_list()\n",
    "#     data_x = load_data_x(preset[\"path\"][\"data_x\"], var_label_list)\n",
    "#     print(data_x.shape)\n",
    "\n",
    "# def test_encode_identity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_identity_onehot_y = encode_identity(data_pd_y)\n",
    "#     print(data_identity_onehot_y.shape)\n",
    "#     print(data_identity_onehot_y[2000])\n",
    "\n",
    "# def test_encode_activity():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_activity_onehot_y = encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     print(data_activity_onehot_y.shape)\n",
    "#     print(data_activity_onehot_y[1560])\n",
    "\n",
    "# def test_encode_location():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_location_onehot_y = encode_location(data_pd_y, preset[\"encoding\"][\"location\"])\n",
    "#     print(data_location_onehot_y.shape)\n",
    "#     print(data_location_onehot_y[1560])\n",
    "\n",
    "# def encode_count(data_pd_y, var_encoding):\n",
    "#     \"\"\"\n",
    "#     Onehot encoding for identity labels.\n",
    "#     \"\"\"\n",
    "#     data_location_pd_y = data_pd_y[[\"user_1_location\", \"user_2_location\", \n",
    "#                                     \"user_3_location\", \"user_4_location\", \n",
    "#                                     \"user_5_location\", \"user_6_location\"]]\n",
    "#     data_identity_y = data_location_pd_y.to_numpy(copy=True).astype(str)\n",
    "#     data_identity_y[data_identity_y != \"nan\"] = 1\n",
    "#     data_identity_y[data_identity_y == \"nan\"] = 0\n",
    "#     data_identity_onehot_y = data_identity_y.astype(\"int8\")\n",
    "#     print(\"data_identity_onehot_y\",data_identity_onehot_y.shape)\n",
    "#     count_data = np.sum(data_identity_onehot_y, axis=1)\n",
    "#     print(\"count_data\",count_data.shape)\n",
    "#     count_data = count_data.reshape(-1, 1)  # shape = (11286, 1)\n",
    "#     encoder = OneHotEncoder(sparse=False)  \n",
    "#     count_data_onehot = encoder.fit_transform(count_data)\n",
    "#     print(count_data_onehot.shape)  \n",
    "#     count_data_onehot = count_data_onehot.astype(\"int8\")\n",
    "\n",
    "#     return count_data_onehot\n",
    "\n",
    "\n",
    "# def test_encode_count():\n",
    "#     data_pd_y = pd.read_csv(preset[\"path\"][\"data_y\"], dtype=str)\n",
    "#     data_count_onehot_y = encode_count(data_pd_y)\n",
    "#     print(data_count_onehot_y.shape)\n",
    "#     print(data_count_onehot_y[20])\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     test_encode_count()\n",
    "# #     test_load_data_y()\n",
    "# #     test_load_data_x()\n",
    "# #     test_encode_identity()\n",
    "# #     test_encode_activity()\n",
    "# #     test_encode_location()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d88c1b",
   "metadata": {
    "papermill": {
     "duration": 0.006321,
     "end_time": "2025-12-25T15:19:18.862092",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.855771",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 4: preprocess.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08baf4e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:19:18.876421Z",
     "iopub.status.busy": "2025-12-25T15:19:18.876173Z",
     "iopub.status.idle": "2025-12-25T15:19:18.891302Z",
     "shell.execute_reply": "2025-12-25T15:19:18.890587Z"
    },
    "papermill": {
     "duration": 0.024038,
     "end_time": "2025-12-25T15:19:18.892439",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.868401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          preprocess.py\n",
    "[description]   preprocess WiFi CSI data\n",
    "\"\"\"\n",
    "\n",
    "# All necessary libraries are already imported in Cell 1.\n",
    "\n",
    "# def mat_to_amp(data_mat):\n",
    "#     \"\"\"\n",
    "#     Calculate amplitude of raw WiFi CSI data.\n",
    "#     \"\"\"\n",
    "#     var_length = data_mat[\"trace\"].shape[0]\n",
    "#     data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "#     data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "#     return data_csi_amp\n",
    "\n",
    "def extract_csi_amp(var_dir_mat, var_dir_amp):\n",
    "    \"\"\"\n",
    "    Read raw WiFi CSI (*.mat) files, calculate CSI amplitude, and save as (*.npy).\n",
    "    \"\"\"\n",
    "    var_path_mat = os.listdir(var_dir_mat)\n",
    "    for var_c, var_path in enumerate(var_path_mat):\n",
    "        data_mat = scio.loadmat(os.path.join(var_dir_mat, var_path))\n",
    "        data_csi_amp = mat_to_amp(data_mat)\n",
    "        # print(var_c, data_csi_amp.shape)\n",
    "        var_path_save = os.path.join(var_dir_amp, var_path.replace(\".mat\", \".npy\"))\n",
    "        with open(var_path_save, \"wb\") as var_file:\n",
    "            np.save(var_file, data_csi_amp)\n",
    "\n",
    "\n",
    "\n",
    "# # تنظیمات low-rank (بدون تغییر ورودی mat_to_amp)\n",
    "# LOW_RANK_ENERGY = 0.95   # مثلاً 95% انرژی\n",
    "# LOW_RANK_RANK = None     # اگر عدد بذاری (مثلاً 5)، به جای ENERGY از rank ثابت استفاده میشه\n",
    "\n",
    "# def _low_rank_approx(X, rank=None, energy=0.95):\n",
    "#     X = np.asarray(X)\n",
    "\n",
    "#     was_1d = (X.ndim == 1)\n",
    "#     if was_1d:\n",
    "#         X = X[:, None]\n",
    "\n",
    "#     U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "#     if rank is None:\n",
    "#         s2 = S**2\n",
    "#         cum = np.cumsum(s2) / (np.sum(s2) + 1e-12)\n",
    "#         rank = int(np.searchsorted(cum, energy) + 1)\n",
    "\n",
    "#     rank = max(1, min(rank, S.shape[0]))\n",
    "#     X_lr = (U[:, :rank] * S[:rank]) @ Vt[:rank, :]\n",
    "\n",
    "#     if was_1d:\n",
    "#         X_lr = X_lr[:, 0]\n",
    "\n",
    "#     return X_lr.astype(np.float32)\n",
    "\n",
    "# def mat_to_amp(data_mat):\n",
    "#     \"\"\"\n",
    "#     Calculate amplitude of raw WiFi CSI data, then return its low-rank approximation.\n",
    "#     (ورودی تابع تغییر نکرده)\n",
    "#     \"\"\"\n",
    "#     var_length = data_mat[\"trace\"].shape[0]\n",
    "#     data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "#     data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "#     # خروجی low-rank با همان ابعاد\n",
    "#     data_csi_amp_lr = _low_rank_approx(\n",
    "#         data_csi_amp,\n",
    "#         rank=LOW_RANK_RANK,\n",
    "#         energy=LOW_RANK_ENERGY\n",
    "#     )\n",
    "#     return data_csi_amp_lr\n",
    "\n",
    "\n",
    "\n",
    "# # تنظیمات sparsity (بدون تغییر ورودی mat_to_amp)\n",
    "# SPARSE_KEEP_RATIO = 0.10   # مثلا فقط 10% بزرگترین مقادیر نگه داشته بشن\n",
    "# SPARSE_MIN_ABS = None      # اگر عدد بذاری (مثلا 0.5)، به جای keep_ratio آستانه ثابت میشه\n",
    "\n",
    "# def _to_sparse(X, keep_ratio=0.10, min_abs=None):\n",
    "#     \"\"\"\n",
    "#     Convert X to a sparse representation by keeping only large-magnitude entries.\n",
    "#     Returns:\n",
    "#       - scipy.sparse.csr_matrix if SciPy is available\n",
    "#       - otherwise returns a dense array with many zeros (still \"sparse\" in content)\n",
    "#     \"\"\"\n",
    "#     X = np.asarray(X)\n",
    "#     flat = X.ravel()\n",
    "#     absflat = np.abs(flat)\n",
    "\n",
    "#     if flat.size == 0:\n",
    "#         return X.astype(np.float32)\n",
    "\n",
    "#     # انتخاب آستانه\n",
    "#     if min_abs is not None:\n",
    "#         thr = float(min_abs)\n",
    "#         mask = absflat >= thr\n",
    "#     else:\n",
    "#         k = int(np.ceil(keep_ratio * flat.size))\n",
    "#         k = max(1, min(k, flat.size))\n",
    "#         if k == flat.size:\n",
    "#             mask = np.ones_like(absflat, dtype=bool)\n",
    "#         else:\n",
    "#             thr = np.partition(absflat, -k)[-k]  # kth largest magnitude\n",
    "#             mask = absflat >= thr\n",
    "\n",
    "#     idx = np.nonzero(mask)[0]\n",
    "#     data = flat[idx].astype(np.float32)\n",
    "\n",
    "#     # اگر SciPy هست: sparse واقعی بساز\n",
    "#     try:\n",
    "#         # معمولاً تو Cell1 یا از قبل import شده؛ اگر هم نشده باشه اینجا تلاش می‌کنه.\n",
    "#         import scipy.sparse as sp\n",
    "\n",
    "#         if X.ndim == 1:\n",
    "#             rows = idx\n",
    "#             cols = np.zeros_like(rows)\n",
    "#             shape = (X.shape[0], 1)\n",
    "#         else:\n",
    "#             rows, cols = np.unravel_index(idx, X.shape)\n",
    "#             shape = X.shape\n",
    "\n",
    "#         return sp.coo_matrix((data, (rows, cols)), shape=shape).tocsr()\n",
    "\n",
    "#     except Exception:\n",
    "#         # fallback: آرایه‌ی dense با صفرهای زیاد\n",
    "#         out = np.zeros_like(flat, dtype=np.float32)\n",
    "#         out[idx] = data\n",
    "#         return out.reshape(X.shape)\n",
    "\n",
    "def mat_to_amp(data_mat):\n",
    "    \"\"\"\n",
    "    Calculate amplitude of raw WiFi CSI data, then return its sparse version.\n",
    "    (ورودی تابع تغییر نکرده)\n",
    "    \"\"\"\n",
    "    var_length = data_mat[\"trace\"].shape[0]\n",
    "    data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "    data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "    # خروجی sparse (CSR اگر SciPy باشد)\n",
    "    return _to_sparse(data_csi_amp, keep_ratio=SPARSE_KEEP_RATIO, min_abs=SPARSE_MIN_ABS)\n",
    "\n",
    "# تنظیمات RPCA (می‌تونی عوضشون کنی)\n",
    "RPCA_MAX_ITER = 500\n",
    "RPCA_TOL = 1e-7\n",
    "RPCA_RHO = 1.5\n",
    "RPCA_MU_INIT = None     # None یعنی خودکار\n",
    "RPCA_LAMBDA = None      # None یعنی 1/sqrt(max(m,n))\n",
    "\n",
    "def _soft_threshold(X, tau):\n",
    "    return np.sign(X) * np.maximum(np.abs(X) - tau, 0.0)\n",
    "\n",
    "def _svt(X, tau):\n",
    "    # Singular Value Thresholding\n",
    "    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    s_thr = np.maximum(s - tau, 0.0)\n",
    "    # اگر همه صفر شد، سریع برگرد\n",
    "    if np.all(s_thr == 0):\n",
    "        return np.zeros_like(X)\n",
    "    return (U * s_thr) @ Vt\n",
    "\n",
    "def _rpca_ialm(M, lam=None, mu=None, rho=1.5, max_iter=500, tol=1e-7):\n",
    "    \"\"\"\n",
    "    Robust PCA via Inexact Augmented Lagrange Multiplier (IALM)\n",
    "    Decompose: M = L + S\n",
    "    Returns: L, S (same shape as M)\n",
    "    \"\"\"\n",
    "    M = M.astype(np.float64, copy=False)\n",
    "    m, n = M.shape\n",
    "\n",
    "    if lam is None:\n",
    "        lam = 1.0 / np.sqrt(max(m, n))\n",
    "\n",
    "    # mu پیشنهادی (خودکار)\n",
    "    if mu is None:\n",
    "        # ||M||_2 تقریباً بزرگ‌ترین singular value است\n",
    "        norm2 = np.linalg.svd(M, compute_uv=False)[0] if M.size else 1.0\n",
    "        mu = 1.25 / (norm2 + 1e-12)\n",
    "\n",
    "    L = np.zeros_like(M)\n",
    "    S = np.zeros_like(M)\n",
    "    Y = np.zeros_like(M)\n",
    "\n",
    "    normM = np.linalg.norm(M, ord='fro') + 1e-12\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # L update\n",
    "        L = _svt(M - S + (1.0/mu)*Y, 1.0/mu)\n",
    "\n",
    "        # S update (sparse)\n",
    "        S = _soft_threshold(M - L + (1.0/mu)*Y, lam/mu)\n",
    "\n",
    "        # dual update\n",
    "        R = M - L - S\n",
    "        Y = Y + mu * R\n",
    "\n",
    "        # stop\n",
    "        err = np.linalg.norm(R, ord='fro') / normM\n",
    "        if err < tol:\n",
    "            break\n",
    "\n",
    "        mu *= rho\n",
    "\n",
    "    return L.astype(np.float32), S.astype(np.float32)\n",
    "\n",
    "# def mat_to_amp(data_mat):\n",
    "#     \"\"\"\n",
    "#     Calculate amplitude of raw WiFi CSI data, then return RPCA sparse component S.\n",
    "#     (ورودی تابع تغییر نکرده)\n",
    "#     \"\"\"\n",
    "#     var_length = data_mat[\"trace\"].shape[0]\n",
    "#     data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "#     data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "#     was_1d = (data_csi_amp.ndim == 1)\n",
    "#     M = data_csi_amp[:, None] if was_1d else data_csi_amp\n",
    "\n",
    "#     _, S = _rpca_ialm(\n",
    "#         M,\n",
    "#         lam=RPCA_LAMBDA,\n",
    "#         mu=RPCA_MU_INIT,\n",
    "#         rho=RPCA_RHO,\n",
    "#         max_iter=RPCA_MAX_ITER,\n",
    "#         tol=RPCA_TOL\n",
    "#     )\n",
    "\n",
    "#     if was_1d:\n",
    "#         S = S[:, 0]\n",
    "\n",
    "#     return S\n",
    "\n",
    "def mat_to_amp(data_mat):\n",
    "    \"\"\"\n",
    "    Calculate amplitude of raw WiFi CSI data, then return RPCA low-rank component L.\n",
    "    (ورودی تابع تغییر نکرده)\n",
    "    \"\"\"\n",
    "    var_length = data_mat[\"trace\"].shape[0]\n",
    "    data_csi_amp = [abs(data_mat[\"trace\"][var_t][0][0][0][-1]) for var_t in range(var_length)]\n",
    "    data_csi_amp = np.array(data_csi_amp, dtype=np.float32)\n",
    "\n",
    "    was_1d = (data_csi_amp.ndim == 1)\n",
    "    M = data_csi_amp[:, None] if was_1d else data_csi_amp\n",
    "\n",
    "    L, _ = _rpca_ialm(\n",
    "        M,\n",
    "        lam=RPCA_LAMBDA,\n",
    "        mu=RPCA_MU_INIT,\n",
    "        rho=RPCA_RHO,\n",
    "        max_iter=RPCA_MAX_ITER,\n",
    "        tol=RPCA_TOL\n",
    "    )\n",
    "\n",
    "    if was_1d:\n",
    "        L = L[:, 0]\n",
    "\n",
    "    return L\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parse arguments from input.\n",
    "    \"\"\"\n",
    "    var_args = argparse.ArgumentParser()\n",
    "    var_args.add_argument(\"--dir_mat\", default=\"/kaggle/input/wimans/wifi_csi/mat\", type=str)\n",
    "    var_args.add_argument(\"--dir_amp\", default=\"/kaggle/input/wimans/wifi_csi/amp\", type=str)\n",
    "    return var_args.parse_args()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     var_args = parse_args()\n",
    "#     extract_csi_amp(var_dir_mat=var_args.dir_mat, var_dir_amp=var_args.dir_amp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad5c40",
   "metadata": {
    "papermill": {
     "duration": 0.006739,
     "end_time": "2025-12-25T15:19:18.906459",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.899720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 5: that.py (WiFi-based Model THAT)\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d9f4e75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:19:18.922598Z",
     "iopub.status.busy": "2025-12-25T15:19:18.922051Z",
     "iopub.status.idle": "2025-12-25T15:19:18.948018Z",
     "shell.execute_reply": "2025-12-25T15:19:18.947292Z"
    },
    "papermill": {
     "duration": 0.035953,
     "end_time": "2025-12-25T15:19:18.949203",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.913250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          that.py\n",
    "[description]   implement and evaluate WiFi-based model THAT\n",
    "                https://github.com/windofshadow/THAT\n",
    "\"\"\"\n",
    "\n",
    "# All necessary libraries are imported in Cell 1.\n",
    "# from train import train   --> Defined in Cell 6.\n",
    "# from preset import preset --> Defined in Cell 2.\n",
    "\n",
    "class Gaussian_Position(torch.nn.Module):\n",
    "    def __init__(self, var_dim_feature, var_dim_time, var_num_gaussian=10):\n",
    "        super(Gaussian_Position, self).__init__()\n",
    "        var_embedding = torch.zeros([var_num_gaussian, var_dim_feature], dtype=torch.float)\n",
    "        self.var_embedding = torch.nn.Parameter(var_embedding, requires_grad=True)\n",
    "        torch.nn.init.xavier_uniform_(self.var_embedding)\n",
    "        var_position = torch.arange(0.0, var_dim_time).unsqueeze(1).repeat(1, var_num_gaussian)\n",
    "        self.var_position = torch.nn.Parameter(var_position, requires_grad=False)\n",
    "        var_mu = torch.arange(0.0, var_dim_time, var_dim_time/var_num_gaussian).unsqueeze(0)\n",
    "        self.var_mu = torch.nn.Parameter(var_mu, requires_grad=True)\n",
    "        var_sigma = torch.tensor([50.0] * var_num_gaussian).unsqueeze(0)\n",
    "        self.var_sigma = torch.nn.Parameter(var_sigma, requires_grad=True)\n",
    "\n",
    "    def calculate_pdf(self, var_position, var_mu, var_sigma):\n",
    "        var_pdf = var_position - var_mu\n",
    "        var_pdf = - var_pdf * var_pdf\n",
    "        var_pdf = var_pdf / var_sigma / var_sigma / 2\n",
    "        var_pdf = var_pdf - torch.log(var_sigma)\n",
    "        return var_pdf\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        var_pdf = self.calculate_pdf(self.var_position, self.var_mu, self.var_sigma)\n",
    "        var_pdf = torch.softmax(var_pdf, dim=-1)\n",
    "        var_position_encoding = torch.matmul(var_pdf, self.var_embedding)\n",
    "        var_output = var_input + var_position_encoding.unsqueeze(0)\n",
    "        return var_output\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, var_dim_feature, var_num_head=10, var_size_cnn=[1, 3, 5]):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layer_norm_0 = torch.nn.LayerNorm(var_dim_feature, eps=1e-6)\n",
    "        self.layer_attention = torch.nn.MultiheadAttention(var_dim_feature, var_num_head, batch_first=True)\n",
    "        self.layer_dropout_0 = torch.nn.Dropout(0.1)\n",
    "        self.layer_norm_1 = torch.nn.LayerNorm(var_dim_feature, 1e-6)\n",
    "        layer_cnn = []\n",
    "        for var_size in var_size_cnn:\n",
    "            layer = torch.nn.Sequential(\n",
    "                torch.nn.Conv1d(var_dim_feature, var_dim_feature, var_size, padding=\"same\"),\n",
    "                torch.nn.BatchNorm1d(var_dim_feature),\n",
    "                torch.nn.Dropout(0.1),\n",
    "                torch.nn.LeakyReLU()\n",
    "            )\n",
    "            layer_cnn.append(layer)\n",
    "        self.layer_cnn = torch.nn.ModuleList(layer_cnn)\n",
    "        self.layer_dropout_1 = torch.nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        var_t = var_input\n",
    "        var_t = self.layer_norm_0(var_t)\n",
    "        var_t, _ = self.layer_attention(var_t, var_t, var_t)\n",
    "        var_t = self.layer_dropout_0(var_t)\n",
    "        var_t = var_t + var_input\n",
    "        var_s = self.layer_norm_1(var_t)\n",
    "        var_s = torch.permute(var_s, (0, 2, 1))\n",
    "        var_c = torch.stack([layer(var_s) for layer in self.layer_cnn], dim=0)\n",
    "        var_s = torch.sum(var_c, dim=0) / len(self.layer_cnn)\n",
    "        var_s = self.layer_dropout_1(var_s)\n",
    "        var_s = torch.permute(var_s, (0, 2, 1))\n",
    "        var_output = var_s + var_t\n",
    "        return var_output\n",
    "\n",
    "class THAT(torch.nn.Module):\n",
    "    def __init__(self, var_x_shape, var_y_shape):\n",
    "        super(THAT, self).__init__()\n",
    "        var_dim_feature = var_x_shape[-1]\n",
    "        var_dim_time = var_x_shape[-2]\n",
    "        var_dim_output = var_y_shape[-1]\n",
    "        # Left branch\n",
    "        self.layer_left_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "        self.layer_left_gaussian = Gaussian_Position(var_dim_feature, var_dim_time // 20)\n",
    "        var_num_left = 4\n",
    "        var_dim_left = var_dim_feature\n",
    "        self.layer_left_encoder = torch.nn.ModuleList([\n",
    "            Encoder(var_dim_feature=var_dim_left, var_num_head=10, var_size_cnn=[1, 3, 5])\n",
    "            for _ in range(var_num_left)\n",
    "        ])\n",
    "        self.layer_left_norm = torch.nn.LayerNorm(var_dim_left, eps=1e-6)\n",
    "        self.layer_left_cnn_0 = torch.nn.Conv1d(in_channels=var_dim_left, out_channels=128, kernel_size=8)\n",
    "        self.layer_left_cnn_1 = torch.nn.Conv1d(in_channels=var_dim_left, out_channels=128, kernel_size=16)\n",
    "        self.layer_left_dropout = torch.nn.Dropout(0.5)\n",
    "        # Right branch\n",
    "        self.layer_right_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "        var_num_right = 1\n",
    "        var_dim_right = var_dim_time // 20\n",
    "        self.layer_right_encoder = torch.nn.ModuleList([\n",
    "            Encoder(var_dim_feature=var_dim_right, var_num_head=10, var_size_cnn=[1, 2, 3])\n",
    "            for _ in range(var_num_right)\n",
    "        ])\n",
    "        self.layer_right_norm = torch.nn.LayerNorm(var_dim_right, eps=1e-6)\n",
    "        self.layer_right_cnn_0 = torch.nn.Conv1d(in_channels=var_dim_right, out_channels=16, kernel_size=2)\n",
    "        self.layer_right_cnn_1 = torch.nn.Conv1d(in_channels=var_dim_right, out_channels=16, kernel_size=4)\n",
    "        self.layer_right_dropout = torch.nn.Dropout(0.5)\n",
    "        self.layer_leakyrelu = torch.nn.LeakyReLU()\n",
    "        self.layer_output = torch.nn.Linear(256 + 32, var_dim_output)\n",
    "\n",
    "    def forward(self, var_input):\n",
    "        var_t = var_input  # shape: (batch_size, time_steps, features)\n",
    "        # Left branch\n",
    "        var_left = torch.permute(var_t, (0, 2, 1))\n",
    "        var_left = self.layer_left_pooling(var_left)\n",
    "        var_left = torch.permute(var_left, (0, 2, 1))\n",
    "        var_left = self.layer_left_gaussian(var_left)\n",
    "        for layer in self.layer_left_encoder:\n",
    "            var_left = layer(var_left)\n",
    "        var_left = self.layer_left_norm(var_left)\n",
    "        var_left = torch.permute(var_left, (0, 2, 1))\n",
    "        var_left_0 = self.layer_leakyrelu(self.layer_left_cnn_0(var_left))\n",
    "        var_left_1 = self.layer_leakyrelu(self.layer_left_cnn_1(var_left))\n",
    "        var_left_0 = torch.sum(var_left_0, dim=-1)\n",
    "        var_left_1 = torch.sum(var_left_1, dim=-1)\n",
    "        var_left = torch.concat([var_left_0, var_left_1], dim=-1)\n",
    "        var_left = self.layer_left_dropout(var_left)\n",
    "        # Right branch\n",
    "        var_right = torch.permute(var_t, (0, 2, 1))\n",
    "        var_right = self.layer_right_pooling(var_right)\n",
    "        for layer in self.layer_right_encoder:\n",
    "            var_right = layer(var_right)\n",
    "        var_right = self.layer_right_norm(var_right)\n",
    "        var_right = torch.permute(var_right, (0, 2, 1))\n",
    "        var_right_0 = self.layer_leakyrelu(self.layer_right_cnn_0(var_right))\n",
    "        var_right_1 = self.layer_leakyrelu(self.layer_right_cnn_1(var_right))\n",
    "        var_right_0 = torch.sum(var_right_0, dim=-1)\n",
    "        var_right_1 = torch.sum(var_right_1, dim=-1)\n",
    "        var_right = torch.concat([var_right_0, var_right_1], dim=-1)\n",
    "        var_right = self.layer_right_dropout(var_right)\n",
    "        # Concatenate branches\n",
    "        var_t = torch.concat([var_left, var_right], dim=-1)\n",
    "        var_output = self.layer_output(var_t)\n",
    "        return var_output\n",
    "\n",
    "def run_that(data_train_x, data_train_y, data_test_x, data_test_y, var_repeat=10, init_model=None):\n",
    "    \"\"\"\n",
    "    Run WiFi-based model THAT.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data_train_x = data_train_x.reshape(data_train_x.shape[0], data_train_x.shape[1], -1)\n",
    "    data_test_x = data_test_x.reshape(data_test_x.shape[0], data_test_x.shape[1], -1)\n",
    "    var_x_shape, var_y_shape = data_train_x[0].shape, data_train_y[0].reshape(-1).shape\n",
    "    data_train_set = TensorDataset(torch.from_numpy(data_train_x), torch.from_numpy(data_train_y))\n",
    "    data_test_set = TensorDataset(torch.from_numpy(data_test_x), torch.from_numpy(data_test_y))\n",
    "    \n",
    "    result = {}\n",
    "    result_accuracy = []\n",
    "    result_time_train = []\n",
    "    result_time_test = []\n",
    "    \n",
    "    # var_macs, var_params = get_model_complexity_info(THAT(var_x_shape, var_y_shape), var_x_shape, as_strings=False)\n",
    "    # print(\"Parameters:\", var_params, \"- FLOPs:\", var_macs * 2)\n",
    "    \n",
    "    for var_r in range(var_repeat):\n",
    "        print(\"Repeat\", var_r)\n",
    "        torch.random.manual_seed(var_r + 39)\n",
    "        if init_model is not None:\n",
    "            model_that = init_model\n",
    "            lr2 = preset[\"nn\"][\"lr\"] /10\n",
    "        else:\n",
    "            model_that = THAT(var_x_shape, var_y_shape).to(device)\n",
    "            lr2 = preset[\"nn\"][\"lr\"]\n",
    "\n",
    "        optimizer = torch.optim.Adam(model_that.parameters(), lr=lr2, weight_decay=0)\n",
    "        loss = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([4] * var_y_shape[-1]).to(device))\n",
    "        var_time_0 = time.time()\n",
    "        \n",
    "        # Train\n",
    "        var_best_weight = train(model=model_that, optimizer=optimizer, loss=loss, \n",
    "                                  data_train_set=data_train_set, data_test_set=data_test_set,\n",
    "                                  var_threshold=preset[\"nn\"][\"threshold\"],\n",
    "                                  var_batch_size=preset[\"nn\"][\"batch_size\"],\n",
    "                                  var_epochs=preset[\"nn\"][\"epoch\"],\n",
    "                                  device=device)\n",
    "        var_time_1 = time.time()\n",
    "        \n",
    "        # Test\n",
    "        model_that.load_state_dict(var_best_weight)\n",
    "        with torch.no_grad():\n",
    "            predict_test_y = model_that(torch.from_numpy(data_test_x).to(device))\n",
    "        predict_test_y = (torch.sigmoid(predict_test_y) > preset[\"nn\"][\"threshold\"]).float()\n",
    "        predict_test_y = predict_test_y.detach().cpu().numpy()\n",
    "        var_time_2 = time.time()\n",
    "        \n",
    "        # Evaluate\n",
    "        data_test_y_c = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "        predict_test_y_c = predict_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "        result_acc = accuracy_score(data_test_y_c.astype(int), predict_test_y_c.astype(int))\n",
    "        result_dict = classification_report(data_test_y_c, predict_test_y_c, digits=6, zero_division=0, output_dict=True)\n",
    "        result[\"repeat_\" + str(var_r)] = result_dict\n",
    "        result_accuracy.append(result_acc)\n",
    "        result_time_train.append(var_time_1 - var_time_0)\n",
    "        result_time_test.append(var_time_2 - var_time_1)\n",
    "        print(\"repeat_\" + str(var_r), result_accuracy)\n",
    "        print(result)\n",
    "    \n",
    "    result[\"accuracy\"] = {\"avg\": np.mean(result_accuracy), \"std\": np.std(result_accuracy)}\n",
    "    result[\"time_train\"] = {\"avg\": np.mean(result_time_train), \"std\": np.std(result_time_train)}\n",
    "    result[\"time_test\"] = {\"avg\": np.mean(result_time_test), \"std\": np.std(result_time_test)}\n",
    "    # result[\"complexity\"] = {\"parameter\": var_params, \"flops\": var_macs * 2}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076dd5d7",
   "metadata": {
    "papermill": {
     "duration": 0.006647,
     "end_time": "2025-12-25T15:19:18.962744",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.956097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15a1f890",
   "metadata": {
    "papermill": {
     "duration": 0.006803,
     "end_time": "2025-12-25T15:19:18.976148",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.969345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell7: for RESNET18 Model\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e828b67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:19:18.990556Z",
     "iopub.status.busy": "2025-12-25T15:19:18.990285Z",
     "iopub.status.idle": "2025-12-25T15:19:18.997054Z",
     "shell.execute_reply": "2025-12-25T15:19:18.996433Z"
    },
    "papermill": {
     "duration": 0.015558,
     "end_time": "2025-12-25T15:19:18.998196",
     "exception": false,
     "start_time": "2025-12-25T15:19:18.982638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# import torch._dynamo\n",
    "# torch._dynamo.config.suppress_errors = True\n",
    "# import time\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "# import numpy as np\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# import torchvision.models as models\n",
    "# from copy import deepcopy\n",
    "\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "# torch._dynamo.config.cache_size_limit = 65536\n",
    "\n",
    "# # فرض می‌کنیم preset قبلاً تعریف شده باشه\n",
    "# # preset = { \"nn\": {\"lr\": 1e-3, \"epoch\": 10, \"batch_size\": 4, \"threshold\": 0.5}, ... }\n",
    "\n",
    "# class ResNet18Model(torch.nn.Module):\n",
    "#     def __init__(self, var_x_shape, var_y_shape):\n",
    "#         super(ResNet18Model, self).__init__()\n",
    "#         model_resnet = models.resnet18(weights=None)\n",
    "#         model_resnet.conv1 = torch.nn.Conv2d(1, 64, 7, 3, 2, bias=False)\n",
    "#         in_features_fc = model_resnet.fc.in_features  # معمولاً 512\n",
    "#         out_features_fc = var_y_shape[-1]\n",
    "#         model_resnet.fc = torch.nn.Linear(in_features_fc, out_features_fc)\n",
    "#         self.resnet = model_resnet\n",
    "\n",
    "#     def forward(self, var_input):\n",
    "#         var_input = var_input.reshape(var_input.size(0), 1, 3000, 270)\n",
    "#         return self.resnet(var_input)\n",
    "\n",
    "# def run_resnet(data_train_x, data_train_y, data_test_x, data_test_y, var_repeat=10, init_model=None):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     var_x_shape = data_train_x[0].shape\n",
    "#     var_y_shape = data_train_y[0].reshape(-1).shape\n",
    "\n",
    "#     # تغییر شکل داده‌ها روی CPU\n",
    "#     data_train_x = data_train_x.reshape(data_train_x.shape[0], 1, data_train_x.shape[1],\n",
    "#                                         data_train_x.shape[2]*data_train_x.shape[3]*data_train_x.shape[4])\n",
    "#     data_test_x  = data_test_x.reshape(data_test_x.shape[0], 1, data_test_x.shape[1],\n",
    "#                                        data_test_x.shape[2]*data_test_x.shape[3]*data_test_x.shape[4])\n",
    "    \n",
    "#     # دیتاست‌ها روی CPU\n",
    "#     data_train_set = TensorDataset(torch.from_numpy(data_train_x).float(),\n",
    "#                                    torch.from_numpy(data_train_y).float())\n",
    "#     data_test_set  = TensorDataset(torch.from_numpy(data_test_x).float(),\n",
    "#                                    torch.from_numpy(data_test_y).float())\n",
    "    \n",
    "#     result = {}\n",
    "#     result_accuracy = []\n",
    "#     result_time_train = []\n",
    "#     result_time_test = []\n",
    "    \n",
    "#     for var_r in range(var_repeat):\n",
    "#         print(\"Repeat\", var_r)\n",
    "#         torch.random.manual_seed(var_r + 39)\n",
    "        \n",
    "#         # ساخت مدل و انتقال به GPU\n",
    "#         if init_model is not None:\n",
    "#             model_resnet = init_model\n",
    "#             lr2 = preset[\"nn\"][\"lr\"] /10\n",
    "            \n",
    "#         else:\n",
    "#             model_resnet = ResNet18Model(var_x_shape, var_y_shape).to(device)\n",
    "#             lr2 = preset[\"nn\"][\"lr\"]\n",
    "\n",
    "#         optimizer = torch.optim.Adam(model_resnet.parameters(), lr=lr2, weight_decay=0)\n",
    "#         loss_func = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([6] * var_y_shape[-1]).to(device))\n",
    "        \n",
    "#         # تابع آموزش داخلی؛ دیتا روی CPU باقی می‌مونه و فقط هنگام محاسبه batch به GPU میره\n",
    "#         def train_inner():\n",
    "#             train_loader = DataLoader(data_train_set, preset[\"nn\"][\"batch_size\"], shuffle=True, pin_memory=False)\n",
    "#             test_loader = DataLoader(data_test_set, preset[\"nn\"][\"batch_size\"], shuffle=False, pin_memory=False)\n",
    "#             best_accuracy = 0\n",
    "#             best_weight = None\n",
    "            \n",
    "#             for epoch in range(preset[\"nn\"][\"epoch\"]):\n",
    "#                 t0 = time.time()\n",
    "#                 model_resnet.train()\n",
    "#                 # متغیرهای مربوط به آخرین batch آموزش\n",
    "#                 last_train_loss = None\n",
    "#                 last_train_acc = None\n",
    "#                 for batch in train_loader:\n",
    "#                     batch_x, batch_y = batch\n",
    "#                     batch_x = batch_x.to(device)\n",
    "#                     batch_y = batch_y.to(device)\n",
    "#                     outputs = model_resnet(batch_x)\n",
    "#                     loss_val = loss_func(outputs, batch_y.reshape(batch_y.shape[0], -1).float())\n",
    "#                     optimizer.zero_grad()\n",
    "#                     loss_val.backward()\n",
    "#                     optimizer.step()\n",
    "#                     last_train_loss = loss_val.item()\n",
    "#                     # محاسبه دقت آخرین batch آموزش\n",
    "#                     train_preds = (torch.sigmoid(outputs) > preset[\"nn\"][\"threshold\"]).float()\n",
    "#                     last_train_acc = accuracy_score(batch_y.reshape(batch_y.shape[0], -1).detach().cpu().numpy().astype(int),\n",
    "#                                                     train_preds.detach().cpu().numpy().astype(int))\n",
    "                \n",
    "#                 # ارزیابی روی دیتاست تست به صورت batch به batch\n",
    "#                 model_resnet.eval()\n",
    "#                 all_preds = []\n",
    "#                 all_labels = []\n",
    "#                 test_loss_val = None\n",
    "#                 with torch.no_grad():\n",
    "#                     for t_batch in test_loader:\n",
    "#                         t_x, t_y = t_batch\n",
    "#                         t_x = t_x.to(device)\n",
    "#                         outputs_test = model_resnet(t_x)\n",
    "#                         outputs_test = (torch.sigmoid(outputs_test) > preset[\"nn\"][\"threshold\"]).float()\n",
    "#                         all_preds.append(outputs_test.detach().cpu().numpy())\n",
    "#                         all_labels.append(t_y.cpu().numpy())  # اینجا تغییر دادیم\n",
    "#                 preds_cat = np.vstack(all_preds)\n",
    "#                 labels_cat = np.vstack(all_labels)\n",
    "#                 print(\"preds_cat\",preds_cat.shape)\n",
    "#                 # تبدیل به شکل (n, 6, 5)\n",
    "                \n",
    "#                 # preds_cat = preds_cat.reshape(-1, 6, 5)\n",
    "#                 # labels_cat = labels_cat.reshape(-1, 6, 5)\n",
    "\n",
    "#                 preds_cat = preds_cat.reshape(-1, 6)\n",
    "#                 labels_cat = labels_cat.reshape(-1, 6)\n",
    "                \n",
    "#                 # برای محاسبه دقت، مسطح می‌کنیم\n",
    "#                 test_acc = accuracy_score(labels_cat.reshape(labels_cat.shape[0], -1).astype(int),\n",
    "#                                           preds_cat.reshape(preds_cat.shape[0], -1).astype(int))\n",
    "#                 epoch_time = time.time() - t0\n",
    "#                 print(f\"Epoch {epoch}/{preset['nn']['epoch']} - \"\n",
    "#                       f\"Train Loss: {(last_train_loss if last_train_loss is not None else 0.0):.6f}, \"\n",
    "#                       f\"Train Acc: {(last_train_acc if last_train_acc is not None else 0.0):.6f}, \"\n",
    "#                       f\"Test Loss: {(test_loss_val if test_loss_val is not None else 0.0):.6f}, \"\n",
    "#                       f\"Test Acc: {(test_acc if test_acc is not None else 0.0):.6f} - \"\n",
    "#                       f\"Time: {epoch_time:.4f}s\")\n",
    "\n",
    "#                 if test_acc > best_accuracy:\n",
    "#                     best_accuracy = test_acc\n",
    "#                     print('-----***-----')\n",
    "#                     print(best_accuracy)\n",
    "#                     best_weight = deepcopy(model_resnet.state_dict())\n",
    "#             return best_weight\n",
    "        \n",
    "#         t0_run = time.time()\n",
    "#         best_weight = train_inner()\n",
    "#         t1_run = time.time()\n",
    "        \n",
    "#         torch.save(model_resnet.state_dict(), f\"{name_run}_model_final.pt\")\n",
    "#         model_resnet.load_state_dict(best_weight)\n",
    "#         torch.save(model_resnet.state_dict(), f\"{name_run}_best_model.pt\")\n",
    "\n",
    "#         # bad age niaz bod load koni\n",
    "#         # model_resnet = ResNet18Model(var_x_shape, var_y_shape).to(device)\n",
    "#         # model_resnet.load_state_dict(torch.load(\"resnet_model_repeat0.pt\"))\n",
    "#         # model_resnet.eval()\n",
    "\n",
    "        \n",
    "#         # ارزیابی نهایی مدل روی دیتاست تست (استفاده از batchهای کوچک)\n",
    "#         model_resnet.eval()\n",
    "#         all_preds = []\n",
    "#         test_loader_final = DataLoader(data_test_set, preset[\"nn\"][\"batch_size\"], shuffle=False, pin_memory=False)\n",
    "#         with torch.no_grad():\n",
    "#             for batch in test_loader_final:\n",
    "#                 batch_x, _ = batch\n",
    "#                 batch_x = batch_x.to(device)\n",
    "#                 all_preds.append(model_resnet(batch_x))\n",
    "#         preds_all = torch.cat(all_preds, dim=0)\n",
    "#         preds_final = (torch.sigmoid(preds_all) > preset[\"nn\"][\"threshold\"]).float().detach().cpu().numpy()\n",
    "#         t2_run = time.time()\n",
    "        \n",
    "#         data_test_y_np = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "#         preds_final = preds_final.reshape(-1, data_test_y.shape[-1])\n",
    "#         acc_final = accuracy_score(data_test_y_np.astype(int), preds_final.astype(int))\n",
    "#         result[f\"repeat_{var_r}\"] = {\"accuracy\": acc_final}\n",
    "#         result_accuracy.append(acc_final)\n",
    "#         result_time_train.append(t1_run - t0_run)\n",
    "#         result_time_test.append(t2_run - t1_run)\n",
    "#         print(\"Repeat\", var_r, \"Final Test Accuracy:\", acc_final)\n",
    "    \n",
    "#     result[\"accuracy\"] = {\"avg\": np.mean(result_accuracy), \"std\": np.std(result_accuracy)}\n",
    "#     result[\"time_train\"] = {\"avg\": np.mean(result_time_train), \"std\": np.std(result_time_train)}\n",
    "#     result[\"time_test\"] = {\"avg\": np.mean(result_time_test), \"std\": np.std(result_time_test)}\n",
    "#     return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160e5f6e",
   "metadata": {
    "papermill": {
     "duration": 0.006755,
     "end_time": "2025-12-25T15:19:19.012052",
     "exception": false,
     "start_time": "2025-12-25T15:19:19.005297",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 9: train.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "910fc0ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:19:19.026438Z",
     "iopub.status.busy": "2025-12-25T15:19:19.026210Z",
     "iopub.status.idle": "2025-12-25T15:19:19.551173Z",
     "shell.execute_reply": "2025-12-25T15:19:19.550633Z"
    },
    "papermill": {
     "duration": 0.534021,
     "end_time": "2025-12-25T15:19:19.552568",
     "exception": false,
     "start_time": "2025-12-25T15:19:19.018547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[file]          train.py\n",
    "[description]   function to train WiFi-based models\n",
    "\"\"\"\n",
    "\n",
    "# All necessary libraries are imported in Cell 1.\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch._dynamo.config.cache_size_limit = 65536\n",
    "\n",
    "def train(model, optimizer, loss, data_train_set, data_test_set, var_threshold, var_batch_size, var_epochs, device):\n",
    "    \"\"\"\n",
    "    Generic training function for WiFi-based models.\n",
    "    \"\"\"\n",
    "    # دیتا رو روی CPU نگه می‌داریم (pin_memory=False)\n",
    "    data_train_loader = DataLoader(data_train_set, var_batch_size, shuffle=True, pin_memory=False)\n",
    "    data_test_loader = DataLoader(data_test_set, batch_size=len(data_test_set), shuffle=False, pin_memory=False)\n",
    "    \n",
    "    var_best_accuracy = -1.0\n",
    "    var_best_weight   = deepcopy(model.state_dict())\n",
    "    \n",
    "    \n",
    "    for var_epoch in range(var_epochs):\n",
    "        var_time_e0 = time.time()\n",
    "        model.train()\n",
    "        for data_batch in data_train_loader:\n",
    "            data_batch_x, data_batch_y = data_batch\n",
    "            # انتقال موقتی داده به GPU فقط برای forward pass\n",
    "            data_batch_x = data_batch_x.to(device)\n",
    "            data_batch_y = data_batch_y.to(device)\n",
    "            predict_train_y = model(data_batch_x)\n",
    "            var_loss_train = loss(predict_train_y, data_batch_y.reshape(data_batch_y.shape[0], -1).float())\n",
    "            optimizer.zero_grad()\n",
    "            var_loss_train.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # محاسبه دقت روی آخرین batch و انتقال نتایج به CPU\n",
    "        predict_train_y = (torch.sigmoid(predict_train_y) > var_threshold).float()\n",
    "        data_batch_y = data_batch_y.detach().cpu().numpy()\n",
    "        predict_train_y = predict_train_y.detach().cpu().numpy()\n",
    "        \n",
    "        predict_train_y = predict_train_y.reshape(-1, data_batch_y.shape[-1])\n",
    "        data_batch_y = data_batch_y.reshape(-1, data_batch_y.shape[-1])\n",
    "        var_accuracy_train = accuracy_score(data_batch_y.astype(int), predict_train_y.astype(int))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            data_test_x, data_test_y = next(iter(data_test_loader))\n",
    "            # انتقال موقتی دیتا تست به GPU برای محاسبات\n",
    "            data_test_x = data_test_x.to(device)\n",
    "            data_test_y = data_test_y.to(device)\n",
    "            \n",
    "            predict_test_y = model(data_test_x)\n",
    "            var_loss_test = loss(predict_test_y, data_test_y.reshape(data_test_y.shape[0], -1).float())\n",
    "            \n",
    "            predict_test_y = (torch.sigmoid(predict_test_y) > var_threshold).float()\n",
    "            \n",
    "            # انتقال نتایج به CPU برای ارزیابی\n",
    "            data_test_y = data_test_y.detach().cpu().numpy()\n",
    "            predict_test_y = predict_test_y.detach().cpu().numpy()\n",
    "            \n",
    "            predict_test_y = predict_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            data_test_y = data_test_y.reshape(-1, data_test_y.shape[-1])\n",
    "            var_accuracy_test = accuracy_score(data_test_y.astype(int), predict_test_y.astype(int))\n",
    "        \n",
    "        print(f\"Epoch {var_epoch}/{var_epochs}\",\n",
    "              \"- %.6fs\"%(time.time() - var_time_e0),\n",
    "              \"- Loss %.6f\"%var_loss_train.cpu(),\n",
    "              \"- Accuracy %.6f\"%var_accuracy_train,\n",
    "              \"- Test Loss %.6f\"%var_loss_test.cpu(),\n",
    "              \"- Test Accuracy %.6f\"%var_accuracy_test)\n",
    "            \n",
    "        if var_accuracy_test > var_best_accuracy:\n",
    "            var_best_accuracy = var_accuracy_test\n",
    "            print('-----***-----')\n",
    "            print(var_best_accuracy)\n",
    "            var_best_weight = deepcopy(model.state_dict())\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{name_run}_model_final.pt\")\n",
    "    torch.save(var_best_weight, f\"{name_run}_best_model.pt\")\n",
    "\n",
    "    \n",
    "    return var_best_weight\n",
    "\n",
    "\n",
    "\n",
    "# === importsِ لازم را یک‌بار بالای فایل اضافه کن ===\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- تابع کمکی ----------\n",
    "def save_confusion_matrix(model, data_loader, threshold, device, pdf_path):\n",
    "    \"\"\"\n",
    "    Runs the model on `data_loader`, builds a confusion matrix and writes it to `pdf_path`.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "\n",
    "            preds = (torch.sigmoid(logits) > threshold).float().cpu().numpy().ravel()\n",
    "            yb    = yb.cpu().numpy().ravel()\n",
    "\n",
    "            y_true.extend(yb)\n",
    "            y_pred.extend(preds)\n",
    "\n",
    "    cm  = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    ConfusionMatrixDisplay(cm).plot(ax=ax)\n",
    "    ax.set_title(\"Confusion Matrix – Test\")\n",
    "\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "# ---------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bf032a",
   "metadata": {
    "papermill": {
     "duration": 0.007121,
     "end_time": "2025-12-25T15:19:19.567164",
     "exception": false,
     "start_time": "2025-12-25T15:19:19.560043",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 11: run.py\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28ef6dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T15:19:19.582740Z",
     "iopub.status.busy": "2025-12-25T15:19:19.582252Z",
     "iopub.status.idle": "2025-12-25T17:01:31.032133Z",
     "shell.execute_reply": "2025-12-25T17:01:31.031317Z"
    },
    "papermill": {
     "duration": 6131.459533,
     "end_time": "2025-12-25T17:01:31.033539",
     "exception": false,
     "start_time": "2025-12-25T15:19:19.574006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "\n",
      "[DEBUG] First MAT sample: act_1_1\n",
      "[DEBUG] shape=(2835, 3, 3, 30), dtype=complex128, complex=True\n",
      "[DEBUG] abs range:  min=1.414214, max=18.027756\n",
      "[DEBUG] phase stats: mean=0.906475, std=1.591276\n",
      "[DEBUG] ==> Input is COMPLEX (phase exists).\n",
      "Running model: THAT\n",
      "Repeat 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py:370: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1036.)\n",
      "  return F.conv1d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100 - 6.008393s - Loss 2.316807 - Accuracy 0.114583 - Test Loss 2.955871 - Test Accuracy 0.038462\n",
      "-----***-----\n",
      "0.038461538461538464\n",
      "Epoch 1/100 - 5.052586s - Loss 1.207754 - Accuracy 0.213542 - Test Loss 1.085181 - Test Accuracy 0.076481\n",
      "-----***-----\n",
      "0.07648099027409372\n",
      "Epoch 2/100 - 5.052267s - Loss 1.203790 - Accuracy 0.276042 - Test Loss 2.433407 - Test Accuracy 0.020778\n",
      "Epoch 3/100 - 5.065821s - Loss 0.921242 - Accuracy 0.229167 - Test Loss 0.991340 - Test Accuracy 0.017683\n",
      "Epoch 4/100 - 5.051083s - Loss 0.784485 - Accuracy 0.291667 - Test Loss 0.851651 - Test Accuracy 0.046861\n",
      "Epoch 5/100 - 5.052563s - Loss 0.840660 - Accuracy 0.296875 - Test Loss 0.857752 - Test Accuracy 0.007958\n",
      "Epoch 6/100 - 5.049228s - Loss 0.697968 - Accuracy 0.276042 - Test Loss 0.840397 - Test Accuracy 0.063218\n",
      "Epoch 7/100 - 5.048348s - Loss 0.765143 - Accuracy 0.291667 - Test Loss 1.235812 - Test Accuracy 0.002210\n",
      "Epoch 8/100 - 5.044760s - Loss 0.805143 - Accuracy 0.328125 - Test Loss 0.802595 - Test Accuracy 0.046419\n",
      "Epoch 9/100 - 5.050987s - Loss 0.612733 - Accuracy 0.364583 - Test Loss 1.735317 - Test Accuracy 0.003095\n",
      "Epoch 10/100 - 5.045579s - Loss 0.624834 - Accuracy 0.380208 - Test Loss 1.338806 - Test Accuracy 0.000442\n",
      "Epoch 11/100 - 5.056728s - Loss 0.643479 - Accuracy 0.395833 - Test Loss 1.506579 - Test Accuracy 0.020778\n",
      "Epoch 12/100 - 5.029617s - Loss 0.705714 - Accuracy 0.312500 - Test Loss 19.014080 - Test Accuracy 0.000000\n",
      "Epoch 13/100 - 5.028070s - Loss 0.507809 - Accuracy 0.453125 - Test Loss 10.796986 - Test Accuracy 0.000000\n",
      "Epoch 14/100 - 5.043696s - Loss 0.584912 - Accuracy 0.416667 - Test Loss 6.932493 - Test Accuracy 0.000000\n",
      "Epoch 15/100 - 5.054542s - Loss 0.641871 - Accuracy 0.432292 - Test Loss 5.573437 - Test Accuracy 0.000000\n",
      "Epoch 16/100 - 5.042500s - Loss 0.538861 - Accuracy 0.447917 - Test Loss 8.200393 - Test Accuracy 0.000000\n",
      "Epoch 17/100 - 5.028369s - Loss 0.591983 - Accuracy 0.432292 - Test Loss 0.687822 - Test Accuracy 0.162688\n",
      "-----***-----\n",
      "0.16268788682581786\n",
      "Epoch 18/100 - 5.051073s - Loss 0.607311 - Accuracy 0.369792 - Test Loss 1.269088 - Test Accuracy 0.015915\n",
      "Epoch 19/100 - 5.026931s - Loss 0.492049 - Accuracy 0.463542 - Test Loss 1.830376 - Test Accuracy 0.011052\n",
      "Epoch 20/100 - 5.044192s - Loss 0.570450 - Accuracy 0.354167 - Test Loss 0.733599 - Test Accuracy 0.087975\n",
      "Epoch 21/100 - 5.024743s - Loss 0.422685 - Accuracy 0.510417 - Test Loss 1.963720 - Test Accuracy 0.009726\n",
      "Epoch 22/100 - 5.052867s - Loss 0.514325 - Accuracy 0.432292 - Test Loss 0.659003 - Test Accuracy 0.301945\n",
      "-----***-----\n",
      "0.3019451812555261\n",
      "Epoch 23/100 - 5.041731s - Loss 0.487008 - Accuracy 0.401042 - Test Loss 1.541904 - Test Accuracy 0.088859\n",
      "Epoch 24/100 - 5.258438s - Loss 0.443594 - Accuracy 0.484375 - Test Loss 0.802808 - Test Accuracy 0.083112\n",
      "Epoch 25/100 - 5.044013s - Loss 0.501111 - Accuracy 0.463542 - Test Loss 0.814969 - Test Accuracy 0.128205\n",
      "Epoch 26/100 - 5.057342s - Loss 0.475764 - Accuracy 0.458333 - Test Loss 0.908614 - Test Accuracy 0.103006\n",
      "Epoch 27/100 - 5.034189s - Loss 0.444859 - Accuracy 0.416667 - Test Loss 0.685249 - Test Accuracy 0.133510\n",
      "Epoch 28/100 - 5.065870s - Loss 0.460946 - Accuracy 0.453125 - Test Loss 0.866184 - Test Accuracy 0.083112\n",
      "Epoch 29/100 - 5.023757s - Loss 0.456276 - Accuracy 0.437500 - Test Loss 0.716075 - Test Accuracy 0.129531\n",
      "Epoch 30/100 - 5.035898s - Loss 0.490292 - Accuracy 0.421875 - Test Loss 1.128453 - Test Accuracy 0.024315\n",
      "Epoch 31/100 - 5.026590s - Loss 0.412643 - Accuracy 0.473958 - Test Loss 0.735618 - Test Accuracy 0.172856\n",
      "Epoch 32/100 - 5.028108s - Loss 0.414421 - Accuracy 0.500000 - Test Loss 0.688677 - Test Accuracy 0.109637\n",
      "Epoch 33/100 - 5.028372s - Loss 0.431445 - Accuracy 0.468750 - Test Loss 0.708106 - Test Accuracy 0.217949\n",
      "Epoch 34/100 - 5.037320s - Loss 0.520780 - Accuracy 0.380208 - Test Loss 0.700571 - Test Accuracy 0.134836\n",
      "Epoch 35/100 - 5.034837s - Loss 0.407476 - Accuracy 0.520833 - Test Loss 0.609922 - Test Accuracy 0.200707\n",
      "Epoch 36/100 - 5.041306s - Loss 0.377183 - Accuracy 0.463542 - Test Loss 0.680155 - Test Accuracy 0.146773\n",
      "Epoch 37/100 - 5.030889s - Loss 0.387597 - Accuracy 0.531250 - Test Loss 0.646741 - Test Accuracy 0.155172\n",
      "Epoch 38/100 - 5.033611s - Loss 0.474285 - Accuracy 0.437500 - Test Loss 0.628967 - Test Accuracy 0.163130\n",
      "Epoch 39/100 - 5.051113s - Loss 0.344141 - Accuracy 0.572917 - Test Loss 0.890169 - Test Accuracy 0.249337\n",
      "Epoch 40/100 - 5.025166s - Loss 0.397342 - Accuracy 0.401042 - Test Loss 0.719027 - Test Accuracy 0.220159\n",
      "Epoch 41/100 - 5.030496s - Loss 0.363059 - Accuracy 0.515625 - Test Loss 0.682143 - Test Accuracy 0.236516\n",
      "Epoch 42/100 - 5.285762s - Loss 0.403548 - Accuracy 0.484375 - Test Loss 0.744919 - Test Accuracy 0.189655\n",
      "Epoch 43/100 - 5.043960s - Loss 0.332149 - Accuracy 0.500000 - Test Loss 0.657198 - Test Accuracy 0.140584\n",
      "Epoch 44/100 - 5.048731s - Loss 0.357675 - Accuracy 0.531250 - Test Loss 0.656143 - Test Accuracy 0.155172\n",
      "Epoch 45/100 - 5.049942s - Loss 0.369446 - Accuracy 0.510417 - Test Loss 0.734753 - Test Accuracy 0.236516\n",
      "Epoch 46/100 - 5.047827s - Loss 0.313326 - Accuracy 0.510417 - Test Loss 0.637339 - Test Accuracy 0.193634\n",
      "Epoch 47/100 - 5.069640s - Loss 0.395552 - Accuracy 0.447917 - Test Loss 0.716538 - Test Accuracy 0.177719\n",
      "Epoch 48/100 - 5.059158s - Loss 0.347091 - Accuracy 0.479167 - Test Loss 0.752880 - Test Accuracy 0.110080\n",
      "Epoch 49/100 - 5.046889s - Loss 0.308643 - Accuracy 0.546875 - Test Loss 0.652453 - Test Accuracy 0.171972\n",
      "Epoch 50/100 - 5.059349s - Loss 0.398221 - Accuracy 0.500000 - Test Loss 0.819028 - Test Accuracy 0.206897\n",
      "Epoch 51/100 - 5.032833s - Loss 0.285881 - Accuracy 0.505208 - Test Loss 0.677317 - Test Accuracy 0.171972\n",
      "Epoch 52/100 - 5.044931s - Loss 0.408587 - Accuracy 0.432292 - Test Loss 0.937854 - Test Accuracy 0.261273\n",
      "Epoch 53/100 - 5.043141s - Loss 0.379474 - Accuracy 0.468750 - Test Loss 0.728033 - Test Accuracy 0.241821\n",
      "Epoch 54/100 - 5.058832s - Loss 0.320928 - Accuracy 0.489583 - Test Loss 0.856935 - Test Accuracy 0.203360\n",
      "Epoch 55/100 - 5.039729s - Loss 0.328668 - Accuracy 0.515625 - Test Loss 0.695868 - Test Accuracy 0.121132\n",
      "Epoch 56/100 - 5.046686s - Loss 0.461144 - Accuracy 0.500000 - Test Loss 0.749751 - Test Accuracy 0.155615\n",
      "Epoch 57/100 - 5.039661s - Loss 0.332191 - Accuracy 0.583333 - Test Loss 0.821717 - Test Accuracy 0.164898\n",
      "Epoch 58/100 - 5.048405s - Loss 0.312987 - Accuracy 0.614583 - Test Loss 0.697113 - Test Accuracy 0.161362\n",
      "Epoch 59/100 - 5.028414s - Loss 0.317860 - Accuracy 0.552083 - Test Loss 0.668056 - Test Accuracy 0.202476\n",
      "Epoch 60/100 - 5.038912s - Loss 0.276609 - Accuracy 0.536458 - Test Loss 0.919780 - Test Accuracy 0.209549\n",
      "Epoch 61/100 - 5.027948s - Loss 0.315217 - Accuracy 0.562500 - Test Loss 0.793373 - Test Accuracy 0.251547\n",
      "Epoch 62/100 - 5.050213s - Loss 0.269959 - Accuracy 0.588542 - Test Loss 0.858058 - Test Accuracy 0.265252\n",
      "Epoch 63/100 - 5.030499s - Loss 0.283688 - Accuracy 0.494792 - Test Loss 0.723210 - Test Accuracy 0.170645\n",
      "Epoch 64/100 - 5.037902s - Loss 0.293240 - Accuracy 0.593750 - Test Loss 0.734287 - Test Accuracy 0.171088\n",
      "Epoch 65/100 - 5.030791s - Loss 0.251022 - Accuracy 0.562500 - Test Loss 0.888302 - Test Accuracy 0.274536\n",
      "Epoch 66/100 - 5.039968s - Loss 0.303560 - Accuracy 0.510417 - Test Loss 0.746932 - Test Accuracy 0.231653\n",
      "Epoch 67/100 - 5.034863s - Loss 0.338412 - Accuracy 0.515625 - Test Loss 0.843759 - Test Accuracy 0.284262\n",
      "Epoch 68/100 - 5.042679s - Loss 0.332633 - Accuracy 0.526042 - Test Loss 0.821132 - Test Accuracy 0.233422\n",
      "Epoch 69/100 - 5.030997s - Loss 0.274721 - Accuracy 0.494792 - Test Loss 0.779157 - Test Accuracy 0.194960\n",
      "Epoch 70/100 - 5.036618s - Loss 0.289521 - Accuracy 0.447917 - Test Loss 0.828897 - Test Accuracy 0.195402\n",
      "Epoch 71/100 - 5.026702s - Loss 0.296935 - Accuracy 0.494792 - Test Loss 1.006181 - Test Accuracy 0.228559\n",
      "Epoch 72/100 - 5.078403s - Loss 0.263702 - Accuracy 0.572917 - Test Loss 1.005646 - Test Accuracy 0.242706\n",
      "Epoch 73/100 - 5.035064s - Loss 0.275885 - Accuracy 0.588542 - Test Loss 0.882495 - Test Accuracy 0.269673\n",
      "Epoch 74/100 - 5.040300s - Loss 0.219509 - Accuracy 0.651042 - Test Loss 1.264093 - Test Accuracy 0.340407\n",
      "-----***-----\n",
      "0.34040671971706454\n",
      "Epoch 75/100 - 5.237848s - Loss 0.314770 - Accuracy 0.520833 - Test Loss 0.807993 - Test Accuracy 0.193634\n",
      "Epoch 76/100 - 5.054987s - Loss 0.286662 - Accuracy 0.578125 - Test Loss 0.947250 - Test Accuracy 0.246684\n",
      "Epoch 77/100 - 5.028254s - Loss 0.225838 - Accuracy 0.625000 - Test Loss 0.944077 - Test Accuracy 0.152078\n",
      "Epoch 78/100 - 5.051457s - Loss 0.293504 - Accuracy 0.557292 - Test Loss 0.785167 - Test Accuracy 0.281167\n",
      "Epoch 79/100 - 5.035151s - Loss 0.361445 - Accuracy 0.406250 - Test Loss 0.895896 - Test Accuracy 0.208223\n",
      "Epoch 80/100 - 5.041760s - Loss 0.260233 - Accuracy 0.562500 - Test Loss 0.739358 - Test Accuracy 0.187887\n",
      "Epoch 81/100 - 5.041413s - Loss 0.202672 - Accuracy 0.562500 - Test Loss 0.830827 - Test Accuracy 0.151194\n",
      "Epoch 82/100 - 5.044732s - Loss 0.286860 - Accuracy 0.557292 - Test Loss 0.998583 - Test Accuracy 0.296640\n",
      "Epoch 83/100 - 5.033378s - Loss 0.259728 - Accuracy 0.583333 - Test Loss 1.002613 - Test Accuracy 0.256410\n",
      "Epoch 84/100 - 5.061704s - Loss 0.242336 - Accuracy 0.552083 - Test Loss 1.074790 - Test Accuracy 0.252431\n",
      "Epoch 85/100 - 5.044176s - Loss 0.265346 - Accuracy 0.588542 - Test Loss 0.877267 - Test Accuracy 0.239169\n",
      "Epoch 86/100 - 5.046002s - Loss 0.241898 - Accuracy 0.552083 - Test Loss 0.911973 - Test Accuracy 0.187445\n",
      "Epoch 87/100 - 5.038673s - Loss 0.268306 - Accuracy 0.494792 - Test Loss 0.953617 - Test Accuracy 0.154288\n",
      "Epoch 88/100 - 5.037109s - Loss 0.274697 - Accuracy 0.541667 - Test Loss 0.971895 - Test Accuracy 0.196729\n",
      "Epoch 89/100 - 5.025434s - Loss 0.267048 - Accuracy 0.531250 - Test Loss 0.844632 - Test Accuracy 0.179487\n",
      "Epoch 90/100 - 5.037812s - Loss 0.195258 - Accuracy 0.656250 - Test Loss 1.084496 - Test Accuracy 0.201592\n",
      "Epoch 91/100 - 5.027174s - Loss 0.222607 - Accuracy 0.619792 - Test Loss 0.955650 - Test Accuracy 0.230327\n",
      "Epoch 92/100 - 5.040096s - Loss 0.205508 - Accuracy 0.614583 - Test Loss 0.932422 - Test Accuracy 0.246242\n",
      "Epoch 93/100 - 5.054696s - Loss 0.230881 - Accuracy 0.552083 - Test Loss 1.062468 - Test Accuracy 0.268347\n",
      "Epoch 94/100 - 5.042129s - Loss 0.255775 - Accuracy 0.515625 - Test Loss 0.897101 - Test Accuracy 0.184350\n",
      "Epoch 95/100 - 5.040760s - Loss 0.183039 - Accuracy 0.645833 - Test Loss 0.919987 - Test Accuracy 0.233422\n",
      "Epoch 96/100 - 5.037246s - Loss 0.209568 - Accuracy 0.593750 - Test Loss 0.894961 - Test Accuracy 0.183466\n",
      "Epoch 97/100 - 5.036738s - Loss 0.242855 - Accuracy 0.635417 - Test Loss 0.899737 - Test Accuracy 0.166667\n",
      "Epoch 98/100 - 5.043795s - Loss 0.196925 - Accuracy 0.687500 - Test Loss 1.127260 - Test Accuracy 0.297524\n",
      "Epoch 99/100 - 5.034559s - Loss 0.227448 - Accuracy 0.661458 - Test Loss 1.429902 - Test Accuracy 0.336870\n",
      "repeat_0 [0.34040671971706454]\n",
      "{'repeat_0': {'0': {'precision': 0.11267605633802817, 'recall': 0.23529411764705882, 'f1-score': 0.15238095238095237, 'support': 102}, '1': {'precision': 0.21238938053097345, 'recall': 0.3609022556390977, 'f1-score': 0.26740947075208915, 'support': 133}, '2': {'precision': 0.10909090909090909, 'recall': 0.15517241379310345, 'f1-score': 0.1281138790035587, 'support': 116}, '3': {'precision': 0.08333333333333333, 'recall': 0.10638297872340426, 'f1-score': 0.09345794392523364, 'support': 94}, '4': {'precision': 0.07317073170731707, 'recall': 0.11214953271028037, 'f1-score': 0.0885608856088561, 'support': 107}, '5': {'precision': 0.09259259259259259, 'recall': 0.2727272727272727, 'f1-score': 0.1382488479262673, 'support': 110}, '6': {'precision': 0.09019607843137255, 'recall': 0.23232323232323232, 'f1-score': 0.12994350282485875, 'support': 99}, '7': {'precision': 0.1036036036036036, 'recall': 0.20909090909090908, 'f1-score': 0.13855421686746988, 'support': 110}, '8': {'precision': 0.12280701754385964, 'recall': 0.1891891891891892, 'f1-score': 0.14893617021276595, 'support': 111}, 'micro avg': {'precision': 0.11236559139784946, 'recall': 0.21283095723014256, 'f1-score': 0.1470795214637579, 'support': 982}, 'macro avg': {'precision': 0.11109552257466551, 'recall': 0.20813687798261646, 'f1-score': 0.14284509661133907, 'support': 982}, 'weighted avg': {'precision': 0.11425709239441542, 'recall': 0.21283095723014256, 'f1-score': 0.1467161374816767, 'support': 982}, 'samples avg': {'precision': 0.06566460359563807, 'recall': 0.09239610963748895, 'f1-score': 0.07390215148835838, 'support': 982}}}\n",
      "{'repeat_0': {'0': {'precision': 0.11267605633802817, 'recall': 0.23529411764705882, 'f1-score': 0.15238095238095237, 'support': 102}, '1': {'precision': 0.21238938053097345, 'recall': 0.3609022556390977, 'f1-score': 0.26740947075208915, 'support': 133}, '2': {'precision': 0.10909090909090909, 'recall': 0.15517241379310345, 'f1-score': 0.1281138790035587, 'support': 116}, '3': {'precision': 0.08333333333333333, 'recall': 0.10638297872340426, 'f1-score': 0.09345794392523364, 'support': 94}, '4': {'precision': 0.07317073170731707, 'recall': 0.11214953271028037, 'f1-score': 0.0885608856088561, 'support': 107}, '5': {'precision': 0.09259259259259259, 'recall': 0.2727272727272727, 'f1-score': 0.1382488479262673, 'support': 110}, '6': {'precision': 0.09019607843137255, 'recall': 0.23232323232323232, 'f1-score': 0.12994350282485875, 'support': 99}, '7': {'precision': 0.1036036036036036, 'recall': 0.20909090909090908, 'f1-score': 0.13855421686746988, 'support': 110}, '8': {'precision': 0.12280701754385964, 'recall': 0.1891891891891892, 'f1-score': 0.14893617021276595, 'support': 111}, 'micro avg': {'precision': 0.11236559139784946, 'recall': 0.21283095723014256, 'f1-score': 0.1470795214637579, 'support': 982}, 'macro avg': {'precision': 0.11109552257466551, 'recall': 0.20813687798261646, 'f1-score': 0.14284509661133907, 'support': 982}, 'weighted avg': {'precision': 0.11425709239441542, 'recall': 0.21283095723014256, 'f1-score': 0.1467161374816767, 'support': 982}, 'samples avg': {'precision': 0.06566460359563807, 'recall': 0.09239610963748895, 'f1-score': 0.07390215148835838, 'support': 982}}, 'accuracy': {'avg': 0.34040671971706454, 'std': 0.0}, 'time_train': {'avg': 505.98475885391235, 'std': 0.0}, 'time_test': {'avg': 0.4763469696044922, 'std': 0.0}, 'model': 'THAT', 'task': 'activity', 'data': {'num_users': ['0', '1', '2', '3', '4', '5'], 'wifi_band': ['2.4'], 'environment': ['classroom'], 'length': 3000}, 'nn': {'lr': 0.001, 'epoch': 100, 'batch_size': 64, 'threshold': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()           \n",
    "torch.cuda.empty_cache()  \n",
    "torch.cuda.ipc_collect()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "[file]          run.py\n",
    "[description]   run WiFi-based models and optionally save a multiclass confusion matrix\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# from preset import preset, name_run\n",
    "# from load_data import load_data_x, load_data_y, encode_data_y\n",
    "# from lstm import run_lstm, LSTMM\n",
    "# from bilstm import run_bilstm, BiLSTMM\n",
    "# from that import run_that, THAT\n",
    "# from resnet import run_resnet, ResNet18Model\n",
    "# from strf import run_strf  # if you have the ST-RF implementation\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model\",   default=preset[\"model\"],  type=str)\n",
    "    parser.add_argument(\"--task\",    default=preset[\"task\"],   type=str)\n",
    "    parser.add_argument(\"--repeat\",  default=preset[\"repeat\"], type=int)\n",
    "    parser.add_argument(\"--save_cm\", action=\"store_true\",\n",
    "                        help=\"Save a multiclass confusion matrix of the best model to PDF\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def save_multiclass_confusion_matrix(model, data_loader, device, pdf_path, num_classes):\n",
    "    \"\"\"\n",
    "    Given a model that outputs one-hot logits for a multiclass task,\n",
    "    convert to predicted classes via argmax, then plot and save a\n",
    "    num_classes × num_classes confusion matrix to pdf_path.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            # predicted class is index of max logit\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            trues = torch.argmax(yb, dim=1).cpu().numpy()\n",
    "            y_pred.extend(preds.tolist())\n",
    "            y_true.extend(trues.tolist())\n",
    "\n",
    "    labels = list(range(num_classes))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    disp.plot(ax=ax, xticks_rotation=\"vertical\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        pdf.savefig(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "def run():\n",
    "    args       = parse_args()\n",
    "    var_model  = args.model\n",
    "    var_task   = args.task\n",
    "    var_repeat = args.repeat\n",
    "\n",
    "    # --- Load and encode the data ---\n",
    "    data_pd_y = load_data_y(\n",
    "        preset[\"path\"][\"data_y\"],\n",
    "        var_environment=preset[\"data\"][\"environment\"],\n",
    "        var_wifi_band=preset[\"data\"][\"wifi_band\"],\n",
    "        var_num_users=preset[\"data\"][\"num_users\"]\n",
    "    )\n",
    "    labels = data_pd_y[\"label\"].tolist()\n",
    "    data_x = load_data_x(preset[\"path\"][\"data_x\"], labels)\n",
    "    data_y = encode_data_y(data_pd_y, var_task)\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(\n",
    "        data_x, data_y, test_size=0.2, shuffle=True, random_state=39\n",
    "    )\n",
    "\n",
    "    # --- Select which model runner to use ---\n",
    "    if var_model == \"ST-RF\":\n",
    "        from strf import run_strf\n",
    "        run_model = run_strf\n",
    "    elif var_model == \"LSTM\":\n",
    "        run_model = run_lstm\n",
    "    elif var_model == \"bi-LSTM\":\n",
    "        run_model = run_bilstm\n",
    "    elif var_model == \"THAT\":\n",
    "        run_model = run_that\n",
    "    elif var_model == \"ResNet18\":\n",
    "        run_model = run_resnet\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {var_model}\")\n",
    "\n",
    "    # --- Train and evaluate ---\n",
    "    print(f\"Running model: {var_model}\")\n",
    "    result = run_model(train_x, train_y, test_x, test_y, var_repeat)\n",
    "    result[\"model\"] = var_model\n",
    "    result[\"task\"]  = var_task\n",
    "    result[\"data\"]  = preset[\"data\"]\n",
    "    result[\"nn\"]    = preset[\"nn\"]\n",
    "    print(result)\n",
    "\n",
    "    # --- Save results to JSON ---\n",
    "    # with open(preset[\"path\"][\"save\"], \"w\") as f:\n",
    "    #     json.dump(result, f, indent=4)\n",
    "\n",
    "    # # --- Optionally save a multiclass confusion matrix ---\n",
    "    # # if args.save_cm:\n",
    "    # if Confusion_matrix == 1:\n",
    "    #     # 1) completely release GPU memory used for training\n",
    "    #     del run_model                      # if 'model' from training is still in scope\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     torch.cuda.ipc_collect()\n",
    "    \n",
    "    #     # 2) reshape input only if the network is sequence‑based\n",
    "    #     if var_model in (\"LSTM\", \"bi-LSTM\", \"THAT\"):\n",
    "    #         test_x_cm = test_x.reshape(test_x.shape[0], test_x.shape[1], -1)\n",
    "    #     else:                           # ResNet18, ST‑RF\n",
    "    #         test_x_cm = test_x\n",
    "    \n",
    "    #     # 3) build the *same* architecture on CPU and load its weights\n",
    "    #     device_cm = torch.device(\"cpu\")\n",
    "    #     if var_model == \"LSTM\":\n",
    "    #         model_cm = LSTMM(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     elif var_model == \"bi-LSTM\":\n",
    "    #         model_cm = BiLSTMM(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     elif var_model == \"THAT\":\n",
    "    #         model_cm = THAT(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     elif var_model == \"ResNet18\":\n",
    "    #         model_cm = ResNet18Model(test_x_cm[0].shape, test_y[0].shape).to(device_cm)\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Confusion matrix not supported for {var_model}\")\n",
    "    \n",
    "    #     # best_path = f\"/kaggle/working/{name_run}_best_model.pt\"\n",
    "    #     # model_cm.load_state_dict(torch.load(best_path, map_location=device_cm))\n",
    "    #     # model_cm.eval()\n",
    "    \n",
    "    #     # 4) DataLoader on CPU with a safe batch size\n",
    "    #     test_ds = TensorDataset(torch.from_numpy(test_x_cm).float(),\n",
    "    #                             torch.from_numpy(test_y).float())\n",
    "    #     test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "    \n",
    "    #     # 5) save the confusion matrix PDF\n",
    "    #     num_classes = test_y.shape[1]\n",
    "    #     pdf_name = f\"{name_run}_confusion_matrix.pdf\"\n",
    "    #     save_multiclass_confusion_matrix(model_cm,test_loader,device_cm,pdf_name,num_classes)\n",
    "    #     print(f\"✅ Saved confusion matrix (classes 0–{num_classes-1}) to {pdf_name}\")\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"start\")\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595dccf8",
   "metadata": {
    "papermill": {
     "duration": 0.012597,
     "end_time": "2025-12-25T17:01:31.061147",
     "exception": false,
     "start_time": "2025-12-25T17:01:31.048550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "Cell 12: Few-shot Learning\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d78733ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T17:01:31.099349Z",
     "iopub.status.busy": "2025-12-25T17:01:31.098946Z",
     "iopub.status.idle": "2025-12-25T17:01:31.107821Z",
     "shell.execute_reply": "2025-12-25T17:01:31.106980Z"
    },
    "papermill": {
     "duration": 0.032088,
     "end_time": "2025-12-25T17:01:31.110140",
     "exception": false,
     "start_time": "2025-12-25T17:01:31.078052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import torch\n",
    "# import shutil\n",
    "# import json\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# gc.collect()           \n",
    "# torch.cuda.empty_cache()  \n",
    "# torch.cuda.ipc_collect()\n",
    "\n",
    "# # ---------- helper: save multiclass confusion matrix ------------------\n",
    "# def save_multiclass_confusion_matrix(model, data_loader, pdf_path, num_classes):\n",
    "#     \"\"\"\n",
    "#     Forward‑pass on CPU, collect predictions, and write an N×N confusion matrix\n",
    "#     to a single‑page PDF (pdf_path).\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     y_true, y_pred = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in data_loader:\n",
    "#             logits = model(xb.cpu())                       # ensure CPU\n",
    "#             preds  = torch.argmax(logits, dim=1).numpy()\n",
    "#             trues  = torch.argmax(yb, dim=1).numpy()\n",
    "#             y_pred.extend(preds.tolist())\n",
    "#             y_true.extend(trues.tolist())\n",
    "\n",
    "#     labels = list(range(num_classes))\n",
    "#     cm  = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "#     disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "#     fig, ax = plt.subplots(figsize=(8, 8))\n",
    "#     disp.plot(ax=ax, xticks_rotation=\"vertical\")\n",
    "#     ax.set_title(\"Few‑shot Confusion Matrix\")\n",
    "#     with PdfPages(pdf_path) as pdf:\n",
    "#         pdf.savefig(fig)\n",
    "#     plt.close(fig)\n",
    "\n",
    "# # -------------------- pick run_* function ------------------------------\n",
    "# if preset[\"model\"] == \"ST-RF\":\n",
    "#     run_model = run_strf\n",
    "# elif preset[\"model\"] == \"LSTM\":\n",
    "#     run_model = run_lstm\n",
    "# elif preset[\"model\"] == \"bi-LSTM\":\n",
    "#     run_model = run_bilstm\n",
    "# elif preset[\"model\"] == \"THAT\":\n",
    "#     run_model = run_that\n",
    "# elif preset[\"model\"] == \"ResNet18\":\n",
    "#     run_model = run_resnet\n",
    "# else:\n",
    "#     raise ValueError(f\"No few‑shot implementation for {preset['model']}.\")\n",
    "\n",
    "# # ------------------------ load / split data ----------------------------\n",
    "# data_pd_y = load_data_y(preset[\"path\"][\"data_y\"],\n",
    "#                         var_environment=[dest_env],\n",
    "#                         var_wifi_band=preset[\"data\"][\"wifi_band\"],\n",
    "#                         var_num_users=preset[\"data\"][\"num_users\"])\n",
    "\n",
    "# labels_list = data_pd_y[\"label\"].tolist()\n",
    "# data_x = load_data_x(preset[\"path\"][\"data_x\"], labels_list)\n",
    "# data_y = encode_data_y(data_pd_y, preset[\"task\"])\n",
    "\n",
    "# train_x, test_x, train_y, test_y = train_test_split(\n",
    "#     data_x, data_y, test_size=0.2, shuffle=True, random_state=39)\n",
    "\n",
    "# # Few-shot sample size\n",
    "# train_x = train_x[:few_shot_num_samples]\n",
    "# train_y = train_y[:few_shot_num_samples]\n",
    "\n",
    "# # ----------------------- few‑shot training -----------------------------\n",
    "# original_epochs = preset[\"nn\"][\"epoch\"]\n",
    "# preset[\"nn\"][\"epoch\"] = few_shot_epochs\n",
    "\n",
    "# # Load the best model weights\n",
    "# best_model_path = f\"{name_run}_best_model.pt\"\n",
    "\n",
    "# # Initialize the model \n",
    "# if preset[\"model\"] == \"LSTM\":\n",
    "#     model = LSTMM(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "#     # print('train_y_[0].shape:', train_y[0].shape)\n",
    "#     # print('train_x_[0].shape:', train_x[0].reshape(train_x[0].shape[0], -1).shape)\n",
    "# elif preset[\"model\"] == \"bi-LSTM\":\n",
    "#     model = BiLSTMM(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "# elif preset[\"model\"] == \"THAT\":\n",
    "#     model = THAT(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "# elif preset[\"model\"] == \"ResNet18\":\n",
    "#     model = ResNet18Model(train_x[0].reshape(train_x[0].shape[0], -1).shape, train_y[0].shape)  # Replace with your model initialization\n",
    "# else:\n",
    "#     raise ValueError(f\"Model {preset['model']} not supported!\")\n",
    "\n",
    "# # Load the weights into the model\n",
    "# model.load_state_dict(torch.load(best_model_path, map_location=\"cpu\"))\n",
    "# model = model.to('cuda')\n",
    "\n",
    "# # Fine-tune the model on few-shot data (note: `run_model` should now return only the result)\n",
    "# result = run_model(train_x, train_y, test_x, test_y, var_repeat=1, init_model=model)\n",
    "# print(result)\n",
    "\n",
    "# # --------------------- save few‑shot checkpoints -----------------------\n",
    "# # After fine-tuning, save the model\n",
    "# torch.save(model.state_dict(), f\"{name_run}_fewshot_final_model.pt\")\n",
    "# torch.save(model.state_dict(), f\"{name_run}_fewshot_best_model.pt\")\n",
    "\n",
    "# # ------------------- confusion matrix on CPU ---------------------------\n",
    "# if Confusion_matrix == 1 and preset[\"model\"] != \"ST-RF\":\n",
    "\n",
    "#     # reshape for sequence models\n",
    "#     test_x_rs = (test_x.reshape(test_x.shape[0], test_x.shape[1], -1)\n",
    "#                  if preset[\"model\"] in (\"LSTM\", \"bi-LSTM\", \"THAT\") else test_x)\n",
    "\n",
    "#     # instantiate identical architecture on CPU\n",
    "#     if preset[\"model\"] == \"LSTM\":\n",
    "#         model_cpu = LSTMM(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "#     elif preset[\"model\"] == \"bi-LSTM\":\n",
    "#         model_cpu = BiLSTMM(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "#     elif preset[\"model\"] == \"THAT\":\n",
    "#         model_cpu = THAT(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "#     else:  # ResNet18\n",
    "#         model_cpu = ResNet18Model(test_x_rs[0].shape, test_y[0].shape).cpu()\n",
    "\n",
    "#     # load weights\n",
    "#     model_cpu.load_state_dict(torch.load(f\"{name_run}_fewshot_best_model.pt\", map_location=\"cpu\"))\n",
    "\n",
    "#     # CPU DataLoader with a safe batch size\n",
    "#     test_ds = TensorDataset(torch.from_numpy(test_x_rs).float(),\n",
    "#                             torch.from_numpy(test_y).float())\n",
    "#     test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "#     pdf_name = f\"{name_run}_fewshot_confusion_matrix.pdf\"\n",
    "#     num_classes = test_y.shape[1]\n",
    "#     save_multiclass_confusion_matrix(model_cpu, test_loader, pdf_name, num_classes)\n",
    "#     print(f\"✅ Saved few‑shot confusion matrix (classes 0–{num_classes-1}) to {pdf_name}\")\n",
    "\n",
    "# # ----------------------- restore & persist -----------------------------\n",
    "# preset[\"nn\"][\"epoch\"] = original_epochs\n",
    "\n",
    "# # Save the final result to JSON\n",
    "# with open(\"result_fewshot.json\", \"w\") as f:\n",
    "#     json.dump(result, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9874f48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T17:01:31.136442Z",
     "iopub.status.busy": "2025-12-25T17:01:31.136237Z",
     "iopub.status.idle": "2025-12-25T17:01:31.145538Z",
     "shell.execute_reply": "2025-12-25T17:01:31.144991Z"
    },
    "papermill": {
     "duration": 0.022233,
     "end_time": "2025-12-25T17:01:31.146529",
     "exception": false,
     "start_time": "2025-12-25T17:01:31.124296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import scipy.io as scio\n",
    "# import time\n",
    "# import torch\n",
    "# import gc\n",
    "# from numpy.linalg import svd\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "# from copy import deepcopy\n",
    "# import json\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# import torch._dynamo\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # --- تنظیمات سیستمی ---\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# # --------------------------\n",
    "# # 1. تنظیمات (Configuration)\n",
    "# # --------------------------\n",
    "# preset = {\n",
    "#     \"model\": \"THAT\",          \n",
    "#     \"task\": \"activity\",       \n",
    "#     \"repeat\": 1,\n",
    "#     \"path\": {\n",
    "#         \"data_x\": \"/kaggle/input/wimans/wifi_csi/amp\",   \n",
    "#         \"data_y\": \"/kaggle/input/wimans/annotation.csv\", \n",
    "#     },\n",
    "#     \"data\": {\n",
    "#         \"num_users\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"],  \n",
    "#         \"wifi_band\": [\"2.4\"],                         \n",
    "#         \"environment\": [\"classroom\"],                 \n",
    "#         \"length\": 3000,\n",
    "        \n",
    "#         # 1.0 = 100% data (Full run) | 0.1 = 10% data (Quick test)\n",
    "#         \"subset_ratio\": 0.5,  \n",
    "#     },\n",
    "#     \"nn\": {\n",
    "#         \"lr\": 1e-3,           \n",
    "#         \"epoch\": 80,          \n",
    "#         \"batch_size\": 32,    \n",
    "#         \"threshold\": 0.5,\n",
    "#         \"patience\": 5,        \n",
    "#         \"factor\": 0.5,        \n",
    "#         \"min_lr\": 1e-6        \n",
    "#     },\n",
    "#     \"encoding\": {\n",
    "#         \"activity\": {\n",
    "#             \"nan\":      [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#             \"nothing\":  [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#             \"walk\":     [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "#             \"rotation\": [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "#             \"jump\":     [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "#             \"wave\":     [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "#             \"lie_down\": [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "#             \"pick_up\":  [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "#             \"sit_down\": [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "#             \"stand_up\": [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "#         },\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# # --------------------------\n",
    "# # 2. توابع RPCA و لود دیتا\n",
    "# # --------------------------\n",
    "# def soft_threshold(x, epsilon):\n",
    "#     return np.maximum(np.abs(x) - epsilon, 0) * np.sign(x)\n",
    "\n",
    "# def robust_pca(M, max_iter=10, tol=1e-4):\n",
    "#     n1, n2 = M.shape\n",
    "#     lambda_param = 1 / np.sqrt(max(n1, n2))\n",
    "#     Y = M / np.maximum(np.linalg.norm(M, 2), np.linalg.norm(M, np.inf) / lambda_param)\n",
    "#     L = np.zeros_like(M)\n",
    "#     S = np.zeros_like(M)\n",
    "#     mu = 1.25 / np.linalg.norm(M, 2)\n",
    "#     rho = 1.5\n",
    "#     for i in range(max_iter):\n",
    "#         temp_L = M - S + (1/mu) * Y\n",
    "#         U, Sigma, Vt = svd(temp_L, full_matrices=False)\n",
    "#         Sigma_thresh = soft_threshold(Sigma, 1/mu)\n",
    "#         L_new = np.dot(U * Sigma_thresh, Vt)\n",
    "#         temp_S = M - L_new + (1/mu) * Y\n",
    "#         S_new = soft_threshold(temp_S, lambda_param/mu)\n",
    "#         error = np.linalg.norm(M - L_new - S_new, 'fro') / np.linalg.norm(M, 'fro')\n",
    "#         L = L_new; S = S_new\n",
    "#         if error < tol: break\n",
    "#         Y = Y + mu * (M - L - S)\n",
    "#         mu = min(mu * rho, 1e7)\n",
    "#     return L, S\n",
    "\n",
    "# def load_data_y(var_path_data_y, var_environment=None, var_wifi_band=None, var_num_users=None):\n",
    "#     data_pd_y = pd.read_csv(var_path_data_y, dtype=str)\n",
    "#     if var_environment is not None: data_pd_y = data_pd_y[data_pd_y[\"environment\"].isin(var_environment)]\n",
    "#     if var_wifi_band is not None: data_pd_y = data_pd_y[data_pd_y[\"wifi_band\"].isin(var_wifi_band)]\n",
    "#     if var_num_users is not None: data_pd_y = data_pd_y[data_pd_y[\"number_of_users\"].isin(var_num_users)]\n",
    "#     return data_pd_y\n",
    "\n",
    "# def load_data_x(var_path_data_x, var_label_list, use_rpca=True):\n",
    "#     var_path_list = [os.path.join(var_path_data_x, var_label + \".npy\") for var_label in var_label_list]\n",
    "#     data_x = []\n",
    "#     mode_str = \"WITH RPCA\" if use_rpca else \"RAW DATA (No RPCA)\"\n",
    "#     print(f\"Loading {len(var_path_list)} samples - Mode: {mode_str}...\")\n",
    "#     for i, var_path in enumerate(var_path_list):\n",
    "#         if i % 100 == 0 and i > 0: print(f\"Processing {i}/{len(var_path_list)}...\")\n",
    "#         data_csi = np.load(var_path) \n",
    "#         data_csi_2d = data_csi.reshape(data_csi.shape[0], -1)\n",
    "#         target_len = preset[\"data\"][\"length\"]\n",
    "#         current_len = data_csi_2d.shape[0]\n",
    "#         var_pad_length = target_len - current_len\n",
    "#         if var_pad_length > 0: data_csi_pad = np.pad(data_csi_2d, ((0, var_pad_length), (0, 0)), mode='constant')\n",
    "#         else: data_csi_pad = data_csi_2d[:target_len, :]\n",
    "#         if use_rpca:\n",
    "#             L, S = robust_pca(data_csi_pad)\n",
    "#             final_sample = np.concatenate([L, S], axis=1) \n",
    "#         else:\n",
    "#             final_sample = data_csi_pad\n",
    "#         data_x.append(final_sample)\n",
    "#     data_x = np.array(data_x)\n",
    "#     return data_x\n",
    "\n",
    "# def encode_data_y(data_pd_y, var_task):\n",
    "#     if var_task == \"activity\": return encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "#     return encode_activity(data_pd_y, preset[\"encoding\"][\"activity\"])\n",
    "\n",
    "# def encode_activity(data_pd_y, var_encoding):\n",
    "#     cols = [f\"user_{i}_activity\" for i in range(1, 7)]\n",
    "#     data = data_pd_y[cols].to_numpy(copy=True).astype(str)\n",
    "#     return np.array([[var_encoding[y] for y in sample] for sample in data])\n",
    "\n",
    "# # --------------------------\n",
    "# # 3. مدل THAT\n",
    "# # --------------------------\n",
    "# class Gaussian_Position(torch.nn.Module):\n",
    "#     def __init__(self, var_dim_feature, var_dim_time, var_num_gaussian=10):\n",
    "#         super(Gaussian_Position, self).__init__()\n",
    "#         self.var_embedding = torch.nn.Parameter(torch.zeros([var_num_gaussian, var_dim_feature]), requires_grad=True)\n",
    "#         torch.nn.init.xavier_uniform_(self.var_embedding)\n",
    "#         self.var_position = torch.nn.Parameter(torch.arange(0.0, var_dim_time).unsqueeze(1).repeat(1, var_num_gaussian), requires_grad=False)\n",
    "#         self.var_mu = torch.nn.Parameter(torch.arange(0.0, var_dim_time, var_dim_time/var_num_gaussian).unsqueeze(0), requires_grad=True)\n",
    "#         self.var_sigma = torch.nn.Parameter(torch.tensor([50.0] * var_num_gaussian).unsqueeze(0), requires_grad=True)\n",
    "#     def forward(self, var_input):\n",
    "#         var_pdf = - (self.var_position - self.var_mu)**2 / (2 * self.var_sigma**2) - torch.log(self.var_sigma)\n",
    "#         var_pdf = torch.softmax(var_pdf, dim=-1)\n",
    "#         return var_input + torch.matmul(var_pdf, self.var_embedding).unsqueeze(0)\n",
    "\n",
    "# class Encoder(torch.nn.Module):\n",
    "#     def __init__(self, var_dim_feature, var_num_head=10, var_size_cnn=[1, 3, 5]):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.layer_norm_0 = torch.nn.LayerNorm(var_dim_feature, eps=1e-6)\n",
    "#         self.layer_attention = torch.nn.MultiheadAttention(var_dim_feature, var_num_head, batch_first=True)\n",
    "#         self.layer_dropout_0 = torch.nn.Dropout(0.1)\n",
    "#         self.layer_norm_1 = torch.nn.LayerNorm(var_dim_feature, 1e-6)\n",
    "#         self.layer_cnn = torch.nn.ModuleList([torch.nn.Sequential(torch.nn.Conv1d(var_dim_feature, var_dim_feature, s, padding=\"same\"), torch.nn.BatchNorm1d(var_dim_feature), torch.nn.Dropout(0.1), torch.nn.LeakyReLU()) for s in var_size_cnn])\n",
    "#         self.layer_dropout_1 = torch.nn.Dropout(0.1)\n",
    "#     def forward(self, var_input):\n",
    "#         var_t = self.layer_norm_0(var_input)\n",
    "#         var_t, _ = self.layer_attention(var_t, var_t, var_t)\n",
    "#         var_t = self.layer_dropout_0(var_t) + var_input\n",
    "#         var_s = self.layer_norm_1(var_t).permute(0, 2, 1)\n",
    "#         var_c = torch.stack([l(var_s) for l in self.layer_cnn], dim=0)\n",
    "#         var_s = self.layer_dropout_1((torch.sum(var_c, dim=0) / len(self.layer_cnn)).permute(0, 2, 1))\n",
    "#         return var_s + var_t\n",
    "\n",
    "# class THAT(torch.nn.Module):\n",
    "#     def __init__(self, var_x_shape, var_y_shape):\n",
    "#         super(THAT, self).__init__()\n",
    "#         var_dim_feature, var_dim_time = var_x_shape[-1], var_x_shape[-2]\n",
    "#         var_dim_output = var_y_shape[-1]\n",
    "#         self.layer_left_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "#         self.layer_left_gaussian = Gaussian_Position(var_dim_feature, var_dim_time // 20)\n",
    "#         self.layer_left_encoder = torch.nn.ModuleList([Encoder(var_dim_feature, 10, [1, 3, 5]) for _ in range(4)])\n",
    "#         self.layer_left_norm = torch.nn.LayerNorm(var_dim_feature, eps=1e-6)\n",
    "#         self.layer_left_cnn = torch.nn.ModuleList([torch.nn.Conv1d(var_dim_feature, 128, k) for k in [8, 16]])\n",
    "#         self.layer_left_dropout = torch.nn.Dropout(0.5)\n",
    "#         var_dim_right = var_dim_time // 20\n",
    "#         self.layer_right_pooling = torch.nn.AvgPool1d(kernel_size=20, stride=20)\n",
    "#         self.layer_right_encoder = torch.nn.ModuleList([Encoder(var_dim_right, 10, [1, 2, 3])])\n",
    "#         self.layer_right_norm = torch.nn.LayerNorm(var_dim_right, eps=1e-6)\n",
    "#         self.layer_right_cnn = torch.nn.ModuleList([torch.nn.Conv1d(var_dim_right, 16, k) for k in [2, 4]])\n",
    "#         self.layer_right_dropout = torch.nn.Dropout(0.5)\n",
    "#         self.layer_leakyrelu = torch.nn.LeakyReLU()\n",
    "#         self.layer_output = torch.nn.Linear(256 + 32, var_dim_output)\n",
    "#     def forward(self, var_input):\n",
    "#         v_l = self.layer_left_gaussian(self.layer_left_pooling(var_input.permute(0, 2, 1)).permute(0, 2, 1))\n",
    "#         for l in self.layer_left_encoder: v_l = l(v_l)\n",
    "#         v_l = self.layer_left_norm(v_l).permute(0, 2, 1)\n",
    "#         v_l = torch.cat([torch.sum(self.layer_leakyrelu(cnn(v_l)), dim=-1) for cnn in self.layer_left_cnn], dim=-1)\n",
    "#         v_l = self.layer_left_dropout(v_l)\n",
    "#         v_r = self.layer_right_pooling(var_input.permute(0, 2, 1))\n",
    "#         for l in self.layer_right_encoder: v_r = l(v_r)\n",
    "#         v_r = self.layer_right_norm(v_r).permute(0, 2, 1)\n",
    "#         v_r = torch.cat([torch.sum(self.layer_leakyrelu(cnn(v_r)), dim=-1) for cnn in self.layer_right_cnn], dim=-1)\n",
    "#         v_r = self.layer_right_dropout(v_r)\n",
    "#         return self.layer_output(torch.cat([v_l, v_r], dim=-1))\n",
    "\n",
    "# # --------------------------\n",
    "# # 4. Training Loop\n",
    "# # --------------------------\n",
    "# def train(model, optimizer, loss_fn, train_loader, test_loader, threshold, epochs, device, model_path):\n",
    "#     best_acc = -1.0\n",
    "#     best_w = deepcopy(model.state_dict())\n",
    "    \n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#         optimizer, mode='max', factor=preset[\"nn\"][\"factor\"], patience=preset[\"nn\"][\"patience\"],\n",
    "#         min_lr=preset[\"nn\"][\"min_lr\"], verbose=True\n",
    "#     )\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         t0 = time.time()\n",
    "#         model.train()\n",
    "        \n",
    "#         # --- [MODIFIED] Using requested variable names ---\n",
    "#         for data_batch_x, data_batch_y in train_loader:\n",
    "#             data_batch_x = data_batch_x.to(device)\n",
    "#             data_batch_y = data_batch_y.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             predict_train_y = model(data_batch_x)\n",
    "            \n",
    "#             # --- [REQUESTED LINE] ---\n",
    "#             loss_value = loss_fn(predict_train_y, data_batch_y.reshape(data_batch_y.shape[0], -1).float())\n",
    "            \n",
    "#             loss_value.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             tx, ty = next(iter(test_loader))\n",
    "#             tx, ty = tx.to(device), ty.to(device)\n",
    "#             pred_t = model(tx)\n",
    "            \n",
    "#             p_cls = (torch.sigmoid(pred_t) > threshold).float().cpu().numpy()\n",
    "#             t_cls = ty.cpu().numpy()\n",
    "#             acc = accuracy_score(t_cls.reshape(-1, t_cls.shape[-1]), p_cls.reshape(-1, t_cls.shape[-1]))\n",
    "            \n",
    "#         scheduler.step(acc)\n",
    "#         current_lr = optimizer.param_groups[0]['lr']\n",
    "#         print(f\"Ep {epoch+1}/{epochs} | LR: {current_lr:.6f} | L_tr: {loss_value.item():.4f} | Acc: {acc:.4f}\")\n",
    "        \n",
    "#         if acc > best_acc:\n",
    "#             best_acc = acc\n",
    "#             best_w = deepcopy(model.state_dict())\n",
    "            \n",
    "#     torch.save(best_w, model_path)\n",
    "#     return best_w\n",
    "\n",
    "# def save_multiclass_confusion_matrix(model, data_loader, device, pdf_path, num_classes, title_text):\n",
    "#     model.eval()\n",
    "#     y_true, y_pred = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in data_loader:\n",
    "#             xb = xb.to(device)\n",
    "#             logits = model(xb) \n",
    "#             logits = logits.reshape(-1, num_classes) \n",
    "#             yb = yb.reshape(-1, num_classes)        \n",
    "#             y_pred.extend(torch.argmax(logits, dim=1).cpu().numpy().tolist())\n",
    "#             y_true.extend(torch.argmax(yb, dim=1).cpu().numpy().tolist())\n",
    "    \n",
    "#     labels = list(range(num_classes))\n",
    "#     cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "#     disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "#     fig, ax = plt.subplots(figsize=(12, 12))\n",
    "#     disp.plot(ax=ax, xticks_rotation=\"vertical\", cmap='Blues')\n",
    "#     ax.set_title(title_text)\n",
    "#     with PdfPages(pdf_path) as pdf: pdf.savefig(fig)\n",
    "#     plt.close(fig)\n",
    "\n",
    "# # --------------------------\n",
    "# # 5. اجرا\n",
    "# # --------------------------\n",
    "# def run_experiment(scenario_name, use_rpca):\n",
    "#     print(f\"\\n################################################\")\n",
    "#     print(f\"STARTING SCENARIO: {scenario_name}\")\n",
    "#     print(f\"################################################\")\n",
    "    \n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     current_run_name = f\"{preset['model']}_{preset['task']}_{scenario_name}\"\n",
    "#     model_save_path = f\"{current_run_name}_best_model.pt\"\n",
    "#     json_save_path = f\"result_{current_run_name}.json\"\n",
    "#     pdf_save_path = f\"Confusion_{current_run_name}.pdf\"\n",
    "    \n",
    "#     # 1. Load Labels\n",
    "#     data_pd_y = load_data_y(preset[\"path\"][\"data_y\"], preset[\"data\"][\"environment\"], preset[\"data\"][\"wifi_band\"], preset[\"data\"][\"num_users\"])\n",
    "    \n",
    "#     # Apply Subset Ratio\n",
    "#     subset_ratio = preset[\"data\"][\"subset_ratio\"]\n",
    "#     if subset_ratio < 1.0:\n",
    "#         data_pd_y = data_pd_y.sample(frac=subset_ratio, random_state=42).reset_index(drop=True)\n",
    "#         print(f\"*** DEBUG MODE: Using {subset_ratio*100}% of data ({len(data_pd_y)} samples) ***\")\n",
    "    \n",
    "#     # 2. Load X\n",
    "#     data_x = load_data_x(preset[\"path\"][\"data_x\"], data_pd_y[\"label\"].tolist(), use_rpca=use_rpca)\n",
    "#     data_y = encode_data_y(data_pd_y, preset[\"task\"])\n",
    "    \n",
    "#     # 3. Split\n",
    "#     train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2, shuffle=True, random_state=39)\n",
    "#     train_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "#     test_ds = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "#     train_loader = DataLoader(train_ds, batch_size=preset[\"nn\"][\"batch_size\"], shuffle=True)\n",
    "#     test_loader = DataLoader(test_ds, batch_size=len(test_ds), shuffle=False)\n",
    "    \n",
    "#     result = {\"accuracy\": []}\n",
    "    \n",
    "#     for r in range(preset[\"repeat\"]):\n",
    "#         print(f\"--- Repeat {r+1}/{preset['repeat']} ---\")\n",
    "#         torch.random.manual_seed(r + 39)\n",
    "        \n",
    "#         model = THAT(train_x[0].shape, train_y[0].reshape(-1).shape).to(device)\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=preset[\"nn\"][\"lr\"])\n",
    "#         loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "#         best_w = train(model, optimizer, loss_fn, train_loader, test_loader, \n",
    "#                        preset[\"nn\"][\"threshold\"], preset[\"nn\"][\"epoch\"], device, model_save_path)\n",
    "        \n",
    "#         model.load_state_dict(best_w)\n",
    "#         with torch.no_grad():\n",
    "#             preds = model(torch.from_numpy(test_x).to(device))\n",
    "#             preds_reshaped = (torch.sigmoid(preds) > preset[\"nn\"][\"threshold\"]).float().cpu().numpy().reshape(-1, 9)\n",
    "#             targets_reshaped = test_y.reshape(-1, 9)\n",
    "#             acc = accuracy_score(targets_reshaped, preds_reshaped)\n",
    "#             result[\"accuracy\"].append(acc)\n",
    "            \n",
    "#     print(f\"Final Accuracy ({scenario_name}): {np.mean(result['accuracy']):.4f}\")\n",
    "#     with open(json_save_path, \"w\") as f: json.dump(result, f, indent=4)\n",
    "    \n",
    "#     print(\"Generating Confusion Matrix...\")\n",
    "#     model_cm = THAT(test_x[0].shape, test_y[0].reshape(-1).shape).to(\"cpu\")\n",
    "#     model_cm.load_state_dict(torch.load(model_save_path, map_location=\"cpu\"))\n",
    "#     cm_loader = DataLoader(TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y)), batch_size=32)\n",
    "#     num_classes = test_y.shape[2] \n",
    "#     title = f\"Confusion Matrix: {scenario_name} (Acc: {np.mean(result['accuracy']):.2f} - {subset_ratio*100}% Data)\"\n",
    "#     save_multiclass_confusion_matrix(model_cm, cm_loader, \"cpu\", pdf_save_path, num_classes, title)\n",
    "    \n",
    "#     del model, model_cm, train_x, test_x, data_x\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     print(f\"Done with {scenario_name}.\\n\")\n",
    "\n",
    "# def run():\n",
    "#     scenarios = [\n",
    "#         (\"RPCA\", True),\n",
    "#         (\"RAW\", False)\n",
    "#     ]\n",
    "#     for name, rpca_flag in scenarios:\n",
    "#         run_experiment(name, rpca_flag)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b344b5",
   "metadata": {
    "papermill": {
     "duration": 0.01044,
     "end_time": "2025-12-25T17:01:31.167968",
     "exception": false,
     "start_time": "2025-12-25T17:01:31.157528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4451316,
     "sourceId": 7638081,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7503373,
     "sourceId": 11934698,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6149.068303,
   "end_time": "2025-12-25T17:01:33.520763",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-25T15:19:04.452460",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
